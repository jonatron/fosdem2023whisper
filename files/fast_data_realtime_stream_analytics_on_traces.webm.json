{"text": " Thanks very much, thank you, so welcome everyone and I'm glad that you're here on Saturday early morning in this first session, so I'd like to make it as easy as possible, thanks for the organizers, Jerez and Yamur, for inviting me to talk today about stream processing. The fact is I don't know your background, so I'm not sure exactly how much experience you'll have with stream processing, so if you see some concepts are easy, just get everyone up and into this concept. So I'll be talking today about stream processing on adaptive and there's a lot, so that's what the main focus will be. Obviously this title as it is could be a startup company, so you would expect to have some ideas today where you can use some of these ideas in your work or in your experience or in your case of study whatever you want or whether you are a Java developer or data scientist or MLO, so it doesn't matter. So there is something for everyone here today, so that's the main focus for this session. So, anyone recognize these guys on the screen here? Right, so that's where I came from, I'm based in Liverpool in the UK and on the right side is the Liverpool football club, which is basically one of the top football teams in the UK, so I wanted just to highlight this screen here, just to tell you that stream processing is not in specific domain, it could be in any domain. And if you look at it, do you know how long it takes, for example, for an eye to blink? Come again? Yeah, so it takes over half a second, so that's pretty fast. So if you think about maybe minutes or hours, probably this is not the right discussion room for you. We're talking about some milliseconds today, so whether it's for example using it in finance, whether you use it in IoT devices, smart devices, whether you use it in sports, hospitals or machine learning or what we're trying to do today for stream processing. So that's the main idea. And obviously, if you're working with real-time stream processing, you focus on the real-time data, right? And I've seen it so many times where platforms and tools focus on how much data you can process and you see these benchmarks everywhere on the internet and this is pretty cool, I think, but the key source and the secret source for this is to use something in combination between real-time data and historical data. So the main reason for this is to look at context. So without knowing what's going on, you probably don't benefit much from the real-time data you're processing. So what you want is always to go back and check what's going on with the context of these data. There is a problem in this secret source, obviously, because what you're looking at is kind of like two different data types and you want to make sure that you process it at the same speed or very close to the same speed. Obviously, it becomes really a problem when you try to scale it. So if you have, I don't know, maybe a few cases of data that you want to process, probably it's not too much trouble for you, but when you start to scale it up, it becomes really a problem to understand how you want to scale it. So do you scale your data or do you scale your compute or do you scale both at what speed? So we will discuss all these concepts today and if I ask you now how much data you process, obviously, because in this room I would assume over a million transactions per second or a few millions or, I don't know, some of you might be processing millions of transactions per second. So that's pretty good. And what we want today is to focus this domain into a very specific area. And this area essentially what we're trying to do today is to analyze traces. So it doesn't matter if it's like writing system traces or platform traces or it's like programming language traces. What we want is to make sure that we have environment and within this environment you can scale your loads, basically, scale your processing and at the same time we'll provide some kind of analytics, right? So again, if you look at how much data you're trying to process, the number by itself doesn't give you much what's going on here. So what you want is to find this specific information you're looking for. Kind of like looking at, you know, finding the needles or finding the hidden areas within your data. So if you look at, you know, how much loads you process per day or per week and you'll store it somewhere on, you know, crystal hard drive or you store it in Mori or you store it in the cloud. So what you want is to, you know, make sense of it. And some companies do this process manually, which means they run software and they go through their loads and this is kind of a patch service and they try to understand what's going on within the load. So obviously this is a problem when you want to scale it and with the scaling you have different loads stored in different places and you want to make sure basically to have a platform where in this platform we kind of like looking at some kind of results. So for the sake of this discussion today, we'll focus on two different solutions. So one of them is trying to provide some kind of alerts and the other is to provide some kind of trends within your data. Obviously I work for a company called Hazardcast. So Hazardcast as a platform, I love you to do so but obviously you might have heard of some companies or, you know, they do some kind of stream processing. So this is kind of like, you know, overview what's going on with this domain at this time. Obviously you can split it depending on if you're looking for open source solution or, I don't know, hardware solution or, you know, some kind of management service. And what you look at is kind of which domain you work so are you looking to capture your data or some kind of, you know, streaming your data or you want to do some kind of transformation on your data or do some kind of electrical machine learning. So you can see that you split it into 12 squares and within these like tools and platforms are, you know, spread it over. Some tools not exist on this screen for whatever reason but obviously this might give you some ideas but it's hard to decide which tool you want to go for. Simply because I think the distribution is not clear here so it tells you basically which tool is open source for example and where in process you can use it but it doesn't give you full picture on, you know, how to do it in practical terms. And so this is where it might be easier to understand what we're talking about. So if you remember from my slide where I discussed the historical data and the new data. So today we're kind of like, you know, trying to split everything into two categories. So on one side you get like stream processing engines. So these engines are pretty fast in, you know, streaming events. And on this far right side you have some kind of fast data stores which are, you know, are pretty fast in handling data at speed. So again the solution for lead time stream processing is kind of a combination and you want to process data in this moment and at the same time you want to actually also access data storage somewhere. So that's where Hazardcast fits into this area here. So the platform itself obviously for those who don't know, by the way we have one of the masterminds of Hazardcast sitting in this room. So this is the platform. So it's open source platform. It doesn't matter where your source is coming from, whether it's Apache Cloud or Apache IoT devices, for example, I don't know, some kind of device applications or even like within Hazardcast or even you can write your own connector and you feed it into the platform. So platform historically used to be two different components. So the IOMTG and the Jet Engine. And essentially now it's all back in one, one jar file. As you see here, it allows you to load your data from hard disks into memory. So you have access to historical data and pretty much like instantaneously and this will, well, you know, you can provide context, what's going on with your data. At the same time, you can actually do stream processing. So that's what Jet Engine is. So from here you can do some kind, I don't know, maybe like data transformation or do some kind of stream processing as we will do today or even like defined machine learning if you want to. You can connect it to some clients. So these are some clients here, so written into various languages. If you're from data science background, which means your programming languages in general are not preferable for you, so you might be considering using SQL to do what I'm planning today. So this is another option you can do. And once you process this data where you load it into memory for historical data and at the same time you have some kind of data coming in. For example, and you do the combination or even you do transformation, you can proceed to do some kind of visualization. So the good thing about Hesicas in general where it comes to scaling is it's partition aware. So which means basically your compute, your Jet Engine or your process essentially can be or can detect where your data is stored. So this is like, you know, we're trying to have as low latency as possible when it comes to processing this data. So this is very important to understand because latency is your enemy when it comes to stream processing. So what you want is kind of like having a platform where you avoid network folks. For example, you avoid IO to your hard disk. You will try to also avoid every time or, sorry, context switching between threads. So you want to avoid all of these, but at the same time you want your process to be as close as possible to your data. You can avoid some kind of, you know, machine learning on this. And the scaling itself could be done in various ways. So the main thing to take away from here is there's no master-worker relationship. So all nodes basically are peers. And we've done this study. It's a bit dated, but it's kind of like one million transactions per second on 45 nodes. So what we're trying to do now is to add one zero into this number here. And even though it's pretty impressive, what is nice about it is the linear scaling, which means more data you can add, you know, more nodes into it. So that's the historical bit of this talk. So let's just move to the technical part. So for this demo, what I wanted is kind of like, you know, show you some ideas, right? So you should be able to take these ideas and apply it, you know, anywhere. Obviously the solution as itself could be like, you know, project by itself. So feel free to edit and change it. All source code is available on GitHub and the documentation as well. So you can go through it. So the main idea when it comes to analyzing or, you know, making sense out of your traces and logs is to store it somewhere close to, you know, your compute, first of all, and shouldn't be stored locally, right? So you want to store it first of all. So the first thing is to store it on the cloud. So for this demo, what I'm doing is I'm storing everything onto the cloud. There is a solution called Hazelgast-Virginian, which is kind of like service. So you don't need to download GR, run your project. You can simply plug in and play. So you can create an account. You'll run everything I'm discussing today. So you create an instance of Hazelgast. And from there, you can pretty much proceed to what I'm planning to do. So the first option we were talking about is kind of like storing everything into the cloud. So we're going to import the data. Obviously, we need some kind of trace message, which makes sense. So this trace message could be, you know, changed based on how you want to approach it, right? So for example, if you're working with machine learning, you probably look for some kind of, I don't know, classification solution for your, you know, for your tests, or you could be looking for NLE. If you don't want to work with machine learning, you probably want to look for some kind of trends. So you look for processing your data. It doesn't matter if you're using machine learning. In this case, you want to have some kind of data stored somewhere. So it could be in JSON format, or it could be like bar charts, strings. So it depends again how much the speed is important to you. So the option, first option is to go through the alerts. So in alerts, what we're trying to do here is to take everything and store it in the cloud. So obviously we don't store it in the cloud on our disks. What we try to do is to store it in memory. My preference in this case is to use some kind of map structure. So map structure allows you to essentially random access and rebalance between various nodes within your cluster. And at the same time, you want to have some key value, so in order to know where this is coming from. So in this case, it could be like ID address, for example, so this is where, and support number, so as key. And the value could be anything that makes sense to you. So in this case, for example, you can track level of this error, sorry, of this loop, and message, for example, if you want to do some kind of NLP processing on it, and some kind of, you know, process or thread name on this. Obviously once you have your key and value, what you can do is proceed and store it into memory. So this is where you get this set to hazard cast. And what we're trying to do is create the IMAP. And once you have the IMAP, it means you should be able to store it, you know, access it and do same processing as I will show you. So first message is to store it in the cloud, store it in memory. In this case, I'm using hazard cast gradient, and I'm using IMAP. And second stage is to do the same processing, right? So there are a couple of options here for you. So first option is to use SQL. So SQL is built within hazard cast, which means, or on top of hazard cast, which means you should be able to query your data, so if you provide some kind of specific messages that you're looking for, obviously depends on your input, you can do some kind of SQL. So whether it's an inner joy, for example, or sales and so on. Or the other option is to do some kind of prediction. So you're getting some logs, you don't know exactly what's going on to happen next, and you try to predict to provide some kind of, you know, alerts or trends. So we need to build the trends. So in order to do this, what I did is kind of like use the same key, but for my value, what I'm using is some kind of log score. So log score is not important. What I'm saying here is I want to give value for every single message, or every single log message. So this could be, for example, how important this specific message is for you, or it could be, for example, how serious or how dangerous the message is. So as levels in logs, you can define scores, so instead of having four levels, for example, you can spread it, I don't know, from one to 100. So this should give you some kind of predictions. Why? Because if you have, for example, 10 messages, 10 local messages, or, for example, warning, you don't know exactly if the event matrix will be warning or not. If you want to predict it, obviously it doesn't give you how much will be warning or not. Whereas if you use some kind of numerical value, you can get as close as possible to this. So we get key from there, we get score from here, and what we do next is to do some kind of predictions on the logs. So in this process, what we have is exactly my key and the value, which is like the score on each log message, and I import it into Hezakas. So Hezakas allows you to basically input and output from two different maps, and do stream processing, so we'll build the train based on previous logs, based on previous log scores, and we'll use the prediction on top of it to provide some kind of alert. So zero means don't alert, one means alert. And as you can see here, the actual workflow, kind of like you build, you read it from math, then you define trend map, so which is like normal map, and from there you can use it to predict what's going to happen next. Obviously you do some kind of visualization, so how does it look like? So this is kind of the prediction part of it. So we take the logs map and we build trend map out of it. So the trend map would start reading messages and the scores and build train for you. And from this trend I can use some kind of machine learning, it doesn't have to be machine learning. In this case it's linear regression, but it could be anything to be honest. And we check the values and we try to use some kind of prediction based on the previous values to decide if you want to send an alert or not. And obviously this is kind of like describing the exact thing, so when there is a one on your values, it's alert, send alert when it's zero, don't. And here there are three ways to do it. So this is where the same processing comes into place. So you could simply use SQL to read from the map and do some query if you want. Obviously this is batch, which means it's not real time same processing, or you can even create a pipeline or create a process. And from this process you can read the logs and do some same thing. So you can use either SQL or Java to do it. But first two options are batch, which means you can process the data in real time, you want to do changes in real time. So the third option is the journal map. So journal map will track all changes, so it is continuous. So you have the logs stored and you have logs coming into Kafka topic and you can basically store both into journal map. So we're on 5.2 version within Hazegas, 5.3 will have the SQL features on top of it, which allows you, for example, data scientists to just do the queries and change the data. And obviously it's ring covered, so this is very important to understand, so you can start processing your data from start or from the end. And what you want is kind of like, you know, using this kind of alerts to it. So the first part is to read it. So this is the actual map we built in the first option. And from here you can define the key, for example, and the value. And you start, for example, to do some kind of filtering. So this happens in real time on continuous and the map itself will allow you to basically track changes. So to give you some takeaways and best practices, so we just try to summarize everything we discussed today. Obviously there are more to discuss, but this should give you something to go out and try. So first of all, you need to store your logs into some kind of data platform. So in this case, I'm using Hazegas, but obviously you don't have to use Hazegas. The idea here is to do some kind of compute on your circle data to provide some context, as well as real time data. And from there, you need to store it on the cloud. So you need to store it somewhere where you can access logs from multiple places. Obviously it has to be stored in memory. And from there, you need to choose the format. If you, for example, looking to provide some, you know, I don't know, some predictions you probably need to use JSON format. Or for example, if you want just to do something that you can't sing and it's faster, if you want to speed the unit, it is some kind of map structure. Obviously when you store it in memory, because this will allow some kind of random access. And also you need to consider how you empty your map. So because you are limited on size, obviously. And finally, you need to consider security. So whatever you send to the cloud, you need to make sure that you don't include some, you know, personal entities or whatever. So if you're interested in this topic, we're running a conference next month. So feel free to scan this code. We provide training for this all free, obviously. And everything I mentioned today is open source, so you should be able to do everything I mentioned today. I'll be steering around here if you want to have a chat or if you want to discuss it a little bit more. Obviously within half an hour, there's not much to give, but hopefully you've got some ideas from this talk. And hopefully it will be also useful for you. So with that being said, thanks very much for listening. I'll open for questions. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.8, "text": " Thanks very much, thank you, so welcome everyone and I'm glad that you're here on Saturday", "tokens": [2561, 588, 709, 11, 1309, 291, 11, 370, 2928, 1518, 293, 286, 478, 5404, 300, 291, 434, 510, 322, 8803], "temperature": 0.0, "avg_logprob": -0.4111890471383427, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.41710612177848816}, {"id": 1, "seek": 0, "start": 10.8, "end": 16.8, "text": " early morning in this first session, so I'd like to make it as easy as possible, thanks", "tokens": [2440, 2446, 294, 341, 700, 5481, 11, 370, 286, 1116, 411, 281, 652, 309, 382, 1858, 382, 1944, 11, 3231], "temperature": 0.0, "avg_logprob": -0.4111890471383427, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.41710612177848816}, {"id": 2, "seek": 0, "start": 16.8, "end": 24.32, "text": " for the organizers, Jerez and Yamur, for inviting me to talk today about stream processing.", "tokens": [337, 264, 35071, 11, 508, 28735, 293, 18992, 374, 11, 337, 18202, 385, 281, 751, 965, 466, 4309, 9007, 13], "temperature": 0.0, "avg_logprob": -0.4111890471383427, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.41710612177848816}, {"id": 3, "seek": 0, "start": 24.32, "end": 28.96, "text": " The fact is I don't know your background, so I'm not sure exactly how much experience", "tokens": [440, 1186, 307, 286, 500, 380, 458, 428, 3678, 11, 370, 286, 478, 406, 988, 2293, 577, 709, 1752], "temperature": 0.0, "avg_logprob": -0.4111890471383427, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.41710612177848816}, {"id": 4, "seek": 2896, "start": 28.96, "end": 34.96, "text": " you'll have with stream processing, so if you see some concepts are easy, just get everyone", "tokens": [291, 603, 362, 365, 4309, 9007, 11, 370, 498, 291, 536, 512, 10392, 366, 1858, 11, 445, 483, 1518], "temperature": 0.0, "avg_logprob": -0.3420857247852144, "compression_ratio": 1.5857142857142856, "no_speech_prob": 0.0007832368719391525}, {"id": 5, "seek": 2896, "start": 34.96, "end": 36.96, "text": " up and into this concept.", "tokens": [493, 293, 666, 341, 3410, 13], "temperature": 0.0, "avg_logprob": -0.3420857247852144, "compression_ratio": 1.5857142857142856, "no_speech_prob": 0.0007832368719391525}, {"id": 6, "seek": 2896, "start": 36.96, "end": 42.96, "text": " So I'll be talking today about stream processing on adaptive and there's a lot, so that's what", "tokens": [407, 286, 603, 312, 1417, 965, 466, 4309, 9007, 322, 27912, 293, 456, 311, 257, 688, 11, 370, 300, 311, 437], "temperature": 0.0, "avg_logprob": -0.3420857247852144, "compression_ratio": 1.5857142857142856, "no_speech_prob": 0.0007832368719391525}, {"id": 7, "seek": 2896, "start": 42.96, "end": 44.96, "text": " the main focus will be.", "tokens": [264, 2135, 1879, 486, 312, 13], "temperature": 0.0, "avg_logprob": -0.3420857247852144, "compression_ratio": 1.5857142857142856, "no_speech_prob": 0.0007832368719391525}, {"id": 8, "seek": 2896, "start": 44.96, "end": 52.96, "text": " Obviously this title as it is could be a startup company, so you would expect to have some ideas", "tokens": [7580, 341, 4876, 382, 309, 307, 727, 312, 257, 18578, 2237, 11, 370, 291, 576, 2066, 281, 362, 512, 3487], "temperature": 0.0, "avg_logprob": -0.3420857247852144, "compression_ratio": 1.5857142857142856, "no_speech_prob": 0.0007832368719391525}, {"id": 9, "seek": 5296, "start": 52.96, "end": 59.96, "text": " today where you can use some of these ideas in your work or in your experience or in your", "tokens": [965, 689, 291, 393, 764, 512, 295, 613, 3487, 294, 428, 589, 420, 294, 428, 1752, 420, 294, 428], "temperature": 0.0, "avg_logprob": -0.2144617321847499, "compression_ratio": 1.6367924528301887, "no_speech_prob": 7.392402767436579e-05}, {"id": 10, "seek": 5296, "start": 59.96, "end": 65.96000000000001, "text": " case of study whatever you want or whether you are a Java developer or data scientist", "tokens": [1389, 295, 2979, 2035, 291, 528, 420, 1968, 291, 366, 257, 10745, 10754, 420, 1412, 12662], "temperature": 0.0, "avg_logprob": -0.2144617321847499, "compression_ratio": 1.6367924528301887, "no_speech_prob": 7.392402767436579e-05}, {"id": 11, "seek": 5296, "start": 65.96000000000001, "end": 68.96000000000001, "text": " or MLO, so it doesn't matter.", "tokens": [420, 376, 20184, 11, 370, 309, 1177, 380, 1871, 13], "temperature": 0.0, "avg_logprob": -0.2144617321847499, "compression_ratio": 1.6367924528301887, "no_speech_prob": 7.392402767436579e-05}, {"id": 12, "seek": 5296, "start": 68.96000000000001, "end": 73.96000000000001, "text": " So there is something for everyone here today, so that's the main focus for this session.", "tokens": [407, 456, 307, 746, 337, 1518, 510, 965, 11, 370, 300, 311, 264, 2135, 1879, 337, 341, 5481, 13], "temperature": 0.0, "avg_logprob": -0.2144617321847499, "compression_ratio": 1.6367924528301887, "no_speech_prob": 7.392402767436579e-05}, {"id": 13, "seek": 5296, "start": 73.96000000000001, "end": 79.96000000000001, "text": " So, anyone recognize these guys on the screen here?", "tokens": [407, 11, 2878, 5521, 613, 1074, 322, 264, 2568, 510, 30], "temperature": 0.0, "avg_logprob": -0.2144617321847499, "compression_ratio": 1.6367924528301887, "no_speech_prob": 7.392402767436579e-05}, {"id": 14, "seek": 7996, "start": 79.96, "end": 86.96, "text": " Right, so that's where I came from, I'm based in Liverpool in the UK and on the right side", "tokens": [1779, 11, 370, 300, 311, 689, 286, 1361, 490, 11, 286, 478, 2361, 294, 32473, 294, 264, 7051, 293, 322, 264, 558, 1252], "temperature": 0.0, "avg_logprob": -0.163911842718357, "compression_ratio": 1.6119402985074627, "no_speech_prob": 0.00040154109592549503}, {"id": 15, "seek": 7996, "start": 86.96, "end": 95.96, "text": " is the Liverpool football club, which is basically one of the top football teams in the UK, so", "tokens": [307, 264, 32473, 7346, 6482, 11, 597, 307, 1936, 472, 295, 264, 1192, 7346, 5491, 294, 264, 7051, 11, 370], "temperature": 0.0, "avg_logprob": -0.163911842718357, "compression_ratio": 1.6119402985074627, "no_speech_prob": 0.00040154109592549503}, {"id": 16, "seek": 7996, "start": 95.96, "end": 100.96, "text": " I wanted just to highlight this screen here, just to tell you that stream processing is", "tokens": [286, 1415, 445, 281, 5078, 341, 2568, 510, 11, 445, 281, 980, 291, 300, 4309, 9007, 307], "temperature": 0.0, "avg_logprob": -0.163911842718357, "compression_ratio": 1.6119402985074627, "no_speech_prob": 0.00040154109592549503}, {"id": 17, "seek": 7996, "start": 100.96, "end": 105.96, "text": " not in specific domain, it could be in any domain.", "tokens": [406, 294, 2685, 9274, 11, 309, 727, 312, 294, 604, 9274, 13], "temperature": 0.0, "avg_logprob": -0.163911842718357, "compression_ratio": 1.6119402985074627, "no_speech_prob": 0.00040154109592549503}, {"id": 18, "seek": 10596, "start": 105.96, "end": 114.96, "text": " And if you look at it, do you know how long it takes, for example, for an eye to blink?", "tokens": [400, 498, 291, 574, 412, 309, 11, 360, 291, 458, 577, 938, 309, 2516, 11, 337, 1365, 11, 337, 364, 3313, 281, 24667, 30], "temperature": 0.0, "avg_logprob": -0.22824110444059076, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.0004746740742120892}, {"id": 19, "seek": 10596, "start": 114.96, "end": 117.96, "text": " Come again?", "tokens": [2492, 797, 30], "temperature": 0.0, "avg_logprob": -0.22824110444059076, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.0004746740742120892}, {"id": 20, "seek": 10596, "start": 117.96, "end": 121.96, "text": " Yeah, so it takes over half a second, so that's pretty fast.", "tokens": [865, 11, 370, 309, 2516, 670, 1922, 257, 1150, 11, 370, 300, 311, 1238, 2370, 13], "temperature": 0.0, "avg_logprob": -0.22824110444059076, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.0004746740742120892}, {"id": 21, "seek": 10596, "start": 121.96, "end": 128.95999999999998, "text": " So if you think about maybe minutes or hours, probably this is not the right discussion", "tokens": [407, 498, 291, 519, 466, 1310, 2077, 420, 2496, 11, 1391, 341, 307, 406, 264, 558, 5017], "temperature": 0.0, "avg_logprob": -0.22824110444059076, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.0004746740742120892}, {"id": 22, "seek": 10596, "start": 128.95999999999998, "end": 129.95999999999998, "text": " room for you.", "tokens": [1808, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.22824110444059076, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.0004746740742120892}, {"id": 23, "seek": 10596, "start": 129.95999999999998, "end": 134.95999999999998, "text": " We're talking about some milliseconds today, so whether it's for example using it in finance,", "tokens": [492, 434, 1417, 466, 512, 34184, 965, 11, 370, 1968, 309, 311, 337, 1365, 1228, 309, 294, 10719, 11], "temperature": 0.0, "avg_logprob": -0.22824110444059076, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.0004746740742120892}, {"id": 24, "seek": 13496, "start": 134.96, "end": 139.96, "text": " whether you use it in IoT devices, smart devices, whether you use it in sports, hospitals or", "tokens": [1968, 291, 764, 309, 294, 30112, 5759, 11, 4069, 5759, 11, 1968, 291, 764, 309, 294, 6573, 11, 13014, 420], "temperature": 0.0, "avg_logprob": -0.13741622924804686, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.00041824800428003073}, {"id": 25, "seek": 13496, "start": 139.96, "end": 143.96, "text": " machine learning or what we're trying to do today for stream processing.", "tokens": [3479, 2539, 420, 437, 321, 434, 1382, 281, 360, 965, 337, 4309, 9007, 13], "temperature": 0.0, "avg_logprob": -0.13741622924804686, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.00041824800428003073}, {"id": 26, "seek": 13496, "start": 143.96, "end": 145.96, "text": " So that's the main idea.", "tokens": [407, 300, 311, 264, 2135, 1558, 13], "temperature": 0.0, "avg_logprob": -0.13741622924804686, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.00041824800428003073}, {"id": 27, "seek": 13496, "start": 145.96, "end": 150.96, "text": " And obviously, if you're working with real-time stream processing, you focus on the real-time", "tokens": [400, 2745, 11, 498, 291, 434, 1364, 365, 957, 12, 3766, 4309, 9007, 11, 291, 1879, 322, 264, 957, 12, 3766], "temperature": 0.0, "avg_logprob": -0.13741622924804686, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.00041824800428003073}, {"id": 28, "seek": 13496, "start": 150.96, "end": 151.96, "text": " data, right?", "tokens": [1412, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13741622924804686, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.00041824800428003073}, {"id": 29, "seek": 13496, "start": 151.96, "end": 159.96, "text": " And I've seen it so many times where platforms and tools focus on how much data you can process", "tokens": [400, 286, 600, 1612, 309, 370, 867, 1413, 689, 9473, 293, 3873, 1879, 322, 577, 709, 1412, 291, 393, 1399], "temperature": 0.0, "avg_logprob": -0.13741622924804686, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.00041824800428003073}, {"id": 30, "seek": 15996, "start": 159.96, "end": 165.96, "text": " and you see these benchmarks everywhere on the internet and this is pretty cool, I think,", "tokens": [293, 291, 536, 613, 43751, 5315, 322, 264, 4705, 293, 341, 307, 1238, 1627, 11, 286, 519, 11], "temperature": 0.0, "avg_logprob": -0.07253510977632256, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.00022242733393795788}, {"id": 31, "seek": 15996, "start": 165.96, "end": 171.96, "text": " but the key source and the secret source for this is to use something in combination between", "tokens": [457, 264, 2141, 4009, 293, 264, 4054, 4009, 337, 341, 307, 281, 764, 746, 294, 6562, 1296], "temperature": 0.0, "avg_logprob": -0.07253510977632256, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.00022242733393795788}, {"id": 32, "seek": 15996, "start": 171.96, "end": 175.96, "text": " real-time data and historical data.", "tokens": [957, 12, 3766, 1412, 293, 8584, 1412, 13], "temperature": 0.0, "avg_logprob": -0.07253510977632256, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.00022242733393795788}, {"id": 33, "seek": 15996, "start": 175.96, "end": 179.96, "text": " So the main reason for this is to look at context.", "tokens": [407, 264, 2135, 1778, 337, 341, 307, 281, 574, 412, 4319, 13], "temperature": 0.0, "avg_logprob": -0.07253510977632256, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.00022242733393795788}, {"id": 34, "seek": 15996, "start": 179.96, "end": 185.96, "text": " So without knowing what's going on, you probably don't benefit much from the real-time data", "tokens": [407, 1553, 5276, 437, 311, 516, 322, 11, 291, 1391, 500, 380, 5121, 709, 490, 264, 957, 12, 3766, 1412], "temperature": 0.0, "avg_logprob": -0.07253510977632256, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.00022242733393795788}, {"id": 35, "seek": 15996, "start": 185.96, "end": 186.96, "text": " you're processing.", "tokens": [291, 434, 9007, 13], "temperature": 0.0, "avg_logprob": -0.07253510977632256, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.00022242733393795788}, {"id": 36, "seek": 18696, "start": 186.96, "end": 191.96, "text": " So what you want is always to go back and check what's going on with the context of these", "tokens": [407, 437, 291, 528, 307, 1009, 281, 352, 646, 293, 1520, 437, 311, 516, 322, 365, 264, 4319, 295, 613], "temperature": 0.0, "avg_logprob": -0.09719655931610421, "compression_ratio": 1.668122270742358, "no_speech_prob": 8.730032277526334e-05}, {"id": 37, "seek": 18696, "start": 191.96, "end": 192.96, "text": " data.", "tokens": [1412, 13], "temperature": 0.0, "avg_logprob": -0.09719655931610421, "compression_ratio": 1.668122270742358, "no_speech_prob": 8.730032277526334e-05}, {"id": 38, "seek": 18696, "start": 192.96, "end": 198.96, "text": " There is a problem in this secret source, obviously, because what you're looking at is kind of", "tokens": [821, 307, 257, 1154, 294, 341, 4054, 4009, 11, 2745, 11, 570, 437, 291, 434, 1237, 412, 307, 733, 295], "temperature": 0.0, "avg_logprob": -0.09719655931610421, "compression_ratio": 1.668122270742358, "no_speech_prob": 8.730032277526334e-05}, {"id": 39, "seek": 18696, "start": 198.96, "end": 204.96, "text": " like two different data types and you want to make sure that you process it at the same", "tokens": [411, 732, 819, 1412, 3467, 293, 291, 528, 281, 652, 988, 300, 291, 1399, 309, 412, 264, 912], "temperature": 0.0, "avg_logprob": -0.09719655931610421, "compression_ratio": 1.668122270742358, "no_speech_prob": 8.730032277526334e-05}, {"id": 40, "seek": 18696, "start": 204.96, "end": 207.96, "text": " speed or very close to the same speed.", "tokens": [3073, 420, 588, 1998, 281, 264, 912, 3073, 13], "temperature": 0.0, "avg_logprob": -0.09719655931610421, "compression_ratio": 1.668122270742358, "no_speech_prob": 8.730032277526334e-05}, {"id": 41, "seek": 18696, "start": 207.96, "end": 212.96, "text": " Obviously, it becomes really a problem when you try to scale it.", "tokens": [7580, 11, 309, 3643, 534, 257, 1154, 562, 291, 853, 281, 4373, 309, 13], "temperature": 0.0, "avg_logprob": -0.09719655931610421, "compression_ratio": 1.668122270742358, "no_speech_prob": 8.730032277526334e-05}, {"id": 42, "seek": 21296, "start": 212.96, "end": 219.96, "text": " So if you have, I don't know, maybe a few cases of data that you want to process, probably", "tokens": [407, 498, 291, 362, 11, 286, 500, 380, 458, 11, 1310, 257, 1326, 3331, 295, 1412, 300, 291, 528, 281, 1399, 11, 1391], "temperature": 0.0, "avg_logprob": -0.13493203233789514, "compression_ratio": 1.7564102564102564, "no_speech_prob": 0.00011199984146514907}, {"id": 43, "seek": 21296, "start": 219.96, "end": 224.96, "text": " it's not too much trouble for you, but when you start to scale it up, it becomes really", "tokens": [309, 311, 406, 886, 709, 5253, 337, 291, 11, 457, 562, 291, 722, 281, 4373, 309, 493, 11, 309, 3643, 534], "temperature": 0.0, "avg_logprob": -0.13493203233789514, "compression_ratio": 1.7564102564102564, "no_speech_prob": 0.00011199984146514907}, {"id": 44, "seek": 21296, "start": 224.96, "end": 227.96, "text": " a problem to understand how you want to scale it.", "tokens": [257, 1154, 281, 1223, 577, 291, 528, 281, 4373, 309, 13], "temperature": 0.0, "avg_logprob": -0.13493203233789514, "compression_ratio": 1.7564102564102564, "no_speech_prob": 0.00011199984146514907}, {"id": 45, "seek": 21296, "start": 227.96, "end": 233.96, "text": " So do you scale your data or do you scale your compute or do you scale both at what speed?", "tokens": [407, 360, 291, 4373, 428, 1412, 420, 360, 291, 4373, 428, 14722, 420, 360, 291, 4373, 1293, 412, 437, 3073, 30], "temperature": 0.0, "avg_logprob": -0.13493203233789514, "compression_ratio": 1.7564102564102564, "no_speech_prob": 0.00011199984146514907}, {"id": 46, "seek": 21296, "start": 233.96, "end": 240.96, "text": " So we will discuss all these concepts today and if I ask you now how much data you process,", "tokens": [407, 321, 486, 2248, 439, 613, 10392, 965, 293, 498, 286, 1029, 291, 586, 577, 709, 1412, 291, 1399, 11], "temperature": 0.0, "avg_logprob": -0.13493203233789514, "compression_ratio": 1.7564102564102564, "no_speech_prob": 0.00011199984146514907}, {"id": 47, "seek": 24096, "start": 240.96, "end": 246.96, "text": " obviously, because in this room I would assume over a million transactions per second or", "tokens": [2745, 11, 570, 294, 341, 1808, 286, 576, 6552, 670, 257, 2459, 16856, 680, 1150, 420], "temperature": 0.0, "avg_logprob": -0.1663884162902832, "compression_ratio": 1.6743119266055047, "no_speech_prob": 0.0004895822494290769}, {"id": 48, "seek": 24096, "start": 246.96, "end": 251.96, "text": " a few millions or, I don't know, some of you might be processing millions of transactions", "tokens": [257, 1326, 6803, 420, 11, 286, 500, 380, 458, 11, 512, 295, 291, 1062, 312, 9007, 6803, 295, 16856], "temperature": 0.0, "avg_logprob": -0.1663884162902832, "compression_ratio": 1.6743119266055047, "no_speech_prob": 0.0004895822494290769}, {"id": 49, "seek": 24096, "start": 251.96, "end": 252.96, "text": " per second.", "tokens": [680, 1150, 13], "temperature": 0.0, "avg_logprob": -0.1663884162902832, "compression_ratio": 1.6743119266055047, "no_speech_prob": 0.0004895822494290769}, {"id": 50, "seek": 24096, "start": 252.96, "end": 253.96, "text": " So that's pretty good.", "tokens": [407, 300, 311, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.1663884162902832, "compression_ratio": 1.6743119266055047, "no_speech_prob": 0.0004895822494290769}, {"id": 51, "seek": 24096, "start": 253.96, "end": 260.96000000000004, "text": " And what we want today is to focus this domain into a very specific area.", "tokens": [400, 437, 321, 528, 965, 307, 281, 1879, 341, 9274, 666, 257, 588, 2685, 1859, 13], "temperature": 0.0, "avg_logprob": -0.1663884162902832, "compression_ratio": 1.6743119266055047, "no_speech_prob": 0.0004895822494290769}, {"id": 52, "seek": 24096, "start": 260.96000000000004, "end": 265.96000000000004, "text": " And this area essentially what we're trying to do today is to analyze traces.", "tokens": [400, 341, 1859, 4476, 437, 321, 434, 1382, 281, 360, 965, 307, 281, 12477, 26076, 13], "temperature": 0.0, "avg_logprob": -0.1663884162902832, "compression_ratio": 1.6743119266055047, "no_speech_prob": 0.0004895822494290769}, {"id": 53, "seek": 26596, "start": 265.96, "end": 271.96, "text": " So it doesn't matter if it's like writing system traces or platform traces or it's like programming", "tokens": [407, 309, 1177, 380, 1871, 498, 309, 311, 411, 3579, 1185, 26076, 420, 3663, 26076, 420, 309, 311, 411, 9410], "temperature": 0.0, "avg_logprob": -0.15217885885152732, "compression_ratio": 1.7159090909090908, "no_speech_prob": 0.00012651811994146556}, {"id": 54, "seek": 26596, "start": 271.96, "end": 272.96, "text": " language traces.", "tokens": [2856, 26076, 13], "temperature": 0.0, "avg_logprob": -0.15217885885152732, "compression_ratio": 1.7159090909090908, "no_speech_prob": 0.00012651811994146556}, {"id": 55, "seek": 26596, "start": 272.96, "end": 278.96, "text": " What we want is to make sure that we have environment and within this environment you", "tokens": [708, 321, 528, 307, 281, 652, 988, 300, 321, 362, 2823, 293, 1951, 341, 2823, 291], "temperature": 0.0, "avg_logprob": -0.15217885885152732, "compression_ratio": 1.7159090909090908, "no_speech_prob": 0.00012651811994146556}, {"id": 56, "seek": 26596, "start": 278.96, "end": 284.96, "text": " can scale your loads, basically, scale your processing and at the same time we'll provide", "tokens": [393, 4373, 428, 12668, 11, 1936, 11, 4373, 428, 9007, 293, 412, 264, 912, 565, 321, 603, 2893], "temperature": 0.0, "avg_logprob": -0.15217885885152732, "compression_ratio": 1.7159090909090908, "no_speech_prob": 0.00012651811994146556}, {"id": 57, "seek": 26596, "start": 284.96, "end": 286.96, "text": " some kind of analytics, right?", "tokens": [512, 733, 295, 15370, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15217885885152732, "compression_ratio": 1.7159090909090908, "no_speech_prob": 0.00012651811994146556}, {"id": 58, "seek": 26596, "start": 286.96, "end": 292.96, "text": " So again, if you look at how much data you're trying to process, the number by itself doesn't", "tokens": [407, 797, 11, 498, 291, 574, 412, 577, 709, 1412, 291, 434, 1382, 281, 1399, 11, 264, 1230, 538, 2564, 1177, 380], "temperature": 0.0, "avg_logprob": -0.15217885885152732, "compression_ratio": 1.7159090909090908, "no_speech_prob": 0.00012651811994146556}, {"id": 59, "seek": 26596, "start": 292.96, "end": 294.96, "text": " give you much what's going on here.", "tokens": [976, 291, 709, 437, 311, 516, 322, 510, 13], "temperature": 0.0, "avg_logprob": -0.15217885885152732, "compression_ratio": 1.7159090909090908, "no_speech_prob": 0.00012651811994146556}, {"id": 60, "seek": 29496, "start": 294.96, "end": 299.96, "text": " So what you want is to find this specific information you're looking for.", "tokens": [407, 437, 291, 528, 307, 281, 915, 341, 2685, 1589, 291, 434, 1237, 337, 13], "temperature": 0.0, "avg_logprob": -0.13553234597911004, "compression_ratio": 1.8873873873873874, "no_speech_prob": 0.0003721315588336438}, {"id": 61, "seek": 29496, "start": 299.96, "end": 305.96, "text": " Kind of like looking at, you know, finding the needles or finding the hidden areas within", "tokens": [9242, 295, 411, 1237, 412, 11, 291, 458, 11, 5006, 264, 24792, 420, 5006, 264, 7633, 3179, 1951], "temperature": 0.0, "avg_logprob": -0.13553234597911004, "compression_ratio": 1.8873873873873874, "no_speech_prob": 0.0003721315588336438}, {"id": 62, "seek": 29496, "start": 305.96, "end": 306.96, "text": " your data.", "tokens": [428, 1412, 13], "temperature": 0.0, "avg_logprob": -0.13553234597911004, "compression_ratio": 1.8873873873873874, "no_speech_prob": 0.0003721315588336438}, {"id": 63, "seek": 29496, "start": 306.96, "end": 313.96, "text": " So if you look at, you know, how much loads you process per day or per week and you'll", "tokens": [407, 498, 291, 574, 412, 11, 291, 458, 11, 577, 709, 12668, 291, 1399, 680, 786, 420, 680, 1243, 293, 291, 603], "temperature": 0.0, "avg_logprob": -0.13553234597911004, "compression_ratio": 1.8873873873873874, "no_speech_prob": 0.0003721315588336438}, {"id": 64, "seek": 29496, "start": 313.96, "end": 318.96, "text": " store it somewhere on, you know, crystal hard drive or you store it in Mori or you store", "tokens": [3531, 309, 4079, 322, 11, 291, 458, 11, 13662, 1152, 3332, 420, 291, 3531, 309, 294, 5146, 72, 420, 291, 3531], "temperature": 0.0, "avg_logprob": -0.13553234597911004, "compression_ratio": 1.8873873873873874, "no_speech_prob": 0.0003721315588336438}, {"id": 65, "seek": 29496, "start": 318.96, "end": 319.96, "text": " it in the cloud.", "tokens": [309, 294, 264, 4588, 13], "temperature": 0.0, "avg_logprob": -0.13553234597911004, "compression_ratio": 1.8873873873873874, "no_speech_prob": 0.0003721315588336438}, {"id": 66, "seek": 29496, "start": 319.96, "end": 323.96, "text": " So what you want is to, you know, make sense of it.", "tokens": [407, 437, 291, 528, 307, 281, 11, 291, 458, 11, 652, 2020, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.13553234597911004, "compression_ratio": 1.8873873873873874, "no_speech_prob": 0.0003721315588336438}, {"id": 67, "seek": 32396, "start": 323.96, "end": 330.96, "text": " And some companies do this process manually, which means they run software and they go", "tokens": [400, 512, 3431, 360, 341, 1399, 16945, 11, 597, 1355, 436, 1190, 4722, 293, 436, 352], "temperature": 0.0, "avg_logprob": -0.1373043608391422, "compression_ratio": 1.7048458149779735, "no_speech_prob": 0.0006409884081222117}, {"id": 68, "seek": 32396, "start": 330.96, "end": 337.96, "text": " through their loads and this is kind of a patch service and they try to understand what's", "tokens": [807, 641, 12668, 293, 341, 307, 733, 295, 257, 9972, 2643, 293, 436, 853, 281, 1223, 437, 311], "temperature": 0.0, "avg_logprob": -0.1373043608391422, "compression_ratio": 1.7048458149779735, "no_speech_prob": 0.0006409884081222117}, {"id": 69, "seek": 32396, "start": 337.96, "end": 339.96, "text": " going on within the load.", "tokens": [516, 322, 1951, 264, 3677, 13], "temperature": 0.0, "avg_logprob": -0.1373043608391422, "compression_ratio": 1.7048458149779735, "no_speech_prob": 0.0006409884081222117}, {"id": 70, "seek": 32396, "start": 339.96, "end": 345.96, "text": " So obviously this is a problem when you want to scale it and with the scaling you have different", "tokens": [407, 2745, 341, 307, 257, 1154, 562, 291, 528, 281, 4373, 309, 293, 365, 264, 21589, 291, 362, 819], "temperature": 0.0, "avg_logprob": -0.1373043608391422, "compression_ratio": 1.7048458149779735, "no_speech_prob": 0.0006409884081222117}, {"id": 71, "seek": 32396, "start": 345.96, "end": 351.96, "text": " loads stored in different places and you want to make sure basically to have a platform", "tokens": [12668, 12187, 294, 819, 3190, 293, 291, 528, 281, 652, 988, 1936, 281, 362, 257, 3663], "temperature": 0.0, "avg_logprob": -0.1373043608391422, "compression_ratio": 1.7048458149779735, "no_speech_prob": 0.0006409884081222117}, {"id": 72, "seek": 35196, "start": 351.96, "end": 358.96, "text": " where in this platform we kind of like looking at some kind of results.", "tokens": [689, 294, 341, 3663, 321, 733, 295, 411, 1237, 412, 512, 733, 295, 3542, 13], "temperature": 0.0, "avg_logprob": -0.18035848641101224, "compression_ratio": 1.6331658291457287, "no_speech_prob": 0.00018086253840010613}, {"id": 73, "seek": 35196, "start": 358.96, "end": 364.96, "text": " So for the sake of this discussion today, we'll focus on two different solutions.", "tokens": [407, 337, 264, 9717, 295, 341, 5017, 965, 11, 321, 603, 1879, 322, 732, 819, 6547, 13], "temperature": 0.0, "avg_logprob": -0.18035848641101224, "compression_ratio": 1.6331658291457287, "no_speech_prob": 0.00018086253840010613}, {"id": 74, "seek": 35196, "start": 364.96, "end": 370.96, "text": " So one of them is trying to provide some kind of alerts and the other is to provide some", "tokens": [407, 472, 295, 552, 307, 1382, 281, 2893, 512, 733, 295, 28061, 293, 264, 661, 307, 281, 2893, 512], "temperature": 0.0, "avg_logprob": -0.18035848641101224, "compression_ratio": 1.6331658291457287, "no_speech_prob": 0.00018086253840010613}, {"id": 75, "seek": 35196, "start": 370.96, "end": 373.96, "text": " kind of trends within your data.", "tokens": [733, 295, 13892, 1951, 428, 1412, 13], "temperature": 0.0, "avg_logprob": -0.18035848641101224, "compression_ratio": 1.6331658291457287, "no_speech_prob": 0.00018086253840010613}, {"id": 76, "seek": 35196, "start": 373.96, "end": 376.96, "text": " Obviously I work for a company called Hazardcast.", "tokens": [7580, 286, 589, 337, 257, 2237, 1219, 15852, 515, 3734, 13], "temperature": 0.0, "avg_logprob": -0.18035848641101224, "compression_ratio": 1.6331658291457287, "no_speech_prob": 0.00018086253840010613}, {"id": 77, "seek": 37696, "start": 376.96, "end": 383.96, "text": " So Hazardcast as a platform, I love you to do so but obviously you might have heard of", "tokens": [407, 15852, 515, 3734, 382, 257, 3663, 11, 286, 959, 291, 281, 360, 370, 457, 2745, 291, 1062, 362, 2198, 295], "temperature": 0.0, "avg_logprob": -0.1517800013224284, "compression_ratio": 1.7238493723849373, "no_speech_prob": 7.822692714398727e-05}, {"id": 78, "seek": 37696, "start": 383.96, "end": 387.96, "text": " some companies or, you know, they do some kind of stream processing.", "tokens": [512, 3431, 420, 11, 291, 458, 11, 436, 360, 512, 733, 295, 4309, 9007, 13], "temperature": 0.0, "avg_logprob": -0.1517800013224284, "compression_ratio": 1.7238493723849373, "no_speech_prob": 7.822692714398727e-05}, {"id": 79, "seek": 37696, "start": 387.96, "end": 394.96, "text": " So this is kind of like, you know, overview what's going on with this domain at this time.", "tokens": [407, 341, 307, 733, 295, 411, 11, 291, 458, 11, 12492, 437, 311, 516, 322, 365, 341, 9274, 412, 341, 565, 13], "temperature": 0.0, "avg_logprob": -0.1517800013224284, "compression_ratio": 1.7238493723849373, "no_speech_prob": 7.822692714398727e-05}, {"id": 80, "seek": 37696, "start": 394.96, "end": 400.96, "text": " Obviously you can split it depending on if you're looking for open source solution or,", "tokens": [7580, 291, 393, 7472, 309, 5413, 322, 498, 291, 434, 1237, 337, 1269, 4009, 3827, 420, 11], "temperature": 0.0, "avg_logprob": -0.1517800013224284, "compression_ratio": 1.7238493723849373, "no_speech_prob": 7.822692714398727e-05}, {"id": 81, "seek": 37696, "start": 400.96, "end": 405.96, "text": " I don't know, hardware solution or, you know, some kind of management service.", "tokens": [286, 500, 380, 458, 11, 8837, 3827, 420, 11, 291, 458, 11, 512, 733, 295, 4592, 2643, 13], "temperature": 0.0, "avg_logprob": -0.1517800013224284, "compression_ratio": 1.7238493723849373, "no_speech_prob": 7.822692714398727e-05}, {"id": 82, "seek": 40596, "start": 405.96, "end": 409.96, "text": " And what you look at is kind of which domain you work so are you looking to capture your", "tokens": [400, 437, 291, 574, 412, 307, 733, 295, 597, 9274, 291, 589, 370, 366, 291, 1237, 281, 7983, 428], "temperature": 0.0, "avg_logprob": -0.13894195347041874, "compression_ratio": 1.8177339901477831, "no_speech_prob": 0.00012434679956641048}, {"id": 83, "seek": 40596, "start": 409.96, "end": 414.96, "text": " data or some kind of, you know, streaming your data or you want to do some kind of", "tokens": [1412, 420, 512, 733, 295, 11, 291, 458, 11, 11791, 428, 1412, 420, 291, 528, 281, 360, 512, 733, 295], "temperature": 0.0, "avg_logprob": -0.13894195347041874, "compression_ratio": 1.8177339901477831, "no_speech_prob": 0.00012434679956641048}, {"id": 84, "seek": 40596, "start": 414.96, "end": 420.96, "text": " transformation on your data or do some kind of electrical machine learning.", "tokens": [9887, 322, 428, 1412, 420, 360, 512, 733, 295, 12147, 3479, 2539, 13], "temperature": 0.0, "avg_logprob": -0.13894195347041874, "compression_ratio": 1.8177339901477831, "no_speech_prob": 0.00012434679956641048}, {"id": 85, "seek": 40596, "start": 420.96, "end": 427.96, "text": " So you can see that you split it into 12 squares and within these like tools and", "tokens": [407, 291, 393, 536, 300, 291, 7472, 309, 666, 2272, 19368, 293, 1951, 613, 411, 3873, 293], "temperature": 0.0, "avg_logprob": -0.13894195347041874, "compression_ratio": 1.8177339901477831, "no_speech_prob": 0.00012434679956641048}, {"id": 86, "seek": 40596, "start": 427.96, "end": 430.96, "text": " platforms are, you know, spread it over.", "tokens": [9473, 366, 11, 291, 458, 11, 3974, 309, 670, 13], "temperature": 0.0, "avg_logprob": -0.13894195347041874, "compression_ratio": 1.8177339901477831, "no_speech_prob": 0.00012434679956641048}, {"id": 87, "seek": 43096, "start": 430.96, "end": 437.96, "text": " Some tools not exist on this screen for whatever reason but obviously this might give", "tokens": [2188, 3873, 406, 2514, 322, 341, 2568, 337, 2035, 1778, 457, 2745, 341, 1062, 976], "temperature": 0.0, "avg_logprob": -0.1459856434872276, "compression_ratio": 1.6751054852320675, "no_speech_prob": 0.00010521009971853346}, {"id": 88, "seek": 43096, "start": 437.96, "end": 442.96, "text": " you some ideas but it's hard to decide which tool you want to go for.", "tokens": [291, 512, 3487, 457, 309, 311, 1152, 281, 4536, 597, 2290, 291, 528, 281, 352, 337, 13], "temperature": 0.0, "avg_logprob": -0.1459856434872276, "compression_ratio": 1.6751054852320675, "no_speech_prob": 0.00010521009971853346}, {"id": 89, "seek": 43096, "start": 442.96, "end": 448.96, "text": " Simply because I think the distribution is not clear here so it tells you basically", "tokens": [19596, 570, 286, 519, 264, 7316, 307, 406, 1850, 510, 370, 309, 5112, 291, 1936], "temperature": 0.0, "avg_logprob": -0.1459856434872276, "compression_ratio": 1.6751054852320675, "no_speech_prob": 0.00010521009971853346}, {"id": 90, "seek": 43096, "start": 448.96, "end": 454.96, "text": " which tool is open source for example and where in process you can use it but it doesn't", "tokens": [597, 2290, 307, 1269, 4009, 337, 1365, 293, 689, 294, 1399, 291, 393, 764, 309, 457, 309, 1177, 380], "temperature": 0.0, "avg_logprob": -0.1459856434872276, "compression_ratio": 1.6751054852320675, "no_speech_prob": 0.00010521009971853346}, {"id": 91, "seek": 43096, "start": 454.96, "end": 458.96, "text": " give you full picture on, you know, how to do it in practical terms.", "tokens": [976, 291, 1577, 3036, 322, 11, 291, 458, 11, 577, 281, 360, 309, 294, 8496, 2115, 13], "temperature": 0.0, "avg_logprob": -0.1459856434872276, "compression_ratio": 1.6751054852320675, "no_speech_prob": 0.00010521009971853346}, {"id": 92, "seek": 45896, "start": 458.96, "end": 463.96, "text": " And so this is where it might be easier to understand what we're talking about.", "tokens": [400, 370, 341, 307, 689, 309, 1062, 312, 3571, 281, 1223, 437, 321, 434, 1417, 466, 13], "temperature": 0.0, "avg_logprob": -0.12885998107574798, "compression_ratio": 1.6519823788546255, "no_speech_prob": 9.743044938659295e-05}, {"id": 93, "seek": 45896, "start": 463.96, "end": 470.96, "text": " So if you remember from my slide where I discussed the historical data and the new data.", "tokens": [407, 498, 291, 1604, 490, 452, 4137, 689, 286, 7152, 264, 8584, 1412, 293, 264, 777, 1412, 13], "temperature": 0.0, "avg_logprob": -0.12885998107574798, "compression_ratio": 1.6519823788546255, "no_speech_prob": 9.743044938659295e-05}, {"id": 94, "seek": 45896, "start": 470.96, "end": 475.96, "text": " So today we're kind of like, you know, trying to split everything into two categories.", "tokens": [407, 965, 321, 434, 733, 295, 411, 11, 291, 458, 11, 1382, 281, 7472, 1203, 666, 732, 10479, 13], "temperature": 0.0, "avg_logprob": -0.12885998107574798, "compression_ratio": 1.6519823788546255, "no_speech_prob": 9.743044938659295e-05}, {"id": 95, "seek": 45896, "start": 475.96, "end": 479.96, "text": " So on one side you get like stream processing engines.", "tokens": [407, 322, 472, 1252, 291, 483, 411, 4309, 9007, 12982, 13], "temperature": 0.0, "avg_logprob": -0.12885998107574798, "compression_ratio": 1.6519823788546255, "no_speech_prob": 9.743044938659295e-05}, {"id": 96, "seek": 45896, "start": 479.96, "end": 484.96, "text": " So these engines are pretty fast in, you know, streaming events.", "tokens": [407, 613, 12982, 366, 1238, 2370, 294, 11, 291, 458, 11, 11791, 3931, 13], "temperature": 0.0, "avg_logprob": -0.12885998107574798, "compression_ratio": 1.6519823788546255, "no_speech_prob": 9.743044938659295e-05}, {"id": 97, "seek": 48496, "start": 484.96, "end": 490.96, "text": " And on this far right side you have some kind of fast data stores which are, you know,", "tokens": [400, 322, 341, 1400, 558, 1252, 291, 362, 512, 733, 295, 2370, 1412, 9512, 597, 366, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.17399060726165771, "compression_ratio": 1.6519337016574585, "no_speech_prob": 5.2156956371618435e-05}, {"id": 98, "seek": 48496, "start": 490.96, "end": 495.96, "text": " are pretty fast in handling data at speed.", "tokens": [366, 1238, 2370, 294, 13175, 1412, 412, 3073, 13], "temperature": 0.0, "avg_logprob": -0.17399060726165771, "compression_ratio": 1.6519337016574585, "no_speech_prob": 5.2156956371618435e-05}, {"id": 99, "seek": 48496, "start": 495.96, "end": 502.96, "text": " So again the solution for lead time stream processing is kind of a combination and you", "tokens": [407, 797, 264, 3827, 337, 1477, 565, 4309, 9007, 307, 733, 295, 257, 6562, 293, 291], "temperature": 0.0, "avg_logprob": -0.17399060726165771, "compression_ratio": 1.6519337016574585, "no_speech_prob": 5.2156956371618435e-05}, {"id": 100, "seek": 48496, "start": 502.96, "end": 508.96, "text": " want to process data in this moment and at the same time you want to actually also", "tokens": [528, 281, 1399, 1412, 294, 341, 1623, 293, 412, 264, 912, 565, 291, 528, 281, 767, 611], "temperature": 0.0, "avg_logprob": -0.17399060726165771, "compression_ratio": 1.6519337016574585, "no_speech_prob": 5.2156956371618435e-05}, {"id": 101, "seek": 50896, "start": 508.96, "end": 515.96, "text": " access data storage somewhere. So that's where Hazardcast fits into this area here.", "tokens": [2105, 1412, 6725, 4079, 13, 407, 300, 311, 689, 15852, 515, 3734, 9001, 666, 341, 1859, 510, 13], "temperature": 0.0, "avg_logprob": -0.23870751230340254, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.0001139090527431108}, {"id": 102, "seek": 50896, "start": 515.96, "end": 520.96, "text": " So the platform itself obviously for those who don't know, by the way we have one of", "tokens": [407, 264, 3663, 2564, 2745, 337, 729, 567, 500, 380, 458, 11, 538, 264, 636, 321, 362, 472, 295], "temperature": 0.0, "avg_logprob": -0.23870751230340254, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.0001139090527431108}, {"id": 103, "seek": 50896, "start": 520.96, "end": 523.96, "text": " the masterminds of Hazardcast sitting in this room.", "tokens": [264, 4505, 13733, 82, 295, 15852, 515, 3734, 3798, 294, 341, 1808, 13], "temperature": 0.0, "avg_logprob": -0.23870751230340254, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.0001139090527431108}, {"id": 104, "seek": 50896, "start": 523.96, "end": 525.96, "text": " So this is the platform.", "tokens": [407, 341, 307, 264, 3663, 13], "temperature": 0.0, "avg_logprob": -0.23870751230340254, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.0001139090527431108}, {"id": 105, "seek": 50896, "start": 525.96, "end": 527.96, "text": " So it's open source platform.", "tokens": [407, 309, 311, 1269, 4009, 3663, 13], "temperature": 0.0, "avg_logprob": -0.23870751230340254, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.0001139090527431108}, {"id": 106, "seek": 50896, "start": 527.96, "end": 532.96, "text": " It doesn't matter where your source is coming from, whether it's Apache Cloud or Apache", "tokens": [467, 1177, 380, 1871, 689, 428, 4009, 307, 1348, 490, 11, 1968, 309, 311, 46597, 8061, 420, 46597], "temperature": 0.0, "avg_logprob": -0.23870751230340254, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.0001139090527431108}, {"id": 107, "seek": 53296, "start": 532.96, "end": 538.96, "text": " IoT devices, for example, I don't know, some kind of device applications or even like within", "tokens": [30112, 5759, 11, 337, 1365, 11, 286, 500, 380, 458, 11, 512, 733, 295, 4302, 5821, 420, 754, 411, 1951], "temperature": 0.0, "avg_logprob": -0.21198557007987545, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.000178300600964576}, {"id": 108, "seek": 53296, "start": 538.96, "end": 543.96, "text": " Hazardcast or even you can write your own connector and you feed it into the platform.", "tokens": [15852, 515, 3734, 420, 754, 291, 393, 2464, 428, 1065, 19127, 293, 291, 3154, 309, 666, 264, 3663, 13], "temperature": 0.0, "avg_logprob": -0.21198557007987545, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.000178300600964576}, {"id": 109, "seek": 53296, "start": 543.96, "end": 547.96, "text": " So platform historically used to be two different components.", "tokens": [407, 3663, 16180, 1143, 281, 312, 732, 819, 6677, 13], "temperature": 0.0, "avg_logprob": -0.21198557007987545, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.000178300600964576}, {"id": 110, "seek": 53296, "start": 547.96, "end": 551.96, "text": " So the IOMTG and the Jet Engine.", "tokens": [407, 264, 286, 5251, 51, 38, 293, 264, 28730, 7659, 13], "temperature": 0.0, "avg_logprob": -0.21198557007987545, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.000178300600964576}, {"id": 111, "seek": 53296, "start": 551.96, "end": 556.96, "text": " And essentially now it's all back in one, one jar file.", "tokens": [400, 4476, 586, 309, 311, 439, 646, 294, 472, 11, 472, 15181, 3991, 13], "temperature": 0.0, "avg_logprob": -0.21198557007987545, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.000178300600964576}, {"id": 112, "seek": 53296, "start": 556.96, "end": 561.96, "text": " As you see here, it allows you to load your data from hard disks into memory.", "tokens": [1018, 291, 536, 510, 11, 309, 4045, 291, 281, 3677, 428, 1412, 490, 1152, 41617, 666, 4675, 13], "temperature": 0.0, "avg_logprob": -0.21198557007987545, "compression_ratio": 1.5692307692307692, "no_speech_prob": 0.000178300600964576}, {"id": 113, "seek": 56196, "start": 561.96, "end": 569.96, "text": " So you have access to historical data and pretty much like instantaneously and this", "tokens": [407, 291, 362, 2105, 281, 8584, 1412, 293, 1238, 709, 411, 9836, 13131, 293, 341], "temperature": 0.0, "avg_logprob": -0.1563062940325056, "compression_ratio": 1.7096774193548387, "no_speech_prob": 6.276258500292897e-05}, {"id": 114, "seek": 56196, "start": 569.96, "end": 575.96, "text": " will, well, you know, you can provide context, what's going on with your data.", "tokens": [486, 11, 731, 11, 291, 458, 11, 291, 393, 2893, 4319, 11, 437, 311, 516, 322, 365, 428, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1563062940325056, "compression_ratio": 1.7096774193548387, "no_speech_prob": 6.276258500292897e-05}, {"id": 115, "seek": 56196, "start": 575.96, "end": 578.96, "text": " At the same time, you can actually do stream processing.", "tokens": [1711, 264, 912, 565, 11, 291, 393, 767, 360, 4309, 9007, 13], "temperature": 0.0, "avg_logprob": -0.1563062940325056, "compression_ratio": 1.7096774193548387, "no_speech_prob": 6.276258500292897e-05}, {"id": 116, "seek": 56196, "start": 578.96, "end": 580.96, "text": " So that's what Jet Engine is.", "tokens": [407, 300, 311, 437, 28730, 7659, 307, 13], "temperature": 0.0, "avg_logprob": -0.1563062940325056, "compression_ratio": 1.7096774193548387, "no_speech_prob": 6.276258500292897e-05}, {"id": 117, "seek": 56196, "start": 580.96, "end": 585.96, "text": " So from here you can do some kind, I don't know, maybe like data transformation or do", "tokens": [407, 490, 510, 291, 393, 360, 512, 733, 11, 286, 500, 380, 458, 11, 1310, 411, 1412, 9887, 420, 360], "temperature": 0.0, "avg_logprob": -0.1563062940325056, "compression_ratio": 1.7096774193548387, "no_speech_prob": 6.276258500292897e-05}, {"id": 118, "seek": 56196, "start": 585.96, "end": 590.96, "text": " some kind of stream processing as we will do today or even like defined machine learning", "tokens": [512, 733, 295, 4309, 9007, 382, 321, 486, 360, 965, 420, 754, 411, 7642, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.1563062940325056, "compression_ratio": 1.7096774193548387, "no_speech_prob": 6.276258500292897e-05}, {"id": 119, "seek": 59096, "start": 590.96, "end": 591.96, "text": " if you want to.", "tokens": [498, 291, 528, 281, 13], "temperature": 0.0, "avg_logprob": -0.12536290006817513, "compression_ratio": 1.6398467432950192, "no_speech_prob": 5.793759191874415e-05}, {"id": 120, "seek": 59096, "start": 591.96, "end": 593.96, "text": " You can connect it to some clients.", "tokens": [509, 393, 1745, 309, 281, 512, 6982, 13], "temperature": 0.0, "avg_logprob": -0.12536290006817513, "compression_ratio": 1.6398467432950192, "no_speech_prob": 5.793759191874415e-05}, {"id": 121, "seek": 59096, "start": 593.96, "end": 598.96, "text": " So these are some clients here, so written into various languages.", "tokens": [407, 613, 366, 512, 6982, 510, 11, 370, 3720, 666, 3683, 8650, 13], "temperature": 0.0, "avg_logprob": -0.12536290006817513, "compression_ratio": 1.6398467432950192, "no_speech_prob": 5.793759191874415e-05}, {"id": 122, "seek": 59096, "start": 598.96, "end": 603.96, "text": " If you're from data science background, which means your programming languages in general", "tokens": [759, 291, 434, 490, 1412, 3497, 3678, 11, 597, 1355, 428, 9410, 8650, 294, 2674], "temperature": 0.0, "avg_logprob": -0.12536290006817513, "compression_ratio": 1.6398467432950192, "no_speech_prob": 5.793759191874415e-05}, {"id": 123, "seek": 59096, "start": 603.96, "end": 609.96, "text": " are not preferable for you, so you might be considering using SQL to do what I'm planning", "tokens": [366, 406, 4382, 712, 337, 291, 11, 370, 291, 1062, 312, 8079, 1228, 19200, 281, 360, 437, 286, 478, 5038], "temperature": 0.0, "avg_logprob": -0.12536290006817513, "compression_ratio": 1.6398467432950192, "no_speech_prob": 5.793759191874415e-05}, {"id": 124, "seek": 59096, "start": 609.96, "end": 610.96, "text": " today.", "tokens": [965, 13], "temperature": 0.0, "avg_logprob": -0.12536290006817513, "compression_ratio": 1.6398467432950192, "no_speech_prob": 5.793759191874415e-05}, {"id": 125, "seek": 59096, "start": 610.96, "end": 612.96, "text": " So this is another option you can do.", "tokens": [407, 341, 307, 1071, 3614, 291, 393, 360, 13], "temperature": 0.0, "avg_logprob": -0.12536290006817513, "compression_ratio": 1.6398467432950192, "no_speech_prob": 5.793759191874415e-05}, {"id": 126, "seek": 59096, "start": 612.96, "end": 619.96, "text": " And once you process this data where you load it into memory for historical data and", "tokens": [400, 1564, 291, 1399, 341, 1412, 689, 291, 3677, 309, 666, 4675, 337, 8584, 1412, 293], "temperature": 0.0, "avg_logprob": -0.12536290006817513, "compression_ratio": 1.6398467432950192, "no_speech_prob": 5.793759191874415e-05}, {"id": 127, "seek": 61996, "start": 619.96, "end": 623.96, "text": " at the same time you have some kind of data coming in.", "tokens": [412, 264, 912, 565, 291, 362, 512, 733, 295, 1412, 1348, 294, 13], "temperature": 0.0, "avg_logprob": -0.20895485926156093, "compression_ratio": 1.680672268907563, "no_speech_prob": 2.791614315356128e-05}, {"id": 128, "seek": 61996, "start": 623.96, "end": 628.96, "text": " For example, and you do the combination or even you do transformation, you can proceed", "tokens": [1171, 1365, 11, 293, 291, 360, 264, 6562, 420, 754, 291, 360, 9887, 11, 291, 393, 8991], "temperature": 0.0, "avg_logprob": -0.20895485926156093, "compression_ratio": 1.680672268907563, "no_speech_prob": 2.791614315356128e-05}, {"id": 129, "seek": 61996, "start": 628.96, "end": 630.96, "text": " to do some kind of visualization.", "tokens": [281, 360, 512, 733, 295, 25801, 13], "temperature": 0.0, "avg_logprob": -0.20895485926156093, "compression_ratio": 1.680672268907563, "no_speech_prob": 2.791614315356128e-05}, {"id": 130, "seek": 61996, "start": 630.96, "end": 637.96, "text": " So the good thing about Hesicas in general where it comes to scaling is it's partition", "tokens": [407, 264, 665, 551, 466, 389, 279, 9150, 294, 2674, 689, 309, 1487, 281, 21589, 307, 309, 311, 24808], "temperature": 0.0, "avg_logprob": -0.20895485926156093, "compression_ratio": 1.680672268907563, "no_speech_prob": 2.791614315356128e-05}, {"id": 131, "seek": 61996, "start": 637.96, "end": 638.96, "text": " aware.", "tokens": [3650, 13], "temperature": 0.0, "avg_logprob": -0.20895485926156093, "compression_ratio": 1.680672268907563, "no_speech_prob": 2.791614315356128e-05}, {"id": 132, "seek": 61996, "start": 638.96, "end": 644.96, "text": " So which means basically your compute, your Jet Engine or your process essentially can", "tokens": [407, 597, 1355, 1936, 428, 14722, 11, 428, 28730, 7659, 420, 428, 1399, 4476, 393], "temperature": 0.0, "avg_logprob": -0.20895485926156093, "compression_ratio": 1.680672268907563, "no_speech_prob": 2.791614315356128e-05}, {"id": 133, "seek": 61996, "start": 644.96, "end": 647.96, "text": " be or can detect where your data is stored.", "tokens": [312, 420, 393, 5531, 689, 428, 1412, 307, 12187, 13], "temperature": 0.0, "avg_logprob": -0.20895485926156093, "compression_ratio": 1.680672268907563, "no_speech_prob": 2.791614315356128e-05}, {"id": 134, "seek": 64796, "start": 647.96, "end": 653.96, "text": " So this is like, you know, we're trying to have as low latency as possible when it comes", "tokens": [407, 341, 307, 411, 11, 291, 458, 11, 321, 434, 1382, 281, 362, 382, 2295, 27043, 382, 1944, 562, 309, 1487], "temperature": 0.0, "avg_logprob": -0.1259914376269812, "compression_ratio": 1.624413145539906, "no_speech_prob": 0.00016834901180118322}, {"id": 135, "seek": 64796, "start": 653.96, "end": 655.96, "text": " to processing this data.", "tokens": [281, 9007, 341, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1259914376269812, "compression_ratio": 1.624413145539906, "no_speech_prob": 0.00016834901180118322}, {"id": 136, "seek": 64796, "start": 655.96, "end": 660.96, "text": " So this is very important to understand because latency is your enemy when it comes to stream", "tokens": [407, 341, 307, 588, 1021, 281, 1223, 570, 27043, 307, 428, 5945, 562, 309, 1487, 281, 4309], "temperature": 0.0, "avg_logprob": -0.1259914376269812, "compression_ratio": 1.624413145539906, "no_speech_prob": 0.00016834901180118322}, {"id": 137, "seek": 64796, "start": 660.96, "end": 661.96, "text": " processing.", "tokens": [9007, 13], "temperature": 0.0, "avg_logprob": -0.1259914376269812, "compression_ratio": 1.624413145539906, "no_speech_prob": 0.00016834901180118322}, {"id": 138, "seek": 64796, "start": 661.96, "end": 667.96, "text": " So what you want is kind of like having a platform where you avoid network folks.", "tokens": [407, 437, 291, 528, 307, 733, 295, 411, 1419, 257, 3663, 689, 291, 5042, 3209, 4024, 13], "temperature": 0.0, "avg_logprob": -0.1259914376269812, "compression_ratio": 1.624413145539906, "no_speech_prob": 0.00016834901180118322}, {"id": 139, "seek": 64796, "start": 667.96, "end": 671.96, "text": " For example, you avoid IO to your hard disk.", "tokens": [1171, 1365, 11, 291, 5042, 39839, 281, 428, 1152, 12355, 13], "temperature": 0.0, "avg_logprob": -0.1259914376269812, "compression_ratio": 1.624413145539906, "no_speech_prob": 0.00016834901180118322}, {"id": 140, "seek": 67196, "start": 671.96, "end": 677.96, "text": " You will try to also avoid every time or, sorry, context switching between threads.", "tokens": [509, 486, 853, 281, 611, 5042, 633, 565, 420, 11, 2597, 11, 4319, 16493, 1296, 19314, 13], "temperature": 0.0, "avg_logprob": -0.13158046389089048, "compression_ratio": 1.6382113821138211, "no_speech_prob": 4.742460805573501e-05}, {"id": 141, "seek": 67196, "start": 677.96, "end": 681.96, "text": " So you want to avoid all of these, but at the same time you want your process to be as", "tokens": [407, 291, 528, 281, 5042, 439, 295, 613, 11, 457, 412, 264, 912, 565, 291, 528, 428, 1399, 281, 312, 382], "temperature": 0.0, "avg_logprob": -0.13158046389089048, "compression_ratio": 1.6382113821138211, "no_speech_prob": 4.742460805573501e-05}, {"id": 142, "seek": 67196, "start": 681.96, "end": 683.96, "text": " close as possible to your data.", "tokens": [1998, 382, 1944, 281, 428, 1412, 13], "temperature": 0.0, "avg_logprob": -0.13158046389089048, "compression_ratio": 1.6382113821138211, "no_speech_prob": 4.742460805573501e-05}, {"id": 143, "seek": 67196, "start": 683.96, "end": 688.96, "text": " You can avoid some kind of, you know, machine learning on this.", "tokens": [509, 393, 5042, 512, 733, 295, 11, 291, 458, 11, 3479, 2539, 322, 341, 13], "temperature": 0.0, "avg_logprob": -0.13158046389089048, "compression_ratio": 1.6382113821138211, "no_speech_prob": 4.742460805573501e-05}, {"id": 144, "seek": 67196, "start": 688.96, "end": 692.96, "text": " And the scaling itself could be done in various ways.", "tokens": [400, 264, 21589, 2564, 727, 312, 1096, 294, 3683, 2098, 13], "temperature": 0.0, "avg_logprob": -0.13158046389089048, "compression_ratio": 1.6382113821138211, "no_speech_prob": 4.742460805573501e-05}, {"id": 145, "seek": 67196, "start": 692.96, "end": 699.96, "text": " So the main thing to take away from here is there's no master-worker relationship.", "tokens": [407, 264, 2135, 551, 281, 747, 1314, 490, 510, 307, 456, 311, 572, 4505, 12, 49402, 2480, 13], "temperature": 0.0, "avg_logprob": -0.13158046389089048, "compression_ratio": 1.6382113821138211, "no_speech_prob": 4.742460805573501e-05}, {"id": 146, "seek": 69996, "start": 699.96, "end": 702.96, "text": " So all nodes basically are peers.", "tokens": [407, 439, 13891, 1936, 366, 16739, 13], "temperature": 0.0, "avg_logprob": -0.11027524967004756, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.00022267714666668326}, {"id": 147, "seek": 69996, "start": 702.96, "end": 704.96, "text": " And we've done this study.", "tokens": [400, 321, 600, 1096, 341, 2979, 13], "temperature": 0.0, "avg_logprob": -0.11027524967004756, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.00022267714666668326}, {"id": 148, "seek": 69996, "start": 704.96, "end": 710.96, "text": " It's a bit dated, but it's kind of like one million transactions per second on 45 nodes.", "tokens": [467, 311, 257, 857, 23804, 11, 457, 309, 311, 733, 295, 411, 472, 2459, 16856, 680, 1150, 322, 6905, 13891, 13], "temperature": 0.0, "avg_logprob": -0.11027524967004756, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.00022267714666668326}, {"id": 149, "seek": 69996, "start": 710.96, "end": 715.96, "text": " So what we're trying to do now is to add one zero into this number here.", "tokens": [407, 437, 321, 434, 1382, 281, 360, 586, 307, 281, 909, 472, 4018, 666, 341, 1230, 510, 13], "temperature": 0.0, "avg_logprob": -0.11027524967004756, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.00022267714666668326}, {"id": 150, "seek": 69996, "start": 715.96, "end": 720.96, "text": " And even though it's pretty impressive, what is nice about it is the linear scaling, which", "tokens": [400, 754, 1673, 309, 311, 1238, 8992, 11, 437, 307, 1481, 466, 309, 307, 264, 8213, 21589, 11, 597], "temperature": 0.0, "avg_logprob": -0.11027524967004756, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.00022267714666668326}, {"id": 151, "seek": 69996, "start": 720.96, "end": 725.96, "text": " means more data you can add, you know, more nodes into it.", "tokens": [1355, 544, 1412, 291, 393, 909, 11, 291, 458, 11, 544, 13891, 666, 309, 13], "temperature": 0.0, "avg_logprob": -0.11027524967004756, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.00022267714666668326}, {"id": 152, "seek": 72596, "start": 725.96, "end": 729.96, "text": " So that's the historical bit of this talk.", "tokens": [407, 300, 311, 264, 8584, 857, 295, 341, 751, 13], "temperature": 0.0, "avg_logprob": -0.09978844273474909, "compression_ratio": 1.695167286245353, "no_speech_prob": 6.908549403306097e-05}, {"id": 153, "seek": 72596, "start": 729.96, "end": 731.96, "text": " So let's just move to the technical part.", "tokens": [407, 718, 311, 445, 1286, 281, 264, 6191, 644, 13], "temperature": 0.0, "avg_logprob": -0.09978844273474909, "compression_ratio": 1.695167286245353, "no_speech_prob": 6.908549403306097e-05}, {"id": 154, "seek": 72596, "start": 731.96, "end": 736.96, "text": " So for this demo, what I wanted is kind of like, you know, show you some ideas, right?", "tokens": [407, 337, 341, 10723, 11, 437, 286, 1415, 307, 733, 295, 411, 11, 291, 458, 11, 855, 291, 512, 3487, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.09978844273474909, "compression_ratio": 1.695167286245353, "no_speech_prob": 6.908549403306097e-05}, {"id": 155, "seek": 72596, "start": 736.96, "end": 740.96, "text": " So you should be able to take these ideas and apply it, you know, anywhere.", "tokens": [407, 291, 820, 312, 1075, 281, 747, 613, 3487, 293, 3079, 309, 11, 291, 458, 11, 4992, 13], "temperature": 0.0, "avg_logprob": -0.09978844273474909, "compression_ratio": 1.695167286245353, "no_speech_prob": 6.908549403306097e-05}, {"id": 156, "seek": 72596, "start": 740.96, "end": 745.96, "text": " Obviously the solution as itself could be like, you know, project by itself.", "tokens": [7580, 264, 3827, 382, 2564, 727, 312, 411, 11, 291, 458, 11, 1716, 538, 2564, 13], "temperature": 0.0, "avg_logprob": -0.09978844273474909, "compression_ratio": 1.695167286245353, "no_speech_prob": 6.908549403306097e-05}, {"id": 157, "seek": 72596, "start": 745.96, "end": 747.96, "text": " So feel free to edit and change it.", "tokens": [407, 841, 1737, 281, 8129, 293, 1319, 309, 13], "temperature": 0.0, "avg_logprob": -0.09978844273474909, "compression_ratio": 1.695167286245353, "no_speech_prob": 6.908549403306097e-05}, {"id": 158, "seek": 72596, "start": 747.96, "end": 752.96, "text": " All source code is available on GitHub and the documentation as well.", "tokens": [1057, 4009, 3089, 307, 2435, 322, 23331, 293, 264, 14333, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.09978844273474909, "compression_ratio": 1.695167286245353, "no_speech_prob": 6.908549403306097e-05}, {"id": 159, "seek": 72596, "start": 752.96, "end": 754.96, "text": " So you can go through it.", "tokens": [407, 291, 393, 352, 807, 309, 13], "temperature": 0.0, "avg_logprob": -0.09978844273474909, "compression_ratio": 1.695167286245353, "no_speech_prob": 6.908549403306097e-05}, {"id": 160, "seek": 75496, "start": 754.96, "end": 760.96, "text": " So the main idea when it comes to analyzing or, you know, making sense out of your traces", "tokens": [407, 264, 2135, 1558, 562, 309, 1487, 281, 23663, 420, 11, 291, 458, 11, 1455, 2020, 484, 295, 428, 26076], "temperature": 0.0, "avg_logprob": -0.137221012300658, "compression_ratio": 1.7971014492753623, "no_speech_prob": 0.00018138086306862533}, {"id": 161, "seek": 75496, "start": 760.96, "end": 768.96, "text": " and logs is to store it somewhere close to, you know, your compute, first of all,", "tokens": [293, 20820, 307, 281, 3531, 309, 4079, 1998, 281, 11, 291, 458, 11, 428, 14722, 11, 700, 295, 439, 11], "temperature": 0.0, "avg_logprob": -0.137221012300658, "compression_ratio": 1.7971014492753623, "no_speech_prob": 0.00018138086306862533}, {"id": 162, "seek": 75496, "start": 768.96, "end": 770.96, "text": " and shouldn't be stored locally, right?", "tokens": [293, 4659, 380, 312, 12187, 16143, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.137221012300658, "compression_ratio": 1.7971014492753623, "no_speech_prob": 0.00018138086306862533}, {"id": 163, "seek": 75496, "start": 770.96, "end": 772.96, "text": " So you want to store it first of all.", "tokens": [407, 291, 528, 281, 3531, 309, 700, 295, 439, 13], "temperature": 0.0, "avg_logprob": -0.137221012300658, "compression_ratio": 1.7971014492753623, "no_speech_prob": 0.00018138086306862533}, {"id": 164, "seek": 75496, "start": 772.96, "end": 774.96, "text": " So the first thing is to store it on the cloud.", "tokens": [407, 264, 700, 551, 307, 281, 3531, 309, 322, 264, 4588, 13], "temperature": 0.0, "avg_logprob": -0.137221012300658, "compression_ratio": 1.7971014492753623, "no_speech_prob": 0.00018138086306862533}, {"id": 165, "seek": 75496, "start": 774.96, "end": 780.96, "text": " So for this demo, what I'm doing is I'm storing everything onto the cloud.", "tokens": [407, 337, 341, 10723, 11, 437, 286, 478, 884, 307, 286, 478, 26085, 1203, 3911, 264, 4588, 13], "temperature": 0.0, "avg_logprob": -0.137221012300658, "compression_ratio": 1.7971014492753623, "no_speech_prob": 0.00018138086306862533}, {"id": 166, "seek": 78096, "start": 780.96, "end": 784.96, "text": " There is a solution called Hazelgast-Virginian, which is kind of like service.", "tokens": [821, 307, 257, 3827, 1219, 15852, 338, 70, 525, 12, 53, 347, 1494, 952, 11, 597, 307, 733, 295, 411, 2643, 13], "temperature": 0.0, "avg_logprob": -0.22825253494386752, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.00019623918342404068}, {"id": 167, "seek": 78096, "start": 784.96, "end": 787.96, "text": " So you don't need to download GR, run your project.", "tokens": [407, 291, 500, 380, 643, 281, 5484, 10903, 11, 1190, 428, 1716, 13], "temperature": 0.0, "avg_logprob": -0.22825253494386752, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.00019623918342404068}, {"id": 168, "seek": 78096, "start": 787.96, "end": 789.96, "text": " You can simply plug in and play.", "tokens": [509, 393, 2935, 5452, 294, 293, 862, 13], "temperature": 0.0, "avg_logprob": -0.22825253494386752, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.00019623918342404068}, {"id": 169, "seek": 78096, "start": 789.96, "end": 791.96, "text": " So you can create an account.", "tokens": [407, 291, 393, 1884, 364, 2696, 13], "temperature": 0.0, "avg_logprob": -0.22825253494386752, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.00019623918342404068}, {"id": 170, "seek": 78096, "start": 791.96, "end": 793.96, "text": " You'll run everything I'm discussing today.", "tokens": [509, 603, 1190, 1203, 286, 478, 10850, 965, 13], "temperature": 0.0, "avg_logprob": -0.22825253494386752, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.00019623918342404068}, {"id": 171, "seek": 78096, "start": 793.96, "end": 795.96, "text": " So you create an instance of Hazelgast.", "tokens": [407, 291, 1884, 364, 5197, 295, 15852, 338, 70, 525, 13], "temperature": 0.0, "avg_logprob": -0.22825253494386752, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.00019623918342404068}, {"id": 172, "seek": 78096, "start": 795.96, "end": 800.96, "text": " And from there, you can pretty much proceed to what I'm planning to do.", "tokens": [400, 490, 456, 11, 291, 393, 1238, 709, 8991, 281, 437, 286, 478, 5038, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.22825253494386752, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.00019623918342404068}, {"id": 173, "seek": 78096, "start": 800.96, "end": 806.96, "text": " So the first option we were talking about is kind of like storing everything into the cloud.", "tokens": [407, 264, 700, 3614, 321, 645, 1417, 466, 307, 733, 295, 411, 26085, 1203, 666, 264, 4588, 13], "temperature": 0.0, "avg_logprob": -0.22825253494386752, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.00019623918342404068}, {"id": 174, "seek": 80696, "start": 806.96, "end": 811.96, "text": " So we're going to import the data. Obviously, we need some kind of trace message,", "tokens": [407, 321, 434, 516, 281, 974, 264, 1412, 13, 7580, 11, 321, 643, 512, 733, 295, 13508, 3636, 11], "temperature": 0.0, "avg_logprob": -0.17571988955948703, "compression_ratio": 1.8679245283018868, "no_speech_prob": 0.00019593944307416677}, {"id": 175, "seek": 80696, "start": 811.96, "end": 813.96, "text": " which makes sense.", "tokens": [597, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.17571988955948703, "compression_ratio": 1.8679245283018868, "no_speech_prob": 0.00019593944307416677}, {"id": 176, "seek": 80696, "start": 813.96, "end": 819.96, "text": " So this trace message could be, you know, changed based on how you want to approach it, right?", "tokens": [407, 341, 13508, 3636, 727, 312, 11, 291, 458, 11, 3105, 2361, 322, 577, 291, 528, 281, 3109, 309, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17571988955948703, "compression_ratio": 1.8679245283018868, "no_speech_prob": 0.00019593944307416677}, {"id": 177, "seek": 80696, "start": 819.96, "end": 821.96, "text": " So for example, if you're working with machine learning,", "tokens": [407, 337, 1365, 11, 498, 291, 434, 1364, 365, 3479, 2539, 11], "temperature": 0.0, "avg_logprob": -0.17571988955948703, "compression_ratio": 1.8679245283018868, "no_speech_prob": 0.00019593944307416677}, {"id": 178, "seek": 80696, "start": 821.96, "end": 827.96, "text": " you probably look for some kind of, I don't know, classification solution for your, you know,", "tokens": [291, 1391, 574, 337, 512, 733, 295, 11, 286, 500, 380, 458, 11, 21538, 3827, 337, 428, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.17571988955948703, "compression_ratio": 1.8679245283018868, "no_speech_prob": 0.00019593944307416677}, {"id": 179, "seek": 80696, "start": 827.96, "end": 830.96, "text": " for your tests, or you could be looking for NLE.", "tokens": [337, 428, 6921, 11, 420, 291, 727, 312, 1237, 337, 426, 2634, 13], "temperature": 0.0, "avg_logprob": -0.17571988955948703, "compression_ratio": 1.8679245283018868, "no_speech_prob": 0.00019593944307416677}, {"id": 180, "seek": 80696, "start": 830.96, "end": 835.96, "text": " If you don't want to work with machine learning, you probably want to look for some kind of trends.", "tokens": [759, 291, 500, 380, 528, 281, 589, 365, 3479, 2539, 11, 291, 1391, 528, 281, 574, 337, 512, 733, 295, 13892, 13], "temperature": 0.0, "avg_logprob": -0.17571988955948703, "compression_ratio": 1.8679245283018868, "no_speech_prob": 0.00019593944307416677}, {"id": 181, "seek": 83596, "start": 835.96, "end": 838.96, "text": " So you look for processing your data.", "tokens": [407, 291, 574, 337, 9007, 428, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1291768139806287, "compression_ratio": 1.66793893129771, "no_speech_prob": 7.005965017015114e-05}, {"id": 182, "seek": 83596, "start": 838.96, "end": 840.96, "text": " It doesn't matter if you're using machine learning.", "tokens": [467, 1177, 380, 1871, 498, 291, 434, 1228, 3479, 2539, 13], "temperature": 0.0, "avg_logprob": -0.1291768139806287, "compression_ratio": 1.66793893129771, "no_speech_prob": 7.005965017015114e-05}, {"id": 183, "seek": 83596, "start": 840.96, "end": 844.96, "text": " In this case, you want to have some kind of data stored somewhere.", "tokens": [682, 341, 1389, 11, 291, 528, 281, 362, 512, 733, 295, 1412, 12187, 4079, 13], "temperature": 0.0, "avg_logprob": -0.1291768139806287, "compression_ratio": 1.66793893129771, "no_speech_prob": 7.005965017015114e-05}, {"id": 184, "seek": 83596, "start": 844.96, "end": 849.96, "text": " So it could be in JSON format, or it could be like bar charts, strings.", "tokens": [407, 309, 727, 312, 294, 31828, 7877, 11, 420, 309, 727, 312, 411, 2159, 17767, 11, 13985, 13], "temperature": 0.0, "avg_logprob": -0.1291768139806287, "compression_ratio": 1.66793893129771, "no_speech_prob": 7.005965017015114e-05}, {"id": 185, "seek": 83596, "start": 849.96, "end": 853.96, "text": " So it depends again how much the speed is important to you.", "tokens": [407, 309, 5946, 797, 577, 709, 264, 3073, 307, 1021, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.1291768139806287, "compression_ratio": 1.66793893129771, "no_speech_prob": 7.005965017015114e-05}, {"id": 186, "seek": 83596, "start": 853.96, "end": 858.96, "text": " So the option, first option is to go through the alerts.", "tokens": [407, 264, 3614, 11, 700, 3614, 307, 281, 352, 807, 264, 28061, 13], "temperature": 0.0, "avg_logprob": -0.1291768139806287, "compression_ratio": 1.66793893129771, "no_speech_prob": 7.005965017015114e-05}, {"id": 187, "seek": 83596, "start": 858.96, "end": 864.96, "text": " So in alerts, what we're trying to do here is to take everything and store it in the cloud.", "tokens": [407, 294, 28061, 11, 437, 321, 434, 1382, 281, 360, 510, 307, 281, 747, 1203, 293, 3531, 309, 294, 264, 4588, 13], "temperature": 0.0, "avg_logprob": -0.1291768139806287, "compression_ratio": 1.66793893129771, "no_speech_prob": 7.005965017015114e-05}, {"id": 188, "seek": 86496, "start": 864.96, "end": 868.96, "text": " So obviously we don't store it in the cloud on our disks.", "tokens": [407, 2745, 321, 500, 380, 3531, 309, 294, 264, 4588, 322, 527, 41617, 13], "temperature": 0.0, "avg_logprob": -0.11614582935969035, "compression_ratio": 1.6324786324786325, "no_speech_prob": 6.903299799887463e-05}, {"id": 189, "seek": 86496, "start": 868.96, "end": 871.96, "text": " What we try to do is to store it in memory.", "tokens": [708, 321, 853, 281, 360, 307, 281, 3531, 309, 294, 4675, 13], "temperature": 0.0, "avg_logprob": -0.11614582935969035, "compression_ratio": 1.6324786324786325, "no_speech_prob": 6.903299799887463e-05}, {"id": 190, "seek": 86496, "start": 871.96, "end": 876.96, "text": " My preference in this case is to use some kind of map structure.", "tokens": [1222, 17502, 294, 341, 1389, 307, 281, 764, 512, 733, 295, 4471, 3877, 13], "temperature": 0.0, "avg_logprob": -0.11614582935969035, "compression_ratio": 1.6324786324786325, "no_speech_prob": 6.903299799887463e-05}, {"id": 191, "seek": 86496, "start": 876.96, "end": 884.96, "text": " So map structure allows you to essentially random access and rebalance between various nodes within your cluster.", "tokens": [407, 4471, 3877, 4045, 291, 281, 4476, 4974, 2105, 293, 319, 29215, 1296, 3683, 13891, 1951, 428, 13630, 13], "temperature": 0.0, "avg_logprob": -0.11614582935969035, "compression_ratio": 1.6324786324786325, "no_speech_prob": 6.903299799887463e-05}, {"id": 192, "seek": 86496, "start": 884.96, "end": 891.96, "text": " And at the same time, you want to have some key value, so in order to know where this is coming from.", "tokens": [400, 412, 264, 912, 565, 11, 291, 528, 281, 362, 512, 2141, 2158, 11, 370, 294, 1668, 281, 458, 689, 341, 307, 1348, 490, 13], "temperature": 0.0, "avg_logprob": -0.11614582935969035, "compression_ratio": 1.6324786324786325, "no_speech_prob": 6.903299799887463e-05}, {"id": 193, "seek": 89196, "start": 891.96, "end": 898.96, "text": " So in this case, it could be like ID address, for example, so this is where, and support number, so as key.", "tokens": [407, 294, 341, 1389, 11, 309, 727, 312, 411, 7348, 2985, 11, 337, 1365, 11, 370, 341, 307, 689, 11, 293, 1406, 1230, 11, 370, 382, 2141, 13], "temperature": 0.0, "avg_logprob": -0.20017003146084872, "compression_ratio": 1.7522522522522523, "no_speech_prob": 0.00013731172657571733}, {"id": 194, "seek": 89196, "start": 898.96, "end": 902.96, "text": " And the value could be anything that makes sense to you.", "tokens": [400, 264, 2158, 727, 312, 1340, 300, 1669, 2020, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.20017003146084872, "compression_ratio": 1.7522522522522523, "no_speech_prob": 0.00013731172657571733}, {"id": 195, "seek": 89196, "start": 902.96, "end": 907.96, "text": " So in this case, for example, you can track level of this error, sorry, of this loop,", "tokens": [407, 294, 341, 1389, 11, 337, 1365, 11, 291, 393, 2837, 1496, 295, 341, 6713, 11, 2597, 11, 295, 341, 6367, 11], "temperature": 0.0, "avg_logprob": -0.20017003146084872, "compression_ratio": 1.7522522522522523, "no_speech_prob": 0.00013731172657571733}, {"id": 196, "seek": 89196, "start": 907.96, "end": 912.96, "text": " and message, for example, if you want to do some kind of NLP processing on it,", "tokens": [293, 3636, 11, 337, 1365, 11, 498, 291, 528, 281, 360, 512, 733, 295, 426, 43, 47, 9007, 322, 309, 11], "temperature": 0.0, "avg_logprob": -0.20017003146084872, "compression_ratio": 1.7522522522522523, "no_speech_prob": 0.00013731172657571733}, {"id": 197, "seek": 89196, "start": 912.96, "end": 916.96, "text": " and some kind of, you know, process or thread name on this.", "tokens": [293, 512, 733, 295, 11, 291, 458, 11, 1399, 420, 7207, 1315, 322, 341, 13], "temperature": 0.0, "avg_logprob": -0.20017003146084872, "compression_ratio": 1.7522522522522523, "no_speech_prob": 0.00013731172657571733}, {"id": 198, "seek": 91696, "start": 916.96, "end": 924.96, "text": " Obviously once you have your key and value, what you can do is proceed and store it into memory.", "tokens": [7580, 1564, 291, 362, 428, 2141, 293, 2158, 11, 437, 291, 393, 360, 307, 8991, 293, 3531, 309, 666, 4675, 13], "temperature": 0.0, "avg_logprob": -0.19781187538788697, "compression_ratio": 1.7268722466960353, "no_speech_prob": 0.00014823643141426146}, {"id": 199, "seek": 91696, "start": 924.96, "end": 927.96, "text": " So this is where you get this set to hazard cast.", "tokens": [407, 341, 307, 689, 291, 483, 341, 992, 281, 20790, 4193, 13], "temperature": 0.0, "avg_logprob": -0.19781187538788697, "compression_ratio": 1.7268722466960353, "no_speech_prob": 0.00014823643141426146}, {"id": 200, "seek": 91696, "start": 927.96, "end": 930.96, "text": " And what we're trying to do is create the IMAP.", "tokens": [400, 437, 321, 434, 1382, 281, 360, 307, 1884, 264, 21463, 4715, 13], "temperature": 0.0, "avg_logprob": -0.19781187538788697, "compression_ratio": 1.7268722466960353, "no_speech_prob": 0.00014823643141426146}, {"id": 201, "seek": 91696, "start": 930.96, "end": 939.96, "text": " And once you have the IMAP, it means you should be able to store it, you know, access it and do same processing as I will show you.", "tokens": [400, 1564, 291, 362, 264, 21463, 4715, 11, 309, 1355, 291, 820, 312, 1075, 281, 3531, 309, 11, 291, 458, 11, 2105, 309, 293, 360, 912, 9007, 382, 286, 486, 855, 291, 13], "temperature": 0.0, "avg_logprob": -0.19781187538788697, "compression_ratio": 1.7268722466960353, "no_speech_prob": 0.00014823643141426146}, {"id": 202, "seek": 91696, "start": 939.96, "end": 944.96, "text": " So first message is to store it in the cloud, store it in memory.", "tokens": [407, 700, 3636, 307, 281, 3531, 309, 294, 264, 4588, 11, 3531, 309, 294, 4675, 13], "temperature": 0.0, "avg_logprob": -0.19781187538788697, "compression_ratio": 1.7268722466960353, "no_speech_prob": 0.00014823643141426146}, {"id": 203, "seek": 94496, "start": 944.96, "end": 948.96, "text": " In this case, I'm using hazard cast gradient, and I'm using IMAP.", "tokens": [682, 341, 1389, 11, 286, 478, 1228, 20790, 4193, 16235, 11, 293, 286, 478, 1228, 21463, 4715, 13], "temperature": 0.0, "avg_logprob": -0.14512240402097623, "compression_ratio": 1.7, "no_speech_prob": 0.00024723290698602796}, {"id": 204, "seek": 94496, "start": 948.96, "end": 952.96, "text": " And second stage is to do the same processing, right?", "tokens": [400, 1150, 3233, 307, 281, 360, 264, 912, 9007, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.14512240402097623, "compression_ratio": 1.7, "no_speech_prob": 0.00024723290698602796}, {"id": 205, "seek": 94496, "start": 952.96, "end": 954.96, "text": " So there are a couple of options here for you.", "tokens": [407, 456, 366, 257, 1916, 295, 3956, 510, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.14512240402097623, "compression_ratio": 1.7, "no_speech_prob": 0.00024723290698602796}, {"id": 206, "seek": 94496, "start": 954.96, "end": 956.96, "text": " So first option is to use SQL.", "tokens": [407, 700, 3614, 307, 281, 764, 19200, 13], "temperature": 0.0, "avg_logprob": -0.14512240402097623, "compression_ratio": 1.7, "no_speech_prob": 0.00024723290698602796}, {"id": 207, "seek": 94496, "start": 956.96, "end": 961.96, "text": " So SQL is built within hazard cast, which means, or on top of hazard cast,", "tokens": [407, 19200, 307, 3094, 1951, 20790, 4193, 11, 597, 1355, 11, 420, 322, 1192, 295, 20790, 4193, 11], "temperature": 0.0, "avg_logprob": -0.14512240402097623, "compression_ratio": 1.7, "no_speech_prob": 0.00024723290698602796}, {"id": 208, "seek": 94496, "start": 961.96, "end": 965.96, "text": " which means you should be able to query your data, so if you provide some kind of specific messages", "tokens": [597, 1355, 291, 820, 312, 1075, 281, 14581, 428, 1412, 11, 370, 498, 291, 2893, 512, 733, 295, 2685, 7897], "temperature": 0.0, "avg_logprob": -0.14512240402097623, "compression_ratio": 1.7, "no_speech_prob": 0.00024723290698602796}, {"id": 209, "seek": 94496, "start": 965.96, "end": 971.96, "text": " that you're looking for, obviously depends on your input, you can do some kind of SQL.", "tokens": [300, 291, 434, 1237, 337, 11, 2745, 5946, 322, 428, 4846, 11, 291, 393, 360, 512, 733, 295, 19200, 13], "temperature": 0.0, "avg_logprob": -0.14512240402097623, "compression_ratio": 1.7, "no_speech_prob": 0.00024723290698602796}, {"id": 210, "seek": 97196, "start": 971.96, "end": 975.96, "text": " So whether it's an inner joy, for example, or sales and so on.", "tokens": [407, 1968, 309, 311, 364, 7284, 6258, 11, 337, 1365, 11, 420, 5763, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.1323933374314081, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.00011920947872567922}, {"id": 211, "seek": 97196, "start": 975.96, "end": 978.96, "text": " Or the other option is to do some kind of prediction.", "tokens": [1610, 264, 661, 3614, 307, 281, 360, 512, 733, 295, 17630, 13], "temperature": 0.0, "avg_logprob": -0.1323933374314081, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.00011920947872567922}, {"id": 212, "seek": 97196, "start": 978.96, "end": 983.96, "text": " So you're getting some logs, you don't know exactly what's going on to happen next,", "tokens": [407, 291, 434, 1242, 512, 20820, 11, 291, 500, 380, 458, 2293, 437, 311, 516, 322, 281, 1051, 958, 11], "temperature": 0.0, "avg_logprob": -0.1323933374314081, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.00011920947872567922}, {"id": 213, "seek": 97196, "start": 983.96, "end": 988.96, "text": " and you try to predict to provide some kind of, you know, alerts or trends.", "tokens": [293, 291, 853, 281, 6069, 281, 2893, 512, 733, 295, 11, 291, 458, 11, 28061, 420, 13892, 13], "temperature": 0.0, "avg_logprob": -0.1323933374314081, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.00011920947872567922}, {"id": 214, "seek": 97196, "start": 988.96, "end": 990.96, "text": " So we need to build the trends.", "tokens": [407, 321, 643, 281, 1322, 264, 13892, 13], "temperature": 0.0, "avg_logprob": -0.1323933374314081, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.00011920947872567922}, {"id": 215, "seek": 97196, "start": 990.96, "end": 994.96, "text": " So in order to do this, what I did is kind of like use the same key,", "tokens": [407, 294, 1668, 281, 360, 341, 11, 437, 286, 630, 307, 733, 295, 411, 764, 264, 912, 2141, 11], "temperature": 0.0, "avg_logprob": -0.1323933374314081, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.00011920947872567922}, {"id": 216, "seek": 97196, "start": 994.96, "end": 1000.96, "text": " but for my value, what I'm using is some kind of log score.", "tokens": [457, 337, 452, 2158, 11, 437, 286, 478, 1228, 307, 512, 733, 295, 3565, 6175, 13], "temperature": 0.0, "avg_logprob": -0.1323933374314081, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.00011920947872567922}, {"id": 217, "seek": 100096, "start": 1000.96, "end": 1003.96, "text": " So log score is not important.", "tokens": [407, 3565, 6175, 307, 406, 1021, 13], "temperature": 0.0, "avg_logprob": -0.09539212544759114, "compression_ratio": 1.7577639751552796, "no_speech_prob": 0.00024295857292599976}, {"id": 218, "seek": 100096, "start": 1003.96, "end": 1009.96, "text": " What I'm saying here is I want to give value for every single message,", "tokens": [708, 286, 478, 1566, 510, 307, 286, 528, 281, 976, 2158, 337, 633, 2167, 3636, 11], "temperature": 0.0, "avg_logprob": -0.09539212544759114, "compression_ratio": 1.7577639751552796, "no_speech_prob": 0.00024295857292599976}, {"id": 219, "seek": 100096, "start": 1009.96, "end": 1012.96, "text": " or every single log message.", "tokens": [420, 633, 2167, 3565, 3636, 13], "temperature": 0.0, "avg_logprob": -0.09539212544759114, "compression_ratio": 1.7577639751552796, "no_speech_prob": 0.00024295857292599976}, {"id": 220, "seek": 100096, "start": 1012.96, "end": 1019.96, "text": " So this could be, for example, how important this specific message is for you,", "tokens": [407, 341, 727, 312, 11, 337, 1365, 11, 577, 1021, 341, 2685, 3636, 307, 337, 291, 11], "temperature": 0.0, "avg_logprob": -0.09539212544759114, "compression_ratio": 1.7577639751552796, "no_speech_prob": 0.00024295857292599976}, {"id": 221, "seek": 100096, "start": 1019.96, "end": 1025.96, "text": " or it could be, for example, how serious or how dangerous the message is.", "tokens": [420, 309, 727, 312, 11, 337, 1365, 11, 577, 3156, 420, 577, 5795, 264, 3636, 307, 13], "temperature": 0.0, "avg_logprob": -0.09539212544759114, "compression_ratio": 1.7577639751552796, "no_speech_prob": 0.00024295857292599976}, {"id": 222, "seek": 102596, "start": 1025.96, "end": 1031.96, "text": " So as levels in logs, you can define scores, so instead of having four levels, for example,", "tokens": [407, 382, 4358, 294, 20820, 11, 291, 393, 6964, 13444, 11, 370, 2602, 295, 1419, 1451, 4358, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.113469664255778, "compression_ratio": 1.78, "no_speech_prob": 5.049832179793157e-05}, {"id": 223, "seek": 102596, "start": 1031.96, "end": 1035.96, "text": " you can spread it, I don't know, from one to 100.", "tokens": [291, 393, 3974, 309, 11, 286, 500, 380, 458, 11, 490, 472, 281, 2319, 13], "temperature": 0.0, "avg_logprob": -0.113469664255778, "compression_ratio": 1.78, "no_speech_prob": 5.049832179793157e-05}, {"id": 224, "seek": 102596, "start": 1035.96, "end": 1037.96, "text": " So this should give you some kind of predictions.", "tokens": [407, 341, 820, 976, 291, 512, 733, 295, 21264, 13], "temperature": 0.0, "avg_logprob": -0.113469664255778, "compression_ratio": 1.78, "no_speech_prob": 5.049832179793157e-05}, {"id": 225, "seek": 102596, "start": 1037.96, "end": 1041.96, "text": " Why? Because if you have, for example, 10 messages, 10 local messages,", "tokens": [1545, 30, 1436, 498, 291, 362, 11, 337, 1365, 11, 1266, 7897, 11, 1266, 2654, 7897, 11], "temperature": 0.0, "avg_logprob": -0.113469664255778, "compression_ratio": 1.78, "no_speech_prob": 5.049832179793157e-05}, {"id": 226, "seek": 102596, "start": 1041.96, "end": 1049.96, "text": " or, for example, warning, you don't know exactly if the event matrix will be warning or not.", "tokens": [420, 11, 337, 1365, 11, 9164, 11, 291, 500, 380, 458, 2293, 498, 264, 2280, 8141, 486, 312, 9164, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.113469664255778, "compression_ratio": 1.78, "no_speech_prob": 5.049832179793157e-05}, {"id": 227, "seek": 102596, "start": 1049.96, "end": 1054.96, "text": " If you want to predict it, obviously it doesn't give you how much will be warning or not.", "tokens": [759, 291, 528, 281, 6069, 309, 11, 2745, 309, 1177, 380, 976, 291, 577, 709, 486, 312, 9164, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.113469664255778, "compression_ratio": 1.78, "no_speech_prob": 5.049832179793157e-05}, {"id": 228, "seek": 105496, "start": 1054.96, "end": 1061.96, "text": " Whereas if you use some kind of numerical value, you can get as close as possible to this.", "tokens": [13813, 498, 291, 764, 512, 733, 295, 29054, 2158, 11, 291, 393, 483, 382, 1998, 382, 1944, 281, 341, 13], "temperature": 0.0, "avg_logprob": -0.0983600026553439, "compression_ratio": 1.6492890995260663, "no_speech_prob": 9.675639739725739e-05}, {"id": 229, "seek": 105496, "start": 1061.96, "end": 1067.96, "text": " So we get key from there, we get score from here,", "tokens": [407, 321, 483, 2141, 490, 456, 11, 321, 483, 6175, 490, 510, 11], "temperature": 0.0, "avg_logprob": -0.0983600026553439, "compression_ratio": 1.6492890995260663, "no_speech_prob": 9.675639739725739e-05}, {"id": 230, "seek": 105496, "start": 1067.96, "end": 1072.96, "text": " and what we do next is to do some kind of predictions on the logs.", "tokens": [293, 437, 321, 360, 958, 307, 281, 360, 512, 733, 295, 21264, 322, 264, 20820, 13], "temperature": 0.0, "avg_logprob": -0.0983600026553439, "compression_ratio": 1.6492890995260663, "no_speech_prob": 9.675639739725739e-05}, {"id": 231, "seek": 105496, "start": 1072.96, "end": 1076.96, "text": " So in this process, what we have is exactly my key and the value,", "tokens": [407, 294, 341, 1399, 11, 437, 321, 362, 307, 2293, 452, 2141, 293, 264, 2158, 11], "temperature": 0.0, "avg_logprob": -0.0983600026553439, "compression_ratio": 1.6492890995260663, "no_speech_prob": 9.675639739725739e-05}, {"id": 232, "seek": 105496, "start": 1076.96, "end": 1082.96, "text": " which is like the score on each log message, and I import it into Hezakas.", "tokens": [597, 307, 411, 264, 6175, 322, 1184, 3565, 3636, 11, 293, 286, 974, 309, 666, 634, 89, 514, 296, 13], "temperature": 0.0, "avg_logprob": -0.0983600026553439, "compression_ratio": 1.6492890995260663, "no_speech_prob": 9.675639739725739e-05}, {"id": 233, "seek": 108296, "start": 1082.96, "end": 1088.96, "text": " So Hezakas allows you to basically input and output from two different maps,", "tokens": [407, 634, 89, 514, 296, 4045, 291, 281, 1936, 4846, 293, 5598, 490, 732, 819, 11317, 11], "temperature": 0.0, "avg_logprob": -0.1541904302743765, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.00024901237338781357}, {"id": 234, "seek": 108296, "start": 1088.96, "end": 1093.96, "text": " and do stream processing, so we'll build the train based on previous logs,", "tokens": [293, 360, 4309, 9007, 11, 370, 321, 603, 1322, 264, 3847, 2361, 322, 3894, 20820, 11], "temperature": 0.0, "avg_logprob": -0.1541904302743765, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.00024901237338781357}, {"id": 235, "seek": 108296, "start": 1093.96, "end": 1099.96, "text": " based on previous log scores, and we'll use the prediction on top of it", "tokens": [2361, 322, 3894, 3565, 13444, 11, 293, 321, 603, 764, 264, 17630, 322, 1192, 295, 309], "temperature": 0.0, "avg_logprob": -0.1541904302743765, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.00024901237338781357}, {"id": 236, "seek": 108296, "start": 1099.96, "end": 1101.96, "text": " to provide some kind of alert.", "tokens": [281, 2893, 512, 733, 295, 9615, 13], "temperature": 0.0, "avg_logprob": -0.1541904302743765, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.00024901237338781357}, {"id": 237, "seek": 108296, "start": 1101.96, "end": 1104.96, "text": " So zero means don't alert, one means alert.", "tokens": [407, 4018, 1355, 500, 380, 9615, 11, 472, 1355, 9615, 13], "temperature": 0.0, "avg_logprob": -0.1541904302743765, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.00024901237338781357}, {"id": 238, "seek": 108296, "start": 1104.96, "end": 1111.96, "text": " And as you can see here, the actual workflow, kind of like you build, you read it from math,", "tokens": [400, 382, 291, 393, 536, 510, 11, 264, 3539, 20993, 11, 733, 295, 411, 291, 1322, 11, 291, 1401, 309, 490, 5221, 11], "temperature": 0.0, "avg_logprob": -0.1541904302743765, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.00024901237338781357}, {"id": 239, "seek": 111196, "start": 1111.96, "end": 1115.96, "text": " then you define trend map, so which is like normal map,", "tokens": [550, 291, 6964, 6028, 4471, 11, 370, 597, 307, 411, 2710, 4471, 11], "temperature": 0.0, "avg_logprob": -0.1374724831911597, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.00017865776317194104}, {"id": 240, "seek": 111196, "start": 1115.96, "end": 1120.96, "text": " and from there you can use it to predict what's going to happen next.", "tokens": [293, 490, 456, 291, 393, 764, 309, 281, 6069, 437, 311, 516, 281, 1051, 958, 13], "temperature": 0.0, "avg_logprob": -0.1374724831911597, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.00017865776317194104}, {"id": 241, "seek": 111196, "start": 1120.96, "end": 1124.96, "text": " Obviously you do some kind of visualization, so how does it look like?", "tokens": [7580, 291, 360, 512, 733, 295, 25801, 11, 370, 577, 775, 309, 574, 411, 30], "temperature": 0.0, "avg_logprob": -0.1374724831911597, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.00017865776317194104}, {"id": 242, "seek": 111196, "start": 1124.96, "end": 1127.96, "text": " So this is kind of the prediction part of it.", "tokens": [407, 341, 307, 733, 295, 264, 17630, 644, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.1374724831911597, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.00017865776317194104}, {"id": 243, "seek": 111196, "start": 1127.96, "end": 1133.96, "text": " So we take the logs map and we build trend map out of it.", "tokens": [407, 321, 747, 264, 20820, 4471, 293, 321, 1322, 6028, 4471, 484, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.1374724831911597, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.00017865776317194104}, {"id": 244, "seek": 111196, "start": 1133.96, "end": 1139.96, "text": " So the trend map would start reading messages and the scores and build train for you.", "tokens": [407, 264, 6028, 4471, 576, 722, 3760, 7897, 293, 264, 13444, 293, 1322, 3847, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.1374724831911597, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.00017865776317194104}, {"id": 245, "seek": 113996, "start": 1139.96, "end": 1143.96, "text": " And from this trend I can use some kind of machine learning,", "tokens": [400, 490, 341, 6028, 286, 393, 764, 512, 733, 295, 3479, 2539, 11], "temperature": 0.0, "avg_logprob": -0.1031968096892039, "compression_ratio": 1.7085201793721974, "no_speech_prob": 8.197230636142194e-05}, {"id": 246, "seek": 113996, "start": 1143.96, "end": 1145.96, "text": " it doesn't have to be machine learning.", "tokens": [309, 1177, 380, 362, 281, 312, 3479, 2539, 13], "temperature": 0.0, "avg_logprob": -0.1031968096892039, "compression_ratio": 1.7085201793721974, "no_speech_prob": 8.197230636142194e-05}, {"id": 247, "seek": 113996, "start": 1145.96, "end": 1149.96, "text": " In this case it's linear regression, but it could be anything to be honest.", "tokens": [682, 341, 1389, 309, 311, 8213, 24590, 11, 457, 309, 727, 312, 1340, 281, 312, 3245, 13], "temperature": 0.0, "avg_logprob": -0.1031968096892039, "compression_ratio": 1.7085201793721974, "no_speech_prob": 8.197230636142194e-05}, {"id": 248, "seek": 113996, "start": 1149.96, "end": 1156.96, "text": " And we check the values and we try to use some kind of prediction based on the previous values", "tokens": [400, 321, 1520, 264, 4190, 293, 321, 853, 281, 764, 512, 733, 295, 17630, 2361, 322, 264, 3894, 4190], "temperature": 0.0, "avg_logprob": -0.1031968096892039, "compression_ratio": 1.7085201793721974, "no_speech_prob": 8.197230636142194e-05}, {"id": 249, "seek": 113996, "start": 1156.96, "end": 1159.96, "text": " to decide if you want to send an alert or not.", "tokens": [281, 4536, 498, 291, 528, 281, 2845, 364, 9615, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.1031968096892039, "compression_ratio": 1.7085201793721974, "no_speech_prob": 8.197230636142194e-05}, {"id": 250, "seek": 113996, "start": 1159.96, "end": 1163.96, "text": " And obviously this is kind of like describing the exact thing,", "tokens": [400, 2745, 341, 307, 733, 295, 411, 16141, 264, 1900, 551, 11], "temperature": 0.0, "avg_logprob": -0.1031968096892039, "compression_ratio": 1.7085201793721974, "no_speech_prob": 8.197230636142194e-05}, {"id": 251, "seek": 116396, "start": 1163.96, "end": 1170.96, "text": " so when there is a one on your values, it's alert, send alert when it's zero, don't.", "tokens": [370, 562, 456, 307, 257, 472, 322, 428, 4190, 11, 309, 311, 9615, 11, 2845, 9615, 562, 309, 311, 4018, 11, 500, 380, 13], "temperature": 0.0, "avg_logprob": -0.1630836668468657, "compression_ratio": 1.6753246753246753, "no_speech_prob": 8.728371176403016e-05}, {"id": 252, "seek": 116396, "start": 1170.96, "end": 1174.96, "text": " And here there are three ways to do it.", "tokens": [400, 510, 456, 366, 1045, 2098, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.1630836668468657, "compression_ratio": 1.6753246753246753, "no_speech_prob": 8.728371176403016e-05}, {"id": 253, "seek": 116396, "start": 1174.96, "end": 1177.96, "text": " So this is where the same processing comes into place.", "tokens": [407, 341, 307, 689, 264, 912, 9007, 1487, 666, 1081, 13], "temperature": 0.0, "avg_logprob": -0.1630836668468657, "compression_ratio": 1.6753246753246753, "no_speech_prob": 8.728371176403016e-05}, {"id": 254, "seek": 116396, "start": 1177.96, "end": 1183.96, "text": " So you could simply use SQL to read from the map and do some query if you want.", "tokens": [407, 291, 727, 2935, 764, 19200, 281, 1401, 490, 264, 4471, 293, 360, 512, 14581, 498, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.1630836668468657, "compression_ratio": 1.6753246753246753, "no_speech_prob": 8.728371176403016e-05}, {"id": 255, "seek": 116396, "start": 1183.96, "end": 1187.96, "text": " Obviously this is batch, which means it's not real time same processing,", "tokens": [7580, 341, 307, 15245, 11, 597, 1355, 309, 311, 406, 957, 565, 912, 9007, 11], "temperature": 0.0, "avg_logprob": -0.1630836668468657, "compression_ratio": 1.6753246753246753, "no_speech_prob": 8.728371176403016e-05}, {"id": 256, "seek": 116396, "start": 1187.96, "end": 1192.96, "text": " or you can even create a pipeline or create a process.", "tokens": [420, 291, 393, 754, 1884, 257, 15517, 420, 1884, 257, 1399, 13], "temperature": 0.0, "avg_logprob": -0.1630836668468657, "compression_ratio": 1.6753246753246753, "no_speech_prob": 8.728371176403016e-05}, {"id": 257, "seek": 119296, "start": 1192.96, "end": 1196.96, "text": " And from this process you can read the logs and do some same thing.", "tokens": [400, 490, 341, 1399, 291, 393, 1401, 264, 20820, 293, 360, 512, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.12097912914348098, "compression_ratio": 1.7478260869565216, "no_speech_prob": 5.636643982143141e-05}, {"id": 258, "seek": 119296, "start": 1196.96, "end": 1200.96, "text": " So you can use either SQL or Java to do it.", "tokens": [407, 291, 393, 764, 2139, 19200, 420, 10745, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.12097912914348098, "compression_ratio": 1.7478260869565216, "no_speech_prob": 5.636643982143141e-05}, {"id": 259, "seek": 119296, "start": 1200.96, "end": 1206.96, "text": " But first two options are batch, which means you can process the data in real time,", "tokens": [583, 700, 732, 3956, 366, 15245, 11, 597, 1355, 291, 393, 1399, 264, 1412, 294, 957, 565, 11], "temperature": 0.0, "avg_logprob": -0.12097912914348098, "compression_ratio": 1.7478260869565216, "no_speech_prob": 5.636643982143141e-05}, {"id": 260, "seek": 119296, "start": 1206.96, "end": 1208.96, "text": " you want to do changes in real time.", "tokens": [291, 528, 281, 360, 2962, 294, 957, 565, 13], "temperature": 0.0, "avg_logprob": -0.12097912914348098, "compression_ratio": 1.7478260869565216, "no_speech_prob": 5.636643982143141e-05}, {"id": 261, "seek": 119296, "start": 1208.96, "end": 1211.96, "text": " So the third option is the journal map.", "tokens": [407, 264, 2636, 3614, 307, 264, 6708, 4471, 13], "temperature": 0.0, "avg_logprob": -0.12097912914348098, "compression_ratio": 1.7478260869565216, "no_speech_prob": 5.636643982143141e-05}, {"id": 262, "seek": 119296, "start": 1211.96, "end": 1214.96, "text": " So journal map will track all changes, so it is continuous.", "tokens": [407, 6708, 4471, 486, 2837, 439, 2962, 11, 370, 309, 307, 10957, 13], "temperature": 0.0, "avg_logprob": -0.12097912914348098, "compression_ratio": 1.7478260869565216, "no_speech_prob": 5.636643982143141e-05}, {"id": 263, "seek": 119296, "start": 1214.96, "end": 1219.96, "text": " So you have the logs stored and you have logs coming into Kafka topic", "tokens": [407, 291, 362, 264, 20820, 12187, 293, 291, 362, 20820, 1348, 666, 47064, 4829], "temperature": 0.0, "avg_logprob": -0.12097912914348098, "compression_ratio": 1.7478260869565216, "no_speech_prob": 5.636643982143141e-05}, {"id": 264, "seek": 121996, "start": 1219.96, "end": 1223.96, "text": " and you can basically store both into journal map.", "tokens": [293, 291, 393, 1936, 3531, 1293, 666, 6708, 4471, 13], "temperature": 0.0, "avg_logprob": -0.1700057586034139, "compression_ratio": 1.512396694214876, "no_speech_prob": 5.377857814892195e-05}, {"id": 265, "seek": 121996, "start": 1223.96, "end": 1230.96, "text": " So we're on 5.2 version within Hazegas, 5.3 will have the SQL features on top of it,", "tokens": [407, 321, 434, 322, 1025, 13, 17, 3037, 1951, 15852, 1146, 296, 11, 1025, 13, 18, 486, 362, 264, 19200, 4122, 322, 1192, 295, 309, 11], "temperature": 0.0, "avg_logprob": -0.1700057586034139, "compression_ratio": 1.512396694214876, "no_speech_prob": 5.377857814892195e-05}, {"id": 266, "seek": 121996, "start": 1230.96, "end": 1236.96, "text": " which allows you, for example, data scientists to just do the queries and change the data.", "tokens": [597, 4045, 291, 11, 337, 1365, 11, 1412, 7708, 281, 445, 360, 264, 24109, 293, 1319, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1700057586034139, "compression_ratio": 1.512396694214876, "no_speech_prob": 5.377857814892195e-05}, {"id": 267, "seek": 121996, "start": 1236.96, "end": 1240.96, "text": " And obviously it's ring covered, so this is very important to understand,", "tokens": [400, 2745, 309, 311, 4875, 5343, 11, 370, 341, 307, 588, 1021, 281, 1223, 11], "temperature": 0.0, "avg_logprob": -0.1700057586034139, "compression_ratio": 1.512396694214876, "no_speech_prob": 5.377857814892195e-05}, {"id": 268, "seek": 121996, "start": 1240.96, "end": 1246.96, "text": " so you can start processing your data from start or from the end.", "tokens": [370, 291, 393, 722, 9007, 428, 1412, 490, 722, 420, 490, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.1700057586034139, "compression_ratio": 1.512396694214876, "no_speech_prob": 5.377857814892195e-05}, {"id": 269, "seek": 124696, "start": 1246.96, "end": 1251.96, "text": " And what you want is kind of like, you know, using this kind of alerts to it.", "tokens": [400, 437, 291, 528, 307, 733, 295, 411, 11, 291, 458, 11, 1228, 341, 733, 295, 28061, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.09913030660377359, "compression_ratio": 1.747787610619469, "no_speech_prob": 0.00013053708244115114}, {"id": 270, "seek": 124696, "start": 1251.96, "end": 1253.96, "text": " So the first part is to read it.", "tokens": [407, 264, 700, 644, 307, 281, 1401, 309, 13], "temperature": 0.0, "avg_logprob": -0.09913030660377359, "compression_ratio": 1.747787610619469, "no_speech_prob": 0.00013053708244115114}, {"id": 271, "seek": 124696, "start": 1253.96, "end": 1257.96, "text": " So this is the actual map we built in the first option.", "tokens": [407, 341, 307, 264, 3539, 4471, 321, 3094, 294, 264, 700, 3614, 13], "temperature": 0.0, "avg_logprob": -0.09913030660377359, "compression_ratio": 1.747787610619469, "no_speech_prob": 0.00013053708244115114}, {"id": 272, "seek": 124696, "start": 1257.96, "end": 1261.96, "text": " And from here you can define the key, for example, and the value.", "tokens": [400, 490, 510, 291, 393, 6964, 264, 2141, 11, 337, 1365, 11, 293, 264, 2158, 13], "temperature": 0.0, "avg_logprob": -0.09913030660377359, "compression_ratio": 1.747787610619469, "no_speech_prob": 0.00013053708244115114}, {"id": 273, "seek": 124696, "start": 1261.96, "end": 1265.96, "text": " And you start, for example, to do some kind of filtering.", "tokens": [400, 291, 722, 11, 337, 1365, 11, 281, 360, 512, 733, 295, 30822, 13], "temperature": 0.0, "avg_logprob": -0.09913030660377359, "compression_ratio": 1.747787610619469, "no_speech_prob": 0.00013053708244115114}, {"id": 274, "seek": 124696, "start": 1265.96, "end": 1274.96, "text": " So this happens in real time on continuous and the map itself will allow you to basically track changes.", "tokens": [407, 341, 2314, 294, 957, 565, 322, 10957, 293, 264, 4471, 2564, 486, 2089, 291, 281, 1936, 2837, 2962, 13], "temperature": 0.0, "avg_logprob": -0.09913030660377359, "compression_ratio": 1.747787610619469, "no_speech_prob": 0.00013053708244115114}, {"id": 275, "seek": 127496, "start": 1274.96, "end": 1278.96, "text": " So to give you some takeaways and best practices,", "tokens": [407, 281, 976, 291, 512, 45584, 293, 1151, 7525, 11], "temperature": 0.0, "avg_logprob": -0.1405358024265455, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.00022911748965270817}, {"id": 276, "seek": 127496, "start": 1278.96, "end": 1282.96, "text": " so we just try to summarize everything we discussed today.", "tokens": [370, 321, 445, 853, 281, 20858, 1203, 321, 7152, 965, 13], "temperature": 0.0, "avg_logprob": -0.1405358024265455, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.00022911748965270817}, {"id": 277, "seek": 127496, "start": 1282.96, "end": 1287.96, "text": " Obviously there are more to discuss, but this should give you something to go out and try.", "tokens": [7580, 456, 366, 544, 281, 2248, 11, 457, 341, 820, 976, 291, 746, 281, 352, 484, 293, 853, 13], "temperature": 0.0, "avg_logprob": -0.1405358024265455, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.00022911748965270817}, {"id": 278, "seek": 127496, "start": 1287.96, "end": 1292.96, "text": " So first of all, you need to store your logs into some kind of data platform.", "tokens": [407, 700, 295, 439, 11, 291, 643, 281, 3531, 428, 20820, 666, 512, 733, 295, 1412, 3663, 13], "temperature": 0.0, "avg_logprob": -0.1405358024265455, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.00022911748965270817}, {"id": 279, "seek": 127496, "start": 1292.96, "end": 1296.96, "text": " So in this case, I'm using Hazegas, but obviously you don't have to use Hazegas.", "tokens": [407, 294, 341, 1389, 11, 286, 478, 1228, 15852, 1146, 296, 11, 457, 2745, 291, 500, 380, 362, 281, 764, 15852, 1146, 296, 13], "temperature": 0.0, "avg_logprob": -0.1405358024265455, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.00022911748965270817}, {"id": 280, "seek": 127496, "start": 1296.96, "end": 1302.96, "text": " The idea here is to do some kind of compute on your circle data to provide some context,", "tokens": [440, 1558, 510, 307, 281, 360, 512, 733, 295, 14722, 322, 428, 6329, 1412, 281, 2893, 512, 4319, 11], "temperature": 0.0, "avg_logprob": -0.1405358024265455, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.00022911748965270817}, {"id": 281, "seek": 130296, "start": 1302.96, "end": 1304.96, "text": " as well as real time data.", "tokens": [382, 731, 382, 957, 565, 1412, 13], "temperature": 0.0, "avg_logprob": -0.161770143816548, "compression_ratio": 1.8031496062992125, "no_speech_prob": 0.00016767748456913978}, {"id": 282, "seek": 130296, "start": 1304.96, "end": 1307.96, "text": " And from there, you need to store it on the cloud.", "tokens": [400, 490, 456, 11, 291, 643, 281, 3531, 309, 322, 264, 4588, 13], "temperature": 0.0, "avg_logprob": -0.161770143816548, "compression_ratio": 1.8031496062992125, "no_speech_prob": 0.00016767748456913978}, {"id": 283, "seek": 130296, "start": 1307.96, "end": 1311.96, "text": " So you need to store it somewhere where you can access logs from multiple places.", "tokens": [407, 291, 643, 281, 3531, 309, 4079, 689, 291, 393, 2105, 20820, 490, 3866, 3190, 13], "temperature": 0.0, "avg_logprob": -0.161770143816548, "compression_ratio": 1.8031496062992125, "no_speech_prob": 0.00016767748456913978}, {"id": 284, "seek": 130296, "start": 1311.96, "end": 1314.96, "text": " Obviously it has to be stored in memory.", "tokens": [7580, 309, 575, 281, 312, 12187, 294, 4675, 13], "temperature": 0.0, "avg_logprob": -0.161770143816548, "compression_ratio": 1.8031496062992125, "no_speech_prob": 0.00016767748456913978}, {"id": 285, "seek": 130296, "start": 1314.96, "end": 1317.96, "text": " And from there, you need to choose the format.", "tokens": [400, 490, 456, 11, 291, 643, 281, 2826, 264, 7877, 13], "temperature": 0.0, "avg_logprob": -0.161770143816548, "compression_ratio": 1.8031496062992125, "no_speech_prob": 0.00016767748456913978}, {"id": 286, "seek": 130296, "start": 1317.96, "end": 1321.96, "text": " If you, for example, looking to provide some, you know, I don't know,", "tokens": [759, 291, 11, 337, 1365, 11, 1237, 281, 2893, 512, 11, 291, 458, 11, 286, 500, 380, 458, 11], "temperature": 0.0, "avg_logprob": -0.161770143816548, "compression_ratio": 1.8031496062992125, "no_speech_prob": 0.00016767748456913978}, {"id": 287, "seek": 130296, "start": 1321.96, "end": 1324.96, "text": " some predictions you probably need to use JSON format.", "tokens": [512, 21264, 291, 1391, 643, 281, 764, 31828, 7877, 13], "temperature": 0.0, "avg_logprob": -0.161770143816548, "compression_ratio": 1.8031496062992125, "no_speech_prob": 0.00016767748456913978}, {"id": 288, "seek": 130296, "start": 1324.96, "end": 1328.96, "text": " Or for example, if you want just to do something that you can't sing and it's faster,", "tokens": [1610, 337, 1365, 11, 498, 291, 528, 445, 281, 360, 746, 300, 291, 393, 380, 1522, 293, 309, 311, 4663, 11], "temperature": 0.0, "avg_logprob": -0.161770143816548, "compression_ratio": 1.8031496062992125, "no_speech_prob": 0.00016767748456913978}, {"id": 289, "seek": 132896, "start": 1328.96, "end": 1332.96, "text": " if you want to speed the unit, it is some kind of map structure.", "tokens": [498, 291, 528, 281, 3073, 264, 4985, 11, 309, 307, 512, 733, 295, 4471, 3877, 13], "temperature": 0.0, "avg_logprob": -0.16607015273150275, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.00011015752534149215}, {"id": 290, "seek": 132896, "start": 1332.96, "end": 1339.96, "text": " Obviously when you store it in memory, because this will allow some kind of random access.", "tokens": [7580, 562, 291, 3531, 309, 294, 4675, 11, 570, 341, 486, 2089, 512, 733, 295, 4974, 2105, 13], "temperature": 0.0, "avg_logprob": -0.16607015273150275, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.00011015752534149215}, {"id": 291, "seek": 132896, "start": 1339.96, "end": 1344.96, "text": " And also you need to consider how you empty your map.", "tokens": [400, 611, 291, 643, 281, 1949, 577, 291, 6707, 428, 4471, 13], "temperature": 0.0, "avg_logprob": -0.16607015273150275, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.00011015752534149215}, {"id": 292, "seek": 132896, "start": 1344.96, "end": 1347.96, "text": " So because you are limited on size, obviously.", "tokens": [407, 570, 291, 366, 5567, 322, 2744, 11, 2745, 13], "temperature": 0.0, "avg_logprob": -0.16607015273150275, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.00011015752534149215}, {"id": 293, "seek": 132896, "start": 1347.96, "end": 1350.96, "text": " And finally, you need to consider security.", "tokens": [400, 2721, 11, 291, 643, 281, 1949, 3825, 13], "temperature": 0.0, "avg_logprob": -0.16607015273150275, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.00011015752534149215}, {"id": 294, "seek": 132896, "start": 1350.96, "end": 1355.96, "text": " So whatever you send to the cloud, you need to make sure that you don't include some, you know,", "tokens": [407, 2035, 291, 2845, 281, 264, 4588, 11, 291, 643, 281, 652, 988, 300, 291, 500, 380, 4090, 512, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.16607015273150275, "compression_ratio": 1.7292576419213974, "no_speech_prob": 0.00011015752534149215}, {"id": 295, "seek": 135596, "start": 1355.96, "end": 1358.96, "text": " personal entities or whatever. So if you're interested in this topic,", "tokens": [2973, 16667, 420, 2035, 13, 407, 498, 291, 434, 3102, 294, 341, 4829, 11], "temperature": 0.0, "avg_logprob": -0.18669467983823834, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.0005747411050833762}, {"id": 296, "seek": 135596, "start": 1358.96, "end": 1361.96, "text": " we're running a conference next month.", "tokens": [321, 434, 2614, 257, 7586, 958, 1618, 13], "temperature": 0.0, "avg_logprob": -0.18669467983823834, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.0005747411050833762}, {"id": 297, "seek": 135596, "start": 1361.96, "end": 1363.96, "text": " So feel free to scan this code.", "tokens": [407, 841, 1737, 281, 11049, 341, 3089, 13], "temperature": 0.0, "avg_logprob": -0.18669467983823834, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.0005747411050833762}, {"id": 298, "seek": 135596, "start": 1363.96, "end": 1366.96, "text": " We provide training for this all free, obviously.", "tokens": [492, 2893, 3097, 337, 341, 439, 1737, 11, 2745, 13], "temperature": 0.0, "avg_logprob": -0.18669467983823834, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.0005747411050833762}, {"id": 299, "seek": 135596, "start": 1366.96, "end": 1368.96, "text": " And everything I mentioned today is open source,", "tokens": [400, 1203, 286, 2835, 965, 307, 1269, 4009, 11], "temperature": 0.0, "avg_logprob": -0.18669467983823834, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.0005747411050833762}, {"id": 300, "seek": 135596, "start": 1368.96, "end": 1371.96, "text": " so you should be able to do everything I mentioned today.", "tokens": [370, 291, 820, 312, 1075, 281, 360, 1203, 286, 2835, 965, 13], "temperature": 0.0, "avg_logprob": -0.18669467983823834, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.0005747411050833762}, {"id": 301, "seek": 135596, "start": 1371.96, "end": 1374.96, "text": " I'll be steering around here if you want to have a chat", "tokens": [286, 603, 312, 14823, 926, 510, 498, 291, 528, 281, 362, 257, 5081], "temperature": 0.0, "avg_logprob": -0.18669467983823834, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.0005747411050833762}, {"id": 302, "seek": 135596, "start": 1374.96, "end": 1376.96, "text": " or if you want to discuss it a little bit more.", "tokens": [420, 498, 291, 528, 281, 2248, 309, 257, 707, 857, 544, 13], "temperature": 0.0, "avg_logprob": -0.18669467983823834, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.0005747411050833762}, {"id": 303, "seek": 135596, "start": 1376.96, "end": 1379.96, "text": " Obviously within half an hour, there's not much to give,", "tokens": [7580, 1951, 1922, 364, 1773, 11, 456, 311, 406, 709, 281, 976, 11], "temperature": 0.0, "avg_logprob": -0.18669467983823834, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.0005747411050833762}, {"id": 304, "seek": 135596, "start": 1379.96, "end": 1382.96, "text": " but hopefully you've got some ideas from this talk.", "tokens": [457, 4696, 291, 600, 658, 512, 3487, 490, 341, 751, 13], "temperature": 0.0, "avg_logprob": -0.18669467983823834, "compression_ratio": 1.7171717171717171, "no_speech_prob": 0.0005747411050833762}, {"id": 305, "seek": 138296, "start": 1382.96, "end": 1385.96, "text": " And hopefully it will be also useful for you.", "tokens": [400, 4696, 309, 486, 312, 611, 4420, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.31431025128031886, "compression_ratio": 1.3389830508474576, "no_speech_prob": 0.0030462590511888266}, {"id": 306, "seek": 138296, "start": 1385.96, "end": 1388.96, "text": " So with that being said, thanks very much for listening.", "tokens": [407, 365, 300, 885, 848, 11, 3231, 588, 709, 337, 4764, 13], "temperature": 0.0, "avg_logprob": -0.31431025128031886, "compression_ratio": 1.3389830508474576, "no_speech_prob": 0.0030462590511888266}, {"id": 307, "seek": 138296, "start": 1388.96, "end": 1390.96, "text": " I'll open for questions.", "tokens": [286, 603, 1269, 337, 1651, 13], "temperature": 0.0, "avg_logprob": -0.31431025128031886, "compression_ratio": 1.3389830508474576, "no_speech_prob": 0.0030462590511888266}, {"id": 308, "seek": 139096, "start": 1390.96, "end": 1413.96, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 51514], "temperature": 0.0, "avg_logprob": -0.43845248222351074, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.0012101847678422928}], "language": "en"}