{"text": " Okay, the next talk is a perfect fit after a previous talk. So Olivier and Axel are going to talk about Troika, a system to easily manage, submit your jobs to any HPC system. Yeah, thanks for inviting me and thanks for letting me talk. So yeah, Troika, as Kenneth said, is a system so that we can interact with job submission systems with one given interface. So just before I start, a bit of context where I work. So I work for the European Center for Medium Range Weather Forecasts, which is a European-based international organization. And we run an operational weather forecasting service four times a day that we send out to national meteorological services and private customers. So we also operate quite a variety of services, like we have our own in-house research to improve the models, to do climate analysis, reforcasts. We operate services linked to climate change, for instance, as part of the EU Copernicus Service, and we've just started a new project called Destination Earth. So I'll talk a bit more about that because it's a nice entry to what I will present. So it's a EU program for weather and climate. It's a large collaboration that we drive with ESA, the European Space Agency and UMEDSAT, the European Meteorological Satellite Organization. And the goal is to run simulations of the Earth at one kilometer resolution. So for those who are wondering, that's about 256 million points per vertical level. So this project is quite big, and it will run on multiple HPC systems across Europe. So for instance, I think Barcelona with BSC and Lumi in Finland, just to name two. And that means we will require some level of flexibility to run our workflows. So you notice I didn't say job because in weather forecasting and also for these projects, we have lots of different tasks that we run together. So here you can see an overview of what we run operationally. But in practice, that's a few thousand tasks that run every time we want to run one of these pipelines. And we have multiple types of workflows in-house. So the main one is the operational one, of course. But then researchers have their own workflows. We have support workflows like CICD, deploying software, or just fetching data and analyzing data and things like that. And that amounts to about half a million tasks per day on our HPC cluster. And so sometimes we run parallel jobs, but most of those tasks are just small, like one CPU or a few CPU tasks, just to do some processing. So for that, we use a workflow manager that we developed called ECFlow, which basically manages a task graph as a tree with additional dependencies. So you can have dependencies on dates, loops, and things like that. And that runs a script for every task. So a task being one leaf in the tree I show here. It stores variables for pre-processing if needed, keeps track of the task status, fetches log files on demand. What it doesn't do to keep it simple is connect to remote systems and talk to specific queuing systems. So ECFlow just runs commands on the server host, which is usually VM, and provides three entry points, which are submit, monitor, and kill for every task. And so if you want to run an actual job on an HPC system, that means you have to have some kind of interface. So first you can start by just saying, oh, yeah, the command is SSH to my cluster and submit a job, and that's it, which works. But when you change cluster, or even like there is an option to put, you're in trouble because you have to change that variable, and it can be a bit painful, especially if you have thousands of tasks, or you don't want to regenerate the whole workflow. So next possible thing, you write a shell script. So you could do multiple actions in your script. You have a bit more flexibility, but I don't know if you tried handling configuration in a shell script. Usually it ends up quite easily into a nightmare. It's very hard to maintain, and if you deal with several people, everyone has their own. So we tried to have something a bit cleaner, and so we want to delegate it to a submit interface that can be made generic, gives you lots of flexibility, and you can also maintain it as a proper piece of software that means versioning, testing, and some level at least of reproducibility. So we call our software Torica because it runs mainly those three actions, submit, monitor, and kill. It's able to handle remote connection to a remote system, mostly using SSH. It's also able to prepare the job script for submission, interact with a queuing system, and optionally you can run hooks at diverse points. So it's written in Python. We put a strong emphasis in making it configurable so everything can be driven by configuration. I'll show how this works afterwards. And we want it to be extensible, so you can add new connection methods if running locally on your server node or running over SSH isn't enough. You could just add a plug-in if you want to support another queuing system, same. And if you want to add some hooks, for instance, to create directories before your job runs or copy files over before or after submitting a job, et cetera, you can also do it. So as an example, that's how you would run Torica. So it has quite a simple command line interface where you can control most of the flags you will need in your day-to-day life. So you choose the action you want to do, submit, monitor, or kill. You give it a machine name which is defined in configuration. Some options like the user, you tell it where to write the output file because that will stay on the server. And it serves as a reference if you want to copy some other files, they would be put alongside this one. And so here you can see the log below that shows the commands that would be actually executed when doing that. So as I said, everything that is configurable. So each site has a name to identify it on the command line. And then you define the connection type, local, SSH, whatever you want to add, a type. So for now we support direct execution, slum, and PBS. And then you can add some hooks, for instance, oh, yeah, before I start doing anything, check the connection just to see whether it will actually work, or, oh, yeah, before submitting the script, just make sure the directory containing the output file exists, or once the job is submitted, copy the log file to the server so that we can see everything in the same place rather than having files scattered around every system. And so that's all good, but just having an alias to SBatch that does it remotely is not really helpful. So we need also to modify the job script to add some options that are understandable by the submission system. So for that we decided to have a new language, because obviously the directives are not interoperable across submission systems. And so we need some kind of translation. We input some generic directives, and we can add some in the configuration as well. And then we translate them, so either for things very simple, like, oh, yeah, the output file in PBS is minus O in slum, it's minus, minus output. So this kind of translation could have also plugins that compute resources, like if someone gives you the number of nodes and the number of tasks per node, and you need the total number of tasks, things like that, so you could add plugins, or if you have some specific resource management in your HPC, you can add that as well. And then on the output side, we have a generator that's site-specific, again, because we need to adapt the directives to the system. It can make the last few translations, for instance, the actual syntax of some options, like mail options, most submission systems allow you to specify an email address to which send an email for some of your tasks. Only the syntax is slightly different for everyone, so it does that translation, and it's able to add code, if you need, for instance, to define environment variables in your software. So the main components that are extensible in Troika are, as I said, the interaction with the queuing system. So you have a parser that reads the native directives so that you can use them if you need them for your processing, generates the job script, it runs the appropriate commands, so either using QSOB, SBATCH, or whatever, it could use APIs if you have another system. And it can also keep track of the submission, so most of the time, just keeping a job ID so that if you want to monitor the task, you just say, oh, yeah, the script was this, and Troika will know, oh, yeah, put the job ID in that file next to the script. I don't need you to tell me where it is. And so you can choose how you want to interact and define new interfaces if you want. Same for the connection. So the connection mostly does the running of commands on the remote system. It's able to copy files over, if needed, both ways. And you can have some hooks at various points at start-up just before submitting, just after killing a job, for instance, if you want to tell a workflow manager that, oh, this task doesn't exist anymore, I just killed it, or at exit if you want to move your log files around, for instance. And that allows you to perform extractions. And then the last thing you can customize is the translation. So if you want to generate more directives than the user provided, you can also do it. And basically, you just pass a function that takes the input set of directives and updates that set to whatever you need. So as a bit of a success story for us, so we've just switched to a new HPC with a new set of EC flow server VMs, new location, new everything. So it's much simpler to actually be able to just change a config file rather than rewrite a whole shell script that does all the submission for us. And also, since we have lots of different users, they have different needs, they have different ways of working. And what we managed to do with Troika is that we managed to bring them all together to use a single tool, which runs the operational workflows where they need to have tight control over what they actually submit and all the options. Research workflows, which need to be very flexible because every researcher might have their own specific needs, but in the end, they run mostly the same kind of code. So we need to have an interface that allows that. And then we run also general purpose servers. If someone has a data processing pipeline, for instance, they can just spawn a server and do their work. And that needs to have an easy to use interface because we don't want to teach people, oh, yeah, you also need to know that to run your job. So now what we do is we provide them with VMs where Troika is pre-installed, and many of them just don't even notice that it's there. And as a summary, so I said at the beginning that we handle about half a million jobs per day, and most of them now pass through Troika, and it hasn't failed yet, so hopefully it works well enough. What it will help us with also going forward is supporting our software development. So it's not necessarily tied to a workflow manager. We want to control our CI CD pipeline also using that because some of the elements of the pipeline have to run on our HPC system. So basically what we could do is from a GitHub runner, we could use Troika to connect to our HPC run jobs there to do testing, deployment, and everything. We, as I said, run our in-house workflows, and we will continue to do that for the foreseeable future. It will help us to adapt to new HPC systems because every time we make a tender, any provider could answer, and we don't control which submission system we will end up with, and even which site-specific variants there will be in the set of options. And then for destination Earth, as I mentioned before, we want to support multiple HPC with minimal changes to the code. And so just to tell a bit more, where do you want, we want to go from here. So we want to support more queuing systems because, I mean, we support two, and one of them quite well because if we use it, the other one a bit less maybe. We want also to add functionality to inquire about the submission systems of, for instance, which are the queues available, the petitions, things like that, so that the user doesn't need to go to the server, check before running, like you could just run a command that fetches all the information in a useful way and gives it to you without, we're abstracting basically the specifics. We also want to add some generic resource computation routines. So we have some in-house, but they are very tied to the way we function, and so there will be some work to make it more generic and then integrate it in the main source code rather than in a plugin. And for improvements to the code, we want to improve script generation. For now it's a bit clunky, but it works. We want to widen the coverage because you never test enough and provide packages to install it on Debian-based machines, for instance, or RPMs for Red Hat systems, et cetera. And if you want to contribute, feel free to talk to me or go to our GitHub page and I'll stop for now and take questions. Hello, thanks for the presentation. So basically I've done something quite similar for my employer, sadly it cannot be open sourced, but the problem that we have is we have legacy clusters with legacy job submission systems. How did you manage to get the traction to migrate to Troika and to convince the user to port their jobs, their developments to this new system? So what we did first is that we made it as seamless as possible. So if you want to interact with your job submission system without using our directives, you can. They will just pass through, but you lose on the generosity. And then what helped us is that we changed our HPC system, and that means we did basically start afresh and everyone had to make changes, so we just pushed that onto them. And I must say many of them have been happy because that meant we can do that for them rather than them having to figure out the details of how do they submit jobs on that new system and everything. We can just tell them, oh yeah, it's reinstalled, it works. And so yeah, that has been really helpful. I actually have a follow-up question to that. So one thing we have been doing, we switched recently, well, four or five years ago from Torq to Slurm, and we didn't want to let all our users retrain themselves and learn the Slurm commands, because in our experience, Slurm is a bit less user-friendly than Torq is. So what we did is we rolled a wrapper that people can still use QSub, but they're actually submitting to Slurm, and it just, it translates the script in the background. Troika doesn't do that now, right? You have to use the Troika command, but it knows about the Slurm header. Yeah, so you could technically do it. We didn't want to encourage that, but technically you could, like, I think you could write a script in three lines, a plugin that just takes the directives. You would probably need to support all the directives you need, but we have a built-in parser that is able to read, like, Slurm commands, for instance. And so you just need to tell Troika, oh yeah, use those on top of whatever is specified in configuration. Is that something you would take pull requests on? Yeah, if you want to. Okay, we had another question. Yeah. Passed them away. Hi, thank you for the presentation, very interesting. So I'm an early programmer myself, and so my question for you is, how does it fail? Like, have you studied or provoked, you know, intentional failures of the system, and have you encountered funny behaviors, like, or plain hilarious faults of the system? Yeah, we had, I mean, getting a new system has its lot of failures, so I don't know if Axel, you want to take over for that, because you probably have handled some of the failures. In the example of the command line provided, you can see that we redirect the output for each submission, and this is a chance to analyze the submission and to decide what's the best approach to deal with erroneous submission, meaning that some of them have to be reflected the hard way to make it clearly visible, this is a problem. And some others can be handled in a hidden way, or not so visible way, in a still deterministic way, and so it may be hidden and still automatically handle the problems when they occur. And this is what we expect with so many jobs to submit, to focus on the critical essential for the human side, and to have a chance to teach the machines through the hook system to manage with the specificities we have identified as problematic, but we want to keep ignored or manage automatically until a fix is coming from the curing system, for example, if it is related to a curing system problem or identified issues that may come with the next release. So this is a way to deal with the failures that can occur at job submission. Thank you. Did I understand correctly that when you're monitoring a job, the reference is the script? Yes. Correct. So that means everyone has to make sure their scripts are uniquely named each time, otherwise, or is it the sort of script and where it is in the file system? It's where it is in the file system. So you are correct. If someone deletes or renames their script, then it can cause a problem. Submits with the same script. So it's not a problem for us because our workflow manager basically does some pre-processing, meaning that the script has some additional things like, oh, yeah, it's your second try at that submission, so I will add.job2 at the end. And so that's how we circumvent this issue, but you are definitely correct, and that's something we will need to improve at some point. But we didn't want to have to link to a database or something so that we can keep it simple. Thanks. You could just copy the script on submission, no? We could copy it. It's just that, yeah, if you have half a million scripts, we need to think, per day, we need to think about cleanup. Yeah. Other questions? Hello. Users like things to be as simple as possible. In order to do that, they would probably be nice to have some sort of central location where recipes of various clusters would be sort of combined accessible for people to be able to get access to. Is that in your plan or? What do you mean under configuration side? So I could imagine a user turning up going, oh, I'm going to download Troika, and I'm going to talk to this cluster that I have access to. How do I get the configuration? Oh, OK. I see. So we don't have that, but if Troika gets attraction, I think we could come up with a website where you can host your configuration files or have some kind of index where you can list them. I think we would have all that's needed to do that pretty easily. I think, hopefully, the configuration is easy enough so that you don't need to do much on top of what's actually provided as examples. But yeah, you are correct. We could, if it gets popular, just provide configuration files for several systems or, I mean, HPC system providers could also just give a configuration file with the system so we can have it where Troika is installed and then the user doesn't even need to bother about it. Very small second one. Given you've just done all this stuff, have you heard of a project called DRMAA, Distributed Resource Manager Application API, it might make the insides of this slightly nicer for your EC flow stuff, maybe it might take some inspiration for that. Thank you. A question, but also an observation. A long time ago, there was a standard called DRMAA, it was an API. Just mentioned. It seems not to be used, maybe I'm wrong, but very quickly, your system, if you had cloud-based resources on AWS, you've got an SSH connector. Could you have, in the future, maybe run up some machines on AWS? Yeah, that could be an option. As long as you can write Python code to spawn up an image, a container somewhere. Yeah, sure. I think the API is for that, that just needs to be a plug-in that does the connection, and that's it. Cool. Okay, we're out of time. Just a comment. I don't think you have any people using Troika outside of ECMEF. No, that's the first time we actually presented outside. All right. Good. So you're trying to start, or trying to get people to start using it? Yes. You're building a community, you're getting yourself into trouble. We're going to get public requests and bug reports, but okay. Thank you very much. Thank you. Very nice. Thank you. I'll just switch. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 15.72, "text": " Okay, the next talk is a perfect fit after a previous talk.", "tokens": [1033, 11, 264, 958, 751, 307, 257, 2176, 3318, 934, 257, 3894, 751, 13], "temperature": 0.0, "avg_logprob": -0.22554858281062198, "compression_ratio": 1.4506172839506173, "no_speech_prob": 0.4755207300186157}, {"id": 1, "seek": 0, "start": 15.72, "end": 21.96, "text": " So Olivier and Axel are going to talk about Troika, a system to easily manage, submit", "tokens": [407, 48075, 293, 20118, 338, 366, 516, 281, 751, 466, 19406, 5439, 11, 257, 1185, 281, 3612, 3067, 11, 10315], "temperature": 0.0, "avg_logprob": -0.22554858281062198, "compression_ratio": 1.4506172839506173, "no_speech_prob": 0.4755207300186157}, {"id": 2, "seek": 0, "start": 21.96, "end": 24.68, "text": " your jobs to any HPC system.", "tokens": [428, 4782, 281, 604, 12557, 34, 1185, 13], "temperature": 0.0, "avg_logprob": -0.22554858281062198, "compression_ratio": 1.4506172839506173, "no_speech_prob": 0.4755207300186157}, {"id": 3, "seek": 0, "start": 24.68, "end": 28.52, "text": " Yeah, thanks for inviting me and thanks for letting me talk.", "tokens": [865, 11, 3231, 337, 18202, 385, 293, 3231, 337, 8295, 385, 751, 13], "temperature": 0.0, "avg_logprob": -0.22554858281062198, "compression_ratio": 1.4506172839506173, "no_speech_prob": 0.4755207300186157}, {"id": 4, "seek": 2852, "start": 28.52, "end": 36.04, "text": " So yeah, Troika, as Kenneth said, is a system so that we can interact with job submission", "tokens": [407, 1338, 11, 19406, 5439, 11, 382, 33735, 848, 11, 307, 257, 1185, 370, 300, 321, 393, 4648, 365, 1691, 23689], "temperature": 0.0, "avg_logprob": -0.14661083221435547, "compression_ratio": 1.625, "no_speech_prob": 0.00017788146215025336}, {"id": 5, "seek": 2852, "start": 36.04, "end": 38.64, "text": " systems with one given interface.", "tokens": [3652, 365, 472, 2212, 9226, 13], "temperature": 0.0, "avg_logprob": -0.14661083221435547, "compression_ratio": 1.625, "no_speech_prob": 0.00017788146215025336}, {"id": 6, "seek": 2852, "start": 38.64, "end": 42.0, "text": " So just before I start, a bit of context where I work.", "tokens": [407, 445, 949, 286, 722, 11, 257, 857, 295, 4319, 689, 286, 589, 13], "temperature": 0.0, "avg_logprob": -0.14661083221435547, "compression_ratio": 1.625, "no_speech_prob": 0.00017788146215025336}, {"id": 7, "seek": 2852, "start": 42.0, "end": 48.84, "text": " So I work for the European Center for Medium Range Weather Forecasts, which is a European-based", "tokens": [407, 286, 589, 337, 264, 6473, 5169, 337, 38915, 33778, 34441, 9018, 3734, 82, 11, 597, 307, 257, 6473, 12, 6032], "temperature": 0.0, "avg_logprob": -0.14661083221435547, "compression_ratio": 1.625, "no_speech_prob": 0.00017788146215025336}, {"id": 8, "seek": 2852, "start": 48.84, "end": 51.120000000000005, "text": " international organization.", "tokens": [5058, 4475, 13], "temperature": 0.0, "avg_logprob": -0.14661083221435547, "compression_ratio": 1.625, "no_speech_prob": 0.00017788146215025336}, {"id": 9, "seek": 2852, "start": 51.120000000000005, "end": 56.4, "text": " And we run an operational weather forecasting service four times a day that we send out", "tokens": [400, 321, 1190, 364, 16607, 5503, 44331, 2643, 1451, 1413, 257, 786, 300, 321, 2845, 484], "temperature": 0.0, "avg_logprob": -0.14661083221435547, "compression_ratio": 1.625, "no_speech_prob": 0.00017788146215025336}, {"id": 10, "seek": 5640, "start": 56.4, "end": 61.96, "text": " to national meteorological services and private customers.", "tokens": [281, 4048, 25313, 4383, 3328, 293, 4551, 4581, 13], "temperature": 0.0, "avg_logprob": -0.15599528948465982, "compression_ratio": 1.5340314136125655, "no_speech_prob": 0.0002612323150970042}, {"id": 11, "seek": 5640, "start": 61.96, "end": 70.92, "text": " So we also operate quite a variety of services, like we have our own in-house research to", "tokens": [407, 321, 611, 9651, 1596, 257, 5673, 295, 3328, 11, 411, 321, 362, 527, 1065, 294, 12, 6410, 2132, 281], "temperature": 0.0, "avg_logprob": -0.15599528948465982, "compression_ratio": 1.5340314136125655, "no_speech_prob": 0.0002612323150970042}, {"id": 12, "seek": 5640, "start": 70.92, "end": 75.48, "text": " improve the models, to do climate analysis, reforcasts.", "tokens": [3470, 264, 5245, 11, 281, 360, 5659, 5215, 11, 1895, 284, 3734, 82, 13], "temperature": 0.0, "avg_logprob": -0.15599528948465982, "compression_ratio": 1.5340314136125655, "no_speech_prob": 0.0002612323150970042}, {"id": 13, "seek": 5640, "start": 75.48, "end": 81.32, "text": " We operate services linked to climate change, for instance, as part of the EU Copernicus", "tokens": [492, 9651, 3328, 9408, 281, 5659, 1319, 11, 337, 5197, 11, 382, 644, 295, 264, 10887, 11579, 1248, 36496], "temperature": 0.0, "avg_logprob": -0.15599528948465982, "compression_ratio": 1.5340314136125655, "no_speech_prob": 0.0002612323150970042}, {"id": 14, "seek": 8132, "start": 81.32, "end": 87.08, "text": " Service, and we've just started a new project called Destination Earth.", "tokens": [9561, 11, 293, 321, 600, 445, 1409, 257, 777, 1716, 1219, 16339, 2486, 4755, 13], "temperature": 0.0, "avg_logprob": -0.1549205426816587, "compression_ratio": 1.5650557620817844, "no_speech_prob": 0.00021275390463415533}, {"id": 15, "seek": 8132, "start": 87.08, "end": 92.11999999999999, "text": " So I'll talk a bit more about that because it's a nice entry to what I will present.", "tokens": [407, 286, 603, 751, 257, 857, 544, 466, 300, 570, 309, 311, 257, 1481, 8729, 281, 437, 286, 486, 1974, 13], "temperature": 0.0, "avg_logprob": -0.1549205426816587, "compression_ratio": 1.5650557620817844, "no_speech_prob": 0.00021275390463415533}, {"id": 16, "seek": 8132, "start": 92.11999999999999, "end": 96.0, "text": " So it's a EU program for weather and climate.", "tokens": [407, 309, 311, 257, 10887, 1461, 337, 5503, 293, 5659, 13], "temperature": 0.0, "avg_logprob": -0.1549205426816587, "compression_ratio": 1.5650557620817844, "no_speech_prob": 0.00021275390463415533}, {"id": 17, "seek": 8132, "start": 96.0, "end": 101.03999999999999, "text": " It's a large collaboration that we drive with ESA, the European Space Agency and UMEDSAT,", "tokens": [467, 311, 257, 2416, 9363, 300, 321, 3332, 365, 462, 8886, 11, 264, 6473, 8705, 21649, 293, 31335, 4731, 50, 2218, 11], "temperature": 0.0, "avg_logprob": -0.1549205426816587, "compression_ratio": 1.5650557620817844, "no_speech_prob": 0.00021275390463415533}, {"id": 18, "seek": 8132, "start": 101.03999999999999, "end": 104.91999999999999, "text": " the European Meteorological Satellite Organization.", "tokens": [264, 6473, 43328, 284, 4383, 318, 10810, 642, 23979, 13], "temperature": 0.0, "avg_logprob": -0.1549205426816587, "compression_ratio": 1.5650557620817844, "no_speech_prob": 0.00021275390463415533}, {"id": 19, "seek": 8132, "start": 104.91999999999999, "end": 108.8, "text": " And the goal is to run simulations of the Earth at one kilometer resolution.", "tokens": [400, 264, 3387, 307, 281, 1190, 35138, 295, 264, 4755, 412, 472, 33795, 8669, 13], "temperature": 0.0, "avg_logprob": -0.1549205426816587, "compression_ratio": 1.5650557620817844, "no_speech_prob": 0.00021275390463415533}, {"id": 20, "seek": 10880, "start": 108.8, "end": 116.44, "text": " So for those who are wondering, that's about 256 million points per vertical level.", "tokens": [407, 337, 729, 567, 366, 6359, 11, 300, 311, 466, 38882, 2459, 2793, 680, 9429, 1496, 13], "temperature": 0.0, "avg_logprob": -0.12571769127478966, "compression_ratio": 1.3351063829787233, "no_speech_prob": 0.00043510631076060236}, {"id": 21, "seek": 10880, "start": 116.44, "end": 122.92, "text": " So this project is quite big, and it will run on multiple HPC systems across Europe.", "tokens": [407, 341, 1716, 307, 1596, 955, 11, 293, 309, 486, 1190, 322, 3866, 12557, 34, 3652, 2108, 3315, 13], "temperature": 0.0, "avg_logprob": -0.12571769127478966, "compression_ratio": 1.3351063829787233, "no_speech_prob": 0.00043510631076060236}, {"id": 22, "seek": 10880, "start": 122.92, "end": 134.24, "text": " So for instance, I think Barcelona with BSC and Lumi in Finland, just to name two.", "tokens": [407, 337, 5197, 11, 286, 519, 21247, 365, 363, 20839, 293, 441, 17800, 294, 24869, 11, 445, 281, 1315, 732, 13], "temperature": 0.0, "avg_logprob": -0.12571769127478966, "compression_ratio": 1.3351063829787233, "no_speech_prob": 0.00043510631076060236}, {"id": 23, "seek": 13424, "start": 134.24, "end": 139.52, "text": " And that means we will require some level of flexibility to run our workflows.", "tokens": [400, 300, 1355, 321, 486, 3651, 512, 1496, 295, 12635, 281, 1190, 527, 43461, 13], "temperature": 0.0, "avg_logprob": -0.10441560745239258, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.00032611863571219146}, {"id": 24, "seek": 13424, "start": 139.52, "end": 146.88, "text": " So you notice I didn't say job because in weather forecasting and also for these projects,", "tokens": [407, 291, 3449, 286, 994, 380, 584, 1691, 570, 294, 5503, 44331, 293, 611, 337, 613, 4455, 11], "temperature": 0.0, "avg_logprob": -0.10441560745239258, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.00032611863571219146}, {"id": 25, "seek": 13424, "start": 146.88, "end": 151.16000000000003, "text": " we have lots of different tasks that we run together.", "tokens": [321, 362, 3195, 295, 819, 9608, 300, 321, 1190, 1214, 13], "temperature": 0.0, "avg_logprob": -0.10441560745239258, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.00032611863571219146}, {"id": 26, "seek": 13424, "start": 151.16000000000003, "end": 157.04000000000002, "text": " So here you can see an overview of what we run operationally.", "tokens": [407, 510, 291, 393, 536, 364, 12492, 295, 437, 321, 1190, 6916, 379, 13], "temperature": 0.0, "avg_logprob": -0.10441560745239258, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.00032611863571219146}, {"id": 27, "seek": 13424, "start": 157.04000000000002, "end": 163.96, "text": " But in practice, that's a few thousand tasks that run every time we want to run one of", "tokens": [583, 294, 3124, 11, 300, 311, 257, 1326, 4714, 9608, 300, 1190, 633, 565, 321, 528, 281, 1190, 472, 295], "temperature": 0.0, "avg_logprob": -0.10441560745239258, "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.00032611863571219146}, {"id": 28, "seek": 16396, "start": 163.96, "end": 167.12, "text": " these pipelines.", "tokens": [613, 40168, 13], "temperature": 0.0, "avg_logprob": -0.14885773866072946, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.00046003522584214807}, {"id": 29, "seek": 16396, "start": 167.12, "end": 170.52, "text": " And we have multiple types of workflows in-house.", "tokens": [400, 321, 362, 3866, 3467, 295, 43461, 294, 12, 6410, 13], "temperature": 0.0, "avg_logprob": -0.14885773866072946, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.00046003522584214807}, {"id": 30, "seek": 16396, "start": 170.52, "end": 173.8, "text": " So the main one is the operational one, of course.", "tokens": [407, 264, 2135, 472, 307, 264, 16607, 472, 11, 295, 1164, 13], "temperature": 0.0, "avg_logprob": -0.14885773866072946, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.00046003522584214807}, {"id": 31, "seek": 16396, "start": 173.8, "end": 178.16, "text": " But then researchers have their own workflows.", "tokens": [583, 550, 10309, 362, 641, 1065, 43461, 13], "temperature": 0.0, "avg_logprob": -0.14885773866072946, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.00046003522584214807}, {"id": 32, "seek": 16396, "start": 178.16, "end": 183.92000000000002, "text": " We have support workflows like CICD, deploying software, or just fetching data and analyzing", "tokens": [492, 362, 1406, 43461, 411, 383, 2532, 35, 11, 34198, 4722, 11, 420, 445, 23673, 278, 1412, 293, 23663], "temperature": 0.0, "avg_logprob": -0.14885773866072946, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.00046003522584214807}, {"id": 33, "seek": 16396, "start": 183.92000000000002, "end": 186.36, "text": " data and things like that.", "tokens": [1412, 293, 721, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.14885773866072946, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.00046003522584214807}, {"id": 34, "seek": 16396, "start": 186.36, "end": 192.72, "text": " And that amounts to about half a million tasks per day on our HPC cluster.", "tokens": [400, 300, 11663, 281, 466, 1922, 257, 2459, 9608, 680, 786, 322, 527, 12557, 34, 13630, 13], "temperature": 0.0, "avg_logprob": -0.14885773866072946, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.00046003522584214807}, {"id": 35, "seek": 19272, "start": 192.72, "end": 199.12, "text": " And so sometimes we run parallel jobs, but most of those tasks are just small, like one", "tokens": [400, 370, 2171, 321, 1190, 8952, 4782, 11, 457, 881, 295, 729, 9608, 366, 445, 1359, 11, 411, 472], "temperature": 0.0, "avg_logprob": -0.16066311344955908, "compression_ratio": 1.5991902834008098, "no_speech_prob": 0.00011745089432224631}, {"id": 36, "seek": 19272, "start": 199.12, "end": 205.04, "text": " CPU or a few CPU tasks, just to do some processing.", "tokens": [13199, 420, 257, 1326, 13199, 9608, 11, 445, 281, 360, 512, 9007, 13], "temperature": 0.0, "avg_logprob": -0.16066311344955908, "compression_ratio": 1.5991902834008098, "no_speech_prob": 0.00011745089432224631}, {"id": 37, "seek": 19272, "start": 205.04, "end": 210.04, "text": " So for that, we use a workflow manager that we developed called ECFlow, which basically", "tokens": [407, 337, 300, 11, 321, 764, 257, 20993, 6598, 300, 321, 4743, 1219, 19081, 31091, 11, 597, 1936], "temperature": 0.0, "avg_logprob": -0.16066311344955908, "compression_ratio": 1.5991902834008098, "no_speech_prob": 0.00011745089432224631}, {"id": 38, "seek": 19272, "start": 210.04, "end": 214.28, "text": " manages a task graph as a tree with additional dependencies.", "tokens": [22489, 257, 5633, 4295, 382, 257, 4230, 365, 4497, 36606, 13], "temperature": 0.0, "avg_logprob": -0.16066311344955908, "compression_ratio": 1.5991902834008098, "no_speech_prob": 0.00011745089432224631}, {"id": 39, "seek": 19272, "start": 214.28, "end": 219.52, "text": " So you can have dependencies on dates, loops, and things like that.", "tokens": [407, 291, 393, 362, 36606, 322, 11691, 11, 16121, 11, 293, 721, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.16066311344955908, "compression_ratio": 1.5991902834008098, "no_speech_prob": 0.00011745089432224631}, {"id": 40, "seek": 19272, "start": 219.52, "end": 222.52, "text": " And that runs a script for every task.", "tokens": [400, 300, 6676, 257, 5755, 337, 633, 5633, 13], "temperature": 0.0, "avg_logprob": -0.16066311344955908, "compression_ratio": 1.5991902834008098, "no_speech_prob": 0.00011745089432224631}, {"id": 41, "seek": 22252, "start": 222.52, "end": 227.16, "text": " So a task being one leaf in the tree I show here.", "tokens": [407, 257, 5633, 885, 472, 10871, 294, 264, 4230, 286, 855, 510, 13], "temperature": 0.0, "avg_logprob": -0.13902271434824953, "compression_ratio": 1.4957627118644068, "no_speech_prob": 0.00015055850963108242}, {"id": 42, "seek": 22252, "start": 227.16, "end": 232.32000000000002, "text": " It stores variables for pre-processing if needed, keeps track of the task status, fetches", "tokens": [467, 9512, 9102, 337, 659, 12, 41075, 278, 498, 2978, 11, 5965, 2837, 295, 264, 5633, 6558, 11, 15136, 3781], "temperature": 0.0, "avg_logprob": -0.13902271434824953, "compression_ratio": 1.4957627118644068, "no_speech_prob": 0.00015055850963108242}, {"id": 43, "seek": 22252, "start": 232.32000000000002, "end": 234.76000000000002, "text": " log files on demand.", "tokens": [3565, 7098, 322, 4733, 13], "temperature": 0.0, "avg_logprob": -0.13902271434824953, "compression_ratio": 1.4957627118644068, "no_speech_prob": 0.00015055850963108242}, {"id": 44, "seek": 22252, "start": 234.76000000000002, "end": 241.28, "text": " What it doesn't do to keep it simple is connect to remote systems and talk to specific queuing", "tokens": [708, 309, 1177, 380, 360, 281, 1066, 309, 2199, 307, 1745, 281, 8607, 3652, 293, 751, 281, 2685, 631, 9635], "temperature": 0.0, "avg_logprob": -0.13902271434824953, "compression_ratio": 1.4957627118644068, "no_speech_prob": 0.00015055850963108242}, {"id": 45, "seek": 22252, "start": 241.28, "end": 242.28, "text": " systems.", "tokens": [3652, 13], "temperature": 0.0, "avg_logprob": -0.13902271434824953, "compression_ratio": 1.4957627118644068, "no_speech_prob": 0.00015055850963108242}, {"id": 46, "seek": 22252, "start": 242.28, "end": 250.48000000000002, "text": " So ECFlow just runs commands on the server host, which is usually VM, and provides three", "tokens": [407, 19081, 31091, 445, 6676, 16901, 322, 264, 7154, 3975, 11, 597, 307, 2673, 18038, 11, 293, 6417, 1045], "temperature": 0.0, "avg_logprob": -0.13902271434824953, "compression_ratio": 1.4957627118644068, "no_speech_prob": 0.00015055850963108242}, {"id": 47, "seek": 25048, "start": 250.48, "end": 256.71999999999997, "text": " entry points, which are submit, monitor, and kill for every task.", "tokens": [8729, 2793, 11, 597, 366, 10315, 11, 6002, 11, 293, 1961, 337, 633, 5633, 13], "temperature": 0.0, "avg_logprob": -0.13973858573219994, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.00011276381701463833}, {"id": 48, "seek": 25048, "start": 256.71999999999997, "end": 261.44, "text": " And so if you want to run an actual job on an HPC system, that means you have to have", "tokens": [400, 370, 498, 291, 528, 281, 1190, 364, 3539, 1691, 322, 364, 12557, 34, 1185, 11, 300, 1355, 291, 362, 281, 362], "temperature": 0.0, "avg_logprob": -0.13973858573219994, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.00011276381701463833}, {"id": 49, "seek": 25048, "start": 261.44, "end": 262.64, "text": " some kind of interface.", "tokens": [512, 733, 295, 9226, 13], "temperature": 0.0, "avg_logprob": -0.13973858573219994, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.00011276381701463833}, {"id": 50, "seek": 25048, "start": 262.64, "end": 267.88, "text": " So first you can start by just saying, oh, yeah, the command is SSH to my cluster and", "tokens": [407, 700, 291, 393, 722, 538, 445, 1566, 11, 1954, 11, 1338, 11, 264, 5622, 307, 12238, 39, 281, 452, 13630, 293], "temperature": 0.0, "avg_logprob": -0.13973858573219994, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.00011276381701463833}, {"id": 51, "seek": 25048, "start": 267.88, "end": 271.15999999999997, "text": " submit a job, and that's it, which works.", "tokens": [10315, 257, 1691, 11, 293, 300, 311, 309, 11, 597, 1985, 13], "temperature": 0.0, "avg_logprob": -0.13973858573219994, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.00011276381701463833}, {"id": 52, "seek": 25048, "start": 271.15999999999997, "end": 276.64, "text": " But when you change cluster, or even like there is an option to put, you're in trouble", "tokens": [583, 562, 291, 1319, 13630, 11, 420, 754, 411, 456, 307, 364, 3614, 281, 829, 11, 291, 434, 294, 5253], "temperature": 0.0, "avg_logprob": -0.13973858573219994, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.00011276381701463833}, {"id": 53, "seek": 27664, "start": 276.64, "end": 282.0, "text": " because you have to change that variable, and it can be a bit painful, especially if", "tokens": [570, 291, 362, 281, 1319, 300, 7006, 11, 293, 309, 393, 312, 257, 857, 11697, 11, 2318, 498], "temperature": 0.0, "avg_logprob": -0.1359217779976981, "compression_ratio": 1.6431372549019607, "no_speech_prob": 0.000161839256179519}, {"id": 54, "seek": 27664, "start": 282.0, "end": 287.71999999999997, "text": " you have thousands of tasks, or you don't want to regenerate the whole workflow.", "tokens": [291, 362, 5383, 295, 9608, 11, 420, 291, 500, 380, 528, 281, 26358, 473, 264, 1379, 20993, 13], "temperature": 0.0, "avg_logprob": -0.1359217779976981, "compression_ratio": 1.6431372549019607, "no_speech_prob": 0.000161839256179519}, {"id": 55, "seek": 27664, "start": 287.71999999999997, "end": 291.52, "text": " So next possible thing, you write a shell script.", "tokens": [407, 958, 1944, 551, 11, 291, 2464, 257, 8720, 5755, 13], "temperature": 0.0, "avg_logprob": -0.1359217779976981, "compression_ratio": 1.6431372549019607, "no_speech_prob": 0.000161839256179519}, {"id": 56, "seek": 27664, "start": 291.52, "end": 294.12, "text": " So you could do multiple actions in your script.", "tokens": [407, 291, 727, 360, 3866, 5909, 294, 428, 5755, 13], "temperature": 0.0, "avg_logprob": -0.1359217779976981, "compression_ratio": 1.6431372549019607, "no_speech_prob": 0.000161839256179519}, {"id": 57, "seek": 27664, "start": 294.12, "end": 299.2, "text": " You have a bit more flexibility, but I don't know if you tried handling configuration in", "tokens": [509, 362, 257, 857, 544, 12635, 11, 457, 286, 500, 380, 458, 498, 291, 3031, 13175, 11694, 294], "temperature": 0.0, "avg_logprob": -0.1359217779976981, "compression_ratio": 1.6431372549019607, "no_speech_prob": 0.000161839256179519}, {"id": 58, "seek": 27664, "start": 299.2, "end": 300.2, "text": " a shell script.", "tokens": [257, 8720, 5755, 13], "temperature": 0.0, "avg_logprob": -0.1359217779976981, "compression_ratio": 1.6431372549019607, "no_speech_prob": 0.000161839256179519}, {"id": 59, "seek": 27664, "start": 300.2, "end": 303.52, "text": " Usually it ends up quite easily into a nightmare.", "tokens": [11419, 309, 5314, 493, 1596, 3612, 666, 257, 18724, 13], "temperature": 0.0, "avg_logprob": -0.1359217779976981, "compression_ratio": 1.6431372549019607, "no_speech_prob": 0.000161839256179519}, {"id": 60, "seek": 30352, "start": 303.52, "end": 310.0, "text": " It's very hard to maintain, and if you deal with several people, everyone has their own.", "tokens": [467, 311, 588, 1152, 281, 6909, 11, 293, 498, 291, 2028, 365, 2940, 561, 11, 1518, 575, 641, 1065, 13], "temperature": 0.0, "avg_logprob": -0.10238460091983571, "compression_ratio": 1.6186046511627907, "no_speech_prob": 0.0001115632985602133}, {"id": 61, "seek": 30352, "start": 310.0, "end": 316.32, "text": " So we tried to have something a bit cleaner, and so we want to delegate it to a submit", "tokens": [407, 321, 3031, 281, 362, 746, 257, 857, 16532, 11, 293, 370, 321, 528, 281, 40999, 309, 281, 257, 10315], "temperature": 0.0, "avg_logprob": -0.10238460091983571, "compression_ratio": 1.6186046511627907, "no_speech_prob": 0.0001115632985602133}, {"id": 62, "seek": 30352, "start": 316.32, "end": 323.79999999999995, "text": " interface that can be made generic, gives you lots of flexibility, and you can also", "tokens": [9226, 300, 393, 312, 1027, 19577, 11, 2709, 291, 3195, 295, 12635, 11, 293, 291, 393, 611], "temperature": 0.0, "avg_logprob": -0.10238460091983571, "compression_ratio": 1.6186046511627907, "no_speech_prob": 0.0001115632985602133}, {"id": 63, "seek": 30352, "start": 323.79999999999995, "end": 329.96, "text": " maintain it as a proper piece of software that means versioning, testing, and some level", "tokens": [6909, 309, 382, 257, 2296, 2522, 295, 4722, 300, 1355, 3037, 278, 11, 4997, 11, 293, 512, 1496], "temperature": 0.0, "avg_logprob": -0.10238460091983571, "compression_ratio": 1.6186046511627907, "no_speech_prob": 0.0001115632985602133}, {"id": 64, "seek": 32996, "start": 329.96, "end": 333.79999999999995, "text": " at least of reproducibility.", "tokens": [412, 1935, 295, 11408, 537, 39802, 13], "temperature": 0.0, "avg_logprob": -0.16100696987575955, "compression_ratio": 1.563063063063063, "no_speech_prob": 0.00012314255582168698}, {"id": 65, "seek": 32996, "start": 333.79999999999995, "end": 339.64, "text": " So we call our software Torica because it runs mainly those three actions, submit, monitor,", "tokens": [407, 321, 818, 527, 4722, 7160, 2262, 570, 309, 6676, 8704, 729, 1045, 5909, 11, 10315, 11, 6002, 11], "temperature": 0.0, "avg_logprob": -0.16100696987575955, "compression_ratio": 1.563063063063063, "no_speech_prob": 0.00012314255582168698}, {"id": 66, "seek": 32996, "start": 339.64, "end": 341.79999999999995, "text": " and kill.", "tokens": [293, 1961, 13], "temperature": 0.0, "avg_logprob": -0.16100696987575955, "compression_ratio": 1.563063063063063, "no_speech_prob": 0.00012314255582168698}, {"id": 67, "seek": 32996, "start": 341.79999999999995, "end": 348.44, "text": " It's able to handle remote connection to a remote system, mostly using SSH.", "tokens": [467, 311, 1075, 281, 4813, 8607, 4984, 281, 257, 8607, 1185, 11, 5240, 1228, 12238, 39, 13], "temperature": 0.0, "avg_logprob": -0.16100696987575955, "compression_ratio": 1.563063063063063, "no_speech_prob": 0.00012314255582168698}, {"id": 68, "seek": 32996, "start": 348.44, "end": 355.28, "text": " It's also able to prepare the job script for submission, interact with a queuing system,", "tokens": [467, 311, 611, 1075, 281, 5940, 264, 1691, 5755, 337, 23689, 11, 4648, 365, 257, 631, 9635, 1185, 11], "temperature": 0.0, "avg_logprob": -0.16100696987575955, "compression_ratio": 1.563063063063063, "no_speech_prob": 0.00012314255582168698}, {"id": 69, "seek": 32996, "start": 355.28, "end": 359.0, "text": " and optionally you can run hooks at diverse points.", "tokens": [293, 3614, 379, 291, 393, 1190, 26485, 412, 9521, 2793, 13], "temperature": 0.0, "avg_logprob": -0.16100696987575955, "compression_ratio": 1.563063063063063, "no_speech_prob": 0.00012314255582168698}, {"id": 70, "seek": 35900, "start": 359.0, "end": 361.44, "text": " So it's written in Python.", "tokens": [407, 309, 311, 3720, 294, 15329, 13], "temperature": 0.0, "avg_logprob": -0.13716595582287722, "compression_ratio": 1.5502008032128514, "no_speech_prob": 0.00016531020810361952}, {"id": 71, "seek": 35900, "start": 361.44, "end": 368.12, "text": " We put a strong emphasis in making it configurable so everything can be driven by configuration.", "tokens": [492, 829, 257, 2068, 16271, 294, 1455, 309, 22192, 712, 370, 1203, 393, 312, 9555, 538, 11694, 13], "temperature": 0.0, "avg_logprob": -0.13716595582287722, "compression_ratio": 1.5502008032128514, "no_speech_prob": 0.00016531020810361952}, {"id": 72, "seek": 35900, "start": 368.12, "end": 371.08, "text": " I'll show how this works afterwards.", "tokens": [286, 603, 855, 577, 341, 1985, 10543, 13], "temperature": 0.0, "avg_logprob": -0.13716595582287722, "compression_ratio": 1.5502008032128514, "no_speech_prob": 0.00016531020810361952}, {"id": 73, "seek": 35900, "start": 371.08, "end": 376.12, "text": " And we want it to be extensible, so you can add new connection methods if running locally", "tokens": [400, 321, 528, 309, 281, 312, 1279, 30633, 11, 370, 291, 393, 909, 777, 4984, 7150, 498, 2614, 16143], "temperature": 0.0, "avg_logprob": -0.13716595582287722, "compression_ratio": 1.5502008032128514, "no_speech_prob": 0.00016531020810361952}, {"id": 74, "seek": 35900, "start": 376.12, "end": 380.44, "text": " on your server node or running over SSH isn't enough.", "tokens": [322, 428, 7154, 9984, 420, 2614, 670, 12238, 39, 1943, 380, 1547, 13], "temperature": 0.0, "avg_logprob": -0.13716595582287722, "compression_ratio": 1.5502008032128514, "no_speech_prob": 0.00016531020810361952}, {"id": 75, "seek": 35900, "start": 380.44, "end": 387.36, "text": " You could just add a plug-in if you want to support another queuing system, same.", "tokens": [509, 727, 445, 909, 257, 5452, 12, 259, 498, 291, 528, 281, 1406, 1071, 631, 9635, 1185, 11, 912, 13], "temperature": 0.0, "avg_logprob": -0.13716595582287722, "compression_ratio": 1.5502008032128514, "no_speech_prob": 0.00016531020810361952}, {"id": 76, "seek": 38736, "start": 387.36, "end": 392.04, "text": " And if you want to add some hooks, for instance, to create directories before your job runs", "tokens": [400, 498, 291, 528, 281, 909, 512, 26485, 11, 337, 5197, 11, 281, 1884, 5391, 530, 949, 428, 1691, 6676], "temperature": 0.0, "avg_logprob": -0.10622426083213404, "compression_ratio": 1.6600790513833992, "no_speech_prob": 8.007023279787973e-05}, {"id": 77, "seek": 38736, "start": 392.04, "end": 399.28000000000003, "text": " or copy files over before or after submitting a job, et cetera, you can also do it.", "tokens": [420, 5055, 7098, 670, 949, 420, 934, 31836, 257, 1691, 11, 1030, 11458, 11, 291, 393, 611, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.10622426083213404, "compression_ratio": 1.6600790513833992, "no_speech_prob": 8.007023279787973e-05}, {"id": 78, "seek": 38736, "start": 399.28000000000003, "end": 402.24, "text": " So as an example, that's how you would run Torica.", "tokens": [407, 382, 364, 1365, 11, 300, 311, 577, 291, 576, 1190, 7160, 2262, 13], "temperature": 0.0, "avg_logprob": -0.10622426083213404, "compression_ratio": 1.6600790513833992, "no_speech_prob": 8.007023279787973e-05}, {"id": 79, "seek": 38736, "start": 402.24, "end": 407.92, "text": " So it has quite a simple command line interface where you can control most of the flags you", "tokens": [407, 309, 575, 1596, 257, 2199, 5622, 1622, 9226, 689, 291, 393, 1969, 881, 295, 264, 23265, 291], "temperature": 0.0, "avg_logprob": -0.10622426083213404, "compression_ratio": 1.6600790513833992, "no_speech_prob": 8.007023279787973e-05}, {"id": 80, "seek": 38736, "start": 407.92, "end": 412.32, "text": " will need in your day-to-day life.", "tokens": [486, 643, 294, 428, 786, 12, 1353, 12, 810, 993, 13], "temperature": 0.0, "avg_logprob": -0.10622426083213404, "compression_ratio": 1.6600790513833992, "no_speech_prob": 8.007023279787973e-05}, {"id": 81, "seek": 38736, "start": 412.32, "end": 417.0, "text": " So you choose the action you want to do, submit, monitor, or kill.", "tokens": [407, 291, 2826, 264, 3069, 291, 528, 281, 360, 11, 10315, 11, 6002, 11, 420, 1961, 13], "temperature": 0.0, "avg_logprob": -0.10622426083213404, "compression_ratio": 1.6600790513833992, "no_speech_prob": 8.007023279787973e-05}, {"id": 82, "seek": 41700, "start": 417.0, "end": 423.16, "text": " You give it a machine name which is defined in configuration.", "tokens": [509, 976, 309, 257, 3479, 1315, 597, 307, 7642, 294, 11694, 13], "temperature": 0.0, "avg_logprob": -0.13372396619132396, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.96411550254561e-05}, {"id": 83, "seek": 41700, "start": 423.16, "end": 428.48, "text": " Some options like the user, you tell it where to write the output file because that will", "tokens": [2188, 3956, 411, 264, 4195, 11, 291, 980, 309, 689, 281, 2464, 264, 5598, 3991, 570, 300, 486], "temperature": 0.0, "avg_logprob": -0.13372396619132396, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.96411550254561e-05}, {"id": 84, "seek": 41700, "start": 428.48, "end": 431.24, "text": " stay on the server.", "tokens": [1754, 322, 264, 7154, 13], "temperature": 0.0, "avg_logprob": -0.13372396619132396, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.96411550254561e-05}, {"id": 85, "seek": 41700, "start": 431.24, "end": 437.24, "text": " And it serves as a reference if you want to copy some other files, they would be put", "tokens": [400, 309, 13451, 382, 257, 6408, 498, 291, 528, 281, 5055, 512, 661, 7098, 11, 436, 576, 312, 829], "temperature": 0.0, "avg_logprob": -0.13372396619132396, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.96411550254561e-05}, {"id": 86, "seek": 41700, "start": 437.24, "end": 438.72, "text": " alongside this one.", "tokens": [12385, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.13372396619132396, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.96411550254561e-05}, {"id": 87, "seek": 41700, "start": 438.72, "end": 444.44, "text": " And so here you can see the log below that shows the commands that would be actually", "tokens": [400, 370, 510, 291, 393, 536, 264, 3565, 2507, 300, 3110, 264, 16901, 300, 576, 312, 767], "temperature": 0.0, "avg_logprob": -0.13372396619132396, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.96411550254561e-05}, {"id": 88, "seek": 44444, "start": 444.44, "end": 447.8, "text": " executed when doing that.", "tokens": [17577, 562, 884, 300, 13], "temperature": 0.0, "avg_logprob": -0.14843488350892678, "compression_ratio": 1.481081081081081, "no_speech_prob": 4.865042501478456e-05}, {"id": 89, "seek": 44444, "start": 447.8, "end": 451.84, "text": " So as I said, everything that is configurable.", "tokens": [407, 382, 286, 848, 11, 1203, 300, 307, 22192, 712, 13], "temperature": 0.0, "avg_logprob": -0.14843488350892678, "compression_ratio": 1.481081081081081, "no_speech_prob": 4.865042501478456e-05}, {"id": 90, "seek": 44444, "start": 451.84, "end": 458.16, "text": " So each site has a name to identify it on the command line.", "tokens": [407, 1184, 3621, 575, 257, 1315, 281, 5876, 309, 322, 264, 5622, 1622, 13], "temperature": 0.0, "avg_logprob": -0.14843488350892678, "compression_ratio": 1.481081081081081, "no_speech_prob": 4.865042501478456e-05}, {"id": 91, "seek": 44444, "start": 458.16, "end": 464.84, "text": " And then you define the connection type, local, SSH, whatever you want to add, a type.", "tokens": [400, 550, 291, 6964, 264, 4984, 2010, 11, 2654, 11, 12238, 39, 11, 2035, 291, 528, 281, 909, 11, 257, 2010, 13], "temperature": 0.0, "avg_logprob": -0.14843488350892678, "compression_ratio": 1.481081081081081, "no_speech_prob": 4.865042501478456e-05}, {"id": 92, "seek": 44444, "start": 464.84, "end": 469.56, "text": " So for now we support direct execution, slum, and PBS.", "tokens": [407, 337, 586, 321, 1406, 2047, 15058, 11, 1061, 449, 11, 293, 33517, 13], "temperature": 0.0, "avg_logprob": -0.14843488350892678, "compression_ratio": 1.481081081081081, "no_speech_prob": 4.865042501478456e-05}, {"id": 93, "seek": 46956, "start": 469.56, "end": 474.4, "text": " And then you can add some hooks, for instance, oh, yeah, before I start doing anything, check", "tokens": [400, 550, 291, 393, 909, 512, 26485, 11, 337, 5197, 11, 1954, 11, 1338, 11, 949, 286, 722, 884, 1340, 11, 1520], "temperature": 0.0, "avg_logprob": -0.1309000946754633, "compression_ratio": 1.669811320754717, "no_speech_prob": 0.00028879111050628126}, {"id": 94, "seek": 46956, "start": 474.4, "end": 480.08, "text": " the connection just to see whether it will actually work, or, oh, yeah, before submitting", "tokens": [264, 4984, 445, 281, 536, 1968, 309, 486, 767, 589, 11, 420, 11, 1954, 11, 1338, 11, 949, 31836], "temperature": 0.0, "avg_logprob": -0.1309000946754633, "compression_ratio": 1.669811320754717, "no_speech_prob": 0.00028879111050628126}, {"id": 95, "seek": 46956, "start": 480.08, "end": 488.8, "text": " the script, just make sure the directory containing the output file exists, or once", "tokens": [264, 5755, 11, 445, 652, 988, 264, 21120, 19273, 264, 5598, 3991, 8198, 11, 420, 1564], "temperature": 0.0, "avg_logprob": -0.1309000946754633, "compression_ratio": 1.669811320754717, "no_speech_prob": 0.00028879111050628126}, {"id": 96, "seek": 46956, "start": 488.8, "end": 494.96, "text": " the job is submitted, copy the log file to the server so that we can see everything in", "tokens": [264, 1691, 307, 14405, 11, 5055, 264, 3565, 3991, 281, 264, 7154, 370, 300, 321, 393, 536, 1203, 294], "temperature": 0.0, "avg_logprob": -0.1309000946754633, "compression_ratio": 1.669811320754717, "no_speech_prob": 0.00028879111050628126}, {"id": 97, "seek": 49496, "start": 494.96, "end": 501.0, "text": " the same place rather than having files scattered around every system.", "tokens": [264, 912, 1081, 2831, 813, 1419, 7098, 21986, 926, 633, 1185, 13], "temperature": 0.0, "avg_logprob": -0.13082543781825473, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.0001858909527072683}, {"id": 98, "seek": 49496, "start": 501.0, "end": 506.88, "text": " And so that's all good, but just having an alias to SBatch that does it remotely is not", "tokens": [400, 370, 300, 311, 439, 665, 11, 457, 445, 1419, 364, 419, 4609, 281, 26944, 852, 300, 775, 309, 20824, 307, 406], "temperature": 0.0, "avg_logprob": -0.13082543781825473, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.0001858909527072683}, {"id": 99, "seek": 49496, "start": 506.88, "end": 507.88, "text": " really helpful.", "tokens": [534, 4961, 13], "temperature": 0.0, "avg_logprob": -0.13082543781825473, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.0001858909527072683}, {"id": 100, "seek": 49496, "start": 507.88, "end": 515.1999999999999, "text": " So we need also to modify the job script to add some options that are understandable by", "tokens": [407, 321, 643, 611, 281, 16927, 264, 1691, 5755, 281, 909, 512, 3956, 300, 366, 25648, 538], "temperature": 0.0, "avg_logprob": -0.13082543781825473, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.0001858909527072683}, {"id": 101, "seek": 49496, "start": 515.1999999999999, "end": 517.0, "text": " the submission system.", "tokens": [264, 23689, 1185, 13], "temperature": 0.0, "avg_logprob": -0.13082543781825473, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.0001858909527072683}, {"id": 102, "seek": 51700, "start": 517.0, "end": 527.12, "text": " So for that we decided to have a new language, because obviously the directives are not interoperable", "tokens": [407, 337, 300, 321, 3047, 281, 362, 257, 777, 2856, 11, 570, 2745, 264, 2047, 1539, 366, 406, 728, 7192, 712], "temperature": 0.0, "avg_logprob": -0.14568782621814358, "compression_ratio": 1.4709302325581395, "no_speech_prob": 3.869572901749052e-05}, {"id": 103, "seek": 51700, "start": 527.12, "end": 530.84, "text": " across submission systems.", "tokens": [2108, 23689, 3652, 13], "temperature": 0.0, "avg_logprob": -0.14568782621814358, "compression_ratio": 1.4709302325581395, "no_speech_prob": 3.869572901749052e-05}, {"id": 104, "seek": 51700, "start": 530.84, "end": 534.52, "text": " And so we need some kind of translation.", "tokens": [400, 370, 321, 643, 512, 733, 295, 12853, 13], "temperature": 0.0, "avg_logprob": -0.14568782621814358, "compression_ratio": 1.4709302325581395, "no_speech_prob": 3.869572901749052e-05}, {"id": 105, "seek": 51700, "start": 534.52, "end": 542.72, "text": " We input some generic directives, and we can add some in the configuration as well.", "tokens": [492, 4846, 512, 19577, 2047, 1539, 11, 293, 321, 393, 909, 512, 294, 264, 11694, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.14568782621814358, "compression_ratio": 1.4709302325581395, "no_speech_prob": 3.869572901749052e-05}, {"id": 106, "seek": 54272, "start": 542.72, "end": 550.6800000000001, "text": " And then we translate them, so either for things very simple, like, oh, yeah, the output", "tokens": [400, 550, 321, 13799, 552, 11, 370, 2139, 337, 721, 588, 2199, 11, 411, 11, 1954, 11, 1338, 11, 264, 5598], "temperature": 0.0, "avg_logprob": -0.16911539973982845, "compression_ratio": 1.5853658536585367, "no_speech_prob": 7.072157313814387e-05}, {"id": 107, "seek": 54272, "start": 550.6800000000001, "end": 558.1600000000001, "text": " file in PBS is minus O in slum, it's minus, minus output.", "tokens": [3991, 294, 33517, 307, 3175, 422, 294, 1061, 449, 11, 309, 311, 3175, 11, 3175, 5598, 13], "temperature": 0.0, "avg_logprob": -0.16911539973982845, "compression_ratio": 1.5853658536585367, "no_speech_prob": 7.072157313814387e-05}, {"id": 108, "seek": 54272, "start": 558.1600000000001, "end": 564.8000000000001, "text": " So this kind of translation could have also plugins that compute resources, like if someone", "tokens": [407, 341, 733, 295, 12853, 727, 362, 611, 33759, 300, 14722, 3593, 11, 411, 498, 1580], "temperature": 0.0, "avg_logprob": -0.16911539973982845, "compression_ratio": 1.5853658536585367, "no_speech_prob": 7.072157313814387e-05}, {"id": 109, "seek": 54272, "start": 564.8000000000001, "end": 568.1600000000001, "text": " gives you the number of nodes and the number of tasks per node, and you need the total", "tokens": [2709, 291, 264, 1230, 295, 13891, 293, 264, 1230, 295, 9608, 680, 9984, 11, 293, 291, 643, 264, 3217], "temperature": 0.0, "avg_logprob": -0.16911539973982845, "compression_ratio": 1.5853658536585367, "no_speech_prob": 7.072157313814387e-05}, {"id": 110, "seek": 56816, "start": 568.16, "end": 574.48, "text": " number of tasks, things like that, so you could add plugins, or if you have some specific", "tokens": [1230, 295, 9608, 11, 721, 411, 300, 11, 370, 291, 727, 909, 33759, 11, 420, 498, 291, 362, 512, 2685], "temperature": 0.0, "avg_logprob": -0.1498027199193051, "compression_ratio": 1.5974025974025974, "no_speech_prob": 0.00010196532093686983}, {"id": 111, "seek": 56816, "start": 574.48, "end": 579.9599999999999, "text": " resource management in your HPC, you can add that as well.", "tokens": [7684, 4592, 294, 428, 12557, 34, 11, 291, 393, 909, 300, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.1498027199193051, "compression_ratio": 1.5974025974025974, "no_speech_prob": 0.00010196532093686983}, {"id": 112, "seek": 56816, "start": 579.9599999999999, "end": 584.64, "text": " And then on the output side, we have a generator that's site-specific, again, because we need", "tokens": [400, 550, 322, 264, 5598, 1252, 11, 321, 362, 257, 19265, 300, 311, 3621, 12, 29258, 11, 797, 11, 570, 321, 643], "temperature": 0.0, "avg_logprob": -0.1498027199193051, "compression_ratio": 1.5974025974025974, "no_speech_prob": 0.00010196532093686983}, {"id": 113, "seek": 56816, "start": 584.64, "end": 588.28, "text": " to adapt the directives to the system.", "tokens": [281, 6231, 264, 2047, 1539, 281, 264, 1185, 13], "temperature": 0.0, "avg_logprob": -0.1498027199193051, "compression_ratio": 1.5974025974025974, "no_speech_prob": 0.00010196532093686983}, {"id": 114, "seek": 56816, "start": 588.28, "end": 593.12, "text": " It can make the last few translations, for instance, the actual syntax of some options,", "tokens": [467, 393, 652, 264, 1036, 1326, 37578, 11, 337, 5197, 11, 264, 3539, 28431, 295, 512, 3956, 11], "temperature": 0.0, "avg_logprob": -0.1498027199193051, "compression_ratio": 1.5974025974025974, "no_speech_prob": 0.00010196532093686983}, {"id": 115, "seek": 59312, "start": 593.12, "end": 600.5600000000001, "text": " like mail options, most submission systems allow you to specify an email address to which", "tokens": [411, 10071, 3956, 11, 881, 23689, 3652, 2089, 291, 281, 16500, 364, 3796, 2985, 281, 597], "temperature": 0.0, "avg_logprob": -0.12968712636869248, "compression_ratio": 1.5678391959798994, "no_speech_prob": 0.00014155442477203906}, {"id": 116, "seek": 59312, "start": 600.5600000000001, "end": 604.24, "text": " send an email for some of your tasks.", "tokens": [2845, 364, 3796, 337, 512, 295, 428, 9608, 13], "temperature": 0.0, "avg_logprob": -0.12968712636869248, "compression_ratio": 1.5678391959798994, "no_speech_prob": 0.00014155442477203906}, {"id": 117, "seek": 59312, "start": 604.24, "end": 609.48, "text": " Only the syntax is slightly different for everyone, so it does that translation, and", "tokens": [5686, 264, 28431, 307, 4748, 819, 337, 1518, 11, 370, 309, 775, 300, 12853, 11, 293], "temperature": 0.0, "avg_logprob": -0.12968712636869248, "compression_ratio": 1.5678391959798994, "no_speech_prob": 0.00014155442477203906}, {"id": 118, "seek": 59312, "start": 609.48, "end": 618.24, "text": " it's able to add code, if you need, for instance, to define environment variables in your software.", "tokens": [309, 311, 1075, 281, 909, 3089, 11, 498, 291, 643, 11, 337, 5197, 11, 281, 6964, 2823, 9102, 294, 428, 4722, 13], "temperature": 0.0, "avg_logprob": -0.12968712636869248, "compression_ratio": 1.5678391959798994, "no_speech_prob": 0.00014155442477203906}, {"id": 119, "seek": 61824, "start": 618.24, "end": 625.36, "text": " So the main components that are extensible in Troika are, as I said, the interaction", "tokens": [407, 264, 2135, 6677, 300, 366, 1279, 30633, 294, 19406, 5439, 366, 11, 382, 286, 848, 11, 264, 9285], "temperature": 0.0, "avg_logprob": -0.1366659535302056, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.381434650393203e-05}, {"id": 120, "seek": 61824, "start": 625.36, "end": 627.2, "text": " with the queuing system.", "tokens": [365, 264, 631, 9635, 1185, 13], "temperature": 0.0, "avg_logprob": -0.1366659535302056, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.381434650393203e-05}, {"id": 121, "seek": 61824, "start": 627.2, "end": 635.44, "text": " So you have a parser that reads the native directives so that you can use them if you", "tokens": [407, 291, 362, 257, 21156, 260, 300, 15700, 264, 8470, 2047, 1539, 370, 300, 291, 393, 764, 552, 498, 291], "temperature": 0.0, "avg_logprob": -0.1366659535302056, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.381434650393203e-05}, {"id": 122, "seek": 61824, "start": 635.44, "end": 643.5600000000001, "text": " need them for your processing, generates the job script, it runs the appropriate commands,", "tokens": [643, 552, 337, 428, 9007, 11, 23815, 264, 1691, 5755, 11, 309, 6676, 264, 6854, 16901, 11], "temperature": 0.0, "avg_logprob": -0.1366659535302056, "compression_ratio": 1.5714285714285714, "no_speech_prob": 6.381434650393203e-05}, {"id": 123, "seek": 64356, "start": 643.56, "end": 655.4799999999999, "text": " so either using QSOB, SBATCH, or whatever, it could use APIs if you have another system.", "tokens": [370, 2139, 1228, 1249, 17188, 33, 11, 26944, 2218, 5462, 11, 420, 2035, 11, 309, 727, 764, 21445, 498, 291, 362, 1071, 1185, 13], "temperature": 0.0, "avg_logprob": -0.16482627868652344, "compression_ratio": 1.573394495412844, "no_speech_prob": 4.409482062328607e-05}, {"id": 124, "seek": 64356, "start": 655.4799999999999, "end": 660.52, "text": " And it can also keep track of the submission, so most of the time, just keeping a job ID", "tokens": [400, 309, 393, 611, 1066, 2837, 295, 264, 23689, 11, 370, 881, 295, 264, 565, 11, 445, 5145, 257, 1691, 7348], "temperature": 0.0, "avg_logprob": -0.16482627868652344, "compression_ratio": 1.573394495412844, "no_speech_prob": 4.409482062328607e-05}, {"id": 125, "seek": 64356, "start": 660.52, "end": 666.2399999999999, "text": " so that if you want to monitor the task, you just say, oh, yeah, the script was this, and", "tokens": [370, 300, 498, 291, 528, 281, 6002, 264, 5633, 11, 291, 445, 584, 11, 1954, 11, 1338, 11, 264, 5755, 390, 341, 11, 293], "temperature": 0.0, "avg_logprob": -0.16482627868652344, "compression_ratio": 1.573394495412844, "no_speech_prob": 4.409482062328607e-05}, {"id": 126, "seek": 64356, "start": 666.2399999999999, "end": 671.76, "text": " Troika will know, oh, yeah, put the job ID in that file next to the script.", "tokens": [19406, 5439, 486, 458, 11, 1954, 11, 1338, 11, 829, 264, 1691, 7348, 294, 300, 3991, 958, 281, 264, 5755, 13], "temperature": 0.0, "avg_logprob": -0.16482627868652344, "compression_ratio": 1.573394495412844, "no_speech_prob": 4.409482062328607e-05}, {"id": 127, "seek": 67176, "start": 671.76, "end": 676.4, "text": " I don't need you to tell me where it is.", "tokens": [286, 500, 380, 643, 291, 281, 980, 385, 689, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.12846181233723958, "compression_ratio": 1.5414364640883977, "no_speech_prob": 0.00023129522742237896}, {"id": 128, "seek": 67176, "start": 676.4, "end": 682.84, "text": " And so you can choose how you want to interact and define new interfaces if you want.", "tokens": [400, 370, 291, 393, 2826, 577, 291, 528, 281, 4648, 293, 6964, 777, 28416, 498, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.12846181233723958, "compression_ratio": 1.5414364640883977, "no_speech_prob": 0.00023129522742237896}, {"id": 129, "seek": 67176, "start": 682.84, "end": 684.2, "text": " Same for the connection.", "tokens": [10635, 337, 264, 4984, 13], "temperature": 0.0, "avg_logprob": -0.12846181233723958, "compression_ratio": 1.5414364640883977, "no_speech_prob": 0.00023129522742237896}, {"id": 130, "seek": 67176, "start": 684.2, "end": 690.4399999999999, "text": " So the connection mostly does the running of commands on the remote system.", "tokens": [407, 264, 4984, 5240, 775, 264, 2614, 295, 16901, 322, 264, 8607, 1185, 13], "temperature": 0.0, "avg_logprob": -0.12846181233723958, "compression_ratio": 1.5414364640883977, "no_speech_prob": 0.00023129522742237896}, {"id": 131, "seek": 67176, "start": 690.4399999999999, "end": 696.4399999999999, "text": " It's able to copy files over, if needed, both ways.", "tokens": [467, 311, 1075, 281, 5055, 7098, 670, 11, 498, 2978, 11, 1293, 2098, 13], "temperature": 0.0, "avg_logprob": -0.12846181233723958, "compression_ratio": 1.5414364640883977, "no_speech_prob": 0.00023129522742237896}, {"id": 132, "seek": 69644, "start": 696.44, "end": 704.5600000000001, "text": " And you can have some hooks at various points at start-up just before submitting, just after", "tokens": [400, 291, 393, 362, 512, 26485, 412, 3683, 2793, 412, 722, 12, 1010, 445, 949, 31836, 11, 445, 934], "temperature": 0.0, "avg_logprob": -0.17429991995934213, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.00010366387868998572}, {"id": 133, "seek": 69644, "start": 704.5600000000001, "end": 708.48, "text": " killing a job, for instance, if you want to tell a workflow manager that, oh, this task", "tokens": [8011, 257, 1691, 11, 337, 5197, 11, 498, 291, 528, 281, 980, 257, 20993, 6598, 300, 11, 1954, 11, 341, 5633], "temperature": 0.0, "avg_logprob": -0.17429991995934213, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.00010366387868998572}, {"id": 134, "seek": 69644, "start": 708.48, "end": 714.5600000000001, "text": " doesn't exist anymore, I just killed it, or at exit if you want to move your log files", "tokens": [1177, 380, 2514, 3602, 11, 286, 445, 4652, 309, 11, 420, 412, 11043, 498, 291, 528, 281, 1286, 428, 3565, 7098], "temperature": 0.0, "avg_logprob": -0.17429991995934213, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.00010366387868998572}, {"id": 135, "seek": 69644, "start": 714.5600000000001, "end": 716.6400000000001, "text": " around, for instance.", "tokens": [926, 11, 337, 5197, 13], "temperature": 0.0, "avg_logprob": -0.17429991995934213, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.00010366387868998572}, {"id": 136, "seek": 69644, "start": 716.6400000000001, "end": 721.0, "text": " And that allows you to perform extractions.", "tokens": [400, 300, 4045, 291, 281, 2042, 8947, 626, 13], "temperature": 0.0, "avg_logprob": -0.17429991995934213, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.00010366387868998572}, {"id": 137, "seek": 69644, "start": 721.0, "end": 724.7800000000001, "text": " And then the last thing you can customize is the translation.", "tokens": [400, 550, 264, 1036, 551, 291, 393, 19734, 307, 264, 12853, 13], "temperature": 0.0, "avg_logprob": -0.17429991995934213, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.00010366387868998572}, {"id": 138, "seek": 72478, "start": 724.78, "end": 730.92, "text": " So if you want to generate more directives than the user provided, you can also do it.", "tokens": [407, 498, 291, 528, 281, 8460, 544, 2047, 1539, 813, 264, 4195, 5649, 11, 291, 393, 611, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.11002773485685649, "compression_ratio": 1.5669642857142858, "no_speech_prob": 9.431521903024986e-05}, {"id": 139, "seek": 72478, "start": 730.92, "end": 737.12, "text": " And basically, you just pass a function that takes the input set of directives and updates", "tokens": [400, 1936, 11, 291, 445, 1320, 257, 2445, 300, 2516, 264, 4846, 992, 295, 2047, 1539, 293, 9205], "temperature": 0.0, "avg_logprob": -0.11002773485685649, "compression_ratio": 1.5669642857142858, "no_speech_prob": 9.431521903024986e-05}, {"id": 140, "seek": 72478, "start": 737.12, "end": 741.56, "text": " that set to whatever you need.", "tokens": [300, 992, 281, 2035, 291, 643, 13], "temperature": 0.0, "avg_logprob": -0.11002773485685649, "compression_ratio": 1.5669642857142858, "no_speech_prob": 9.431521903024986e-05}, {"id": 141, "seek": 72478, "start": 741.56, "end": 747.24, "text": " So as a bit of a success story for us, so we've just switched to a new HPC with a new", "tokens": [407, 382, 257, 857, 295, 257, 2245, 1657, 337, 505, 11, 370, 321, 600, 445, 16858, 281, 257, 777, 12557, 34, 365, 257, 777], "temperature": 0.0, "avg_logprob": -0.11002773485685649, "compression_ratio": 1.5669642857142858, "no_speech_prob": 9.431521903024986e-05}, {"id": 142, "seek": 72478, "start": 747.24, "end": 753.12, "text": " set of EC flow server VMs, new location, new everything.", "tokens": [992, 295, 19081, 3095, 7154, 18038, 82, 11, 777, 4914, 11, 777, 1203, 13], "temperature": 0.0, "avg_logprob": -0.11002773485685649, "compression_ratio": 1.5669642857142858, "no_speech_prob": 9.431521903024986e-05}, {"id": 143, "seek": 75312, "start": 753.12, "end": 758.96, "text": " So it's much simpler to actually be able to just change a config file rather than rewrite", "tokens": [407, 309, 311, 709, 18587, 281, 767, 312, 1075, 281, 445, 1319, 257, 6662, 3991, 2831, 813, 28132], "temperature": 0.0, "avg_logprob": -0.1271688776108825, "compression_ratio": 1.75, "no_speech_prob": 0.00010444050712976605}, {"id": 144, "seek": 75312, "start": 758.96, "end": 763.84, "text": " a whole shell script that does all the submission for us.", "tokens": [257, 1379, 8720, 5755, 300, 775, 439, 264, 23689, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.1271688776108825, "compression_ratio": 1.75, "no_speech_prob": 0.00010444050712976605}, {"id": 145, "seek": 75312, "start": 763.84, "end": 768.16, "text": " And also, since we have lots of different users, they have different needs, they have", "tokens": [400, 611, 11, 1670, 321, 362, 3195, 295, 819, 5022, 11, 436, 362, 819, 2203, 11, 436, 362], "temperature": 0.0, "avg_logprob": -0.1271688776108825, "compression_ratio": 1.75, "no_speech_prob": 0.00010444050712976605}, {"id": 146, "seek": 75312, "start": 768.16, "end": 770.08, "text": " different ways of working.", "tokens": [819, 2098, 295, 1364, 13], "temperature": 0.0, "avg_logprob": -0.1271688776108825, "compression_ratio": 1.75, "no_speech_prob": 0.00010444050712976605}, {"id": 147, "seek": 75312, "start": 770.08, "end": 775.96, "text": " And what we managed to do with Troika is that we managed to bring them all together to use", "tokens": [400, 437, 321, 6453, 281, 360, 365, 19406, 5439, 307, 300, 321, 6453, 281, 1565, 552, 439, 1214, 281, 764], "temperature": 0.0, "avg_logprob": -0.1271688776108825, "compression_ratio": 1.75, "no_speech_prob": 0.00010444050712976605}, {"id": 148, "seek": 75312, "start": 775.96, "end": 782.16, "text": " a single tool, which runs the operational workflows where they need to have tight control", "tokens": [257, 2167, 2290, 11, 597, 6676, 264, 16607, 43461, 689, 436, 643, 281, 362, 4524, 1969], "temperature": 0.0, "avg_logprob": -0.1271688776108825, "compression_ratio": 1.75, "no_speech_prob": 0.00010444050712976605}, {"id": 149, "seek": 78216, "start": 782.16, "end": 787.04, "text": " over what they actually submit and all the options.", "tokens": [670, 437, 436, 767, 10315, 293, 439, 264, 3956, 13], "temperature": 0.0, "avg_logprob": -0.14906486551812353, "compression_ratio": 1.6104417670682731, "no_speech_prob": 0.00019156061171088368}, {"id": 150, "seek": 78216, "start": 787.04, "end": 792.68, "text": " Research workflows, which need to be very flexible because every researcher might have", "tokens": [10303, 43461, 11, 597, 643, 281, 312, 588, 11358, 570, 633, 21751, 1062, 362], "temperature": 0.0, "avg_logprob": -0.14906486551812353, "compression_ratio": 1.6104417670682731, "no_speech_prob": 0.00019156061171088368}, {"id": 151, "seek": 78216, "start": 792.68, "end": 797.24, "text": " their own specific needs, but in the end, they run mostly the same kind of code.", "tokens": [641, 1065, 2685, 2203, 11, 457, 294, 264, 917, 11, 436, 1190, 5240, 264, 912, 733, 295, 3089, 13], "temperature": 0.0, "avg_logprob": -0.14906486551812353, "compression_ratio": 1.6104417670682731, "no_speech_prob": 0.00019156061171088368}, {"id": 152, "seek": 78216, "start": 797.24, "end": 802.4, "text": " So we need to have an interface that allows that.", "tokens": [407, 321, 643, 281, 362, 364, 9226, 300, 4045, 300, 13], "temperature": 0.0, "avg_logprob": -0.14906486551812353, "compression_ratio": 1.6104417670682731, "no_speech_prob": 0.00019156061171088368}, {"id": 153, "seek": 78216, "start": 802.4, "end": 805.3199999999999, "text": " And then we run also general purpose servers.", "tokens": [400, 550, 321, 1190, 611, 2674, 4334, 15909, 13], "temperature": 0.0, "avg_logprob": -0.14906486551812353, "compression_ratio": 1.6104417670682731, "no_speech_prob": 0.00019156061171088368}, {"id": 154, "seek": 78216, "start": 805.3199999999999, "end": 810.56, "text": " If someone has a data processing pipeline, for instance, they can just spawn a server", "tokens": [759, 1580, 575, 257, 1412, 9007, 15517, 11, 337, 5197, 11, 436, 393, 445, 17088, 257, 7154], "temperature": 0.0, "avg_logprob": -0.14906486551812353, "compression_ratio": 1.6104417670682731, "no_speech_prob": 0.00019156061171088368}, {"id": 155, "seek": 81056, "start": 810.56, "end": 813.4799999999999, "text": " and do their work.", "tokens": [293, 360, 641, 589, 13], "temperature": 0.0, "avg_logprob": -0.13845143494782625, "compression_ratio": 1.5761316872427984, "no_speech_prob": 0.00016073501319624484}, {"id": 156, "seek": 81056, "start": 813.4799999999999, "end": 818.1199999999999, "text": " And that needs to have an easy to use interface because we don't want to teach people, oh,", "tokens": [400, 300, 2203, 281, 362, 364, 1858, 281, 764, 9226, 570, 321, 500, 380, 528, 281, 2924, 561, 11, 1954, 11], "temperature": 0.0, "avg_logprob": -0.13845143494782625, "compression_ratio": 1.5761316872427984, "no_speech_prob": 0.00016073501319624484}, {"id": 157, "seek": 81056, "start": 818.1199999999999, "end": 822.04, "text": " yeah, you also need to know that to run your job.", "tokens": [1338, 11, 291, 611, 643, 281, 458, 300, 281, 1190, 428, 1691, 13], "temperature": 0.0, "avg_logprob": -0.13845143494782625, "compression_ratio": 1.5761316872427984, "no_speech_prob": 0.00016073501319624484}, {"id": 158, "seek": 81056, "start": 822.04, "end": 827.64, "text": " So now what we do is we provide them with VMs where Troika is pre-installed, and many", "tokens": [407, 586, 437, 321, 360, 307, 321, 2893, 552, 365, 18038, 82, 689, 19406, 5439, 307, 659, 12, 13911, 8907, 11, 293, 867], "temperature": 0.0, "avg_logprob": -0.13845143494782625, "compression_ratio": 1.5761316872427984, "no_speech_prob": 0.00016073501319624484}, {"id": 159, "seek": 81056, "start": 827.64, "end": 832.1199999999999, "text": " of them just don't even notice that it's there.", "tokens": [295, 552, 445, 500, 380, 754, 3449, 300, 309, 311, 456, 13], "temperature": 0.0, "avg_logprob": -0.13845143494782625, "compression_ratio": 1.5761316872427984, "no_speech_prob": 0.00016073501319624484}, {"id": 160, "seek": 81056, "start": 832.1199999999999, "end": 838.52, "text": " And as a summary, so I said at the beginning that we handle about half a million jobs per", "tokens": [400, 382, 257, 12691, 11, 370, 286, 848, 412, 264, 2863, 300, 321, 4813, 466, 1922, 257, 2459, 4782, 680], "temperature": 0.0, "avg_logprob": -0.13845143494782625, "compression_ratio": 1.5761316872427984, "no_speech_prob": 0.00016073501319624484}, {"id": 161, "seek": 83852, "start": 838.52, "end": 845.96, "text": " day, and most of them now pass through Troika, and it hasn't failed yet, so hopefully it", "tokens": [786, 11, 293, 881, 295, 552, 586, 1320, 807, 19406, 5439, 11, 293, 309, 6132, 380, 7612, 1939, 11, 370, 4696, 309], "temperature": 0.0, "avg_logprob": -0.15318822272029925, "compression_ratio": 1.5137614678899083, "no_speech_prob": 8.907362644094974e-05}, {"id": 162, "seek": 83852, "start": 845.96, "end": 848.12, "text": " works well enough.", "tokens": [1985, 731, 1547, 13], "temperature": 0.0, "avg_logprob": -0.15318822272029925, "compression_ratio": 1.5137614678899083, "no_speech_prob": 8.907362644094974e-05}, {"id": 163, "seek": 83852, "start": 848.12, "end": 853.96, "text": " What it will help us with also going forward is supporting our software development.", "tokens": [708, 309, 486, 854, 505, 365, 611, 516, 2128, 307, 7231, 527, 4722, 3250, 13], "temperature": 0.0, "avg_logprob": -0.15318822272029925, "compression_ratio": 1.5137614678899083, "no_speech_prob": 8.907362644094974e-05}, {"id": 164, "seek": 83852, "start": 853.96, "end": 858.6, "text": " So it's not necessarily tied to a workflow manager.", "tokens": [407, 309, 311, 406, 4725, 9601, 281, 257, 20993, 6598, 13], "temperature": 0.0, "avg_logprob": -0.15318822272029925, "compression_ratio": 1.5137614678899083, "no_speech_prob": 8.907362644094974e-05}, {"id": 165, "seek": 83852, "start": 858.6, "end": 865.24, "text": " We want to control our CI CD pipeline also using that because some of the elements of", "tokens": [492, 528, 281, 1969, 527, 37777, 6743, 15517, 611, 1228, 300, 570, 512, 295, 264, 4959, 295], "temperature": 0.0, "avg_logprob": -0.15318822272029925, "compression_ratio": 1.5137614678899083, "no_speech_prob": 8.907362644094974e-05}, {"id": 166, "seek": 86524, "start": 865.24, "end": 868.6800000000001, "text": " the pipeline have to run on our HPC system.", "tokens": [264, 15517, 362, 281, 1190, 322, 527, 12557, 34, 1185, 13], "temperature": 0.0, "avg_logprob": -0.11759063685051749, "compression_ratio": 1.592741935483871, "no_speech_prob": 9.976246656151488e-05}, {"id": 167, "seek": 86524, "start": 868.6800000000001, "end": 874.12, "text": " So basically what we could do is from a GitHub runner, we could use Troika to connect to", "tokens": [407, 1936, 437, 321, 727, 360, 307, 490, 257, 23331, 24376, 11, 321, 727, 764, 19406, 5439, 281, 1745, 281], "temperature": 0.0, "avg_logprob": -0.11759063685051749, "compression_ratio": 1.592741935483871, "no_speech_prob": 9.976246656151488e-05}, {"id": 168, "seek": 86524, "start": 874.12, "end": 881.6, "text": " our HPC run jobs there to do testing, deployment, and everything.", "tokens": [527, 12557, 34, 1190, 4782, 456, 281, 360, 4997, 11, 19317, 11, 293, 1203, 13], "temperature": 0.0, "avg_logprob": -0.11759063685051749, "compression_ratio": 1.592741935483871, "no_speech_prob": 9.976246656151488e-05}, {"id": 169, "seek": 86524, "start": 881.6, "end": 886.12, "text": " We, as I said, run our in-house workflows, and we will continue to do that for the foreseeable", "tokens": [492, 11, 382, 286, 848, 11, 1190, 527, 294, 12, 6410, 43461, 11, 293, 321, 486, 2354, 281, 360, 300, 337, 264, 38736, 712], "temperature": 0.0, "avg_logprob": -0.11759063685051749, "compression_ratio": 1.592741935483871, "no_speech_prob": 9.976246656151488e-05}, {"id": 170, "seek": 86524, "start": 886.12, "end": 887.12, "text": " future.", "tokens": [2027, 13], "temperature": 0.0, "avg_logprob": -0.11759063685051749, "compression_ratio": 1.592741935483871, "no_speech_prob": 9.976246656151488e-05}, {"id": 171, "seek": 86524, "start": 887.12, "end": 893.5600000000001, "text": " It will help us to adapt to new HPC systems because every time we make a tender, any provider", "tokens": [467, 486, 854, 505, 281, 6231, 281, 777, 12557, 34, 3652, 570, 633, 565, 321, 652, 257, 15036, 11, 604, 12398], "temperature": 0.0, "avg_logprob": -0.11759063685051749, "compression_ratio": 1.592741935483871, "no_speech_prob": 9.976246656151488e-05}, {"id": 172, "seek": 89356, "start": 893.56, "end": 901.16, "text": " could answer, and we don't control which submission system we will end up with, and even which", "tokens": [727, 1867, 11, 293, 321, 500, 380, 1969, 597, 23689, 1185, 321, 486, 917, 493, 365, 11, 293, 754, 597], "temperature": 0.0, "avg_logprob": -0.13309341318467083, "compression_ratio": 1.478494623655914, "no_speech_prob": 0.00025187089340761304}, {"id": 173, "seek": 89356, "start": 901.16, "end": 906.5999999999999, "text": " site-specific variants there will be in the set of options.", "tokens": [3621, 12, 29258, 21669, 456, 486, 312, 294, 264, 992, 295, 3956, 13], "temperature": 0.0, "avg_logprob": -0.13309341318467083, "compression_ratio": 1.478494623655914, "no_speech_prob": 0.00025187089340761304}, {"id": 174, "seek": 89356, "start": 906.5999999999999, "end": 912.92, "text": " And then for destination Earth, as I mentioned before, we want to support multiple HPC with", "tokens": [400, 550, 337, 12236, 4755, 11, 382, 286, 2835, 949, 11, 321, 528, 281, 1406, 3866, 12557, 34, 365], "temperature": 0.0, "avg_logprob": -0.13309341318467083, "compression_ratio": 1.478494623655914, "no_speech_prob": 0.00025187089340761304}, {"id": 175, "seek": 89356, "start": 912.92, "end": 918.1199999999999, "text": " minimal changes to the code.", "tokens": [13206, 2962, 281, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.13309341318467083, "compression_ratio": 1.478494623655914, "no_speech_prob": 0.00025187089340761304}, {"id": 176, "seek": 91812, "start": 918.12, "end": 925.04, "text": " And so just to tell a bit more, where do you want, we want to go from here.", "tokens": [400, 370, 445, 281, 980, 257, 857, 544, 11, 689, 360, 291, 528, 11, 321, 528, 281, 352, 490, 510, 13], "temperature": 0.0, "avg_logprob": -0.15665857819305068, "compression_ratio": 1.6281407035175879, "no_speech_prob": 0.0001682437869021669}, {"id": 177, "seek": 91812, "start": 925.04, "end": 933.4, "text": " So we want to support more queuing systems because, I mean, we support two, and one of", "tokens": [407, 321, 528, 281, 1406, 544, 631, 9635, 3652, 570, 11, 286, 914, 11, 321, 1406, 732, 11, 293, 472, 295], "temperature": 0.0, "avg_logprob": -0.15665857819305068, "compression_ratio": 1.6281407035175879, "no_speech_prob": 0.0001682437869021669}, {"id": 178, "seek": 91812, "start": 933.4, "end": 938.52, "text": " them quite well because if we use it, the other one a bit less maybe.", "tokens": [552, 1596, 731, 570, 498, 321, 764, 309, 11, 264, 661, 472, 257, 857, 1570, 1310, 13], "temperature": 0.0, "avg_logprob": -0.15665857819305068, "compression_ratio": 1.6281407035175879, "no_speech_prob": 0.0001682437869021669}, {"id": 179, "seek": 91812, "start": 938.52, "end": 944.28, "text": " We want also to add functionality to inquire about the submission systems of, for instance,", "tokens": [492, 528, 611, 281, 909, 14980, 281, 13570, 621, 466, 264, 23689, 3652, 295, 11, 337, 5197, 11], "temperature": 0.0, "avg_logprob": -0.15665857819305068, "compression_ratio": 1.6281407035175879, "no_speech_prob": 0.0001682437869021669}, {"id": 180, "seek": 94428, "start": 944.28, "end": 949.1999999999999, "text": " which are the queues available, the petitions, things like that, so that the user doesn't", "tokens": [597, 366, 264, 631, 1247, 2435, 11, 264, 3817, 2451, 11, 721, 411, 300, 11, 370, 300, 264, 4195, 1177, 380], "temperature": 0.0, "avg_logprob": -0.18740576666754646, "compression_ratio": 1.5614973262032086, "no_speech_prob": 0.00018708742572925985}, {"id": 181, "seek": 94428, "start": 949.1999999999999, "end": 958.12, "text": " need to go to the server, check before running, like you could just run a command that fetches", "tokens": [643, 281, 352, 281, 264, 7154, 11, 1520, 949, 2614, 11, 411, 291, 727, 445, 1190, 257, 5622, 300, 15136, 3781], "temperature": 0.0, "avg_logprob": -0.18740576666754646, "compression_ratio": 1.5614973262032086, "no_speech_prob": 0.00018708742572925985}, {"id": 182, "seek": 94428, "start": 958.12, "end": 967.04, "text": " all the information in a useful way and gives it to you without, we're abstracting basically", "tokens": [439, 264, 1589, 294, 257, 4420, 636, 293, 2709, 309, 281, 291, 1553, 11, 321, 434, 12649, 278, 1936], "temperature": 0.0, "avg_logprob": -0.18740576666754646, "compression_ratio": 1.5614973262032086, "no_speech_prob": 0.00018708742572925985}, {"id": 183, "seek": 94428, "start": 967.04, "end": 970.24, "text": " the specifics.", "tokens": [264, 28454, 13], "temperature": 0.0, "avg_logprob": -0.18740576666754646, "compression_ratio": 1.5614973262032086, "no_speech_prob": 0.00018708742572925985}, {"id": 184, "seek": 97024, "start": 970.24, "end": 974.92, "text": " We also want to add some generic resource computation routines.", "tokens": [492, 611, 528, 281, 909, 512, 19577, 7684, 24903, 33827, 13], "temperature": 0.0, "avg_logprob": -0.12501256319941306, "compression_ratio": 1.6519823788546255, "no_speech_prob": 8.229475497500971e-05}, {"id": 185, "seek": 97024, "start": 974.92, "end": 980.84, "text": " So we have some in-house, but they are very tied to the way we function, and so there", "tokens": [407, 321, 362, 512, 294, 12, 6410, 11, 457, 436, 366, 588, 9601, 281, 264, 636, 321, 2445, 11, 293, 370, 456], "temperature": 0.0, "avg_logprob": -0.12501256319941306, "compression_ratio": 1.6519823788546255, "no_speech_prob": 8.229475497500971e-05}, {"id": 186, "seek": 97024, "start": 980.84, "end": 986.44, "text": " will be some work to make it more generic and then integrate it in the main source code", "tokens": [486, 312, 512, 589, 281, 652, 309, 544, 19577, 293, 550, 13365, 309, 294, 264, 2135, 4009, 3089], "temperature": 0.0, "avg_logprob": -0.12501256319941306, "compression_ratio": 1.6519823788546255, "no_speech_prob": 8.229475497500971e-05}, {"id": 187, "seek": 97024, "start": 986.44, "end": 990.08, "text": " rather than in a plugin.", "tokens": [2831, 813, 294, 257, 23407, 13], "temperature": 0.0, "avg_logprob": -0.12501256319941306, "compression_ratio": 1.6519823788546255, "no_speech_prob": 8.229475497500971e-05}, {"id": 188, "seek": 97024, "start": 990.08, "end": 995.24, "text": " And for improvements to the code, we want to improve script generation.", "tokens": [400, 337, 13797, 281, 264, 3089, 11, 321, 528, 281, 3470, 5755, 5125, 13], "temperature": 0.0, "avg_logprob": -0.12501256319941306, "compression_ratio": 1.6519823788546255, "no_speech_prob": 8.229475497500971e-05}, {"id": 189, "seek": 97024, "start": 995.24, "end": 998.4, "text": " For now it's a bit clunky, but it works.", "tokens": [1171, 586, 309, 311, 257, 857, 596, 25837, 11, 457, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.12501256319941306, "compression_ratio": 1.6519823788546255, "no_speech_prob": 8.229475497500971e-05}, {"id": 190, "seek": 99840, "start": 998.4, "end": 1005.4, "text": " We want to widen the coverage because you never test enough and provide packages to", "tokens": [492, 528, 281, 32552, 264, 9645, 570, 291, 1128, 1500, 1547, 293, 2893, 17401, 281], "temperature": 0.0, "avg_logprob": -0.27130808549768787, "compression_ratio": 1.398936170212766, "no_speech_prob": 0.00012235251779202372}, {"id": 191, "seek": 99840, "start": 1005.4, "end": 1012.28, "text": " install it on Debian-based machines, for instance, or RPMs for Red Hat systems, et cetera.", "tokens": [3625, 309, 322, 1346, 20196, 12, 6032, 8379, 11, 337, 5197, 11, 420, 14105, 26386, 337, 4477, 15867, 3652, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.27130808549768787, "compression_ratio": 1.398936170212766, "no_speech_prob": 0.00012235251779202372}, {"id": 192, "seek": 99840, "start": 1012.28, "end": 1017.52, "text": " And if you want to contribute, feel free to talk to me or go to our GitHub page and I'll", "tokens": [400, 498, 291, 528, 281, 10586, 11, 841, 1737, 281, 751, 281, 385, 420, 352, 281, 527, 23331, 3028, 293, 286, 603], "temperature": 0.0, "avg_logprob": -0.27130808549768787, "compression_ratio": 1.398936170212766, "no_speech_prob": 0.00012235251779202372}, {"id": 193, "seek": 101752, "start": 1017.52, "end": 1030.4, "text": " stop for now and take questions.", "tokens": [1590, 337, 586, 293, 747, 1651, 13], "temperature": 0.0, "avg_logprob": -0.3433172135126023, "compression_ratio": 1.2790697674418605, "no_speech_prob": 0.00029481129604391754}, {"id": 194, "seek": 101752, "start": 1030.4, "end": 1038.4, "text": " Hello, thanks for the presentation.", "tokens": [2425, 11, 3231, 337, 264, 5860, 13], "temperature": 0.0, "avg_logprob": -0.3433172135126023, "compression_ratio": 1.2790697674418605, "no_speech_prob": 0.00029481129604391754}, {"id": 195, "seek": 101752, "start": 1038.4, "end": 1044.44, "text": " So basically I've done something quite similar for my employer, sadly it cannot be open sourced,", "tokens": [407, 1936, 286, 600, 1096, 746, 1596, 2531, 337, 452, 16205, 11, 22023, 309, 2644, 312, 1269, 11006, 1232, 11], "temperature": 0.0, "avg_logprob": -0.3433172135126023, "compression_ratio": 1.2790697674418605, "no_speech_prob": 0.00029481129604391754}, {"id": 196, "seek": 104444, "start": 1044.44, "end": 1050.28, "text": " but the problem that we have is we have legacy clusters with legacy job submission systems.", "tokens": [457, 264, 1154, 300, 321, 362, 307, 321, 362, 11711, 23313, 365, 11711, 1691, 23689, 3652, 13], "temperature": 0.0, "avg_logprob": -0.16409871933308054, "compression_ratio": 1.7304347826086957, "no_speech_prob": 0.00034485652577131987}, {"id": 197, "seek": 104444, "start": 1050.28, "end": 1056.8400000000001, "text": " How did you manage to get the traction to migrate to Troika and to convince the user", "tokens": [1012, 630, 291, 3067, 281, 483, 264, 23558, 281, 31821, 281, 19406, 5439, 293, 281, 13447, 264, 4195], "temperature": 0.0, "avg_logprob": -0.16409871933308054, "compression_ratio": 1.7304347826086957, "no_speech_prob": 0.00034485652577131987}, {"id": 198, "seek": 104444, "start": 1056.8400000000001, "end": 1063.0, "text": " to port their jobs, their developments to this new system?", "tokens": [281, 2436, 641, 4782, 11, 641, 20862, 281, 341, 777, 1185, 30], "temperature": 0.0, "avg_logprob": -0.16409871933308054, "compression_ratio": 1.7304347826086957, "no_speech_prob": 0.00034485652577131987}, {"id": 199, "seek": 104444, "start": 1063.0, "end": 1067.16, "text": " So what we did first is that we made it as seamless as possible.", "tokens": [407, 437, 321, 630, 700, 307, 300, 321, 1027, 309, 382, 28677, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.16409871933308054, "compression_ratio": 1.7304347826086957, "no_speech_prob": 0.00034485652577131987}, {"id": 200, "seek": 104444, "start": 1067.16, "end": 1072.24, "text": " So if you want to interact with your job submission system without using our directives, you can.", "tokens": [407, 498, 291, 528, 281, 4648, 365, 428, 1691, 23689, 1185, 1553, 1228, 527, 2047, 1539, 11, 291, 393, 13], "temperature": 0.0, "avg_logprob": -0.16409871933308054, "compression_ratio": 1.7304347826086957, "no_speech_prob": 0.00034485652577131987}, {"id": 201, "seek": 107224, "start": 1072.24, "end": 1078.8, "text": " They will just pass through, but you lose on the generosity.", "tokens": [814, 486, 445, 1320, 807, 11, 457, 291, 3624, 322, 264, 30178, 13], "temperature": 0.0, "avg_logprob": -0.15367026329040528, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.00048267541569657624}, {"id": 202, "seek": 107224, "start": 1078.8, "end": 1087.2, "text": " And then what helped us is that we changed our HPC system, and that means we did basically", "tokens": [400, 550, 437, 4254, 505, 307, 300, 321, 3105, 527, 12557, 34, 1185, 11, 293, 300, 1355, 321, 630, 1936], "temperature": 0.0, "avg_logprob": -0.15367026329040528, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.00048267541569657624}, {"id": 203, "seek": 107224, "start": 1087.2, "end": 1092.16, "text": " start afresh and everyone had to make changes, so we just pushed that onto them.", "tokens": [722, 3238, 3644, 293, 1518, 632, 281, 652, 2962, 11, 370, 321, 445, 9152, 300, 3911, 552, 13], "temperature": 0.0, "avg_logprob": -0.15367026329040528, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.00048267541569657624}, {"id": 204, "seek": 107224, "start": 1092.16, "end": 1098.84, "text": " And I must say many of them have been happy because that meant we can do that for them", "tokens": [400, 286, 1633, 584, 867, 295, 552, 362, 668, 2055, 570, 300, 4140, 321, 393, 360, 300, 337, 552], "temperature": 0.0, "avg_logprob": -0.15367026329040528, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.00048267541569657624}, {"id": 205, "seek": 109884, "start": 1098.84, "end": 1104.9599999999998, "text": " rather than them having to figure out the details of how do they submit jobs on that", "tokens": [2831, 813, 552, 1419, 281, 2573, 484, 264, 4365, 295, 577, 360, 436, 10315, 4782, 322, 300], "temperature": 0.0, "avg_logprob": -0.18960170256785858, "compression_ratio": 1.6037037037037036, "no_speech_prob": 0.0008051846525631845}, {"id": 206, "seek": 109884, "start": 1104.9599999999998, "end": 1107.24, "text": " new system and everything.", "tokens": [777, 1185, 293, 1203, 13], "temperature": 0.0, "avg_logprob": -0.18960170256785858, "compression_ratio": 1.6037037037037036, "no_speech_prob": 0.0008051846525631845}, {"id": 207, "seek": 109884, "start": 1107.24, "end": 1110.6399999999999, "text": " We can just tell them, oh yeah, it's reinstalled, it works.", "tokens": [492, 393, 445, 980, 552, 11, 1954, 1338, 11, 309, 311, 35056, 8907, 11, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.18960170256785858, "compression_ratio": 1.6037037037037036, "no_speech_prob": 0.0008051846525631845}, {"id": 208, "seek": 109884, "start": 1110.6399999999999, "end": 1112.9599999999998, "text": " And so yeah, that has been really helpful.", "tokens": [400, 370, 1338, 11, 300, 575, 668, 534, 4961, 13], "temperature": 0.0, "avg_logprob": -0.18960170256785858, "compression_ratio": 1.6037037037037036, "no_speech_prob": 0.0008051846525631845}, {"id": 209, "seek": 109884, "start": 1112.9599999999998, "end": 1115.28, "text": " I actually have a follow-up question to that.", "tokens": [286, 767, 362, 257, 1524, 12, 1010, 1168, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.18960170256785858, "compression_ratio": 1.6037037037037036, "no_speech_prob": 0.0008051846525631845}, {"id": 210, "seek": 109884, "start": 1115.28, "end": 1120.76, "text": " So one thing we have been doing, we switched recently, well, four or five years ago from", "tokens": [407, 472, 551, 321, 362, 668, 884, 11, 321, 16858, 3938, 11, 731, 11, 1451, 420, 1732, 924, 2057, 490], "temperature": 0.0, "avg_logprob": -0.18960170256785858, "compression_ratio": 1.6037037037037036, "no_speech_prob": 0.0008051846525631845}, {"id": 211, "seek": 109884, "start": 1120.76, "end": 1126.04, "text": " Torq to Slurm, and we didn't want to let all our users retrain themselves and learn", "tokens": [7160, 80, 281, 6187, 26717, 11, 293, 321, 994, 380, 528, 281, 718, 439, 527, 5022, 1533, 7146, 2969, 293, 1466], "temperature": 0.0, "avg_logprob": -0.18960170256785858, "compression_ratio": 1.6037037037037036, "no_speech_prob": 0.0008051846525631845}, {"id": 212, "seek": 112604, "start": 1126.04, "end": 1130.24, "text": " the Slurm commands, because in our experience, Slurm is a bit less user-friendly than Torq", "tokens": [264, 6187, 26717, 16901, 11, 570, 294, 527, 1752, 11, 6187, 26717, 307, 257, 857, 1570, 4195, 12, 22864, 813, 7160, 80], "temperature": 0.0, "avg_logprob": -0.19123740818189539, "compression_ratio": 1.6733333333333333, "no_speech_prob": 0.0005458187661133707}, {"id": 213, "seek": 112604, "start": 1130.24, "end": 1131.24, "text": " is.", "tokens": [307, 13], "temperature": 0.0, "avg_logprob": -0.19123740818189539, "compression_ratio": 1.6733333333333333, "no_speech_prob": 0.0005458187661133707}, {"id": 214, "seek": 112604, "start": 1131.24, "end": 1135.12, "text": " So what we did is we rolled a wrapper that people can still use QSub, but they're actually", "tokens": [407, 437, 321, 630, 307, 321, 14306, 257, 46906, 300, 561, 393, 920, 764, 1249, 39582, 11, 457, 436, 434, 767], "temperature": 0.0, "avg_logprob": -0.19123740818189539, "compression_ratio": 1.6733333333333333, "no_speech_prob": 0.0005458187661133707}, {"id": 215, "seek": 112604, "start": 1135.12, "end": 1139.72, "text": " submitting to Slurm, and it just, it translates the script in the background.", "tokens": [31836, 281, 6187, 26717, 11, 293, 309, 445, 11, 309, 28468, 264, 5755, 294, 264, 3678, 13], "temperature": 0.0, "avg_logprob": -0.19123740818189539, "compression_ratio": 1.6733333333333333, "no_speech_prob": 0.0005458187661133707}, {"id": 216, "seek": 112604, "start": 1139.72, "end": 1141.1599999999999, "text": " Troika doesn't do that now, right?", "tokens": [19406, 5439, 1177, 380, 360, 300, 586, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.19123740818189539, "compression_ratio": 1.6733333333333333, "no_speech_prob": 0.0005458187661133707}, {"id": 217, "seek": 112604, "start": 1141.1599999999999, "end": 1145.92, "text": " You have to use the Troika command, but it knows about the Slurm header.", "tokens": [509, 362, 281, 764, 264, 19406, 5439, 5622, 11, 457, 309, 3255, 466, 264, 6187, 26717, 23117, 13], "temperature": 0.0, "avg_logprob": -0.19123740818189539, "compression_ratio": 1.6733333333333333, "no_speech_prob": 0.0005458187661133707}, {"id": 218, "seek": 112604, "start": 1145.92, "end": 1149.0, "text": " Yeah, so you could technically do it.", "tokens": [865, 11, 370, 291, 727, 12120, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.19123740818189539, "compression_ratio": 1.6733333333333333, "no_speech_prob": 0.0005458187661133707}, {"id": 219, "seek": 112604, "start": 1149.0, "end": 1156.0, "text": " We didn't want to encourage that, but technically you could, like, I think you could write a", "tokens": [492, 994, 380, 528, 281, 5373, 300, 11, 457, 12120, 291, 727, 11, 411, 11, 286, 519, 291, 727, 2464, 257], "temperature": 0.0, "avg_logprob": -0.19123740818189539, "compression_ratio": 1.6733333333333333, "no_speech_prob": 0.0005458187661133707}, {"id": 220, "seek": 115600, "start": 1156.0, "end": 1161.24, "text": " script in three lines, a plugin that just takes the directives.", "tokens": [5755, 294, 1045, 3876, 11, 257, 23407, 300, 445, 2516, 264, 2047, 1539, 13], "temperature": 0.0, "avg_logprob": -0.19883634425975658, "compression_ratio": 1.576, "no_speech_prob": 0.0003002762096002698}, {"id": 221, "seek": 115600, "start": 1161.24, "end": 1166.56, "text": " You would probably need to support all the directives you need, but we have a built-in", "tokens": [509, 576, 1391, 643, 281, 1406, 439, 264, 2047, 1539, 291, 643, 11, 457, 321, 362, 257, 3094, 12, 259], "temperature": 0.0, "avg_logprob": -0.19883634425975658, "compression_ratio": 1.576, "no_speech_prob": 0.0003002762096002698}, {"id": 222, "seek": 115600, "start": 1166.56, "end": 1171.36, "text": " parser that is able to read, like, Slurm commands, for instance.", "tokens": [21156, 260, 300, 307, 1075, 281, 1401, 11, 411, 11, 6187, 26717, 16901, 11, 337, 5197, 13], "temperature": 0.0, "avg_logprob": -0.19883634425975658, "compression_ratio": 1.576, "no_speech_prob": 0.0003002762096002698}, {"id": 223, "seek": 115600, "start": 1171.36, "end": 1177.28, "text": " And so you just need to tell Troika, oh yeah, use those on top of whatever is specified", "tokens": [400, 370, 291, 445, 643, 281, 980, 19406, 5439, 11, 1954, 1338, 11, 764, 729, 322, 1192, 295, 2035, 307, 22206], "temperature": 0.0, "avg_logprob": -0.19883634425975658, "compression_ratio": 1.576, "no_speech_prob": 0.0003002762096002698}, {"id": 224, "seek": 115600, "start": 1177.28, "end": 1179.28, "text": " in configuration.", "tokens": [294, 11694, 13], "temperature": 0.0, "avg_logprob": -0.19883634425975658, "compression_ratio": 1.576, "no_speech_prob": 0.0003002762096002698}, {"id": 225, "seek": 115600, "start": 1179.28, "end": 1181.6, "text": " Is that something you would take pull requests on?", "tokens": [1119, 300, 746, 291, 576, 747, 2235, 12475, 322, 30], "temperature": 0.0, "avg_logprob": -0.19883634425975658, "compression_ratio": 1.576, "no_speech_prob": 0.0003002762096002698}, {"id": 226, "seek": 115600, "start": 1181.6, "end": 1183.6, "text": " Yeah, if you want to.", "tokens": [865, 11, 498, 291, 528, 281, 13], "temperature": 0.0, "avg_logprob": -0.19883634425975658, "compression_ratio": 1.576, "no_speech_prob": 0.0003002762096002698}, {"id": 227, "seek": 118360, "start": 1183.6, "end": 1186.1999999999998, "text": " Okay, we had another question.", "tokens": [1033, 11, 321, 632, 1071, 1168, 13], "temperature": 0.0, "avg_logprob": -0.3636160373687744, "compression_ratio": 1.4739583333333333, "no_speech_prob": 0.006190244108438492}, {"id": 228, "seek": 118360, "start": 1186.1999999999998, "end": 1187.1999999999998, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3636160373687744, "compression_ratio": 1.4739583333333333, "no_speech_prob": 0.006190244108438492}, {"id": 229, "seek": 118360, "start": 1187.1999999999998, "end": 1190.1999999999998, "text": " Passed them away.", "tokens": [10319, 292, 552, 1314, 13], "temperature": 0.0, "avg_logprob": -0.3636160373687744, "compression_ratio": 1.4739583333333333, "no_speech_prob": 0.006190244108438492}, {"id": 230, "seek": 118360, "start": 1190.1999999999998, "end": 1197.84, "text": " Hi, thank you for the presentation, very interesting.", "tokens": [2421, 11, 1309, 291, 337, 264, 5860, 11, 588, 1880, 13], "temperature": 0.0, "avg_logprob": -0.3636160373687744, "compression_ratio": 1.4739583333333333, "no_speech_prob": 0.006190244108438492}, {"id": 231, "seek": 118360, "start": 1197.84, "end": 1205.1999999999998, "text": " So I'm an early programmer myself, and so my question for you is, how does it fail?", "tokens": [407, 286, 478, 364, 2440, 32116, 2059, 11, 293, 370, 452, 1168, 337, 291, 307, 11, 577, 775, 309, 3061, 30], "temperature": 0.0, "avg_logprob": -0.3636160373687744, "compression_ratio": 1.4739583333333333, "no_speech_prob": 0.006190244108438492}, {"id": 232, "seek": 118360, "start": 1205.1999999999998, "end": 1211.1999999999998, "text": " Like, have you studied or provoked, you know, intentional failures of the system, and have", "tokens": [1743, 11, 362, 291, 9454, 420, 1439, 9511, 11, 291, 458, 11, 21935, 20774, 295, 264, 1185, 11, 293, 362], "temperature": 0.0, "avg_logprob": -0.3636160373687744, "compression_ratio": 1.4739583333333333, "no_speech_prob": 0.006190244108438492}, {"id": 233, "seek": 121120, "start": 1211.2, "end": 1218.28, "text": " you encountered funny behaviors, like, or plain hilarious faults of the system?", "tokens": [291, 20381, 4074, 15501, 11, 411, 11, 420, 11121, 19796, 36090, 295, 264, 1185, 30], "temperature": 0.0, "avg_logprob": -0.20356561920859598, "compression_ratio": 1.551111111111111, "no_speech_prob": 0.0009896161500364542}, {"id": 234, "seek": 121120, "start": 1218.28, "end": 1224.56, "text": " Yeah, we had, I mean, getting a new system has its lot of failures, so I don't know if", "tokens": [865, 11, 321, 632, 11, 286, 914, 11, 1242, 257, 777, 1185, 575, 1080, 688, 295, 20774, 11, 370, 286, 500, 380, 458, 498], "temperature": 0.0, "avg_logprob": -0.20356561920859598, "compression_ratio": 1.551111111111111, "no_speech_prob": 0.0009896161500364542}, {"id": 235, "seek": 121120, "start": 1224.56, "end": 1230.76, "text": " Axel, you want to take over for that, because you probably have handled some of the failures.", "tokens": [20118, 338, 11, 291, 528, 281, 747, 670, 337, 300, 11, 570, 291, 1391, 362, 18033, 512, 295, 264, 20774, 13], "temperature": 0.0, "avg_logprob": -0.20356561920859598, "compression_ratio": 1.551111111111111, "no_speech_prob": 0.0009896161500364542}, {"id": 236, "seek": 121120, "start": 1230.76, "end": 1237.04, "text": " In the example of the command line provided, you can see that we redirect the output for", "tokens": [682, 264, 1365, 295, 264, 5622, 1622, 5649, 11, 291, 393, 536, 300, 321, 29066, 264, 5598, 337], "temperature": 0.0, "avg_logprob": -0.20356561920859598, "compression_ratio": 1.551111111111111, "no_speech_prob": 0.0009896161500364542}, {"id": 237, "seek": 123704, "start": 1237.04, "end": 1243.44, "text": " each submission, and this is a chance to analyze the submission and to decide what's the best", "tokens": [1184, 23689, 11, 293, 341, 307, 257, 2931, 281, 12477, 264, 23689, 293, 281, 4536, 437, 311, 264, 1151], "temperature": 0.0, "avg_logprob": -0.15821525425586885, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0008326813112944365}, {"id": 238, "seek": 123704, "start": 1243.44, "end": 1248.8799999999999, "text": " approach to deal with erroneous submission, meaning that some of them have to be reflected", "tokens": [3109, 281, 2028, 365, 1189, 26446, 563, 23689, 11, 3620, 300, 512, 295, 552, 362, 281, 312, 15502], "temperature": 0.0, "avg_logprob": -0.15821525425586885, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0008326813112944365}, {"id": 239, "seek": 123704, "start": 1248.8799999999999, "end": 1252.48, "text": " the hard way to make it clearly visible, this is a problem.", "tokens": [264, 1152, 636, 281, 652, 309, 4448, 8974, 11, 341, 307, 257, 1154, 13], "temperature": 0.0, "avg_logprob": -0.15821525425586885, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0008326813112944365}, {"id": 240, "seek": 123704, "start": 1252.48, "end": 1258.6399999999999, "text": " And some others can be handled in a hidden way, or not so visible way, in a still deterministic", "tokens": [400, 512, 2357, 393, 312, 18033, 294, 257, 7633, 636, 11, 420, 406, 370, 8974, 636, 11, 294, 257, 920, 15957, 3142], "temperature": 0.0, "avg_logprob": -0.15821525425586885, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0008326813112944365}, {"id": 241, "seek": 123704, "start": 1258.6399999999999, "end": 1265.36, "text": " way, and so it may be hidden and still automatically handle the problems when they occur.", "tokens": [636, 11, 293, 370, 309, 815, 312, 7633, 293, 920, 6772, 4813, 264, 2740, 562, 436, 5160, 13], "temperature": 0.0, "avg_logprob": -0.15821525425586885, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.0008326813112944365}, {"id": 242, "seek": 126536, "start": 1265.36, "end": 1271.24, "text": " And this is what we expect with so many jobs to submit, to focus on the critical essential", "tokens": [400, 341, 307, 437, 321, 2066, 365, 370, 867, 4782, 281, 10315, 11, 281, 1879, 322, 264, 4924, 7115], "temperature": 0.0, "avg_logprob": -0.1806253819238572, "compression_ratio": 1.6306306306306306, "no_speech_prob": 0.00036796260974369943}, {"id": 243, "seek": 126536, "start": 1271.24, "end": 1277.04, "text": " for the human side, and to have a chance to teach the machines through the hook system", "tokens": [337, 264, 1952, 1252, 11, 293, 281, 362, 257, 2931, 281, 2924, 264, 8379, 807, 264, 6328, 1185], "temperature": 0.0, "avg_logprob": -0.1806253819238572, "compression_ratio": 1.6306306306306306, "no_speech_prob": 0.00036796260974369943}, {"id": 244, "seek": 126536, "start": 1277.04, "end": 1282.24, "text": " to manage with the specificities we have identified as problematic, but we want to keep ignored", "tokens": [281, 3067, 365, 264, 2685, 1088, 321, 362, 9234, 382, 19011, 11, 457, 321, 528, 281, 1066, 19735], "temperature": 0.0, "avg_logprob": -0.1806253819238572, "compression_ratio": 1.6306306306306306, "no_speech_prob": 0.00036796260974369943}, {"id": 245, "seek": 126536, "start": 1282.24, "end": 1287.52, "text": " or manage automatically until a fix is coming from the curing system, for example, if it", "tokens": [420, 3067, 6772, 1826, 257, 3191, 307, 1348, 490, 264, 1262, 278, 1185, 11, 337, 1365, 11, 498, 309], "temperature": 0.0, "avg_logprob": -0.1806253819238572, "compression_ratio": 1.6306306306306306, "no_speech_prob": 0.00036796260974369943}, {"id": 246, "seek": 128752, "start": 1287.52, "end": 1296.96, "text": " is related to a curing system problem or identified issues that may come with the next release.", "tokens": [307, 4077, 281, 257, 1262, 278, 1185, 1154, 420, 9234, 2663, 300, 815, 808, 365, 264, 958, 4374, 13], "temperature": 0.0, "avg_logprob": -0.3410988665641622, "compression_ratio": 1.3863636363636365, "no_speech_prob": 0.001444173394702375}, {"id": 247, "seek": 128752, "start": 1296.96, "end": 1305.12, "text": " So this is a way to deal with the failures that can occur at job submission.", "tokens": [407, 341, 307, 257, 636, 281, 2028, 365, 264, 20774, 300, 393, 5160, 412, 1691, 23689, 13], "temperature": 0.0, "avg_logprob": -0.3410988665641622, "compression_ratio": 1.3863636363636365, "no_speech_prob": 0.001444173394702375}, {"id": 248, "seek": 128752, "start": 1305.12, "end": 1313.48, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.3410988665641622, "compression_ratio": 1.3863636363636365, "no_speech_prob": 0.001444173394702375}, {"id": 249, "seek": 131348, "start": 1313.48, "end": 1320.32, "text": " Did I understand correctly that when you're monitoring a job, the reference is the script?", "tokens": [2589, 286, 1223, 8944, 300, 562, 291, 434, 11028, 257, 1691, 11, 264, 6408, 307, 264, 5755, 30], "temperature": 0.0, "avg_logprob": -0.2068868673072671, "compression_ratio": 1.6939655172413792, "no_speech_prob": 0.001978481188416481}, {"id": 250, "seek": 131348, "start": 1320.32, "end": 1321.32, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.2068868673072671, "compression_ratio": 1.6939655172413792, "no_speech_prob": 0.001978481188416481}, {"id": 251, "seek": 131348, "start": 1321.32, "end": 1322.32, "text": " Correct.", "tokens": [12753, 13], "temperature": 0.0, "avg_logprob": -0.2068868673072671, "compression_ratio": 1.6939655172413792, "no_speech_prob": 0.001978481188416481}, {"id": 252, "seek": 131348, "start": 1322.32, "end": 1328.04, "text": " So that means everyone has to make sure their scripts are uniquely named each time, otherwise,", "tokens": [407, 300, 1355, 1518, 575, 281, 652, 988, 641, 23294, 366, 31474, 4926, 1184, 565, 11, 5911, 11], "temperature": 0.0, "avg_logprob": -0.2068868673072671, "compression_ratio": 1.6939655172413792, "no_speech_prob": 0.001978481188416481}, {"id": 253, "seek": 131348, "start": 1328.04, "end": 1332.08, "text": " or is it the sort of script and where it is in the file system?", "tokens": [420, 307, 309, 264, 1333, 295, 5755, 293, 689, 309, 307, 294, 264, 3991, 1185, 30], "temperature": 0.0, "avg_logprob": -0.2068868673072671, "compression_ratio": 1.6939655172413792, "no_speech_prob": 0.001978481188416481}, {"id": 254, "seek": 131348, "start": 1332.08, "end": 1334.48, "text": " It's where it is in the file system.", "tokens": [467, 311, 689, 309, 307, 294, 264, 3991, 1185, 13], "temperature": 0.0, "avg_logprob": -0.2068868673072671, "compression_ratio": 1.6939655172413792, "no_speech_prob": 0.001978481188416481}, {"id": 255, "seek": 131348, "start": 1334.48, "end": 1336.04, "text": " So you are correct.", "tokens": [407, 291, 366, 3006, 13], "temperature": 0.0, "avg_logprob": -0.2068868673072671, "compression_ratio": 1.6939655172413792, "no_speech_prob": 0.001978481188416481}, {"id": 256, "seek": 131348, "start": 1336.04, "end": 1341.44, "text": " If someone deletes or renames their script, then it can cause a problem.", "tokens": [759, 1580, 1103, 37996, 420, 8124, 1632, 641, 5755, 11, 550, 309, 393, 3082, 257, 1154, 13], "temperature": 0.0, "avg_logprob": -0.2068868673072671, "compression_ratio": 1.6939655172413792, "no_speech_prob": 0.001978481188416481}, {"id": 257, "seek": 134144, "start": 1341.44, "end": 1344.68, "text": " Submits with the same script.", "tokens": [8511, 76, 1208, 365, 264, 912, 5755, 13], "temperature": 0.0, "avg_logprob": -0.17646623165049452, "compression_ratio": 1.5374449339207048, "no_speech_prob": 0.0014240988530218601}, {"id": 258, "seek": 134144, "start": 1344.68, "end": 1351.8400000000001, "text": " So it's not a problem for us because our workflow manager basically does some pre-processing,", "tokens": [407, 309, 311, 406, 257, 1154, 337, 505, 570, 527, 20993, 6598, 1936, 775, 512, 659, 12, 41075, 278, 11], "temperature": 0.0, "avg_logprob": -0.17646623165049452, "compression_ratio": 1.5374449339207048, "no_speech_prob": 0.0014240988530218601}, {"id": 259, "seek": 134144, "start": 1351.8400000000001, "end": 1359.1200000000001, "text": " meaning that the script has some additional things like, oh, yeah, it's your second try", "tokens": [3620, 300, 264, 5755, 575, 512, 4497, 721, 411, 11, 1954, 11, 1338, 11, 309, 311, 428, 1150, 853], "temperature": 0.0, "avg_logprob": -0.17646623165049452, "compression_ratio": 1.5374449339207048, "no_speech_prob": 0.0014240988530218601}, {"id": 260, "seek": 134144, "start": 1359.1200000000001, "end": 1364.96, "text": " at that submission, so I will add.job2 at the end.", "tokens": [412, 300, 23689, 11, 370, 286, 486, 909, 2411, 50208, 17, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.17646623165049452, "compression_ratio": 1.5374449339207048, "no_speech_prob": 0.0014240988530218601}, {"id": 261, "seek": 134144, "start": 1364.96, "end": 1370.52, "text": " And so that's how we circumvent this issue, but you are definitely correct, and that's", "tokens": [400, 370, 300, 311, 577, 321, 7125, 2475, 341, 2734, 11, 457, 291, 366, 2138, 3006, 11, 293, 300, 311], "temperature": 0.0, "avg_logprob": -0.17646623165049452, "compression_ratio": 1.5374449339207048, "no_speech_prob": 0.0014240988530218601}, {"id": 262, "seek": 137052, "start": 1370.52, "end": 1373.92, "text": " something we will need to improve at some point.", "tokens": [746, 321, 486, 643, 281, 3470, 412, 512, 935, 13], "temperature": 0.0, "avg_logprob": -0.22997196197509764, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.001755397068336606}, {"id": 263, "seek": 137052, "start": 1373.92, "end": 1381.76, "text": " But we didn't want to have to link to a database or something so that we can keep it simple.", "tokens": [583, 321, 994, 380, 528, 281, 362, 281, 2113, 281, 257, 8149, 420, 746, 370, 300, 321, 393, 1066, 309, 2199, 13], "temperature": 0.0, "avg_logprob": -0.22997196197509764, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.001755397068336606}, {"id": 264, "seek": 137052, "start": 1381.76, "end": 1383.12, "text": " Thanks.", "tokens": [2561, 13], "temperature": 0.0, "avg_logprob": -0.22997196197509764, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.001755397068336606}, {"id": 265, "seek": 137052, "start": 1383.12, "end": 1385.76, "text": " You could just copy the script on submission, no?", "tokens": [509, 727, 445, 5055, 264, 5755, 322, 23689, 11, 572, 30], "temperature": 0.0, "avg_logprob": -0.22997196197509764, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.001755397068336606}, {"id": 266, "seek": 137052, "start": 1385.76, "end": 1386.76, "text": " We could copy it.", "tokens": [492, 727, 5055, 309, 13], "temperature": 0.0, "avg_logprob": -0.22997196197509764, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.001755397068336606}, {"id": 267, "seek": 137052, "start": 1386.76, "end": 1393.08, "text": " It's just that, yeah, if you have half a million scripts, we need to think, per day, we need", "tokens": [467, 311, 445, 300, 11, 1338, 11, 498, 291, 362, 1922, 257, 2459, 23294, 11, 321, 643, 281, 519, 11, 680, 786, 11, 321, 643], "temperature": 0.0, "avg_logprob": -0.22997196197509764, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.001755397068336606}, {"id": 268, "seek": 137052, "start": 1393.08, "end": 1394.92, "text": " to think about cleanup.", "tokens": [281, 519, 466, 40991, 13], "temperature": 0.0, "avg_logprob": -0.22997196197509764, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.001755397068336606}, {"id": 269, "seek": 137052, "start": 1394.92, "end": 1395.92, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.22997196197509764, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.001755397068336606}, {"id": 270, "seek": 139592, "start": 1395.92, "end": 1400.92, "text": " Other questions?", "tokens": [5358, 1651, 30], "temperature": 0.0, "avg_logprob": -0.2415686640246161, "compression_ratio": 1.5774647887323943, "no_speech_prob": 0.0006704005063511431}, {"id": 271, "seek": 139592, "start": 1400.92, "end": 1403.72, "text": " Hello.", "tokens": [2425, 13], "temperature": 0.0, "avg_logprob": -0.2415686640246161, "compression_ratio": 1.5774647887323943, "no_speech_prob": 0.0006704005063511431}, {"id": 272, "seek": 139592, "start": 1403.72, "end": 1407.04, "text": " Users like things to be as simple as possible.", "tokens": [47092, 411, 721, 281, 312, 382, 2199, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.2415686640246161, "compression_ratio": 1.5774647887323943, "no_speech_prob": 0.0006704005063511431}, {"id": 273, "seek": 139592, "start": 1407.04, "end": 1411.8400000000001, "text": " In order to do that, they would probably be nice to have some sort of central location", "tokens": [682, 1668, 281, 360, 300, 11, 436, 576, 1391, 312, 1481, 281, 362, 512, 1333, 295, 5777, 4914], "temperature": 0.0, "avg_logprob": -0.2415686640246161, "compression_ratio": 1.5774647887323943, "no_speech_prob": 0.0006704005063511431}, {"id": 274, "seek": 139592, "start": 1411.8400000000001, "end": 1416.88, "text": " where recipes of various clusters would be sort of combined accessible for people to", "tokens": [689, 13035, 295, 3683, 23313, 576, 312, 1333, 295, 9354, 9515, 337, 561, 281], "temperature": 0.0, "avg_logprob": -0.2415686640246161, "compression_ratio": 1.5774647887323943, "no_speech_prob": 0.0006704005063511431}, {"id": 275, "seek": 139592, "start": 1416.88, "end": 1418.0800000000002, "text": " be able to get access to.", "tokens": [312, 1075, 281, 483, 2105, 281, 13], "temperature": 0.0, "avg_logprob": -0.2415686640246161, "compression_ratio": 1.5774647887323943, "no_speech_prob": 0.0006704005063511431}, {"id": 276, "seek": 139592, "start": 1418.0800000000002, "end": 1421.2, "text": " Is that in your plan or?", "tokens": [1119, 300, 294, 428, 1393, 420, 30], "temperature": 0.0, "avg_logprob": -0.2415686640246161, "compression_ratio": 1.5774647887323943, "no_speech_prob": 0.0006704005063511431}, {"id": 277, "seek": 139592, "start": 1421.2, "end": 1423.0, "text": " What do you mean under configuration side?", "tokens": [708, 360, 291, 914, 833, 11694, 1252, 30], "temperature": 0.0, "avg_logprob": -0.2415686640246161, "compression_ratio": 1.5774647887323943, "no_speech_prob": 0.0006704005063511431}, {"id": 278, "seek": 142300, "start": 1423.0, "end": 1428.12, "text": " So I could imagine a user turning up going, oh, I'm going to download Troika, and I'm", "tokens": [407, 286, 727, 3811, 257, 4195, 6246, 493, 516, 11, 1954, 11, 286, 478, 516, 281, 5484, 19406, 5439, 11, 293, 286, 478], "temperature": 0.0, "avg_logprob": -0.21195234946154673, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0006192168220877647}, {"id": 279, "seek": 142300, "start": 1428.12, "end": 1430.4, "text": " going to talk to this cluster that I have access to.", "tokens": [516, 281, 751, 281, 341, 13630, 300, 286, 362, 2105, 281, 13], "temperature": 0.0, "avg_logprob": -0.21195234946154673, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0006192168220877647}, {"id": 280, "seek": 142300, "start": 1430.4, "end": 1432.0, "text": " How do I get the configuration?", "tokens": [1012, 360, 286, 483, 264, 11694, 30], "temperature": 0.0, "avg_logprob": -0.21195234946154673, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0006192168220877647}, {"id": 281, "seek": 142300, "start": 1432.0, "end": 1433.0, "text": " Oh, OK.", "tokens": [876, 11, 2264, 13], "temperature": 0.0, "avg_logprob": -0.21195234946154673, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0006192168220877647}, {"id": 282, "seek": 142300, "start": 1433.0, "end": 1434.36, "text": " I see.", "tokens": [286, 536, 13], "temperature": 0.0, "avg_logprob": -0.21195234946154673, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0006192168220877647}, {"id": 283, "seek": 142300, "start": 1434.36, "end": 1440.36, "text": " So we don't have that, but if Troika gets attraction, I think we could come up with a", "tokens": [407, 321, 500, 380, 362, 300, 11, 457, 498, 19406, 5439, 2170, 17672, 11, 286, 519, 321, 727, 808, 493, 365, 257], "temperature": 0.0, "avg_logprob": -0.21195234946154673, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0006192168220877647}, {"id": 284, "seek": 142300, "start": 1440.36, "end": 1446.76, "text": " website where you can host your configuration files or have some kind of index where you", "tokens": [3144, 689, 291, 393, 3975, 428, 11694, 7098, 420, 362, 512, 733, 295, 8186, 689, 291], "temperature": 0.0, "avg_logprob": -0.21195234946154673, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0006192168220877647}, {"id": 285, "seek": 142300, "start": 1446.76, "end": 1448.56, "text": " can list them.", "tokens": [393, 1329, 552, 13], "temperature": 0.0, "avg_logprob": -0.21195234946154673, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0006192168220877647}, {"id": 286, "seek": 144856, "start": 1448.56, "end": 1456.52, "text": " I think we would have all that's needed to do that pretty easily.", "tokens": [286, 519, 321, 576, 362, 439, 300, 311, 2978, 281, 360, 300, 1238, 3612, 13], "temperature": 0.0, "avg_logprob": -0.16873806715011597, "compression_ratio": 1.5621890547263682, "no_speech_prob": 0.00027975684497505426}, {"id": 287, "seek": 144856, "start": 1456.52, "end": 1463.24, "text": " I think, hopefully, the configuration is easy enough so that you don't need to do much", "tokens": [286, 519, 11, 4696, 11, 264, 11694, 307, 1858, 1547, 370, 300, 291, 500, 380, 643, 281, 360, 709], "temperature": 0.0, "avg_logprob": -0.16873806715011597, "compression_ratio": 1.5621890547263682, "no_speech_prob": 0.00027975684497505426}, {"id": 288, "seek": 144856, "start": 1463.24, "end": 1466.8, "text": " on top of what's actually provided as examples.", "tokens": [322, 1192, 295, 437, 311, 767, 5649, 382, 5110, 13], "temperature": 0.0, "avg_logprob": -0.16873806715011597, "compression_ratio": 1.5621890547263682, "no_speech_prob": 0.00027975684497505426}, {"id": 289, "seek": 144856, "start": 1466.8, "end": 1468.04, "text": " But yeah, you are correct.", "tokens": [583, 1338, 11, 291, 366, 3006, 13], "temperature": 0.0, "avg_logprob": -0.16873806715011597, "compression_ratio": 1.5621890547263682, "no_speech_prob": 0.00027975684497505426}, {"id": 290, "seek": 144856, "start": 1468.04, "end": 1475.28, "text": " We could, if it gets popular, just provide configuration files for several systems or,", "tokens": [492, 727, 11, 498, 309, 2170, 3743, 11, 445, 2893, 11694, 7098, 337, 2940, 3652, 420, 11], "temperature": 0.0, "avg_logprob": -0.16873806715011597, "compression_ratio": 1.5621890547263682, "no_speech_prob": 0.00027975684497505426}, {"id": 291, "seek": 147528, "start": 1475.28, "end": 1484.52, "text": " I mean, HPC system providers could also just give a configuration file with the system so", "tokens": [286, 914, 11, 12557, 34, 1185, 11330, 727, 611, 445, 976, 257, 11694, 3991, 365, 264, 1185, 370], "temperature": 0.0, "avg_logprob": -0.1939852237701416, "compression_ratio": 1.4312796208530805, "no_speech_prob": 0.00017416190530639142}, {"id": 292, "seek": 147528, "start": 1484.52, "end": 1488.04, "text": " we can have it where Troika is installed and then the user doesn't even need to bother", "tokens": [321, 393, 362, 309, 689, 19406, 5439, 307, 8899, 293, 550, 264, 4195, 1177, 380, 754, 643, 281, 8677], "temperature": 0.0, "avg_logprob": -0.1939852237701416, "compression_ratio": 1.4312796208530805, "no_speech_prob": 0.00017416190530639142}, {"id": 293, "seek": 147528, "start": 1488.04, "end": 1493.52, "text": " about it.", "tokens": [466, 309, 13], "temperature": 0.0, "avg_logprob": -0.1939852237701416, "compression_ratio": 1.4312796208530805, "no_speech_prob": 0.00017416190530639142}, {"id": 294, "seek": 147528, "start": 1493.52, "end": 1494.52, "text": " Very small second one.", "tokens": [4372, 1359, 1150, 472, 13], "temperature": 0.0, "avg_logprob": -0.1939852237701416, "compression_ratio": 1.4312796208530805, "no_speech_prob": 0.00017416190530639142}, {"id": 295, "seek": 147528, "start": 1494.52, "end": 1500.08, "text": " Given you've just done all this stuff, have you heard of a project called DRMAA, Distributed", "tokens": [18600, 291, 600, 445, 1096, 439, 341, 1507, 11, 362, 291, 2198, 295, 257, 1716, 1219, 12118, 44, 5265, 11, 9840, 2024, 4866], "temperature": 0.0, "avg_logprob": -0.1939852237701416, "compression_ratio": 1.4312796208530805, "no_speech_prob": 0.00017416190530639142}, {"id": 296, "seek": 150008, "start": 1500.08, "end": 1506.24, "text": " Resource Manager Application API, it might make the insides of this slightly nicer for", "tokens": [35200, 13821, 39512, 9362, 11, 309, 1062, 652, 264, 1028, 1875, 295, 341, 4748, 22842, 337], "temperature": 0.0, "avg_logprob": -0.4083635899927709, "compression_ratio": 1.4343434343434343, "no_speech_prob": 0.0004014265723526478}, {"id": 297, "seek": 150008, "start": 1506.24, "end": 1515.04, "text": " your EC flow stuff, maybe it might take some inspiration for that.", "tokens": [428, 19081, 3095, 1507, 11, 1310, 309, 1062, 747, 512, 10249, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.4083635899927709, "compression_ratio": 1.4343434343434343, "no_speech_prob": 0.0004014265723526478}, {"id": 298, "seek": 150008, "start": 1515.04, "end": 1519.56, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.4083635899927709, "compression_ratio": 1.4343434343434343, "no_speech_prob": 0.0004014265723526478}, {"id": 299, "seek": 150008, "start": 1519.56, "end": 1521.1999999999998, "text": " A question, but also an observation.", "tokens": [316, 1168, 11, 457, 611, 364, 14816, 13], "temperature": 0.0, "avg_logprob": -0.4083635899927709, "compression_ratio": 1.4343434343434343, "no_speech_prob": 0.0004014265723526478}, {"id": 300, "seek": 150008, "start": 1521.1999999999998, "end": 1525.72, "text": " A long time ago, there was a standard called DRMAA, it was an API.", "tokens": [316, 938, 565, 2057, 11, 456, 390, 257, 3832, 1219, 12118, 44, 5265, 11, 309, 390, 364, 9362, 13], "temperature": 0.0, "avg_logprob": -0.4083635899927709, "compression_ratio": 1.4343434343434343, "no_speech_prob": 0.0004014265723526478}, {"id": 301, "seek": 150008, "start": 1525.72, "end": 1526.72, "text": " Just mentioned.", "tokens": [1449, 2835, 13], "temperature": 0.0, "avg_logprob": -0.4083635899927709, "compression_ratio": 1.4343434343434343, "no_speech_prob": 0.0004014265723526478}, {"id": 302, "seek": 152672, "start": 1526.72, "end": 1532.16, "text": " It seems not to be used, maybe I'm wrong, but very quickly, your system, if you had", "tokens": [467, 2544, 406, 281, 312, 1143, 11, 1310, 286, 478, 2085, 11, 457, 588, 2661, 11, 428, 1185, 11, 498, 291, 632], "temperature": 0.0, "avg_logprob": -0.25888836260923404, "compression_ratio": 1.48, "no_speech_prob": 0.0007659540860913694}, {"id": 303, "seek": 152672, "start": 1532.16, "end": 1536.2, "text": " cloud-based resources on AWS, you've got an SSH connector.", "tokens": [4588, 12, 6032, 3593, 322, 17650, 11, 291, 600, 658, 364, 12238, 39, 19127, 13], "temperature": 0.0, "avg_logprob": -0.25888836260923404, "compression_ratio": 1.48, "no_speech_prob": 0.0007659540860913694}, {"id": 304, "seek": 152672, "start": 1536.2, "end": 1540.48, "text": " Could you have, in the future, maybe run up some machines on AWS?", "tokens": [7497, 291, 362, 11, 294, 264, 2027, 11, 1310, 1190, 493, 512, 8379, 322, 17650, 30], "temperature": 0.0, "avg_logprob": -0.25888836260923404, "compression_ratio": 1.48, "no_speech_prob": 0.0007659540860913694}, {"id": 305, "seek": 152672, "start": 1540.48, "end": 1542.56, "text": " Yeah, that could be an option.", "tokens": [865, 11, 300, 727, 312, 364, 3614, 13], "temperature": 0.0, "avg_logprob": -0.25888836260923404, "compression_ratio": 1.48, "no_speech_prob": 0.0007659540860913694}, {"id": 306, "seek": 152672, "start": 1542.56, "end": 1550.44, "text": " As long as you can write Python code to spawn up an image, a container somewhere.", "tokens": [1018, 938, 382, 291, 393, 2464, 15329, 3089, 281, 17088, 493, 364, 3256, 11, 257, 10129, 4079, 13], "temperature": 0.0, "avg_logprob": -0.25888836260923404, "compression_ratio": 1.48, "no_speech_prob": 0.0007659540860913694}, {"id": 307, "seek": 152672, "start": 1550.44, "end": 1551.84, "text": " Yeah, sure.", "tokens": [865, 11, 988, 13], "temperature": 0.0, "avg_logprob": -0.25888836260923404, "compression_ratio": 1.48, "no_speech_prob": 0.0007659540860913694}, {"id": 308, "seek": 155184, "start": 1551.84, "end": 1557.76, "text": " I think the API is for that, that just needs to be a plug-in that does the connection,", "tokens": [286, 519, 264, 9362, 307, 337, 300, 11, 300, 445, 2203, 281, 312, 257, 5452, 12, 259, 300, 775, 264, 4984, 11], "temperature": 0.0, "avg_logprob": -0.22351567548020443, "compression_ratio": 1.6007462686567164, "no_speech_prob": 0.000929873960558325}, {"id": 309, "seek": 155184, "start": 1557.76, "end": 1558.76, "text": " and that's it.", "tokens": [293, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.22351567548020443, "compression_ratio": 1.6007462686567164, "no_speech_prob": 0.000929873960558325}, {"id": 310, "seek": 155184, "start": 1558.76, "end": 1559.76, "text": " Cool.", "tokens": [8561, 13], "temperature": 0.0, "avg_logprob": -0.22351567548020443, "compression_ratio": 1.6007462686567164, "no_speech_prob": 0.000929873960558325}, {"id": 311, "seek": 155184, "start": 1559.76, "end": 1562.32, "text": " Okay, we're out of time.", "tokens": [1033, 11, 321, 434, 484, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.22351567548020443, "compression_ratio": 1.6007462686567164, "no_speech_prob": 0.000929873960558325}, {"id": 312, "seek": 155184, "start": 1562.32, "end": 1563.32, "text": " Just a comment.", "tokens": [1449, 257, 2871, 13], "temperature": 0.0, "avg_logprob": -0.22351567548020443, "compression_ratio": 1.6007462686567164, "no_speech_prob": 0.000929873960558325}, {"id": 313, "seek": 155184, "start": 1563.32, "end": 1567.24, "text": " I don't think you have any people using Troika outside of ECMEF.", "tokens": [286, 500, 380, 519, 291, 362, 604, 561, 1228, 19406, 5439, 2380, 295, 19081, 15454, 37, 13], "temperature": 0.0, "avg_logprob": -0.22351567548020443, "compression_ratio": 1.6007462686567164, "no_speech_prob": 0.000929873960558325}, {"id": 314, "seek": 155184, "start": 1567.24, "end": 1569.76, "text": " No, that's the first time we actually presented outside.", "tokens": [883, 11, 300, 311, 264, 700, 565, 321, 767, 8212, 2380, 13], "temperature": 0.0, "avg_logprob": -0.22351567548020443, "compression_ratio": 1.6007462686567164, "no_speech_prob": 0.000929873960558325}, {"id": 315, "seek": 155184, "start": 1569.76, "end": 1570.76, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.22351567548020443, "compression_ratio": 1.6007462686567164, "no_speech_prob": 0.000929873960558325}, {"id": 316, "seek": 155184, "start": 1570.76, "end": 1571.76, "text": " Good.", "tokens": [2205, 13], "temperature": 0.0, "avg_logprob": -0.22351567548020443, "compression_ratio": 1.6007462686567164, "no_speech_prob": 0.000929873960558325}, {"id": 317, "seek": 155184, "start": 1571.76, "end": 1574.4399999999998, "text": " So you're trying to start, or trying to get people to start using it?", "tokens": [407, 291, 434, 1382, 281, 722, 11, 420, 1382, 281, 483, 561, 281, 722, 1228, 309, 30], "temperature": 0.0, "avg_logprob": -0.22351567548020443, "compression_ratio": 1.6007462686567164, "no_speech_prob": 0.000929873960558325}, {"id": 318, "seek": 155184, "start": 1574.4399999999998, "end": 1575.4399999999998, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.22351567548020443, "compression_ratio": 1.6007462686567164, "no_speech_prob": 0.000929873960558325}, {"id": 319, "seek": 155184, "start": 1575.4399999999998, "end": 1577.4399999999998, "text": " You're building a community, you're getting yourself into trouble.", "tokens": [509, 434, 2390, 257, 1768, 11, 291, 434, 1242, 1803, 666, 5253, 13], "temperature": 0.0, "avg_logprob": -0.22351567548020443, "compression_ratio": 1.6007462686567164, "no_speech_prob": 0.000929873960558325}, {"id": 320, "seek": 157744, "start": 1577.44, "end": 1582.72, "text": " We're going to get public requests and bug reports, but okay.", "tokens": [492, 434, 516, 281, 483, 1908, 12475, 293, 7426, 7122, 11, 457, 1392, 13], "temperature": 0.0, "avg_logprob": -0.39347576272898704, "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.0041812025010585785}, {"id": 321, "seek": 157744, "start": 1582.72, "end": 1583.72, "text": " Thank you very much.", "tokens": [1044, 291, 588, 709, 13], "temperature": 0.0, "avg_logprob": -0.39347576272898704, "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.0041812025010585785}, {"id": 322, "seek": 157744, "start": 1583.72, "end": 1584.72, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.39347576272898704, "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.0041812025010585785}, {"id": 323, "seek": 157744, "start": 1584.72, "end": 1585.72, "text": " Very nice.", "tokens": [4372, 1481, 13], "temperature": 0.0, "avg_logprob": -0.39347576272898704, "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.0041812025010585785}, {"id": 324, "seek": 157744, "start": 1585.72, "end": 1586.72, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.39347576272898704, "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.0041812025010585785}, {"id": 325, "seek": 157744, "start": 1586.72, "end": 1587.72, "text": " I'll just switch.", "tokens": [286, 603, 445, 3679, 13], "temperature": 0.0, "avg_logprob": -0.39347576272898704, "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.0041812025010585785}, {"id": 326, "seek": 157744, "start": 1587.72, "end": 1588.72, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.39347576272898704, "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.0041812025010585785}, {"id": 327, "seek": 157744, "start": 1588.72, "end": 1589.72, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.39347576272898704, "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.0041812025010585785}, {"id": 328, "seek": 157744, "start": 1589.72, "end": 1590.72, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.39347576272898704, "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.0041812025010585785}, {"id": 329, "seek": 157744, "start": 1590.72, "end": 1591.72, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.39347576272898704, "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.0041812025010585785}, {"id": 330, "seek": 157744, "start": 1591.72, "end": 1592.72, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.39347576272898704, "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.0041812025010585785}, {"id": 331, "seek": 157744, "start": 1592.72, "end": 1593.72, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.39347576272898704, "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.0041812025010585785}, {"id": 332, "seek": 157744, "start": 1593.72, "end": 1594.72, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.39347576272898704, "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.0041812025010585785}, {"id": 333, "seek": 157744, "start": 1594.72, "end": 1595.72, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.39347576272898704, "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.0041812025010585785}, {"id": 334, "seek": 157744, "start": 1595.72, "end": 1596.72, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.39347576272898704, "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.0041812025010585785}, {"id": 335, "seek": 157744, "start": 1596.72, "end": 1597.72, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.39347576272898704, "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.0041812025010585785}, {"id": 336, "seek": 159772, "start": 1597.72, "end": 1607.72, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.8674294948577881, "compression_ratio": 0.38461538461538464, "no_speech_prob": 0.0006528906524181366}], "language": "en"}