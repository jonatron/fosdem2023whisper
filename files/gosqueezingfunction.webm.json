{"text": " Okay, thank you. So our next speaker is Jesus, we've been talking a few times in the GoDev room about everything that has to do deeply within the language and today he's going to talk to us about what's going on in functions. A round of applause. Okay. Hello, everybody. Well, my name is Jesus. I'm software engineer and I'm going to talk about squeezing a Go function. So what is optimization? I think it's important to know that optimization is not being faster or consuming less memory, it depends on your needs. So it's better for squeeze use, probably everybody will say yes, but it depends if you are looking for convenience or for something that lasts forever. So in that case, it's not the best option. Optimizing is about what you need and trying to address that. It's important to optimize at the right level. You can buy the best car, you can get an F1 car and it's not going to be fast if this is the road. So try to optimize always at the upper level because this kind of optimization, the ones that we are going to see in this talk are micro optimizations that probably are not the first place that you should be starting. Optimize what you need and when you need it. It's not about taking a Go function and try to optimize forever and try to make that run super efficiently and scratch every single nanosecond because probably the bottleneck is no longer there. You have to search for the bottleneck, you have to optimize where the bottleneck is and then look again if the bottleneck is still there because if it's no longer there, you are over-optimizing that function without much gain. So just take that into consideration, optimizing is an interactive cycle and you need to keep moving and keep searching for the bottleneck. Do not guess, please. Yeah, I know everybody has instincts and all that stuff but guessing about performance is an awful thing because there's so many things that comes into play that is just impossible. There's the operating system, the compiler, the optimizations of the compiler, if you are in the cloud, maybe a noisy neighbor, all that stuff comes into play with performance. So you have to, you are not good at guessing almost for sure in performance. So just measure everything. The important thing here is try to measure everything and work with that data. Probably is what, probably the talk that is after the next one is about. So I will suggest to go there also because it probably is a very interesting talk. So let's talk about benchmarks. The way that you measure performance in micro-optimization, so micro-benchmarks, is through Go benchmarks. Go benchmark is a tool that comes with Go and is similar to the testing framework that comes in Go but very focused on benchmark. In this case, we can see here an example to have two benchmark, one for MD5SAM and one for SHA256SAM. That's it. It's just a function that starts with benchmark. I'm going to receive a testing.b argument and that's this four, I have this four loop inside. And that is going to do all the job to give you the numbers and I show you now the numbers. If I run this with Go bench, we got this dash bench dot. The dot is a regular expression that means everything. So you can use like the Go test run a regular expression for only executing certain benchmarks. And here you can see that MD5SAM is around twice time faster per operation than SHA. So well, just a number. It's that important. It depends. If you need more security, probably MD5 is not the best option. So it depends on your needs. Another interesting thing is the allocations. One thing that you maybe have heard is about counting allocations. Counting allocations, why is that important? It's because every time we allocate something, when we talk allocation, we're talking about allocation in the heap. If every time we allocate something in the heap, allocating that is going to introduce an overhead. And not only that, it's going to add more pressure to the garbage collector. That's why it's important to count the allocations when you are talking about performance. If you are not worried about performance at that point, don't count the allocation. It's not that important and you are not going to gain a massive amount of performance from there if you are not in that point there. Okay. Let's see an example here in MD5 and SHA SAMs. We have zero allocations. So well, this data is not very useful for us now. So let's use another thing. Let's open a file. Let's open a file thousands of times and see how it goes. Now I see that every single operation of opening a file, just opening the file, is going to generate three allocations. And it's going to consume 120 bytes per operation. Interesting. So now you are measuring things. You are measuring how much time it takes, how much time is gone in processing something, is going in allocating things, how much memory is gone there. So let's talk about profiling because once you, well, actually normally you do the profiling first to find your bottleneck and then you do the benchmark to tune that bottleneck. But I'm playing with the fact that I already have the benchmark and I'm going to do the profiling on top of the benchmark. So I'm going to execute the gobench, I'm going to pass the mem profile, I'm going to generate the mem profile and I'm going to use the people of tool. The people of tool is going to allow me to analyze that profile. In this case, I'm just asking for a text output and that text output is going to show me the top consumers of memory in this case. And I can see there that 84% of the memory is gone in OS new file. Okay, let's see what happened, okay, it's that file but I need more information, well, it's that function, sorry, I need more information. Actually I cannot like this output but if you don't like this output, you can, for example, use SVG and you are going to get something like this that is very visual and actually is kind of obvious that where is the bottleneck there and in this case, again, is OS new file. If I go to the people of tool again and instead of that, I use the list of a function and I'm seeing here where is the memory going by line and here I can see that in the line 127 of the file, fileunix.go, I'm consuming the memory. Actually there you see 74 megabytes, that is because it's counting all the allocation and aggregating all the allocations, it's not, every operation here is consuming only 120 bytes. Okay, the same with CPU profile, in this case, this is generating the most of the CPU consumption is in Cisco 6, I can see in SVG, this time it's more scattered, so the CPU is consuming in way more places but still the Cisco 6 is the biggest one. So I'm going to list that and I see some assembly code, probably you are not going to optimize more this function, so probably this is not the place that you should be looking for optimizations anyway, this is an example of getting to the root cause during the profiling. Okay, this talk is going to be more by examples, I'm going to try to show you some examples of optimizations, it's just to show you the process more than the specific optimization, I expect you learn something in between but it's more about the process, okay. One of the things that you can do is reducing the CPU usage, this is a kind of silly example, you have a fine function that have a needle and a high stack and just go through the high stack and search for that needle and give you the result. This is looping over the whole string or the whole slice, I'm going to do a benchmark, the first thing, I'm going to do the benchmark, I'm going to generate a lot of strings and I'm going to do a benchmark looking for something around in the middle, it's not exactly in the middle but it's around there and the benchmark is saying that it's taking nearly 300 nanoseconds. If I just early return that is just a kind of silly optimization, it's not super smart or something like that, I'm going to save basically almost the half of the performance, this is because the benchmark is doing something really silly and it can vary depending on the data that it inputs but it's an optimization is just doing less, that is one of the best ways of optimizing things. Reducing allocations, one of the classic example of reducing allocations is when you are dealing with slices, when you have a slice, for example this is a common way of constructing a slice, I create a slice, I loop over this, generate a loop and start appending things to that slice, okay fine, I'm going to do a benchmark for checking that and it's taking 39 allocations and around 41 megabytes per operation, okay sounds like a lot, okay let's do it, let's do this, let's build the slice but we are going to give an initial size of a million and the time I'm just setting that, the final result is exactly the same but now we have one allocation and we have consumed only one megabyte and actually if you see there is around 800 microseconds and here you have around 10 milliseconds, so it's a lot of time actually, a lot of CPU time too but you can squeeze it more, if you know that at compile time, if you know exactly the size that you want to have at compile time, you can build an array, it's faster than any slice actually, so if I build an array I'm now doing zero allocation, zero heap allocations, it's going to go in the stack or in binary somehow, whatever but it's not consuming my heap allocations and this time is 300 microseconds approximately, so an interesting thing if you know that information at compile time, okay another thing is packing, if you are concerned about memory you can build this struct and say okay I have a Boolean, I have a float, I have an N32 and the goal compiler is going to align my struct to make it more efficient and work better with the CPU and all that stuff and in this case it's just adding seven bytes between the Boolean and the float and four bytes after the integer to get everything aligned, okay I built a slice and initialized a slice and I'm allocating one time because that's what the slice is doing and I'm consuming around 24 megabytes per operation, if I just organize the struct, in this case I put the float at the beginning then the integer 32 and then the Boolean, the compiler is only going to add three bytes so the whole structure is going to be smaller in memory and in this case now is 16 megabytes per operation, so this kind of optimization is not going to save your day, if you are just creating some structs but if you are creating millions of instances of an struct it can be a significant amount of memory. Function in lining, function in lining is something that the goal compiler does for us is just taking a function and replacing any call to that function with the code that is generated by the function. I'm going to show you a very damn example, I'm not inlining this function explicitly and I'm using the inlined version that is going to be inlined by the compiler because it's simple enough and then I'm going to execute that, I'm saving a whole nanosecond there, so yeah it's not a great optimization to be honest, probably you don't care about that nanosecond but we are going to see why that is important later, not because of the nanosecond. I'm going to talk now about escape analysis, escape analysis is another thing that the compiler does for us and basically analyzes our variables and decides when a variable escapes from the context of the stack, it's something that is no longer able to get the information from the stack or store the information from the stack and be accessible where it needs to be accessible so it needs to escape to the heap, so it's what generates that allocations and we have seen that allocations have certain implications, so let's see an example here, this is another inline function that returns a pointer that is going to generate an allocation, this is something that returns by value, a value is going to copy the value to the stack of the caller so it's not going to generate allocations, so we can see that in the benchmark that is saying the first version have one allocation and it's allocating 8 bytes and the second one have 0 allocations and actually you can see there is one allocation and it's taking 10 times more to do that, 10 times more in this case is around 12 nanoseconds that is not a lot but everything adds up at the end especially when you are calling millions of times of things, okay and one interesting thing is escape analysis plus inlining, why? Well imagine this situation you have a struct, a function that generates or instantiate that struct and the constructor of that extract, okay, the constructor returns me a pointer and do all the stuff that it needs, okay great, it is generating 3 allocations and it's consuming 56 bytes per operation, okay, what happen if I just move the logic of that initialization process into a different function, if we do that suddenly the new document is simple enough to be inlined and because it's inlined it's no longer escaped so it's no longer needed that allocation, something that simple allows you to just reduce the number of allocations of certain types when you have a constructor, what I would suggest is just keep your constructor as simple as possible and if you have to do certain complex logic do it in an initialization function, well if that doesn't hurt the readability, okay, let's see here we have less allocations, we have now 2 allocations and 32 bytes per operation and the time consumed is you are saving 50 nanoseconds every time you instantiate that, so this is a good chunk, okay, well this is optimization sometimes it's a matter of trade-offs, sometimes you just can do less, like less allocations, less CPU work, less garbage collector pressure, all that stuff is things that you can be done, but sometimes it's not about doing less, it's about consuming different kind of resources, I care less about memory and I care more about CPU or all the way around, so concurrency is one of the cases where you need to decide what you want to consume because go-routines are really cheap but are not free at all, so let's see an example with IO, this is two functions that I created, one is a fake IO that is going to generate some kind of IO simulation by time-sleep and then you have the fake IO parallel that received the number of go-routines and it's doing basically the same but distributing all that hundred cycles between different go-routines and I built a benchmark to do that using three different approaches, one is serial one, the non-concurrency, the other one is concurrency using the number of CPUs in my machine and the other one is using the number of tasks that I have, and because this is IO, this is the result, I'm going to see that if I create one go-routine per job, the number of bytes per operation and the number of allocation is going to spike but the time that is going to be consumed is going to be way lower, actually I'm able to execute hundred times this function using this one go-routine per job approach and only 12 using one CPU per job because this is IO, so let's see what happens if I do that with CPU. Using the CPU, this is to simulate some CPU load and using MD5 sum and it's more or less the same approach as we saw in the fake IO, the benchmark is exactly the same approach, we are using the number of jobs and the number of CPUs and using no go-routines and here is interesting because if you use the number of CPUs and this is a CPU workload, that is what is going to do the best efficiency. You can see here that executing one go-routine per job is going to be even slower than executing that in serial and actually you have the worst of both worlds. You have plenty of allocations, plenty of memory consumption, plenty of time consumption and you are not gaining anything. In the case of CPU, you are consuming more memory and you are getting better CPU performance because you are basically spreading the job all over your physical CPUs and the serial one is just doing everything and is using only one core of your CPU. Whenever you want to optimize using concurrency, you have to take in consideration what the kind of workload that you are using is the CPU workload, is your workload, do you care about memory, do you care about CPU, what do you care about? That is the whole idea. I just want to explain that all this is about measuring everything, measuring all this, doing all these benchmarks, doing all these kind of experiments to see if you are getting improvement on the performance and iterate over that. That is the main idea. I show some examples of how you can improve things and some of them can be applied in general basics like using the, try to keep constructors small or using the constructor for slices when you know the size and things like that. Some references. Efficient Go is a really book that is really, really interesting. If you are really interested into efficiency, Bartolome Plocca wrote that book and actually is going to give a talk after the next one. I am sure it is going to be super interesting. High-performance workshop from Dave Cheney. There is a lot of documentation about that workshop that Dave Cheney did and it is really interesting also. The Go Perf book is a good lecture also. An Ultimate Go course from Ardon Labs is also an interesting course because it is giving you a lot of foundation and the course takes a lot of, cares a lot about hardware sympathy and all that stuff. Well, some creative common, all the images are creative common so I put the reference here because it is creative common. Thank you. That is it. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.8, "text": " Okay, thank you.", "tokens": [1033, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.3548101078380238, "compression_ratio": 1.462686567164179, "no_speech_prob": 0.5457528829574585}, {"id": 1, "seek": 0, "start": 7.8, "end": 12.16, "text": " So our next speaker is Jesus, we've been talking a few times in the GoDev room about everything", "tokens": [407, 527, 958, 8145, 307, 2705, 11, 321, 600, 668, 1417, 257, 1326, 1413, 294, 264, 1037, 11089, 85, 1808, 466, 1203], "temperature": 0.0, "avg_logprob": -0.3548101078380238, "compression_ratio": 1.462686567164179, "no_speech_prob": 0.5457528829574585}, {"id": 2, "seek": 0, "start": 12.16, "end": 16.28, "text": " that has to do deeply within the language and today he's going to talk to us about what's", "tokens": [300, 575, 281, 360, 8760, 1951, 264, 2856, 293, 965, 415, 311, 516, 281, 751, 281, 505, 466, 437, 311], "temperature": 0.0, "avg_logprob": -0.3548101078380238, "compression_ratio": 1.462686567164179, "no_speech_prob": 0.5457528829574585}, {"id": 3, "seek": 0, "start": 16.28, "end": 17.28, "text": " going on in functions.", "tokens": [516, 322, 294, 6828, 13], "temperature": 0.0, "avg_logprob": -0.3548101078380238, "compression_ratio": 1.462686567164179, "no_speech_prob": 0.5457528829574585}, {"id": 4, "seek": 0, "start": 17.28, "end": 18.28, "text": " A round of applause.", "tokens": [316, 3098, 295, 9969, 13], "temperature": 0.0, "avg_logprob": -0.3548101078380238, "compression_ratio": 1.462686567164179, "no_speech_prob": 0.5457528829574585}, {"id": 5, "seek": 0, "start": 18.28, "end": 19.28, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.3548101078380238, "compression_ratio": 1.462686567164179, "no_speech_prob": 0.5457528829574585}, {"id": 6, "seek": 0, "start": 19.28, "end": 20.28, "text": " Hello, everybody.", "tokens": [2425, 11, 2201, 13], "temperature": 0.0, "avg_logprob": -0.3548101078380238, "compression_ratio": 1.462686567164179, "no_speech_prob": 0.5457528829574585}, {"id": 7, "seek": 0, "start": 20.28, "end": 21.28, "text": " Well, my name is Jesus.", "tokens": [1042, 11, 452, 1315, 307, 2705, 13], "temperature": 0.0, "avg_logprob": -0.3548101078380238, "compression_ratio": 1.462686567164179, "no_speech_prob": 0.5457528829574585}, {"id": 8, "seek": 2128, "start": 21.28, "end": 32.28, "text": " I'm software engineer and I'm going to talk about squeezing a Go function.", "tokens": [286, 478, 4722, 11403, 293, 286, 478, 516, 281, 751, 466, 36645, 257, 1037, 2445, 13], "temperature": 0.0, "avg_logprob": -0.24203043717604417, "compression_ratio": 1.5336538461538463, "no_speech_prob": 0.0009239374194294214}, {"id": 9, "seek": 2128, "start": 32.28, "end": 34.92, "text": " So what is optimization?", "tokens": [407, 437, 307, 19618, 30], "temperature": 0.0, "avg_logprob": -0.24203043717604417, "compression_ratio": 1.5336538461538463, "no_speech_prob": 0.0009239374194294214}, {"id": 10, "seek": 2128, "start": 34.92, "end": 40.480000000000004, "text": " I think it's important to know that optimization is not being faster or consuming less memory,", "tokens": [286, 519, 309, 311, 1021, 281, 458, 300, 19618, 307, 406, 885, 4663, 420, 19867, 1570, 4675, 11], "temperature": 0.0, "avg_logprob": -0.24203043717604417, "compression_ratio": 1.5336538461538463, "no_speech_prob": 0.0009239374194294214}, {"id": 11, "seek": 2128, "start": 40.480000000000004, "end": 41.96, "text": " it depends on your needs.", "tokens": [309, 5946, 322, 428, 2203, 13], "temperature": 0.0, "avg_logprob": -0.24203043717604417, "compression_ratio": 1.5336538461538463, "no_speech_prob": 0.0009239374194294214}, {"id": 12, "seek": 2128, "start": 41.96, "end": 48.760000000000005, "text": " So it's better for squeeze use, probably everybody will say yes, but it depends if you are looking", "tokens": [407, 309, 311, 1101, 337, 13578, 764, 11, 1391, 2201, 486, 584, 2086, 11, 457, 309, 5946, 498, 291, 366, 1237], "temperature": 0.0, "avg_logprob": -0.24203043717604417, "compression_ratio": 1.5336538461538463, "no_speech_prob": 0.0009239374194294214}, {"id": 13, "seek": 4876, "start": 48.76, "end": 52.519999999999996, "text": " for convenience or for something that lasts forever.", "tokens": [337, 19283, 420, 337, 746, 300, 20669, 5680, 13], "temperature": 0.0, "avg_logprob": -0.13761950648108193, "compression_ratio": 1.5538461538461539, "no_speech_prob": 0.00013001706975046545}, {"id": 14, "seek": 4876, "start": 52.519999999999996, "end": 55.48, "text": " So in that case, it's not the best option.", "tokens": [407, 294, 300, 1389, 11, 309, 311, 406, 264, 1151, 3614, 13], "temperature": 0.0, "avg_logprob": -0.13761950648108193, "compression_ratio": 1.5538461538461539, "no_speech_prob": 0.00013001706975046545}, {"id": 15, "seek": 4876, "start": 55.48, "end": 61.32, "text": " Optimizing is about what you need and trying to address that.", "tokens": [35013, 3319, 307, 466, 437, 291, 643, 293, 1382, 281, 2985, 300, 13], "temperature": 0.0, "avg_logprob": -0.13761950648108193, "compression_ratio": 1.5538461538461539, "no_speech_prob": 0.00013001706975046545}, {"id": 16, "seek": 4876, "start": 61.32, "end": 63.32, "text": " It's important to optimize at the right level.", "tokens": [467, 311, 1021, 281, 19719, 412, 264, 558, 1496, 13], "temperature": 0.0, "avg_logprob": -0.13761950648108193, "compression_ratio": 1.5538461538461539, "no_speech_prob": 0.00013001706975046545}, {"id": 17, "seek": 4876, "start": 63.32, "end": 70.2, "text": " You can buy the best car, you can get an F1 car and it's not going to be fast if this", "tokens": [509, 393, 2256, 264, 1151, 1032, 11, 291, 393, 483, 364, 479, 16, 1032, 293, 309, 311, 406, 516, 281, 312, 2370, 498, 341], "temperature": 0.0, "avg_logprob": -0.13761950648108193, "compression_ratio": 1.5538461538461539, "no_speech_prob": 0.00013001706975046545}, {"id": 18, "seek": 4876, "start": 70.2, "end": 71.44, "text": " is the road.", "tokens": [307, 264, 3060, 13], "temperature": 0.0, "avg_logprob": -0.13761950648108193, "compression_ratio": 1.5538461538461539, "no_speech_prob": 0.00013001706975046545}, {"id": 19, "seek": 7144, "start": 71.44, "end": 80.0, "text": " So try to optimize always at the upper level because this kind of optimization, the ones", "tokens": [407, 853, 281, 19719, 1009, 412, 264, 6597, 1496, 570, 341, 733, 295, 19618, 11, 264, 2306], "temperature": 0.0, "avg_logprob": -0.17765055263743681, "compression_ratio": 1.7376237623762376, "no_speech_prob": 1.9335964680067264e-05}, {"id": 20, "seek": 7144, "start": 80.0, "end": 85.96, "text": " that we are going to see in this talk are micro optimizations that probably are not", "tokens": [300, 321, 366, 516, 281, 536, 294, 341, 751, 366, 4532, 5028, 14455, 300, 1391, 366, 406], "temperature": 0.0, "avg_logprob": -0.17765055263743681, "compression_ratio": 1.7376237623762376, "no_speech_prob": 1.9335964680067264e-05}, {"id": 21, "seek": 7144, "start": 85.96, "end": 91.12, "text": " the first place that you should be starting.", "tokens": [264, 700, 1081, 300, 291, 820, 312, 2891, 13], "temperature": 0.0, "avg_logprob": -0.17765055263743681, "compression_ratio": 1.7376237623762376, "no_speech_prob": 1.9335964680067264e-05}, {"id": 22, "seek": 7144, "start": 91.12, "end": 95.08, "text": " Optimize what you need and when you need it.", "tokens": [35013, 1125, 437, 291, 643, 293, 562, 291, 643, 309, 13], "temperature": 0.0, "avg_logprob": -0.17765055263743681, "compression_ratio": 1.7376237623762376, "no_speech_prob": 1.9335964680067264e-05}, {"id": 23, "seek": 7144, "start": 95.08, "end": 101.0, "text": " It's not about taking a Go function and try to optimize forever and try to make that run", "tokens": [467, 311, 406, 466, 1940, 257, 1037, 2445, 293, 853, 281, 19719, 5680, 293, 853, 281, 652, 300, 1190], "temperature": 0.0, "avg_logprob": -0.17765055263743681, "compression_ratio": 1.7376237623762376, "no_speech_prob": 1.9335964680067264e-05}, {"id": 24, "seek": 10100, "start": 101.0, "end": 108.16, "text": " super efficiently and scratch every single nanosecond because probably the bottleneck", "tokens": [1687, 19621, 293, 8459, 633, 2167, 14067, 541, 18882, 570, 1391, 264, 44641, 547], "temperature": 0.0, "avg_logprob": -0.13919411176516686, "compression_ratio": 1.8241758241758241, "no_speech_prob": 3.511666363920085e-05}, {"id": 25, "seek": 10100, "start": 108.16, "end": 109.6, "text": " is no longer there.", "tokens": [307, 572, 2854, 456, 13], "temperature": 0.0, "avg_logprob": -0.13919411176516686, "compression_ratio": 1.8241758241758241, "no_speech_prob": 3.511666363920085e-05}, {"id": 26, "seek": 10100, "start": 109.6, "end": 114.4, "text": " You have to search for the bottleneck, you have to optimize where the bottleneck is and", "tokens": [509, 362, 281, 3164, 337, 264, 44641, 547, 11, 291, 362, 281, 19719, 689, 264, 44641, 547, 307, 293], "temperature": 0.0, "avg_logprob": -0.13919411176516686, "compression_ratio": 1.8241758241758241, "no_speech_prob": 3.511666363920085e-05}, {"id": 27, "seek": 10100, "start": 114.4, "end": 119.16, "text": " then look again if the bottleneck is still there because if it's no longer there, you", "tokens": [550, 574, 797, 498, 264, 44641, 547, 307, 920, 456, 570, 498, 309, 311, 572, 2854, 456, 11, 291], "temperature": 0.0, "avg_logprob": -0.13919411176516686, "compression_ratio": 1.8241758241758241, "no_speech_prob": 3.511666363920085e-05}, {"id": 28, "seek": 10100, "start": 119.16, "end": 123.68, "text": " are over-optimizing that function without much gain.", "tokens": [366, 670, 12, 5747, 332, 3319, 300, 2445, 1553, 709, 6052, 13], "temperature": 0.0, "avg_logprob": -0.13919411176516686, "compression_ratio": 1.8241758241758241, "no_speech_prob": 3.511666363920085e-05}, {"id": 29, "seek": 12368, "start": 123.68, "end": 133.32, "text": " So just take that into consideration, optimizing is an interactive cycle and you need to keep", "tokens": [407, 445, 747, 300, 666, 12381, 11, 40425, 307, 364, 15141, 6586, 293, 291, 643, 281, 1066], "temperature": 0.0, "avg_logprob": -0.20883599305764222, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.9668394088512287e-05}, {"id": 30, "seek": 12368, "start": 133.32, "end": 138.36, "text": " moving and keep searching for the bottleneck.", "tokens": [2684, 293, 1066, 10808, 337, 264, 44641, 547, 13], "temperature": 0.0, "avg_logprob": -0.20883599305764222, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.9668394088512287e-05}, {"id": 31, "seek": 12368, "start": 138.36, "end": 139.36, "text": " Do not guess, please.", "tokens": [1144, 406, 2041, 11, 1767, 13], "temperature": 0.0, "avg_logprob": -0.20883599305764222, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.9668394088512287e-05}, {"id": 32, "seek": 12368, "start": 139.36, "end": 144.56, "text": " Yeah, I know everybody has instincts and all that stuff but guessing about performance", "tokens": [865, 11, 286, 458, 2201, 575, 38997, 293, 439, 300, 1507, 457, 17939, 466, 3389], "temperature": 0.0, "avg_logprob": -0.20883599305764222, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.9668394088512287e-05}, {"id": 33, "seek": 12368, "start": 144.56, "end": 152.32, "text": " is an awful thing because there's so many things that comes into play that is just impossible.", "tokens": [307, 364, 11232, 551, 570, 456, 311, 370, 867, 721, 300, 1487, 666, 862, 300, 307, 445, 6243, 13], "temperature": 0.0, "avg_logprob": -0.20883599305764222, "compression_ratio": 1.5806451612903225, "no_speech_prob": 1.9668394088512287e-05}, {"id": 34, "seek": 15232, "start": 152.32, "end": 158.51999999999998, "text": " There's the operating system, the compiler, the optimizations of the compiler, if you", "tokens": [821, 311, 264, 7447, 1185, 11, 264, 31958, 11, 264, 5028, 14455, 295, 264, 31958, 11, 498, 291], "temperature": 0.0, "avg_logprob": -0.1700201478115348, "compression_ratio": 1.722488038277512, "no_speech_prob": 4.177657319814898e-05}, {"id": 35, "seek": 15232, "start": 158.51999999999998, "end": 165.04, "text": " are in the cloud, maybe a noisy neighbor, all that stuff comes into play with performance.", "tokens": [366, 294, 264, 4588, 11, 1310, 257, 24518, 5987, 11, 439, 300, 1507, 1487, 666, 862, 365, 3389, 13], "temperature": 0.0, "avg_logprob": -0.1700201478115348, "compression_ratio": 1.722488038277512, "no_speech_prob": 4.177657319814898e-05}, {"id": 36, "seek": 15232, "start": 165.04, "end": 172.24, "text": " So you have to, you are not good at guessing almost for sure in performance.", "tokens": [407, 291, 362, 281, 11, 291, 366, 406, 665, 412, 17939, 1920, 337, 988, 294, 3389, 13], "temperature": 0.0, "avg_logprob": -0.1700201478115348, "compression_ratio": 1.722488038277512, "no_speech_prob": 4.177657319814898e-05}, {"id": 37, "seek": 15232, "start": 172.24, "end": 174.12, "text": " So just measure everything.", "tokens": [407, 445, 3481, 1203, 13], "temperature": 0.0, "avg_logprob": -0.1700201478115348, "compression_ratio": 1.722488038277512, "no_speech_prob": 4.177657319814898e-05}, {"id": 38, "seek": 15232, "start": 174.12, "end": 178.6, "text": " The important thing here is try to measure everything and work with that data.", "tokens": [440, 1021, 551, 510, 307, 853, 281, 3481, 1203, 293, 589, 365, 300, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1700201478115348, "compression_ratio": 1.722488038277512, "no_speech_prob": 4.177657319814898e-05}, {"id": 39, "seek": 17860, "start": 178.6, "end": 184.4, "text": " Probably is what, probably the talk that is after the next one is about.", "tokens": [9210, 307, 437, 11, 1391, 264, 751, 300, 307, 934, 264, 958, 472, 307, 466, 13], "temperature": 0.0, "avg_logprob": -0.20095115579584594, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.00014552967331837863}, {"id": 40, "seek": 17860, "start": 184.4, "end": 191.64, "text": " So I will suggest to go there also because it probably is a very interesting talk.", "tokens": [407, 286, 486, 3402, 281, 352, 456, 611, 570, 309, 1391, 307, 257, 588, 1880, 751, 13], "temperature": 0.0, "avg_logprob": -0.20095115579584594, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.00014552967331837863}, {"id": 41, "seek": 17860, "start": 191.64, "end": 193.56, "text": " So let's talk about benchmarks.", "tokens": [407, 718, 311, 751, 466, 43751, 13], "temperature": 0.0, "avg_logprob": -0.20095115579584594, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.00014552967331837863}, {"id": 42, "seek": 17860, "start": 193.56, "end": 202.44, "text": " The way that you measure performance in micro-optimization, so micro-benchmarks, is through Go benchmarks.", "tokens": [440, 636, 300, 291, 3481, 3389, 294, 4532, 12, 5747, 332, 2144, 11, 370, 4532, 12, 47244, 37307, 11, 307, 807, 1037, 43751, 13], "temperature": 0.0, "avg_logprob": -0.20095115579584594, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.00014552967331837863}, {"id": 43, "seek": 17860, "start": 202.44, "end": 208.24, "text": " Go benchmark is a tool that comes with Go and is similar to the testing framework that", "tokens": [1037, 18927, 307, 257, 2290, 300, 1487, 365, 1037, 293, 307, 2531, 281, 264, 4997, 8388, 300], "temperature": 0.0, "avg_logprob": -0.20095115579584594, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.00014552967331837863}, {"id": 44, "seek": 20824, "start": 208.24, "end": 211.36, "text": " comes in Go but very focused on benchmark.", "tokens": [1487, 294, 1037, 457, 588, 5178, 322, 18927, 13], "temperature": 0.0, "avg_logprob": -0.307489768318508, "compression_ratio": 1.4752475247524752, "no_speech_prob": 9.164682705886662e-05}, {"id": 45, "seek": 20824, "start": 211.36, "end": 218.28, "text": " In this case, we can see here an example to have two benchmark, one for MD5SAM and one", "tokens": [682, 341, 1389, 11, 321, 393, 536, 510, 364, 1365, 281, 362, 732, 18927, 11, 472, 337, 22521, 20, 50, 2865, 293, 472], "temperature": 0.0, "avg_logprob": -0.307489768318508, "compression_ratio": 1.4752475247524752, "no_speech_prob": 9.164682705886662e-05}, {"id": 46, "seek": 20824, "start": 218.28, "end": 224.48000000000002, "text": " for SHA256SAM.", "tokens": [337, 38820, 6074, 21, 50, 2865, 13], "temperature": 0.0, "avg_logprob": -0.307489768318508, "compression_ratio": 1.4752475247524752, "no_speech_prob": 9.164682705886662e-05}, {"id": 47, "seek": 20824, "start": 224.48000000000002, "end": 225.48000000000002, "text": " That's it.", "tokens": [663, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.307489768318508, "compression_ratio": 1.4752475247524752, "no_speech_prob": 9.164682705886662e-05}, {"id": 48, "seek": 20824, "start": 225.48000000000002, "end": 226.48000000000002, "text": " It's just a function that starts with benchmark.", "tokens": [467, 311, 445, 257, 2445, 300, 3719, 365, 18927, 13], "temperature": 0.0, "avg_logprob": -0.307489768318508, "compression_ratio": 1.4752475247524752, "no_speech_prob": 9.164682705886662e-05}, {"id": 49, "seek": 20824, "start": 226.48000000000002, "end": 231.68, "text": " I'm going to receive a testing.b argument and that's this four, I have this four loop", "tokens": [286, 478, 516, 281, 4774, 257, 4997, 13, 65, 6770, 293, 300, 311, 341, 1451, 11, 286, 362, 341, 1451, 6367], "temperature": 0.0, "avg_logprob": -0.307489768318508, "compression_ratio": 1.4752475247524752, "no_speech_prob": 9.164682705886662e-05}, {"id": 50, "seek": 20824, "start": 231.68, "end": 232.68, "text": " inside.", "tokens": [1854, 13], "temperature": 0.0, "avg_logprob": -0.307489768318508, "compression_ratio": 1.4752475247524752, "no_speech_prob": 9.164682705886662e-05}, {"id": 51, "seek": 23268, "start": 232.68, "end": 239.6, "text": " And that is going to do all the job to give you the numbers and I show you now the numbers.", "tokens": [400, 300, 307, 516, 281, 360, 439, 264, 1691, 281, 976, 291, 264, 3547, 293, 286, 855, 291, 586, 264, 3547, 13], "temperature": 0.0, "avg_logprob": -0.2183216094970703, "compression_ratio": 1.5989304812834224, "no_speech_prob": 0.00013939078780822456}, {"id": 52, "seek": 23268, "start": 239.6, "end": 245.20000000000002, "text": " If I run this with Go bench, we got this dash bench dot.", "tokens": [759, 286, 1190, 341, 365, 1037, 10638, 11, 321, 658, 341, 8240, 10638, 5893, 13], "temperature": 0.0, "avg_logprob": -0.2183216094970703, "compression_ratio": 1.5989304812834224, "no_speech_prob": 0.00013939078780822456}, {"id": 53, "seek": 23268, "start": 245.20000000000002, "end": 248.76000000000002, "text": " The dot is a regular expression that means everything.", "tokens": [440, 5893, 307, 257, 3890, 6114, 300, 1355, 1203, 13], "temperature": 0.0, "avg_logprob": -0.2183216094970703, "compression_ratio": 1.5989304812834224, "no_speech_prob": 0.00013939078780822456}, {"id": 54, "seek": 23268, "start": 248.76000000000002, "end": 256.76, "text": " So you can use like the Go test run a regular expression for only executing certain benchmarks.", "tokens": [407, 291, 393, 764, 411, 264, 1037, 1500, 1190, 257, 3890, 6114, 337, 787, 32368, 1629, 43751, 13], "temperature": 0.0, "avg_logprob": -0.2183216094970703, "compression_ratio": 1.5989304812834224, "no_speech_prob": 0.00013939078780822456}, {"id": 55, "seek": 25676, "start": 256.76, "end": 266.88, "text": " And here you can see that MD5SAM is around twice time faster per operation than SHA.", "tokens": [400, 510, 291, 393, 536, 300, 22521, 20, 50, 2865, 307, 926, 6091, 565, 4663, 680, 6916, 813, 38820, 13], "temperature": 0.0, "avg_logprob": -0.12441321949899932, "compression_ratio": 1.450777202072539, "no_speech_prob": 7.44404605939053e-05}, {"id": 56, "seek": 25676, "start": 266.88, "end": 268.68, "text": " So well, just a number.", "tokens": [407, 731, 11, 445, 257, 1230, 13], "temperature": 0.0, "avg_logprob": -0.12441321949899932, "compression_ratio": 1.450777202072539, "no_speech_prob": 7.44404605939053e-05}, {"id": 57, "seek": 25676, "start": 268.68, "end": 269.68, "text": " It's that important.", "tokens": [467, 311, 300, 1021, 13], "temperature": 0.0, "avg_logprob": -0.12441321949899932, "compression_ratio": 1.450777202072539, "no_speech_prob": 7.44404605939053e-05}, {"id": 58, "seek": 25676, "start": 269.68, "end": 270.68, "text": " It depends.", "tokens": [467, 5946, 13], "temperature": 0.0, "avg_logprob": -0.12441321949899932, "compression_ratio": 1.450777202072539, "no_speech_prob": 7.44404605939053e-05}, {"id": 59, "seek": 25676, "start": 270.68, "end": 274.34, "text": " If you need more security, probably MD5 is not the best option.", "tokens": [759, 291, 643, 544, 3825, 11, 1391, 22521, 20, 307, 406, 264, 1151, 3614, 13], "temperature": 0.0, "avg_logprob": -0.12441321949899932, "compression_ratio": 1.450777202072539, "no_speech_prob": 7.44404605939053e-05}, {"id": 60, "seek": 25676, "start": 274.34, "end": 278.64, "text": " So it depends on your needs.", "tokens": [407, 309, 5946, 322, 428, 2203, 13], "temperature": 0.0, "avg_logprob": -0.12441321949899932, "compression_ratio": 1.450777202072539, "no_speech_prob": 7.44404605939053e-05}, {"id": 61, "seek": 25676, "start": 278.64, "end": 282.59999999999997, "text": " Another interesting thing is the allocations.", "tokens": [3996, 1880, 551, 307, 264, 12660, 763, 13], "temperature": 0.0, "avg_logprob": -0.12441321949899932, "compression_ratio": 1.450777202072539, "no_speech_prob": 7.44404605939053e-05}, {"id": 62, "seek": 28260, "start": 282.6, "end": 287.16, "text": " One thing that you maybe have heard is about counting allocations.", "tokens": [1485, 551, 300, 291, 1310, 362, 2198, 307, 466, 13251, 12660, 763, 13], "temperature": 0.0, "avg_logprob": -0.14012086691976594, "compression_ratio": 1.9721115537848606, "no_speech_prob": 0.00010463176295161247}, {"id": 63, "seek": 28260, "start": 287.16, "end": 289.52000000000004, "text": " Counting allocations, why is that important?", "tokens": [5247, 278, 12660, 763, 11, 983, 307, 300, 1021, 30], "temperature": 0.0, "avg_logprob": -0.14012086691976594, "compression_ratio": 1.9721115537848606, "no_speech_prob": 0.00010463176295161247}, {"id": 64, "seek": 28260, "start": 289.52000000000004, "end": 294.04, "text": " It's because every time we allocate something, when we talk allocation, we're talking about", "tokens": [467, 311, 570, 633, 565, 321, 35713, 746, 11, 562, 321, 751, 27599, 11, 321, 434, 1417, 466], "temperature": 0.0, "avg_logprob": -0.14012086691976594, "compression_ratio": 1.9721115537848606, "no_speech_prob": 0.00010463176295161247}, {"id": 65, "seek": 28260, "start": 294.04, "end": 295.92, "text": " allocation in the heap.", "tokens": [27599, 294, 264, 33591, 13], "temperature": 0.0, "avg_logprob": -0.14012086691976594, "compression_ratio": 1.9721115537848606, "no_speech_prob": 0.00010463176295161247}, {"id": 66, "seek": 28260, "start": 295.92, "end": 300.8, "text": " If every time we allocate something in the heap, allocating that is going to introduce", "tokens": [759, 633, 565, 321, 35713, 746, 294, 264, 33591, 11, 12660, 990, 300, 307, 516, 281, 5366], "temperature": 0.0, "avg_logprob": -0.14012086691976594, "compression_ratio": 1.9721115537848606, "no_speech_prob": 0.00010463176295161247}, {"id": 67, "seek": 28260, "start": 300.8, "end": 301.8, "text": " an overhead.", "tokens": [364, 19922, 13], "temperature": 0.0, "avg_logprob": -0.14012086691976594, "compression_ratio": 1.9721115537848606, "no_speech_prob": 0.00010463176295161247}, {"id": 68, "seek": 28260, "start": 301.8, "end": 306.08000000000004, "text": " And not only that, it's going to add more pressure to the garbage collector.", "tokens": [400, 406, 787, 300, 11, 309, 311, 516, 281, 909, 544, 3321, 281, 264, 14150, 23960, 13], "temperature": 0.0, "avg_logprob": -0.14012086691976594, "compression_ratio": 1.9721115537848606, "no_speech_prob": 0.00010463176295161247}, {"id": 69, "seek": 28260, "start": 306.08000000000004, "end": 311.08000000000004, "text": " That's why it's important to count the allocations when you are talking about performance.", "tokens": [663, 311, 983, 309, 311, 1021, 281, 1207, 264, 12660, 763, 562, 291, 366, 1417, 466, 3389, 13], "temperature": 0.0, "avg_logprob": -0.14012086691976594, "compression_ratio": 1.9721115537848606, "no_speech_prob": 0.00010463176295161247}, {"id": 70, "seek": 31108, "start": 311.08, "end": 317.0, "text": " If you are not worried about performance at that point, don't count the allocation.", "tokens": [759, 291, 366, 406, 5804, 466, 3389, 412, 300, 935, 11, 500, 380, 1207, 264, 27599, 13], "temperature": 0.0, "avg_logprob": -0.1772216189224108, "compression_ratio": 1.6610169491525424, "no_speech_prob": 2.035938268818427e-05}, {"id": 71, "seek": 31108, "start": 317.0, "end": 323.08, "text": " It's not that important and you are not going to gain a massive amount of performance from", "tokens": [467, 311, 406, 300, 1021, 293, 291, 366, 406, 516, 281, 6052, 257, 5994, 2372, 295, 3389, 490], "temperature": 0.0, "avg_logprob": -0.1772216189224108, "compression_ratio": 1.6610169491525424, "no_speech_prob": 2.035938268818427e-05}, {"id": 72, "seek": 31108, "start": 323.08, "end": 326.71999999999997, "text": " there if you are not in that point there.", "tokens": [456, 498, 291, 366, 406, 294, 300, 935, 456, 13], "temperature": 0.0, "avg_logprob": -0.1772216189224108, "compression_ratio": 1.6610169491525424, "no_speech_prob": 2.035938268818427e-05}, {"id": 73, "seek": 31108, "start": 326.71999999999997, "end": 327.71999999999997, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1772216189224108, "compression_ratio": 1.6610169491525424, "no_speech_prob": 2.035938268818427e-05}, {"id": 74, "seek": 31108, "start": 327.71999999999997, "end": 332.79999999999995, "text": " Let's see an example here in MD5 and SHA SAMs.", "tokens": [961, 311, 536, 364, 1365, 510, 294, 22521, 20, 293, 38820, 9617, 82, 13], "temperature": 0.0, "avg_logprob": -0.1772216189224108, "compression_ratio": 1.6610169491525424, "no_speech_prob": 2.035938268818427e-05}, {"id": 75, "seek": 31108, "start": 332.79999999999995, "end": 334.64, "text": " We have zero allocations.", "tokens": [492, 362, 4018, 12660, 763, 13], "temperature": 0.0, "avg_logprob": -0.1772216189224108, "compression_ratio": 1.6610169491525424, "no_speech_prob": 2.035938268818427e-05}, {"id": 76, "seek": 31108, "start": 334.64, "end": 337.84, "text": " So well, this data is not very useful for us now.", "tokens": [407, 731, 11, 341, 1412, 307, 406, 588, 4420, 337, 505, 586, 13], "temperature": 0.0, "avg_logprob": -0.1772216189224108, "compression_ratio": 1.6610169491525424, "no_speech_prob": 2.035938268818427e-05}, {"id": 77, "seek": 31108, "start": 337.84, "end": 339.4, "text": " So let's use another thing.", "tokens": [407, 718, 311, 764, 1071, 551, 13], "temperature": 0.0, "avg_logprob": -0.1772216189224108, "compression_ratio": 1.6610169491525424, "no_speech_prob": 2.035938268818427e-05}, {"id": 78, "seek": 31108, "start": 339.4, "end": 340.91999999999996, "text": " Let's open a file.", "tokens": [961, 311, 1269, 257, 3991, 13], "temperature": 0.0, "avg_logprob": -0.1772216189224108, "compression_ratio": 1.6610169491525424, "no_speech_prob": 2.035938268818427e-05}, {"id": 79, "seek": 34092, "start": 340.92, "end": 347.6, "text": " Let's open a file thousands of times and see how it goes.", "tokens": [961, 311, 1269, 257, 3991, 5383, 295, 1413, 293, 536, 577, 309, 1709, 13], "temperature": 0.0, "avg_logprob": -0.16945257494526525, "compression_ratio": 1.704225352112676, "no_speech_prob": 6.237327761482447e-05}, {"id": 80, "seek": 34092, "start": 347.6, "end": 352.24, "text": " Now I see that every single operation of opening a file, just opening the file, is going to", "tokens": [823, 286, 536, 300, 633, 2167, 6916, 295, 5193, 257, 3991, 11, 445, 5193, 264, 3991, 11, 307, 516, 281], "temperature": 0.0, "avg_logprob": -0.16945257494526525, "compression_ratio": 1.704225352112676, "no_speech_prob": 6.237327761482447e-05}, {"id": 81, "seek": 34092, "start": 352.24, "end": 354.08000000000004, "text": " generate three allocations.", "tokens": [8460, 1045, 12660, 763, 13], "temperature": 0.0, "avg_logprob": -0.16945257494526525, "compression_ratio": 1.704225352112676, "no_speech_prob": 6.237327761482447e-05}, {"id": 82, "seek": 34092, "start": 354.08000000000004, "end": 357.68, "text": " And it's going to consume 120 bytes per operation.", "tokens": [400, 309, 311, 516, 281, 14732, 10411, 36088, 680, 6916, 13], "temperature": 0.0, "avg_logprob": -0.16945257494526525, "compression_ratio": 1.704225352112676, "no_speech_prob": 6.237327761482447e-05}, {"id": 83, "seek": 34092, "start": 357.68, "end": 359.48, "text": " Interesting.", "tokens": [14711, 13], "temperature": 0.0, "avg_logprob": -0.16945257494526525, "compression_ratio": 1.704225352112676, "no_speech_prob": 6.237327761482447e-05}, {"id": 84, "seek": 34092, "start": 359.48, "end": 361.92, "text": " So now you are measuring things.", "tokens": [407, 586, 291, 366, 13389, 721, 13], "temperature": 0.0, "avg_logprob": -0.16945257494526525, "compression_ratio": 1.704225352112676, "no_speech_prob": 6.237327761482447e-05}, {"id": 85, "seek": 34092, "start": 361.92, "end": 368.24, "text": " You are measuring how much time it takes, how much time is gone in processing something,", "tokens": [509, 366, 13389, 577, 709, 565, 309, 2516, 11, 577, 709, 565, 307, 2780, 294, 9007, 746, 11], "temperature": 0.0, "avg_logprob": -0.16945257494526525, "compression_ratio": 1.704225352112676, "no_speech_prob": 6.237327761482447e-05}, {"id": 86, "seek": 36824, "start": 368.24, "end": 374.76, "text": " is going in allocating things, how much memory is gone there.", "tokens": [307, 516, 294, 12660, 990, 721, 11, 577, 709, 4675, 307, 2780, 456, 13], "temperature": 0.0, "avg_logprob": -0.1511580218439517, "compression_ratio": 1.7572815533980584, "no_speech_prob": 3.793350697378628e-05}, {"id": 87, "seek": 36824, "start": 374.76, "end": 381.40000000000003, "text": " So let's talk about profiling because once you, well, actually normally you do the profiling", "tokens": [407, 718, 311, 751, 466, 1740, 4883, 570, 1564, 291, 11, 731, 11, 767, 5646, 291, 360, 264, 1740, 4883], "temperature": 0.0, "avg_logprob": -0.1511580218439517, "compression_ratio": 1.7572815533980584, "no_speech_prob": 3.793350697378628e-05}, {"id": 88, "seek": 36824, "start": 381.40000000000003, "end": 389.92, "text": " first to find your bottleneck and then you do the benchmark to tune that bottleneck.", "tokens": [700, 281, 915, 428, 44641, 547, 293, 550, 291, 360, 264, 18927, 281, 10864, 300, 44641, 547, 13], "temperature": 0.0, "avg_logprob": -0.1511580218439517, "compression_ratio": 1.7572815533980584, "no_speech_prob": 3.793350697378628e-05}, {"id": 89, "seek": 36824, "start": 389.92, "end": 395.8, "text": " But I'm playing with the fact that I already have the benchmark and I'm going to do the", "tokens": [583, 286, 478, 2433, 365, 264, 1186, 300, 286, 1217, 362, 264, 18927, 293, 286, 478, 516, 281, 360, 264], "temperature": 0.0, "avg_logprob": -0.1511580218439517, "compression_ratio": 1.7572815533980584, "no_speech_prob": 3.793350697378628e-05}, {"id": 90, "seek": 36824, "start": 395.8, "end": 397.96000000000004, "text": " profiling on top of the benchmark.", "tokens": [1740, 4883, 322, 1192, 295, 264, 18927, 13], "temperature": 0.0, "avg_logprob": -0.1511580218439517, "compression_ratio": 1.7572815533980584, "no_speech_prob": 3.793350697378628e-05}, {"id": 91, "seek": 39796, "start": 397.96, "end": 402.52, "text": " So I'm going to execute the gobench, I'm going to pass the mem profile, I'm going to generate", "tokens": [407, 286, 478, 516, 281, 14483, 264, 352, 47244, 11, 286, 478, 516, 281, 1320, 264, 1334, 7964, 11, 286, 478, 516, 281, 8460], "temperature": 0.0, "avg_logprob": -0.2197964818854081, "compression_ratio": 1.9666666666666666, "no_speech_prob": 8.649792289361358e-05}, {"id": 92, "seek": 39796, "start": 402.52, "end": 405.96, "text": " the mem profile and I'm going to use the people of tool.", "tokens": [264, 1334, 7964, 293, 286, 478, 516, 281, 764, 264, 561, 295, 2290, 13], "temperature": 0.0, "avg_logprob": -0.2197964818854081, "compression_ratio": 1.9666666666666666, "no_speech_prob": 8.649792289361358e-05}, {"id": 93, "seek": 39796, "start": 405.96, "end": 409.2, "text": " The people of tool is going to allow me to analyze that profile.", "tokens": [440, 561, 295, 2290, 307, 516, 281, 2089, 385, 281, 12477, 300, 7964, 13], "temperature": 0.0, "avg_logprob": -0.2197964818854081, "compression_ratio": 1.9666666666666666, "no_speech_prob": 8.649792289361358e-05}, {"id": 94, "seek": 39796, "start": 409.2, "end": 416.71999999999997, "text": " In this case, I'm just asking for a text output and that text output is going to show me the", "tokens": [682, 341, 1389, 11, 286, 478, 445, 3365, 337, 257, 2487, 5598, 293, 300, 2487, 5598, 307, 516, 281, 855, 385, 264], "temperature": 0.0, "avg_logprob": -0.2197964818854081, "compression_ratio": 1.9666666666666666, "no_speech_prob": 8.649792289361358e-05}, {"id": 95, "seek": 39796, "start": 416.71999999999997, "end": 419.52, "text": " top consumers of memory in this case.", "tokens": [1192, 11883, 295, 4675, 294, 341, 1389, 13], "temperature": 0.0, "avg_logprob": -0.2197964818854081, "compression_ratio": 1.9666666666666666, "no_speech_prob": 8.649792289361358e-05}, {"id": 96, "seek": 39796, "start": 419.52, "end": 426.52, "text": " And I can see there that 84% of the memory is gone in OS new file.", "tokens": [400, 286, 393, 536, 456, 300, 29018, 4, 295, 264, 4675, 307, 2780, 294, 12731, 777, 3991, 13], "temperature": 0.0, "avg_logprob": -0.2197964818854081, "compression_ratio": 1.9666666666666666, "no_speech_prob": 8.649792289361358e-05}, {"id": 97, "seek": 42652, "start": 426.52, "end": 434.03999999999996, "text": " Okay, let's see what happened, okay, it's that file but I need more information, well,", "tokens": [1033, 11, 718, 311, 536, 437, 2011, 11, 1392, 11, 309, 311, 300, 3991, 457, 286, 643, 544, 1589, 11, 731, 11], "temperature": 0.0, "avg_logprob": -0.22224554958113704, "compression_ratio": 1.6719576719576719, "no_speech_prob": 3.522013867041096e-05}, {"id": 98, "seek": 42652, "start": 434.03999999999996, "end": 437.52, "text": " it's that function, sorry, I need more information.", "tokens": [309, 311, 300, 2445, 11, 2597, 11, 286, 643, 544, 1589, 13], "temperature": 0.0, "avg_logprob": -0.22224554958113704, "compression_ratio": 1.6719576719576719, "no_speech_prob": 3.522013867041096e-05}, {"id": 99, "seek": 42652, "start": 437.52, "end": 444.44, "text": " Actually I cannot like this output but if you don't like this output, you can, for example,", "tokens": [5135, 286, 2644, 411, 341, 5598, 457, 498, 291, 500, 380, 411, 341, 5598, 11, 291, 393, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.22224554958113704, "compression_ratio": 1.6719576719576719, "no_speech_prob": 3.522013867041096e-05}, {"id": 100, "seek": 42652, "start": 444.44, "end": 449.24, "text": " use SVG and you are going to get something like this that is very visual and actually", "tokens": [764, 31910, 38, 293, 291, 366, 516, 281, 483, 746, 411, 341, 300, 307, 588, 5056, 293, 767], "temperature": 0.0, "avg_logprob": -0.22224554958113704, "compression_ratio": 1.6719576719576719, "no_speech_prob": 3.522013867041096e-05}, {"id": 101, "seek": 44924, "start": 449.24, "end": 459.08, "text": " is kind of obvious that where is the bottleneck there and in this case, again, is OS new file.", "tokens": [307, 733, 295, 6322, 300, 689, 307, 264, 44641, 547, 456, 293, 294, 341, 1389, 11, 797, 11, 307, 12731, 777, 3991, 13], "temperature": 0.0, "avg_logprob": -0.1743215225838326, "compression_ratio": 1.583815028901734, "no_speech_prob": 1.3580351151176728e-05}, {"id": 102, "seek": 44924, "start": 459.08, "end": 465.36, "text": " If I go to the people of tool again and instead of that, I use the list of a function and", "tokens": [759, 286, 352, 281, 264, 561, 295, 2290, 797, 293, 2602, 295, 300, 11, 286, 764, 264, 1329, 295, 257, 2445, 293], "temperature": 0.0, "avg_logprob": -0.1743215225838326, "compression_ratio": 1.583815028901734, "no_speech_prob": 1.3580351151176728e-05}, {"id": 103, "seek": 44924, "start": 465.36, "end": 473.96000000000004, "text": " I'm seeing here where is the memory going by line and here I can see that in the line 127", "tokens": [286, 478, 2577, 510, 689, 307, 264, 4675, 516, 538, 1622, 293, 510, 286, 393, 536, 300, 294, 264, 1622, 47561], "temperature": 0.0, "avg_logprob": -0.1743215225838326, "compression_ratio": 1.583815028901734, "no_speech_prob": 1.3580351151176728e-05}, {"id": 104, "seek": 47396, "start": 473.96, "end": 479.52, "text": " of the file, fileunix.go, I'm consuming the memory.", "tokens": [295, 264, 3991, 11, 3991, 409, 970, 13, 1571, 11, 286, 478, 19867, 264, 4675, 13], "temperature": 0.0, "avg_logprob": -0.23116747538248697, "compression_ratio": 1.6116504854368932, "no_speech_prob": 6.704842235194519e-05}, {"id": 105, "seek": 47396, "start": 479.52, "end": 484.4, "text": " Actually there you see 74 megabytes, that is because it's counting all the allocation", "tokens": [5135, 456, 291, 536, 28868, 10816, 24538, 11, 300, 307, 570, 309, 311, 13251, 439, 264, 27599], "temperature": 0.0, "avg_logprob": -0.23116747538248697, "compression_ratio": 1.6116504854368932, "no_speech_prob": 6.704842235194519e-05}, {"id": 106, "seek": 47396, "start": 484.4, "end": 490.44, "text": " and aggregating all the allocations, it's not, every operation here is consuming only 120", "tokens": [293, 16743, 990, 439, 264, 12660, 763, 11, 309, 311, 406, 11, 633, 6916, 510, 307, 19867, 787, 10411], "temperature": 0.0, "avg_logprob": -0.23116747538248697, "compression_ratio": 1.6116504854368932, "no_speech_prob": 6.704842235194519e-05}, {"id": 107, "seek": 47396, "start": 490.44, "end": 492.44, "text": " bytes.", "tokens": [36088, 13], "temperature": 0.0, "avg_logprob": -0.23116747538248697, "compression_ratio": 1.6116504854368932, "no_speech_prob": 6.704842235194519e-05}, {"id": 108, "seek": 47396, "start": 492.44, "end": 503.08, "text": " Okay, the same with CPU profile, in this case, this is generating the most of the CPU consumption", "tokens": [1033, 11, 264, 912, 365, 13199, 7964, 11, 294, 341, 1389, 11, 341, 307, 17746, 264, 881, 295, 264, 13199, 12126], "temperature": 0.0, "avg_logprob": -0.23116747538248697, "compression_ratio": 1.6116504854368932, "no_speech_prob": 6.704842235194519e-05}, {"id": 109, "seek": 50308, "start": 503.08, "end": 513.12, "text": " is in Cisco 6, I can see in SVG, this time it's more scattered, so the CPU is consuming", "tokens": [307, 294, 38528, 1386, 11, 286, 393, 536, 294, 31910, 38, 11, 341, 565, 309, 311, 544, 21986, 11, 370, 264, 13199, 307, 19867], "temperature": 0.0, "avg_logprob": -0.1878849376331676, "compression_ratio": 1.6132075471698113, "no_speech_prob": 1.0427604138385504e-05}, {"id": 110, "seek": 50308, "start": 513.12, "end": 518.88, "text": " in way more places but still the Cisco 6 is the biggest one.", "tokens": [294, 636, 544, 3190, 457, 920, 264, 38528, 1386, 307, 264, 3880, 472, 13], "temperature": 0.0, "avg_logprob": -0.1878849376331676, "compression_ratio": 1.6132075471698113, "no_speech_prob": 1.0427604138385504e-05}, {"id": 111, "seek": 50308, "start": 518.88, "end": 524.84, "text": " So I'm going to list that and I see some assembly code, probably you are not going to optimize", "tokens": [407, 286, 478, 516, 281, 1329, 300, 293, 286, 536, 512, 12103, 3089, 11, 1391, 291, 366, 406, 516, 281, 19719], "temperature": 0.0, "avg_logprob": -0.1878849376331676, "compression_ratio": 1.6132075471698113, "no_speech_prob": 1.0427604138385504e-05}, {"id": 112, "seek": 50308, "start": 524.84, "end": 533.0, "text": " more this function, so probably this is not the place that you should be looking for optimizations", "tokens": [544, 341, 2445, 11, 370, 1391, 341, 307, 406, 264, 1081, 300, 291, 820, 312, 1237, 337, 5028, 14455], "temperature": 0.0, "avg_logprob": -0.1878849376331676, "compression_ratio": 1.6132075471698113, "no_speech_prob": 1.0427604138385504e-05}, {"id": 113, "seek": 53300, "start": 533.0, "end": 541.24, "text": " anyway, this is an example of getting to the root cause during the profiling.", "tokens": [4033, 11, 341, 307, 364, 1365, 295, 1242, 281, 264, 5593, 3082, 1830, 264, 1740, 4883, 13], "temperature": 0.0, "avg_logprob": -0.18560088248479933, "compression_ratio": 1.7461139896373057, "no_speech_prob": 1.423402954969788e-05}, {"id": 114, "seek": 53300, "start": 541.24, "end": 549.08, "text": " Okay, this talk is going to be more by examples, I'm going to try to show you some examples", "tokens": [1033, 11, 341, 751, 307, 516, 281, 312, 544, 538, 5110, 11, 286, 478, 516, 281, 853, 281, 855, 291, 512, 5110], "temperature": 0.0, "avg_logprob": -0.18560088248479933, "compression_ratio": 1.7461139896373057, "no_speech_prob": 1.423402954969788e-05}, {"id": 115, "seek": 53300, "start": 549.08, "end": 556.44, "text": " of optimizations, it's just to show you the process more than the specific optimization,", "tokens": [295, 5028, 14455, 11, 309, 311, 445, 281, 855, 291, 264, 1399, 544, 813, 264, 2685, 19618, 11], "temperature": 0.0, "avg_logprob": -0.18560088248479933, "compression_ratio": 1.7461139896373057, "no_speech_prob": 1.423402954969788e-05}, {"id": 116, "seek": 53300, "start": 556.44, "end": 561.96, "text": " I expect you learn something in between but it's more about the process, okay.", "tokens": [286, 2066, 291, 1466, 746, 294, 1296, 457, 309, 311, 544, 466, 264, 1399, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.18560088248479933, "compression_ratio": 1.7461139896373057, "no_speech_prob": 1.423402954969788e-05}, {"id": 117, "seek": 56196, "start": 561.96, "end": 568.48, "text": " One of the things that you can do is reducing the CPU usage, this is a kind of silly example,", "tokens": [1485, 295, 264, 721, 300, 291, 393, 360, 307, 12245, 264, 13199, 14924, 11, 341, 307, 257, 733, 295, 11774, 1365, 11], "temperature": 0.0, "avg_logprob": -0.16340989158267066, "compression_ratio": 1.6318407960199004, "no_speech_prob": 1.4712835763930343e-05}, {"id": 118, "seek": 56196, "start": 568.48, "end": 573.6800000000001, "text": " you have a fine function that have a needle and a high stack and just go through the high", "tokens": [291, 362, 257, 2489, 2445, 300, 362, 257, 11037, 293, 257, 1090, 8630, 293, 445, 352, 807, 264, 1090], "temperature": 0.0, "avg_logprob": -0.16340989158267066, "compression_ratio": 1.6318407960199004, "no_speech_prob": 1.4712835763930343e-05}, {"id": 119, "seek": 56196, "start": 573.6800000000001, "end": 580.12, "text": " stack and search for that needle and give you the result.", "tokens": [8630, 293, 3164, 337, 300, 11037, 293, 976, 291, 264, 1874, 13], "temperature": 0.0, "avg_logprob": -0.16340989158267066, "compression_ratio": 1.6318407960199004, "no_speech_prob": 1.4712835763930343e-05}, {"id": 120, "seek": 56196, "start": 580.12, "end": 588.5600000000001, "text": " This is looping over the whole string or the whole slice, I'm going to do a benchmark,", "tokens": [639, 307, 6367, 278, 670, 264, 1379, 6798, 420, 264, 1379, 13153, 11, 286, 478, 516, 281, 360, 257, 18927, 11], "temperature": 0.0, "avg_logprob": -0.16340989158267066, "compression_ratio": 1.6318407960199004, "no_speech_prob": 1.4712835763930343e-05}, {"id": 121, "seek": 58856, "start": 588.56, "end": 595.56, "text": " the first thing, I'm going to do the benchmark, I'm going to generate a lot of strings and", "tokens": [264, 700, 551, 11, 286, 478, 516, 281, 360, 264, 18927, 11, 286, 478, 516, 281, 8460, 257, 688, 295, 13985, 293], "temperature": 0.0, "avg_logprob": -0.162497878074646, "compression_ratio": 1.7861635220125787, "no_speech_prob": 4.4300824811216444e-05}, {"id": 122, "seek": 58856, "start": 595.56, "end": 602.56, "text": " I'm going to do a benchmark looking for something around in the middle, it's not exactly in", "tokens": [286, 478, 516, 281, 360, 257, 18927, 1237, 337, 746, 926, 294, 264, 2808, 11, 309, 311, 406, 2293, 294], "temperature": 0.0, "avg_logprob": -0.162497878074646, "compression_ratio": 1.7861635220125787, "no_speech_prob": 4.4300824811216444e-05}, {"id": 123, "seek": 58856, "start": 602.56, "end": 613.88, "text": " the middle but it's around there and the benchmark is saying that it's taking nearly 300 nanoseconds.", "tokens": [264, 2808, 457, 309, 311, 926, 456, 293, 264, 18927, 307, 1566, 300, 309, 311, 1940, 6217, 6641, 14067, 541, 28750, 13], "temperature": 0.0, "avg_logprob": -0.162497878074646, "compression_ratio": 1.7861635220125787, "no_speech_prob": 4.4300824811216444e-05}, {"id": 124, "seek": 61388, "start": 613.88, "end": 620.12, "text": " If I just early return that is just a kind of silly optimization, it's not super smart", "tokens": [759, 286, 445, 2440, 2736, 300, 307, 445, 257, 733, 295, 11774, 19618, 11, 309, 311, 406, 1687, 4069], "temperature": 0.0, "avg_logprob": -0.17571149553571427, "compression_ratio": 1.7129186602870814, "no_speech_prob": 1.4664819900644943e-05}, {"id": 125, "seek": 61388, "start": 620.12, "end": 627.36, "text": " or something like that, I'm going to save basically almost the half of the performance,", "tokens": [420, 746, 411, 300, 11, 286, 478, 516, 281, 3155, 1936, 1920, 264, 1922, 295, 264, 3389, 11], "temperature": 0.0, "avg_logprob": -0.17571149553571427, "compression_ratio": 1.7129186602870814, "no_speech_prob": 1.4664819900644943e-05}, {"id": 126, "seek": 61388, "start": 627.36, "end": 633.36, "text": " this is because the benchmark is doing something really silly and it can vary depending on", "tokens": [341, 307, 570, 264, 18927, 307, 884, 746, 534, 11774, 293, 309, 393, 10559, 5413, 322], "temperature": 0.0, "avg_logprob": -0.17571149553571427, "compression_ratio": 1.7129186602870814, "no_speech_prob": 1.4664819900644943e-05}, {"id": 127, "seek": 61388, "start": 633.36, "end": 640.56, "text": " the data that it inputs but it's an optimization is just doing less, that is one of the best", "tokens": [264, 1412, 300, 309, 15743, 457, 309, 311, 364, 19618, 307, 445, 884, 1570, 11, 300, 307, 472, 295, 264, 1151], "temperature": 0.0, "avg_logprob": -0.17571149553571427, "compression_ratio": 1.7129186602870814, "no_speech_prob": 1.4664819900644943e-05}, {"id": 128, "seek": 64056, "start": 640.56, "end": 644.68, "text": " ways of optimizing things.", "tokens": [2098, 295, 40425, 721, 13], "temperature": 0.0, "avg_logprob": -0.21435318495097913, "compression_ratio": 1.7175141242937852, "no_speech_prob": 2.9130738766980357e-05}, {"id": 129, "seek": 64056, "start": 644.68, "end": 649.0799999999999, "text": " Reducing allocations, one of the classic example of reducing allocations is when you are dealing", "tokens": [4477, 1311, 278, 12660, 763, 11, 472, 295, 264, 7230, 1365, 295, 12245, 12660, 763, 307, 562, 291, 366, 6260], "temperature": 0.0, "avg_logprob": -0.21435318495097913, "compression_ratio": 1.7175141242937852, "no_speech_prob": 2.9130738766980357e-05}, {"id": 130, "seek": 64056, "start": 649.0799999999999, "end": 655.52, "text": " with slices, when you have a slice, for example this is a common way of constructing a slice,", "tokens": [365, 19793, 11, 562, 291, 362, 257, 13153, 11, 337, 1365, 341, 307, 257, 2689, 636, 295, 39969, 257, 13153, 11], "temperature": 0.0, "avg_logprob": -0.21435318495097913, "compression_ratio": 1.7175141242937852, "no_speech_prob": 2.9130738766980357e-05}, {"id": 131, "seek": 64056, "start": 655.52, "end": 661.04, "text": " I create a slice, I loop over this, generate a loop and start appending things to that", "tokens": [286, 1884, 257, 13153, 11, 286, 6367, 670, 341, 11, 8460, 257, 6367, 293, 722, 724, 2029, 721, 281, 300], "temperature": 0.0, "avg_logprob": -0.21435318495097913, "compression_ratio": 1.7175141242937852, "no_speech_prob": 2.9130738766980357e-05}, {"id": 132, "seek": 66104, "start": 661.04, "end": 671.8399999999999, "text": " slice, okay fine, I'm going to do a benchmark for checking that and it's taking 39 allocations", "tokens": [13153, 11, 1392, 2489, 11, 286, 478, 516, 281, 360, 257, 18927, 337, 8568, 300, 293, 309, 311, 1940, 15238, 12660, 763], "temperature": 0.0, "avg_logprob": -0.21288702011108399, "compression_ratio": 1.381679389312977, "no_speech_prob": 4.77717439935077e-05}, {"id": 133, "seek": 66104, "start": 671.8399999999999, "end": 683.56, "text": " and around 41 megabytes per operation, okay sounds like a lot, okay let's do it, let's", "tokens": [293, 926, 18173, 10816, 24538, 680, 6916, 11, 1392, 3263, 411, 257, 688, 11, 1392, 718, 311, 360, 309, 11, 718, 311], "temperature": 0.0, "avg_logprob": -0.21288702011108399, "compression_ratio": 1.381679389312977, "no_speech_prob": 4.77717439935077e-05}, {"id": 134, "seek": 68356, "start": 683.56, "end": 692.7199999999999, "text": " do this, let's build the slice but we are going to give an initial size of a million", "tokens": [360, 341, 11, 718, 311, 1322, 264, 13153, 457, 321, 366, 516, 281, 976, 364, 5883, 2744, 295, 257, 2459], "temperature": 0.0, "avg_logprob": -0.14720343053340912, "compression_ratio": 1.5766871165644172, "no_speech_prob": 2.4917662813095376e-05}, {"id": 135, "seek": 68356, "start": 692.7199999999999, "end": 697.92, "text": " and the time I'm just setting that, the final result is exactly the same but now we have", "tokens": [293, 264, 565, 286, 478, 445, 3287, 300, 11, 264, 2572, 1874, 307, 2293, 264, 912, 457, 586, 321, 362], "temperature": 0.0, "avg_logprob": -0.14720343053340912, "compression_ratio": 1.5766871165644172, "no_speech_prob": 2.4917662813095376e-05}, {"id": 136, "seek": 68356, "start": 697.92, "end": 702.8, "text": " one allocation and we have consumed only one megabyte and actually if you see there", "tokens": [472, 27599, 293, 321, 362, 21226, 787, 472, 10816, 34529, 293, 767, 498, 291, 536, 456], "temperature": 0.0, "avg_logprob": -0.14720343053340912, "compression_ratio": 1.5766871165644172, "no_speech_prob": 2.4917662813095376e-05}, {"id": 137, "seek": 70280, "start": 702.8, "end": 721.4399999999999, "text": " is around 800 microseconds and here you have around 10 milliseconds, so it's a lot of time", "tokens": [307, 926, 13083, 3123, 37841, 28750, 293, 510, 291, 362, 926, 1266, 34184, 11, 370, 309, 311, 257, 688, 295, 565], "temperature": 0.0, "avg_logprob": -0.1874146064122518, "compression_ratio": 1.366412213740458, "no_speech_prob": 2.949154077214189e-05}, {"id": 138, "seek": 70280, "start": 721.4399999999999, "end": 728.16, "text": " actually, a lot of CPU time too but you can squeeze it more, if you know that at compile", "tokens": [767, 11, 257, 688, 295, 13199, 565, 886, 457, 291, 393, 13578, 309, 544, 11, 498, 291, 458, 300, 412, 31413], "temperature": 0.0, "avg_logprob": -0.1874146064122518, "compression_ratio": 1.366412213740458, "no_speech_prob": 2.949154077214189e-05}, {"id": 139, "seek": 72816, "start": 728.16, "end": 732.7199999999999, "text": " time, if you know exactly the size that you want to have at compile time, you can build", "tokens": [565, 11, 498, 291, 458, 2293, 264, 2744, 300, 291, 528, 281, 362, 412, 31413, 565, 11, 291, 393, 1322], "temperature": 0.0, "avg_logprob": -0.19977778973786728, "compression_ratio": 1.6621004566210045, "no_speech_prob": 8.39233907754533e-05}, {"id": 140, "seek": 72816, "start": 732.7199999999999, "end": 740.7199999999999, "text": " an array, it's faster than any slice actually, so if I build an array I'm now doing zero", "tokens": [364, 10225, 11, 309, 311, 4663, 813, 604, 13153, 767, 11, 370, 498, 286, 1322, 364, 10225, 286, 478, 586, 884, 4018], "temperature": 0.0, "avg_logprob": -0.19977778973786728, "compression_ratio": 1.6621004566210045, "no_speech_prob": 8.39233907754533e-05}, {"id": 141, "seek": 72816, "start": 740.7199999999999, "end": 748.64, "text": " allocation, zero heap allocations, it's going to go in the stack or in binary somehow, whatever", "tokens": [27599, 11, 4018, 33591, 12660, 763, 11, 309, 311, 516, 281, 352, 294, 264, 8630, 420, 294, 17434, 6063, 11, 2035], "temperature": 0.0, "avg_logprob": -0.19977778973786728, "compression_ratio": 1.6621004566210045, "no_speech_prob": 8.39233907754533e-05}, {"id": 142, "seek": 72816, "start": 748.64, "end": 758.0, "text": " but it's not consuming my heap allocations and this time is 300 microseconds approximately,", "tokens": [457, 309, 311, 406, 19867, 452, 33591, 12660, 763, 293, 341, 565, 307, 6641, 3123, 37841, 28750, 10447, 11], "temperature": 0.0, "avg_logprob": -0.19977778973786728, "compression_ratio": 1.6621004566210045, "no_speech_prob": 8.39233907754533e-05}, {"id": 143, "seek": 75800, "start": 758.0, "end": 766.48, "text": " so an interesting thing if you know that information at compile time, okay another thing is packing,", "tokens": [370, 364, 1880, 551, 498, 291, 458, 300, 1589, 412, 31413, 565, 11, 1392, 1071, 551, 307, 20815, 11], "temperature": 0.0, "avg_logprob": -0.1776330206129286, "compression_ratio": 1.6383928571428572, "no_speech_prob": 1.5118277588044293e-05}, {"id": 144, "seek": 75800, "start": 766.48, "end": 772.32, "text": " if you are concerned about memory you can build this struct and say okay I have a Boolean,", "tokens": [498, 291, 366, 5922, 466, 4675, 291, 393, 1322, 341, 6594, 293, 584, 1392, 286, 362, 257, 23351, 28499, 11], "temperature": 0.0, "avg_logprob": -0.1776330206129286, "compression_ratio": 1.6383928571428572, "no_speech_prob": 1.5118277588044293e-05}, {"id": 145, "seek": 75800, "start": 772.32, "end": 781.28, "text": " I have a float, I have an N32 and the goal compiler is going to align my struct to make", "tokens": [286, 362, 257, 15706, 11, 286, 362, 364, 426, 11440, 293, 264, 3387, 31958, 307, 516, 281, 7975, 452, 6594, 281, 652], "temperature": 0.0, "avg_logprob": -0.1776330206129286, "compression_ratio": 1.6383928571428572, "no_speech_prob": 1.5118277588044293e-05}, {"id": 146, "seek": 75800, "start": 781.28, "end": 787.08, "text": " it more efficient and work better with the CPU and all that stuff and in this case it's", "tokens": [309, 544, 7148, 293, 589, 1101, 365, 264, 13199, 293, 439, 300, 1507, 293, 294, 341, 1389, 309, 311], "temperature": 0.0, "avg_logprob": -0.1776330206129286, "compression_ratio": 1.6383928571428572, "no_speech_prob": 1.5118277588044293e-05}, {"id": 147, "seek": 78708, "start": 787.08, "end": 792.5600000000001, "text": " just adding seven bytes between the Boolean and the float and four bytes after the integer", "tokens": [445, 5127, 3407, 36088, 1296, 264, 23351, 28499, 293, 264, 15706, 293, 1451, 36088, 934, 264, 24922], "temperature": 0.0, "avg_logprob": -0.19812806073357078, "compression_ratio": 1.662037037037037, "no_speech_prob": 5.6557026255177334e-05}, {"id": 148, "seek": 78708, "start": 792.5600000000001, "end": 801.32, "text": " to get everything aligned, okay I built a slice and initialized a slice and I'm allocating", "tokens": [281, 483, 1203, 17962, 11, 1392, 286, 3094, 257, 13153, 293, 5883, 1602, 257, 13153, 293, 286, 478, 12660, 990], "temperature": 0.0, "avg_logprob": -0.19812806073357078, "compression_ratio": 1.662037037037037, "no_speech_prob": 5.6557026255177334e-05}, {"id": 149, "seek": 78708, "start": 801.32, "end": 807.88, "text": " one time because that's what the slice is doing and I'm consuming around 24 megabytes", "tokens": [472, 565, 570, 300, 311, 437, 264, 13153, 307, 884, 293, 286, 478, 19867, 926, 4022, 10816, 24538], "temperature": 0.0, "avg_logprob": -0.19812806073357078, "compression_ratio": 1.662037037037037, "no_speech_prob": 5.6557026255177334e-05}, {"id": 150, "seek": 78708, "start": 807.88, "end": 814.48, "text": " per operation, if I just organize the struct, in this case I put the float at the beginning", "tokens": [680, 6916, 11, 498, 286, 445, 13859, 264, 6594, 11, 294, 341, 1389, 286, 829, 264, 15706, 412, 264, 2863], "temperature": 0.0, "avg_logprob": -0.19812806073357078, "compression_ratio": 1.662037037037037, "no_speech_prob": 5.6557026255177334e-05}, {"id": 151, "seek": 81448, "start": 814.48, "end": 819.12, "text": " then the integer 32 and then the Boolean, the compiler is only going to add three bytes", "tokens": [550, 264, 24922, 8858, 293, 550, 264, 23351, 28499, 11, 264, 31958, 307, 787, 516, 281, 909, 1045, 36088], "temperature": 0.0, "avg_logprob": -0.14138240814208985, "compression_ratio": 1.751111111111111, "no_speech_prob": 1.9774970496655442e-05}, {"id": 152, "seek": 81448, "start": 819.12, "end": 826.32, "text": " so the whole structure is going to be smaller in memory and in this case now is 16 megabytes", "tokens": [370, 264, 1379, 3877, 307, 516, 281, 312, 4356, 294, 4675, 293, 294, 341, 1389, 586, 307, 3165, 10816, 24538], "temperature": 0.0, "avg_logprob": -0.14138240814208985, "compression_ratio": 1.751111111111111, "no_speech_prob": 1.9774970496655442e-05}, {"id": 153, "seek": 81448, "start": 826.32, "end": 830.96, "text": " per operation, so this kind of optimization is not going to save your day, if you are", "tokens": [680, 6916, 11, 370, 341, 733, 295, 19618, 307, 406, 516, 281, 3155, 428, 786, 11, 498, 291, 366], "temperature": 0.0, "avg_logprob": -0.14138240814208985, "compression_ratio": 1.751111111111111, "no_speech_prob": 1.9774970496655442e-05}, {"id": 154, "seek": 81448, "start": 830.96, "end": 837.2, "text": " just creating some structs but if you are creating millions of instances of an struct", "tokens": [445, 4084, 512, 6594, 82, 457, 498, 291, 366, 4084, 6803, 295, 14519, 295, 364, 6594], "temperature": 0.0, "avg_logprob": -0.14138240814208985, "compression_ratio": 1.751111111111111, "no_speech_prob": 1.9774970496655442e-05}, {"id": 155, "seek": 81448, "start": 837.2, "end": 842.04, "text": " it can be a significant amount of memory.", "tokens": [309, 393, 312, 257, 4776, 2372, 295, 4675, 13], "temperature": 0.0, "avg_logprob": -0.14138240814208985, "compression_ratio": 1.751111111111111, "no_speech_prob": 1.9774970496655442e-05}, {"id": 156, "seek": 84204, "start": 842.04, "end": 846.7199999999999, "text": " Function in lining, function in lining is something that the goal compiler does for us", "tokens": [11166, 882, 294, 19628, 11, 2445, 294, 19628, 307, 746, 300, 264, 3387, 31958, 775, 337, 505], "temperature": 0.0, "avg_logprob": -0.1804674770055192, "compression_ratio": 1.832512315270936, "no_speech_prob": 2.3986129235709086e-05}, {"id": 157, "seek": 84204, "start": 846.7199999999999, "end": 852.64, "text": " is just taking a function and replacing any call to that function with the code that is", "tokens": [307, 445, 1940, 257, 2445, 293, 19139, 604, 818, 281, 300, 2445, 365, 264, 3089, 300, 307], "temperature": 0.0, "avg_logprob": -0.1804674770055192, "compression_ratio": 1.832512315270936, "no_speech_prob": 2.3986129235709086e-05}, {"id": 158, "seek": 84204, "start": 852.64, "end": 857.28, "text": " generated by the function.", "tokens": [10833, 538, 264, 2445, 13], "temperature": 0.0, "avg_logprob": -0.1804674770055192, "compression_ratio": 1.832512315270936, "no_speech_prob": 2.3986129235709086e-05}, {"id": 159, "seek": 84204, "start": 857.28, "end": 863.76, "text": " I'm going to show you a very damn example, I'm not inlining this function explicitly", "tokens": [286, 478, 516, 281, 855, 291, 257, 588, 8151, 1365, 11, 286, 478, 406, 294, 31079, 341, 2445, 20803], "temperature": 0.0, "avg_logprob": -0.1804674770055192, "compression_ratio": 1.832512315270936, "no_speech_prob": 2.3986129235709086e-05}, {"id": 160, "seek": 84204, "start": 863.76, "end": 869.24, "text": " and I'm using the inlined version that is going to be inlined by the compiler because", "tokens": [293, 286, 478, 1228, 264, 294, 13564, 3037, 300, 307, 516, 281, 312, 294, 13564, 538, 264, 31958, 570], "temperature": 0.0, "avg_logprob": -0.1804674770055192, "compression_ratio": 1.832512315270936, "no_speech_prob": 2.3986129235709086e-05}, {"id": 161, "seek": 86924, "start": 869.24, "end": 878.04, "text": " it's simple enough and then I'm going to execute that, I'm saving a whole nanosecond there,", "tokens": [309, 311, 2199, 1547, 293, 550, 286, 478, 516, 281, 14483, 300, 11, 286, 478, 6816, 257, 1379, 14067, 541, 18882, 456, 11], "temperature": 0.0, "avg_logprob": -0.16306854330975076, "compression_ratio": 1.7647058823529411, "no_speech_prob": 2.0910612875013612e-05}, {"id": 162, "seek": 86924, "start": 878.04, "end": 886.24, "text": " so yeah it's not a great optimization to be honest, probably you don't care about that", "tokens": [370, 1338, 309, 311, 406, 257, 869, 19618, 281, 312, 3245, 11, 1391, 291, 500, 380, 1127, 466, 300], "temperature": 0.0, "avg_logprob": -0.16306854330975076, "compression_ratio": 1.7647058823529411, "no_speech_prob": 2.0910612875013612e-05}, {"id": 163, "seek": 86924, "start": 886.24, "end": 893.12, "text": " nanosecond but we are going to see why that is important later, not because of the nanosecond.", "tokens": [14067, 541, 18882, 457, 321, 366, 516, 281, 536, 983, 300, 307, 1021, 1780, 11, 406, 570, 295, 264, 14067, 541, 18882, 13], "temperature": 0.0, "avg_logprob": -0.16306854330975076, "compression_ratio": 1.7647058823529411, "no_speech_prob": 2.0910612875013612e-05}, {"id": 164, "seek": 86924, "start": 893.12, "end": 896.6, "text": " I'm going to talk now about escape analysis, escape analysis is another thing that the", "tokens": [286, 478, 516, 281, 751, 586, 466, 7615, 5215, 11, 7615, 5215, 307, 1071, 551, 300, 264], "temperature": 0.0, "avg_logprob": -0.16306854330975076, "compression_ratio": 1.7647058823529411, "no_speech_prob": 2.0910612875013612e-05}, {"id": 165, "seek": 89660, "start": 896.6, "end": 902.08, "text": " compiler does for us and basically analyzes our variables and decides when a variable", "tokens": [31958, 775, 337, 505, 293, 1936, 6459, 12214, 527, 9102, 293, 14898, 562, 257, 7006], "temperature": 0.0, "avg_logprob": -0.16588577223412784, "compression_ratio": 1.8814432989690721, "no_speech_prob": 0.00011840194201795384}, {"id": 166, "seek": 89660, "start": 902.08, "end": 909.48, "text": " escapes from the context of the stack, it's something that is no longer able to get the", "tokens": [43769, 490, 264, 4319, 295, 264, 8630, 11, 309, 311, 746, 300, 307, 572, 2854, 1075, 281, 483, 264], "temperature": 0.0, "avg_logprob": -0.16588577223412784, "compression_ratio": 1.8814432989690721, "no_speech_prob": 0.00011840194201795384}, {"id": 167, "seek": 89660, "start": 909.48, "end": 914.72, "text": " information from the stack or store the information from the stack and be accessible where it", "tokens": [1589, 490, 264, 8630, 420, 3531, 264, 1589, 490, 264, 8630, 293, 312, 9515, 689, 309], "temperature": 0.0, "avg_logprob": -0.16588577223412784, "compression_ratio": 1.8814432989690721, "no_speech_prob": 0.00011840194201795384}, {"id": 168, "seek": 89660, "start": 914.72, "end": 922.52, "text": " needs to be accessible so it needs to escape to the heap, so it's what generates that allocations", "tokens": [2203, 281, 312, 9515, 370, 309, 2203, 281, 7615, 281, 264, 33591, 11, 370, 309, 311, 437, 23815, 300, 12660, 763], "temperature": 0.0, "avg_logprob": -0.16588577223412784, "compression_ratio": 1.8814432989690721, "no_speech_prob": 0.00011840194201795384}, {"id": 169, "seek": 92252, "start": 922.52, "end": 929.48, "text": " and we have seen that allocations have certain implications, so let's see an example here,", "tokens": [293, 321, 362, 1612, 300, 12660, 763, 362, 1629, 16602, 11, 370, 718, 311, 536, 364, 1365, 510, 11], "temperature": 0.0, "avg_logprob": -0.1451615958378233, "compression_ratio": 1.9067357512953367, "no_speech_prob": 3.915390698239207e-05}, {"id": 170, "seek": 92252, "start": 929.48, "end": 938.12, "text": " this is another inline function that returns a pointer that is going to generate an allocation,", "tokens": [341, 307, 1071, 294, 1889, 2445, 300, 11247, 257, 23918, 300, 307, 516, 281, 8460, 364, 27599, 11], "temperature": 0.0, "avg_logprob": -0.1451615958378233, "compression_ratio": 1.9067357512953367, "no_speech_prob": 3.915390698239207e-05}, {"id": 171, "seek": 92252, "start": 938.12, "end": 943.28, "text": " this is something that returns by value, a value is going to copy the value to the stack", "tokens": [341, 307, 746, 300, 11247, 538, 2158, 11, 257, 2158, 307, 516, 281, 5055, 264, 2158, 281, 264, 8630], "temperature": 0.0, "avg_logprob": -0.1451615958378233, "compression_ratio": 1.9067357512953367, "no_speech_prob": 3.915390698239207e-05}, {"id": 172, "seek": 92252, "start": 943.28, "end": 950.04, "text": " of the caller so it's not going to generate allocations, so we can see that in the benchmark", "tokens": [295, 264, 48324, 370, 309, 311, 406, 516, 281, 8460, 12660, 763, 11, 370, 321, 393, 536, 300, 294, 264, 18927], "temperature": 0.0, "avg_logprob": -0.1451615958378233, "compression_ratio": 1.9067357512953367, "no_speech_prob": 3.915390698239207e-05}, {"id": 173, "seek": 95004, "start": 950.04, "end": 956.16, "text": " that is saying the first version have one allocation and it's allocating 8 bytes and", "tokens": [300, 307, 1566, 264, 700, 3037, 362, 472, 27599, 293, 309, 311, 12660, 990, 1649, 36088, 293], "temperature": 0.0, "avg_logprob": -0.15455037071591332, "compression_ratio": 1.778894472361809, "no_speech_prob": 0.00018184399232268333}, {"id": 174, "seek": 95004, "start": 956.16, "end": 961.8399999999999, "text": " the second one have 0 allocations and actually you can see there is one allocation and it's", "tokens": [264, 1150, 472, 362, 1958, 12660, 763, 293, 767, 291, 393, 536, 456, 307, 472, 27599, 293, 309, 311], "temperature": 0.0, "avg_logprob": -0.15455037071591332, "compression_ratio": 1.778894472361809, "no_speech_prob": 0.00018184399232268333}, {"id": 175, "seek": 95004, "start": 961.8399999999999, "end": 972.0, "text": " taking 10 times more to do that, 10 times more in this case is around 12 nanoseconds", "tokens": [1940, 1266, 1413, 544, 281, 360, 300, 11, 1266, 1413, 544, 294, 341, 1389, 307, 926, 2272, 14067, 541, 28750], "temperature": 0.0, "avg_logprob": -0.15455037071591332, "compression_ratio": 1.778894472361809, "no_speech_prob": 0.00018184399232268333}, {"id": 176, "seek": 95004, "start": 972.0, "end": 978.8399999999999, "text": " that is not a lot but everything adds up at the end especially when you are calling millions", "tokens": [300, 307, 406, 257, 688, 457, 1203, 10860, 493, 412, 264, 917, 2318, 562, 291, 366, 5141, 6803], "temperature": 0.0, "avg_logprob": -0.15455037071591332, "compression_ratio": 1.778894472361809, "no_speech_prob": 0.00018184399232268333}, {"id": 177, "seek": 97884, "start": 978.84, "end": 987.9200000000001, "text": " of times of things, okay and one interesting thing is escape analysis plus inlining, why?", "tokens": [295, 1413, 295, 721, 11, 1392, 293, 472, 1880, 551, 307, 7615, 5215, 1804, 294, 31079, 11, 983, 30], "temperature": 0.0, "avg_logprob": -0.22690534591674805, "compression_ratio": 1.7230046948356808, "no_speech_prob": 2.598849641799461e-05}, {"id": 178, "seek": 97884, "start": 987.9200000000001, "end": 994.6800000000001, "text": " Well imagine this situation you have a struct, a function that generates or instantiate that", "tokens": [1042, 3811, 341, 2590, 291, 362, 257, 6594, 11, 257, 2445, 300, 23815, 420, 9836, 13024, 300], "temperature": 0.0, "avg_logprob": -0.22690534591674805, "compression_ratio": 1.7230046948356808, "no_speech_prob": 2.598849641799461e-05}, {"id": 179, "seek": 97884, "start": 994.6800000000001, "end": 1000.52, "text": " struct and the constructor of that extract, okay, the constructor returns me a pointer", "tokens": [6594, 293, 264, 47479, 295, 300, 8947, 11, 1392, 11, 264, 47479, 11247, 385, 257, 23918], "temperature": 0.0, "avg_logprob": -0.22690534591674805, "compression_ratio": 1.7230046948356808, "no_speech_prob": 2.598849641799461e-05}, {"id": 180, "seek": 97884, "start": 1000.52, "end": 1008.8000000000001, "text": " and do all the stuff that it needs, okay great, it is generating 3 allocations and it's consuming", "tokens": [293, 360, 439, 264, 1507, 300, 309, 2203, 11, 1392, 869, 11, 309, 307, 17746, 805, 12660, 763, 293, 309, 311, 19867], "temperature": 0.0, "avg_logprob": -0.22690534591674805, "compression_ratio": 1.7230046948356808, "no_speech_prob": 2.598849641799461e-05}, {"id": 181, "seek": 100880, "start": 1008.8, "end": 1018.52, "text": " 56 bytes per operation, okay, what happen if I just move the logic of that initialization", "tokens": [19687, 36088, 680, 6916, 11, 1392, 11, 437, 1051, 498, 286, 445, 1286, 264, 9952, 295, 300, 5883, 2144], "temperature": 0.0, "avg_logprob": -0.14762436050966563, "compression_ratio": 1.6775700934579438, "no_speech_prob": 3.240539081161842e-05}, {"id": 182, "seek": 100880, "start": 1018.52, "end": 1025.3999999999999, "text": " process into a different function, if we do that suddenly the new document is simple enough", "tokens": [1399, 666, 257, 819, 2445, 11, 498, 321, 360, 300, 5800, 264, 777, 4166, 307, 2199, 1547], "temperature": 0.0, "avg_logprob": -0.14762436050966563, "compression_ratio": 1.6775700934579438, "no_speech_prob": 3.240539081161842e-05}, {"id": 183, "seek": 100880, "start": 1025.3999999999999, "end": 1032.3999999999999, "text": " to be inlined and because it's inlined it's no longer escaped so it's no longer needed", "tokens": [281, 312, 294, 13564, 293, 570, 309, 311, 294, 13564, 309, 311, 572, 2854, 20397, 370, 309, 311, 572, 2854, 2978], "temperature": 0.0, "avg_logprob": -0.14762436050966563, "compression_ratio": 1.6775700934579438, "no_speech_prob": 3.240539081161842e-05}, {"id": 184, "seek": 100880, "start": 1032.3999999999999, "end": 1038.28, "text": " that allocation, something that simple allows you to just reduce the number of allocations", "tokens": [300, 27599, 11, 746, 300, 2199, 4045, 291, 281, 445, 5407, 264, 1230, 295, 12660, 763], "temperature": 0.0, "avg_logprob": -0.14762436050966563, "compression_ratio": 1.6775700934579438, "no_speech_prob": 3.240539081161842e-05}, {"id": 185, "seek": 103828, "start": 1038.28, "end": 1045.36, "text": " of certain types when you have a constructor, what I would suggest is just keep your constructor", "tokens": [295, 1629, 3467, 562, 291, 362, 257, 47479, 11, 437, 286, 576, 3402, 307, 445, 1066, 428, 47479], "temperature": 0.0, "avg_logprob": -0.153498787810837, "compression_ratio": 1.5879120879120878, "no_speech_prob": 2.0688859876827337e-05}, {"id": 186, "seek": 103828, "start": 1045.36, "end": 1051.52, "text": " as simple as possible and if you have to do certain complex logic do it in an initialization", "tokens": [382, 2199, 382, 1944, 293, 498, 291, 362, 281, 360, 1629, 3997, 9952, 360, 309, 294, 364, 5883, 2144], "temperature": 0.0, "avg_logprob": -0.153498787810837, "compression_ratio": 1.5879120879120878, "no_speech_prob": 2.0688859876827337e-05}, {"id": 187, "seek": 103828, "start": 1051.52, "end": 1061.76, "text": " function, well if that doesn't hurt the readability, okay, let's see here we have less allocations,", "tokens": [2445, 11, 731, 498, 300, 1177, 380, 4607, 264, 1401, 2310, 11, 1392, 11, 718, 311, 536, 510, 321, 362, 1570, 12660, 763, 11], "temperature": 0.0, "avg_logprob": -0.153498787810837, "compression_ratio": 1.5879120879120878, "no_speech_prob": 2.0688859876827337e-05}, {"id": 188, "seek": 106176, "start": 1061.76, "end": 1068.4, "text": " we have now 2 allocations and 32 bytes per operation and the time consumed is you are", "tokens": [321, 362, 586, 568, 12660, 763, 293, 8858, 36088, 680, 6916, 293, 264, 565, 21226, 307, 291, 366], "temperature": 0.0, "avg_logprob": -0.2366789930007037, "compression_ratio": 1.5227272727272727, "no_speech_prob": 1.867318496806547e-05}, {"id": 189, "seek": 106176, "start": 1068.4, "end": 1081.2, "text": " saving 50 nanoseconds every time you instantiate that, so this is a good chunk, okay, well", "tokens": [6816, 2625, 14067, 541, 28750, 633, 565, 291, 9836, 13024, 300, 11, 370, 341, 307, 257, 665, 16635, 11, 1392, 11, 731], "temperature": 0.0, "avg_logprob": -0.2366789930007037, "compression_ratio": 1.5227272727272727, "no_speech_prob": 1.867318496806547e-05}, {"id": 190, "seek": 106176, "start": 1081.2, "end": 1089.48, "text": " this is optimization sometimes it's a matter of trade-offs, sometimes you just can do less,", "tokens": [341, 307, 19618, 2171, 309, 311, 257, 1871, 295, 4923, 12, 19231, 11, 2171, 291, 445, 393, 360, 1570, 11], "temperature": 0.0, "avg_logprob": -0.2366789930007037, "compression_ratio": 1.5227272727272727, "no_speech_prob": 1.867318496806547e-05}, {"id": 191, "seek": 108948, "start": 1089.48, "end": 1094.56, "text": " like less allocations, less CPU work, less garbage collector pressure, all that stuff", "tokens": [411, 1570, 12660, 763, 11, 1570, 13199, 589, 11, 1570, 14150, 23960, 3321, 11, 439, 300, 1507], "temperature": 0.0, "avg_logprob": -0.13046922354862608, "compression_ratio": 1.669767441860465, "no_speech_prob": 5.4794429161120206e-05}, {"id": 192, "seek": 108948, "start": 1094.56, "end": 1103.08, "text": " is things that you can be done, but sometimes it's not about doing less, it's about consuming", "tokens": [307, 721, 300, 291, 393, 312, 1096, 11, 457, 2171, 309, 311, 406, 466, 884, 1570, 11, 309, 311, 466, 19867], "temperature": 0.0, "avg_logprob": -0.13046922354862608, "compression_ratio": 1.669767441860465, "no_speech_prob": 5.4794429161120206e-05}, {"id": 193, "seek": 108948, "start": 1103.08, "end": 1108.8, "text": " different kind of resources, I care less about memory and I care more about CPU or all the", "tokens": [819, 733, 295, 3593, 11, 286, 1127, 1570, 466, 4675, 293, 286, 1127, 544, 466, 13199, 420, 439, 264], "temperature": 0.0, "avg_logprob": -0.13046922354862608, "compression_ratio": 1.669767441860465, "no_speech_prob": 5.4794429161120206e-05}, {"id": 194, "seek": 108948, "start": 1108.8, "end": 1114.64, "text": " way around, so concurrency is one of the cases where you need to decide what you want to", "tokens": [636, 926, 11, 370, 23702, 10457, 307, 472, 295, 264, 3331, 689, 291, 643, 281, 4536, 437, 291, 528, 281], "temperature": 0.0, "avg_logprob": -0.13046922354862608, "compression_ratio": 1.669767441860465, "no_speech_prob": 5.4794429161120206e-05}, {"id": 195, "seek": 111464, "start": 1114.64, "end": 1123.0400000000002, "text": " consume because go-routines are really cheap but are not free at all, so let's see an example", "tokens": [14732, 570, 352, 12, 81, 346, 1652, 366, 534, 7084, 457, 366, 406, 1737, 412, 439, 11, 370, 718, 311, 536, 364, 1365], "temperature": 0.0, "avg_logprob": -0.23825576570298937, "compression_ratio": 1.489247311827957, "no_speech_prob": 3.312928674858995e-05}, {"id": 196, "seek": 111464, "start": 1123.0400000000002, "end": 1129.76, "text": " with IO, this is two functions that I created, one is a fake IO that is going to generate", "tokens": [365, 39839, 11, 341, 307, 732, 6828, 300, 286, 2942, 11, 472, 307, 257, 7592, 39839, 300, 307, 516, 281, 8460], "temperature": 0.0, "avg_logprob": -0.23825576570298937, "compression_ratio": 1.489247311827957, "no_speech_prob": 3.312928674858995e-05}, {"id": 197, "seek": 111464, "start": 1129.76, "end": 1140.64, "text": " some kind of IO simulation by time-sleep and then you have the fake IO parallel that received", "tokens": [512, 733, 295, 39839, 16575, 538, 565, 12, 82, 7927, 293, 550, 291, 362, 264, 7592, 39839, 8952, 300, 4613], "temperature": 0.0, "avg_logprob": -0.23825576570298937, "compression_ratio": 1.489247311827957, "no_speech_prob": 3.312928674858995e-05}, {"id": 198, "seek": 114064, "start": 1140.64, "end": 1146.4, "text": " the number of go-routines and it's doing basically the same but distributing all that hundred", "tokens": [264, 1230, 295, 352, 12, 81, 346, 1652, 293, 309, 311, 884, 1936, 264, 912, 457, 41406, 439, 300, 3262], "temperature": 0.0, "avg_logprob": -0.19223356958645493, "compression_ratio": 1.6047904191616766, "no_speech_prob": 5.4815234761917964e-05}, {"id": 199, "seek": 114064, "start": 1146.4, "end": 1159.3600000000001, "text": " cycles between different go-routines and I built a benchmark to do that using three", "tokens": [17796, 1296, 819, 352, 12, 81, 346, 1652, 293, 286, 3094, 257, 18927, 281, 360, 300, 1228, 1045], "temperature": 0.0, "avg_logprob": -0.19223356958645493, "compression_ratio": 1.6047904191616766, "no_speech_prob": 5.4815234761917964e-05}, {"id": 200, "seek": 114064, "start": 1159.3600000000001, "end": 1165.3600000000001, "text": " different approaches, one is serial one, the non-concurrency, the other one is concurrency", "tokens": [819, 11587, 11, 472, 307, 17436, 472, 11, 264, 2107, 12, 1671, 14112, 10457, 11, 264, 661, 472, 307, 23702, 10457], "temperature": 0.0, "avg_logprob": -0.19223356958645493, "compression_ratio": 1.6047904191616766, "no_speech_prob": 5.4815234761917964e-05}, {"id": 201, "seek": 116536, "start": 1165.36, "end": 1175.1599999999999, "text": " using the number of CPUs in my machine and the other one is using the number of tasks", "tokens": [1228, 264, 1230, 295, 13199, 82, 294, 452, 3479, 293, 264, 661, 472, 307, 1228, 264, 1230, 295, 9608], "temperature": 0.0, "avg_logprob": -0.14386215209960937, "compression_ratio": 1.6645569620253164, "no_speech_prob": 3.1204042898025364e-05}, {"id": 202, "seek": 116536, "start": 1175.1599999999999, "end": 1181.6799999999998, "text": " that I have, and because this is IO, this is the result, I'm going to see that if I", "tokens": [300, 286, 362, 11, 293, 570, 341, 307, 39839, 11, 341, 307, 264, 1874, 11, 286, 478, 516, 281, 536, 300, 498, 286], "temperature": 0.0, "avg_logprob": -0.14386215209960937, "compression_ratio": 1.6645569620253164, "no_speech_prob": 3.1204042898025364e-05}, {"id": 203, "seek": 116536, "start": 1181.6799999999998, "end": 1188.8, "text": " create one go-routine per job, the number of bytes per operation and the number of allocation", "tokens": [1884, 472, 352, 12, 81, 45075, 680, 1691, 11, 264, 1230, 295, 36088, 680, 6916, 293, 264, 1230, 295, 27599], "temperature": 0.0, "avg_logprob": -0.14386215209960937, "compression_ratio": 1.6645569620253164, "no_speech_prob": 3.1204042898025364e-05}, {"id": 204, "seek": 118880, "start": 1188.8, "end": 1197.84, "text": " is going to spike but the time that is going to be consumed is going to be way lower, actually", "tokens": [307, 516, 281, 21053, 457, 264, 565, 300, 307, 516, 281, 312, 21226, 307, 516, 281, 312, 636, 3126, 11, 767], "temperature": 0.0, "avg_logprob": -0.1366850949715877, "compression_ratio": 1.4917127071823204, "no_speech_prob": 3.84721570299007e-05}, {"id": 205, "seek": 118880, "start": 1197.84, "end": 1209.12, "text": " I'm able to execute hundred times this function using this one go-routine per job approach", "tokens": [286, 478, 1075, 281, 14483, 3262, 1413, 341, 2445, 1228, 341, 472, 352, 12, 81, 45075, 680, 1691, 3109], "temperature": 0.0, "avg_logprob": -0.1366850949715877, "compression_ratio": 1.4917127071823204, "no_speech_prob": 3.84721570299007e-05}, {"id": 206, "seek": 118880, "start": 1209.12, "end": 1215.28, "text": " and only 12 using one CPU per job because this is IO, so let's see what happens if I", "tokens": [293, 787, 2272, 1228, 472, 13199, 680, 1691, 570, 341, 307, 39839, 11, 370, 718, 311, 536, 437, 2314, 498, 286], "temperature": 0.0, "avg_logprob": -0.1366850949715877, "compression_ratio": 1.4917127071823204, "no_speech_prob": 3.84721570299007e-05}, {"id": 207, "seek": 121528, "start": 1215.28, "end": 1223.2, "text": " do that with CPU. Using the CPU, this is to simulate some CPU load and using MD5 sum", "tokens": [360, 300, 365, 13199, 13, 11142, 264, 13199, 11, 341, 307, 281, 27817, 512, 13199, 3677, 293, 1228, 22521, 20, 2408], "temperature": 0.0, "avg_logprob": -0.13788834438529066, "compression_ratio": 1.7227722772277227, "no_speech_prob": 3.430807191762142e-05}, {"id": 208, "seek": 121528, "start": 1223.2, "end": 1230.84, "text": " and it's more or less the same approach as we saw in the fake IO, the benchmark is exactly", "tokens": [293, 309, 311, 544, 420, 1570, 264, 912, 3109, 382, 321, 1866, 294, 264, 7592, 39839, 11, 264, 18927, 307, 2293], "temperature": 0.0, "avg_logprob": -0.13788834438529066, "compression_ratio": 1.7227722772277227, "no_speech_prob": 3.430807191762142e-05}, {"id": 209, "seek": 121528, "start": 1230.84, "end": 1237.56, "text": " the same approach, we are using the number of jobs and the number of CPUs and using no", "tokens": [264, 912, 3109, 11, 321, 366, 1228, 264, 1230, 295, 4782, 293, 264, 1230, 295, 13199, 82, 293, 1228, 572], "temperature": 0.0, "avg_logprob": -0.13788834438529066, "compression_ratio": 1.7227722772277227, "no_speech_prob": 3.430807191762142e-05}, {"id": 210, "seek": 121528, "start": 1237.56, "end": 1243.3999999999999, "text": " go-routines and here is interesting because if you use the number of CPUs and this is", "tokens": [352, 12, 81, 346, 1652, 293, 510, 307, 1880, 570, 498, 291, 764, 264, 1230, 295, 13199, 82, 293, 341, 307], "temperature": 0.0, "avg_logprob": -0.13788834438529066, "compression_ratio": 1.7227722772277227, "no_speech_prob": 3.430807191762142e-05}, {"id": 211, "seek": 124340, "start": 1243.4, "end": 1251.0800000000002, "text": " a CPU workload, that is what is going to do the best efficiency. You can see here that", "tokens": [257, 13199, 20139, 11, 300, 307, 437, 307, 516, 281, 360, 264, 1151, 10493, 13, 509, 393, 536, 510, 300], "temperature": 0.0, "avg_logprob": -0.16144560604560665, "compression_ratio": 1.70935960591133, "no_speech_prob": 3.0127015634207055e-05}, {"id": 212, "seek": 124340, "start": 1251.0800000000002, "end": 1257.2, "text": " executing one go-routine per job is going to be even slower than executing that in", "tokens": [32368, 472, 352, 12, 81, 45075, 680, 1691, 307, 516, 281, 312, 754, 14009, 813, 32368, 300, 294], "temperature": 0.0, "avg_logprob": -0.16144560604560665, "compression_ratio": 1.70935960591133, "no_speech_prob": 3.0127015634207055e-05}, {"id": 213, "seek": 124340, "start": 1257.2, "end": 1267.44, "text": " serial and actually you have the worst of both worlds. You have plenty of allocations,", "tokens": [17436, 293, 767, 291, 362, 264, 5855, 295, 1293, 13401, 13, 509, 362, 7140, 295, 12660, 763, 11], "temperature": 0.0, "avg_logprob": -0.16144560604560665, "compression_ratio": 1.70935960591133, "no_speech_prob": 3.0127015634207055e-05}, {"id": 214, "seek": 124340, "start": 1267.44, "end": 1273.1200000000001, "text": " plenty of memory consumption, plenty of time consumption and you are not gaining anything.", "tokens": [7140, 295, 4675, 12126, 11, 7140, 295, 565, 12126, 293, 291, 366, 406, 19752, 1340, 13], "temperature": 0.0, "avg_logprob": -0.16144560604560665, "compression_ratio": 1.70935960591133, "no_speech_prob": 3.0127015634207055e-05}, {"id": 215, "seek": 127312, "start": 1273.12, "end": 1282.28, "text": " In the case of CPU, you are consuming more memory and you are getting better CPU performance", "tokens": [682, 264, 1389, 295, 13199, 11, 291, 366, 19867, 544, 4675, 293, 291, 366, 1242, 1101, 13199, 3389], "temperature": 0.0, "avg_logprob": -0.14894781233389168, "compression_ratio": 1.6227272727272728, "no_speech_prob": 4.073086165590212e-05}, {"id": 216, "seek": 127312, "start": 1282.28, "end": 1288.9599999999998, "text": " because you are basically spreading the job all over your physical CPUs and the serial", "tokens": [570, 291, 366, 1936, 15232, 264, 1691, 439, 670, 428, 4001, 13199, 82, 293, 264, 17436], "temperature": 0.0, "avg_logprob": -0.14894781233389168, "compression_ratio": 1.6227272727272728, "no_speech_prob": 4.073086165590212e-05}, {"id": 217, "seek": 127312, "start": 1288.9599999999998, "end": 1295.8, "text": " one is just doing everything and is using only one core of your CPU. Whenever you want", "tokens": [472, 307, 445, 884, 1203, 293, 307, 1228, 787, 472, 4965, 295, 428, 13199, 13, 14159, 291, 528], "temperature": 0.0, "avg_logprob": -0.14894781233389168, "compression_ratio": 1.6227272727272728, "no_speech_prob": 4.073086165590212e-05}, {"id": 218, "seek": 127312, "start": 1295.8, "end": 1299.8799999999999, "text": " to optimize using concurrency, you have to take in consideration what the kind of workload", "tokens": [281, 19719, 1228, 23702, 10457, 11, 291, 362, 281, 747, 294, 12381, 437, 264, 733, 295, 20139], "temperature": 0.0, "avg_logprob": -0.14894781233389168, "compression_ratio": 1.6227272727272728, "no_speech_prob": 4.073086165590212e-05}, {"id": 219, "seek": 129988, "start": 1299.88, "end": 1305.68, "text": " that you are using is the CPU workload, is your workload, do you care about memory, do", "tokens": [300, 291, 366, 1228, 307, 264, 13199, 20139, 11, 307, 428, 20139, 11, 360, 291, 1127, 466, 4675, 11, 360], "temperature": 0.0, "avg_logprob": -0.18521286124613748, "compression_ratio": 1.7197452229299364, "no_speech_prob": 6.721243698848411e-05}, {"id": 220, "seek": 129988, "start": 1305.68, "end": 1317.8000000000002, "text": " you care about CPU, what do you care about? That is the whole idea. I just want to explain", "tokens": [291, 1127, 466, 13199, 11, 437, 360, 291, 1127, 466, 30, 663, 307, 264, 1379, 1558, 13, 286, 445, 528, 281, 2903], "temperature": 0.0, "avg_logprob": -0.18521286124613748, "compression_ratio": 1.7197452229299364, "no_speech_prob": 6.721243698848411e-05}, {"id": 221, "seek": 129988, "start": 1317.8000000000002, "end": 1324.5200000000002, "text": " that all this is about measuring everything, measuring all this, doing all these benchmarks,", "tokens": [300, 439, 341, 307, 466, 13389, 1203, 11, 13389, 439, 341, 11, 884, 439, 613, 43751, 11], "temperature": 0.0, "avg_logprob": -0.18521286124613748, "compression_ratio": 1.7197452229299364, "no_speech_prob": 6.721243698848411e-05}, {"id": 222, "seek": 132452, "start": 1324.52, "end": 1332.28, "text": " doing all these kind of experiments to see if you are getting improvement on the performance", "tokens": [884, 439, 613, 733, 295, 12050, 281, 536, 498, 291, 366, 1242, 10444, 322, 264, 3389], "temperature": 0.0, "avg_logprob": -0.19048184156417847, "compression_ratio": 1.655813953488372, "no_speech_prob": 3.6242308851797134e-05}, {"id": 223, "seek": 132452, "start": 1332.28, "end": 1338.92, "text": " and iterate over that. That is the main idea. I show some examples of how you can improve", "tokens": [293, 44497, 670, 300, 13, 663, 307, 264, 2135, 1558, 13, 286, 855, 512, 5110, 295, 577, 291, 393, 3470], "temperature": 0.0, "avg_logprob": -0.19048184156417847, "compression_ratio": 1.655813953488372, "no_speech_prob": 3.6242308851797134e-05}, {"id": 224, "seek": 132452, "start": 1338.92, "end": 1346.8, "text": " things and some of them can be applied in general basics like using the, try to keep", "tokens": [721, 293, 512, 295, 552, 393, 312, 6456, 294, 2674, 14688, 411, 1228, 264, 11, 853, 281, 1066], "temperature": 0.0, "avg_logprob": -0.19048184156417847, "compression_ratio": 1.655813953488372, "no_speech_prob": 3.6242308851797134e-05}, {"id": 225, "seek": 132452, "start": 1346.8, "end": 1351.4, "text": " constructors small or using the constructor for slices when you know the size and things", "tokens": [7690, 830, 1359, 420, 1228, 264, 47479, 337, 19793, 562, 291, 458, 264, 2744, 293, 721], "temperature": 0.0, "avg_logprob": -0.19048184156417847, "compression_ratio": 1.655813953488372, "no_speech_prob": 3.6242308851797134e-05}, {"id": 226, "seek": 135140, "start": 1351.4, "end": 1361.1200000000001, "text": " like that. Some references. Efficient Go is a really book that is really, really interesting.", "tokens": [411, 300, 13, 2188, 15400, 13, 462, 7816, 1037, 307, 257, 534, 1446, 300, 307, 534, 11, 534, 1880, 13], "temperature": 0.0, "avg_logprob": -0.22416755590545998, "compression_ratio": 1.65, "no_speech_prob": 0.0001377159933326766}, {"id": 227, "seek": 135140, "start": 1361.1200000000001, "end": 1367.16, "text": " If you are really interested into efficiency, Bartolome Plocca wrote that book and actually", "tokens": [759, 291, 366, 534, 3102, 666, 10493, 11, 22338, 401, 423, 430, 5842, 496, 4114, 300, 1446, 293, 767], "temperature": 0.0, "avg_logprob": -0.22416755590545998, "compression_ratio": 1.65, "no_speech_prob": 0.0001377159933326766}, {"id": 228, "seek": 135140, "start": 1367.16, "end": 1374.96, "text": " is going to give a talk after the next one. I am sure it is going to be super interesting.", "tokens": [307, 516, 281, 976, 257, 751, 934, 264, 958, 472, 13, 286, 669, 988, 309, 307, 516, 281, 312, 1687, 1880, 13], "temperature": 0.0, "avg_logprob": -0.22416755590545998, "compression_ratio": 1.65, "no_speech_prob": 0.0001377159933326766}, {"id": 229, "seek": 135140, "start": 1374.96, "end": 1379.0, "text": " High-performance workshop from Dave Cheney. There is a lot of documentation about that", "tokens": [5229, 12, 50242, 13541, 490, 11017, 761, 1450, 88, 13, 821, 307, 257, 688, 295, 14333, 466, 300], "temperature": 0.0, "avg_logprob": -0.22416755590545998, "compression_ratio": 1.65, "no_speech_prob": 0.0001377159933326766}, {"id": 230, "seek": 137900, "start": 1379.0, "end": 1383.96, "text": " workshop that Dave Cheney did and it is really interesting also. The Go Perf book is a good", "tokens": [13541, 300, 11017, 761, 1450, 88, 630, 293, 309, 307, 534, 1880, 611, 13, 440, 1037, 3026, 69, 1446, 307, 257, 665], "temperature": 0.0, "avg_logprob": -0.21820412982593884, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.00012608777615241706}, {"id": 231, "seek": 137900, "start": 1383.96, "end": 1391.08, "text": " lecture also. An Ultimate Go course from Ardon Labs is also an interesting course because", "tokens": [7991, 611, 13, 1107, 26570, 1037, 1164, 490, 1587, 13966, 40047, 307, 611, 364, 1880, 1164, 570], "temperature": 0.0, "avg_logprob": -0.21820412982593884, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.00012608777615241706}, {"id": 232, "seek": 137900, "start": 1391.08, "end": 1398.48, "text": " it is giving you a lot of foundation and the course takes a lot of, cares a lot about hardware", "tokens": [309, 307, 2902, 291, 257, 688, 295, 7030, 293, 264, 1164, 2516, 257, 688, 295, 11, 12310, 257, 688, 466, 8837], "temperature": 0.0, "avg_logprob": -0.21820412982593884, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.00012608777615241706}, {"id": 233, "seek": 137900, "start": 1398.48, "end": 1404.88, "text": " sympathy and all that stuff. Well, some creative common, all the images are creative common", "tokens": [33240, 293, 439, 300, 1507, 13, 1042, 11, 512, 5880, 2689, 11, 439, 264, 5267, 366, 5880, 2689], "temperature": 0.0, "avg_logprob": -0.21820412982593884, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.00012608777615241706}, {"id": 234, "seek": 140488, "start": 1404.88, "end": 1412.5200000000002, "text": " so I put the reference here because it is creative common. Thank you. That is it.", "tokens": [370, 286, 829, 264, 6408, 510, 570, 309, 307, 5880, 2689, 13, 1044, 291, 13, 663, 307, 309, 13], "temperature": 0.0, "avg_logprob": -0.3189059716683847, "compression_ratio": 1.1794871794871795, "no_speech_prob": 0.0003773466742131859}, {"id": 235, "seek": 141252, "start": 1412.52, "end": 1438.48, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 51662], "temperature": 0.0, "avg_logprob": -0.7123762766520182, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.002149727661162615}], "language": "en"}