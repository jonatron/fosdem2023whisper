{"text": " First thing first, the dagger team was kind enough to send me some stickers. This is really the reason why I go to conferences. So if you want to pick some up, I will leave it somewhere here. Okay, cool. So, good evening, everyone. Thank you for joining my presentation today. My name is Mark, and for the last couple years or maybe for the better part of the decade, I've been helping engineering teams focus on their business, building their business applications instead of worrying about things like deployments or CI CD or stuff like that. And I'm currently, my current title at Cisco is Tech Lead, but by the way, is there anyone here who saw my presentation in the morning in the go-to room? Okay. Okay, so I'm going to make it up to you guys. So, and please laugh at my jokes as well again, even though you already heard them. So I decided that I would come clean here today, that this is a completely fake title, and that in fact my real job is a YAML engineer. So anyone else want to come clean, unburden themselves? Okay, okay, cool. Oh, yeah, yeah, that's engineer probably, that's an overstatement. So let's talk about CI CD, and we do have a bunch of CI CD services available today, and it's still evolving continuously, but we do have a couple of challenges that causes pain to developers and others, people every day, and I've already kind of hinted at one of them, YAML, like, man, you put a space in the wrong place and it just breaks over, and you don't even know why, because the CI tool may not even tell you where that extra space is. So YAML is really one of the core pains of all the CI solutions today, and yeah, I know there is Jenkins and Groovy, which is inverse, but YAML is really the standard of CI CD languages these days. There are some places, like, there are a couple of solutions where it works kind of okay for simple pipelines, but for more complex cases it's just a nightmare. Then CI has this tendency to break for no obvious reason, like, one day the pipeline works and the other way it just doesn't, and, well, for operations, for deploying your application to a production environment, you can always say, okay, OAPS problem, let them solve, but for CI that's not really a case, like, developers have to interface with interactive CI, and if the CI is breaking, then it's probably the developers who have to fix it. And the problem with current CI solutions today is that we don't really have, like, an easy way to debug CI issues, like, if there is something wrong, you probably have to guess where the problem is, maybe add a few echo lines to the YAML file, push the whole thing to the repository, wait for the CI to get triggered by the repository, and then go through this whole long feedback loop over and over again, you see a lot of people nodding. So when something goes wrong, it's your job to fix it, and it takes a lot of time, and it's just painful. And sometimes, sometimes, it's actually not the CI's fault, but your fault. The code doesn't, or the test doesn't pass, the linter doesn't pass, and that's often caused by things like having different versions in the CI and different versions in your development environment, and there are tools, and there are, like, ways to make those as close to each other as possible, but still this is happening very often, like, I don't know, I had this problem like a week ago. So sometimes it's just your code that's not working with the CI, and you have to go through the same feedback loop trying to push a change, hoping that it will fix your problem, and of course, it doesn't work for the first time, so you do it over and over again, until after an hour, maybe, maybe if you sacrifice something to the CI gods, it works. So I'm pretty sure there are other challenges with CI, but let's see how Dagger may be able to solve some of these challenges here. So first of all, who has heard about Dagger? Who knows what Dagger is, oh, cool. So Dagger is a program about portable CI-CD solution, and portable is a pretty great feature here, because instead of going through the total feedback loop I was talking about, you can run your CI-CD pipeline on your own local machine and figure out what's wrong much sooner than by pushing to the Git repository and waiting for the CI over and over again. So it's much quicker that way to debug issues either related to the CI or your code, and it's also much easier to build the pipeline in the first place, like when you build a new CI pipeline for a new project, you have to go through the same feedback loop, because you have to add new steps and figure out if it works or not, and if it doesn't, then you have to figure out how to add the right parameters. So even building new pipelines is way easier, because the whole thing is portable. The other thing that makes Dagger great is that you can basically write your pipelines in any language. Dagger officially supports a couple of languages, like Go, Python, TypeScript, and Q, but basically any language that can talk to a GraphQL API, because that's what's under the hood, any language that can talk to a GraphQL API basically can be used to build your own pipelines with Dagger. And if you combine these two traits, like being portable and being able to write pipelines in your own language, it also points to the fact that you can completely avoid vendor locking, like you can, obviously you would still need some sort of CI service and you would need like a thin layer of integration that would run Dagger itself, but once you have a portable pipeline written in your own format in your own language, not in a proprietary or CI-specific general format, you are not logged into the CI vendor you are using right now. And that you don't really switch like CI providers often, but that happens, like when they buy your company and then you have to move from one provider to another and then you have to move again because reasons. And the fourth reason or the fourth thing that makes Dagger great is caching. Now, most CI services already have some sort of caching solution that you can use to cache like intermediary artifacts or dependencies or whatever you want to store in a cache that you don't want to download or compile every single time when your CI pipeline runs. But you still have to configure it properly and you have to make sure that you have the right caching keys, you have to add the right paths to the caching configuration. And you may either end up with a huge cache at the end of the day or you may not use cache at all. So if you don't configure it properly, it may not work. With Dagger, you get caching by default. And by default, I mean every single step in your pipeline, the result of that run will be cached similarly how a Docker file works, like every single instruction, the result of that instruction will be cached if there are no changes before that step, actually. So similarly to that, Dagger caches every step in your CI pipeline. Now how does it do that? How does it work? How is this portable? Any guesses? One word? Yeah, Docker, yeah, that's right. So containers, of course, containers. So in order to be portable and to do all this magic that Dagger does, it needs to have like a reasonable level of isolation so that you can be confident that it will run on your local machine and on your CI the same way. So it runs your builds in containers. And I already mentioned that Dagger has a few official SDKs that you can use to build the pipeline in your own code. Using that Dagger SDK, you can talk to the so-called Dagger engine, which is the API that implements the GraphQL specification. And the Dagger SDK will call this API with the steps in your pipeline. And the Dagger engine will build a DAG from these steps. And then we'll pass that to basically to run through a container runtime. And that's how your pipeline will run. And the good thing about this is that you can actually change this pipeline so the output of one pipeline can be the input of another. And this whole thing goes through a single thing called session. So in a single session, you can have multiple, like, these container executions. And you can change the results into each other if you want to. Now let's actually take a look at how these things run. And the reason why I asked if there is anyone here who was in my presentation in the morning because I completely botched the demo and it didn't work at all. So let's hope it works this time. So the example is in Go. But again, it could be, like, TypeScript and Python or even Q. And I'm not going to go into that much detail about the Go specific here. But basically, you need to import this Dagger SDK in order to, by the way, can you see the screen or the code? Make it bigger? OK. Better? Cool. So you have to import this Dagger SDK if you want to use Go. And then the first thing you need to do is connect to the Dagger Engine. Now if the Dagger Engine doesn't run locally, then the SDK will actually run it using, as a simple Docker container. So the first thing you need to do is connect to this Dagger Engine. And then you can start launching these containers and start building your pipelines. And if it looks very similar, it's because it's basically the same, uses the same language and it looks very similar to Docker files. And it works basically the same way. So you have, like, a base image. You have a bunch of mounted volumes for caching. And then you mount the source code and you run some sort of command. And that's it. That's your pipeline. Now let's see if it actually runs. So I use this make file authority for Go called mage. So this is how I have this whole code implemented in a test function. Let's see if it runs. Okay. So it did run. Cool. Let's try just for, let's debug to see what's happening in the background. So it pulls an image, the goal and image. It mounts the code. It mounts the volumes. And then runs my Go test on the mounted code. And then basically exits and outputs the result of the test. So, well, that's it. If you want to get started with Dagger, check out the documentation. It's getting better by the day. They actually released, well, either today or yesterday, a new quick start guide, which is pretty awesome. It has all the three or four supported languages in a single document. So you can switch between languages if you want to. There is even a playground for the lower level GraphQL API. So if you don't want to start a new project, you can play directly with the GraphQL API with the hosted version of the Dagger engine. So thank you for your attention. If you have any questions, I'm happy to answer if you have time for that. Thank you so much. Awesome. I have a question with regards to implementation, so let's say that you roll your pipeline, you commit that you want to run that somewhere like in a CI environment. GitHub Actions, or Gaffer Bids, Jenkins, or whatever. How do you go about that? I can imagine that you need to expose the Docker socket to the pipeline, or how does it work? Yeah, so basically, if you have Docker running in your environment, you can run this pipeline. And you can run Docker anywhere, basically, today. You can run it in Jenkins or GitHub. You have it in GitHub Actions, actually. And you probably have it on your machine, as well. So wherever you have Docker running today, this pipeline will run. So you just invoke the Dagger command, that command that you just showed us. Yeah, it's not even a Dagger command. This is entirely my code. This is my go binary, basically. Right, okay. And it will find the Docker API socket, and if you just start containers there, yeah. Before I, very cool stuff. So before I switch all my CI to Dagger, let's frame it like this. What would be the two things that you would really love to see an improved implementation of in the next version? Can you repeat the question? What are the two things that really need to be improved about the current state of Dagger? What improves? To improve Dagger, in your opinion, what are the two things that need the most improvement? Okay, so one thing is secret management. Right now, Dagger, it's not that easy to work with secrets, so that needs to be improved, and they are actually working on it, so that's great. The other thing is that right now, if you build something in one language, for example, if I build a reusable library in Go to run my pipelines, I can't reuse it in TypeScript, for example, today. And for that, there is actually a feature called extensions, so they are working on a feature so you can build extensions to the Dagger engine, so you can build these reliable or reusable pipeline PCs, like running liters and stuff like that, so you don't have to build that in your own code, you just have to build the extensions, and you can call it from whatever language you want to call them. Basically GraphQL API extensions. Thank you. Last question. Hi, does Dagger support spinning up Service A concurrently with Service B, because the tests need something else to run while the test is running, and then afterward you can continue to other stuff? Right now, I don't think it does. Again, this is something that they are thinking about, but it's not a trivial thing to do, so no. Currently. Okay, someone is working on it. Thank you. Of course. Thank you. Thank you very much.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 11.72, "text": " First thing first, the dagger team was kind enough to send me some stickers.", "tokens": [2386, 551, 700, 11, 264, 36972, 1469, 390, 733, 1547, 281, 2845, 385, 512, 21019, 13], "temperature": 0.0, "avg_logprob": -0.29943057110435084, "compression_ratio": 1.4759358288770053, "no_speech_prob": 0.306131511926651}, {"id": 1, "seek": 0, "start": 11.72, "end": 13.64, "text": " This is really the reason why I go to conferences.", "tokens": [639, 307, 534, 264, 1778, 983, 286, 352, 281, 22032, 13], "temperature": 0.0, "avg_logprob": -0.29943057110435084, "compression_ratio": 1.4759358288770053, "no_speech_prob": 0.306131511926651}, {"id": 2, "seek": 0, "start": 13.64, "end": 17.36, "text": " So if you want to pick some up, I will leave it somewhere here.", "tokens": [407, 498, 291, 528, 281, 1888, 512, 493, 11, 286, 486, 1856, 309, 4079, 510, 13], "temperature": 0.0, "avg_logprob": -0.29943057110435084, "compression_ratio": 1.4759358288770053, "no_speech_prob": 0.306131511926651}, {"id": 3, "seek": 0, "start": 17.36, "end": 20.64, "text": " Okay, cool.", "tokens": [1033, 11, 1627, 13], "temperature": 0.0, "avg_logprob": -0.29943057110435084, "compression_ratio": 1.4759358288770053, "no_speech_prob": 0.306131511926651}, {"id": 4, "seek": 0, "start": 20.64, "end": 23.96, "text": " So, good evening, everyone.", "tokens": [407, 11, 665, 5634, 11, 1518, 13], "temperature": 0.0, "avg_logprob": -0.29943057110435084, "compression_ratio": 1.4759358288770053, "no_speech_prob": 0.306131511926651}, {"id": 5, "seek": 0, "start": 23.96, "end": 27.12, "text": " Thank you for joining my presentation today.", "tokens": [1044, 291, 337, 5549, 452, 5860, 965, 13], "temperature": 0.0, "avg_logprob": -0.29943057110435084, "compression_ratio": 1.4759358288770053, "no_speech_prob": 0.306131511926651}, {"id": 6, "seek": 2712, "start": 27.12, "end": 33.36, "text": " My name is Mark, and for the last couple years or maybe for the better part of the decade,", "tokens": [1222, 1315, 307, 3934, 11, 293, 337, 264, 1036, 1916, 924, 420, 1310, 337, 264, 1101, 644, 295, 264, 10378, 11], "temperature": 0.0, "avg_logprob": -0.1971433367047991, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0002477778762113303}, {"id": 7, "seek": 2712, "start": 33.36, "end": 37.92, "text": " I've been helping engineering teams focus on their business, building their business", "tokens": [286, 600, 668, 4315, 7043, 5491, 1879, 322, 641, 1606, 11, 2390, 641, 1606], "temperature": 0.0, "avg_logprob": -0.1971433367047991, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0002477778762113303}, {"id": 8, "seek": 2712, "start": 37.92, "end": 44.400000000000006, "text": " applications instead of worrying about things like deployments or CI CD or stuff like that.", "tokens": [5821, 2602, 295, 18788, 466, 721, 411, 7274, 1117, 420, 37777, 6743, 420, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.1971433367047991, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0002477778762113303}, {"id": 9, "seek": 2712, "start": 44.400000000000006, "end": 50.52, "text": " And I'm currently, my current title at Cisco is Tech Lead, but by the way, is there anyone", "tokens": [400, 286, 478, 4362, 11, 452, 2190, 4876, 412, 38528, 307, 13795, 31025, 11, 457, 538, 264, 636, 11, 307, 456, 2878], "temperature": 0.0, "avg_logprob": -0.1971433367047991, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0002477778762113303}, {"id": 10, "seek": 2712, "start": 50.52, "end": 55.2, "text": " here who saw my presentation in the morning in the go-to room?", "tokens": [510, 567, 1866, 452, 5860, 294, 264, 2446, 294, 264, 352, 12, 1353, 1808, 30], "temperature": 0.0, "avg_logprob": -0.1971433367047991, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0002477778762113303}, {"id": 11, "seek": 2712, "start": 55.2, "end": 56.2, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1971433367047991, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.0002477778762113303}, {"id": 12, "seek": 5620, "start": 56.2, "end": 60.84, "text": " Okay, so I'm going to make it up to you guys.", "tokens": [1033, 11, 370, 286, 478, 516, 281, 652, 309, 493, 281, 291, 1074, 13], "temperature": 0.0, "avg_logprob": -0.27103019483161694, "compression_ratio": 1.5133928571428572, "no_speech_prob": 0.00011221093154745176}, {"id": 13, "seek": 5620, "start": 60.84, "end": 65.64, "text": " So, and please laugh at my jokes as well again, even though you already heard them.", "tokens": [407, 11, 293, 1767, 5801, 412, 452, 14439, 382, 731, 797, 11, 754, 1673, 291, 1217, 2198, 552, 13], "temperature": 0.0, "avg_logprob": -0.27103019483161694, "compression_ratio": 1.5133928571428572, "no_speech_prob": 0.00011221093154745176}, {"id": 14, "seek": 5620, "start": 65.64, "end": 71.68, "text": " So I decided that I would come clean here today, that this is a completely fake title, and", "tokens": [407, 286, 3047, 300, 286, 576, 808, 2541, 510, 965, 11, 300, 341, 307, 257, 2584, 7592, 4876, 11, 293], "temperature": 0.0, "avg_logprob": -0.27103019483161694, "compression_ratio": 1.5133928571428572, "no_speech_prob": 0.00011221093154745176}, {"id": 15, "seek": 5620, "start": 71.68, "end": 76.84, "text": " that in fact my real job is a YAML engineer.", "tokens": [300, 294, 1186, 452, 957, 1691, 307, 257, 398, 2865, 43, 11403, 13], "temperature": 0.0, "avg_logprob": -0.27103019483161694, "compression_ratio": 1.5133928571428572, "no_speech_prob": 0.00011221093154745176}, {"id": 16, "seek": 5620, "start": 76.84, "end": 81.88, "text": " So anyone else want to come clean, unburden themselves?", "tokens": [407, 2878, 1646, 528, 281, 808, 2541, 11, 517, 13243, 1556, 2969, 30], "temperature": 0.0, "avg_logprob": -0.27103019483161694, "compression_ratio": 1.5133928571428572, "no_speech_prob": 0.00011221093154745176}, {"id": 17, "seek": 5620, "start": 81.88, "end": 84.0, "text": " Okay, okay, cool.", "tokens": [1033, 11, 1392, 11, 1627, 13], "temperature": 0.0, "avg_logprob": -0.27103019483161694, "compression_ratio": 1.5133928571428572, "no_speech_prob": 0.00011221093154745176}, {"id": 18, "seek": 8400, "start": 84.0, "end": 90.76, "text": " Oh, yeah, yeah, that's engineer probably, that's an overstatement.", "tokens": [876, 11, 1338, 11, 1338, 11, 300, 311, 11403, 1391, 11, 300, 311, 364, 670, 19435, 1712, 13], "temperature": 0.0, "avg_logprob": -0.23274670090786245, "compression_ratio": 1.5721153846153846, "no_speech_prob": 9.863768354989588e-05}, {"id": 19, "seek": 8400, "start": 90.76, "end": 97.32, "text": " So let's talk about CI CD, and we do have a bunch of CI CD services available today,", "tokens": [407, 718, 311, 751, 466, 37777, 6743, 11, 293, 321, 360, 362, 257, 3840, 295, 37777, 6743, 3328, 2435, 965, 11], "temperature": 0.0, "avg_logprob": -0.23274670090786245, "compression_ratio": 1.5721153846153846, "no_speech_prob": 9.863768354989588e-05}, {"id": 20, "seek": 8400, "start": 97.32, "end": 103.84, "text": " and it's still evolving continuously, but we do have a couple of challenges that causes", "tokens": [293, 309, 311, 920, 21085, 15684, 11, 457, 321, 360, 362, 257, 1916, 295, 4759, 300, 7700], "temperature": 0.0, "avg_logprob": -0.23274670090786245, "compression_ratio": 1.5721153846153846, "no_speech_prob": 9.863768354989588e-05}, {"id": 21, "seek": 8400, "start": 103.84, "end": 110.2, "text": " pain to developers and others, people every day, and I've already kind of hinted at one", "tokens": [1822, 281, 8849, 293, 2357, 11, 561, 633, 786, 11, 293, 286, 600, 1217, 733, 295, 12075, 292, 412, 472], "temperature": 0.0, "avg_logprob": -0.23274670090786245, "compression_ratio": 1.5721153846153846, "no_speech_prob": 9.863768354989588e-05}, {"id": 22, "seek": 11020, "start": 110.2, "end": 118.44, "text": " of them, YAML, like, man, you put a space in the wrong place and it just breaks over,", "tokens": [295, 552, 11, 398, 2865, 43, 11, 411, 11, 587, 11, 291, 829, 257, 1901, 294, 264, 2085, 1081, 293, 309, 445, 9857, 670, 11], "temperature": 0.0, "avg_logprob": -0.14770250062684756, "compression_ratio": 1.6008403361344539, "no_speech_prob": 4.364754568086937e-05}, {"id": 23, "seek": 11020, "start": 118.44, "end": 123.12, "text": " and you don't even know why, because the CI tool may not even tell you where that extra", "tokens": [293, 291, 500, 380, 754, 458, 983, 11, 570, 264, 37777, 2290, 815, 406, 754, 980, 291, 689, 300, 2857], "temperature": 0.0, "avg_logprob": -0.14770250062684756, "compression_ratio": 1.6008403361344539, "no_speech_prob": 4.364754568086937e-05}, {"id": 24, "seek": 11020, "start": 123.12, "end": 124.92, "text": " space is.", "tokens": [1901, 307, 13], "temperature": 0.0, "avg_logprob": -0.14770250062684756, "compression_ratio": 1.6008403361344539, "no_speech_prob": 4.364754568086937e-05}, {"id": 25, "seek": 11020, "start": 124.92, "end": 130.88, "text": " So YAML is really one of the core pains of all the CI solutions today, and yeah, I know", "tokens": [407, 398, 2865, 43, 307, 534, 472, 295, 264, 4965, 29774, 295, 439, 264, 37777, 6547, 965, 11, 293, 1338, 11, 286, 458], "temperature": 0.0, "avg_logprob": -0.14770250062684756, "compression_ratio": 1.6008403361344539, "no_speech_prob": 4.364754568086937e-05}, {"id": 26, "seek": 11020, "start": 130.88, "end": 136.88, "text": " there is Jenkins and Groovy, which is inverse, but YAML is really the standard of CI CD languages", "tokens": [456, 307, 41273, 293, 12981, 38223, 11, 597, 307, 17340, 11, 457, 398, 2865, 43, 307, 534, 264, 3832, 295, 37777, 6743, 8650], "temperature": 0.0, "avg_logprob": -0.14770250062684756, "compression_ratio": 1.6008403361344539, "no_speech_prob": 4.364754568086937e-05}, {"id": 27, "seek": 11020, "start": 136.88, "end": 137.88, "text": " these days.", "tokens": [613, 1708, 13], "temperature": 0.0, "avg_logprob": -0.14770250062684756, "compression_ratio": 1.6008403361344539, "no_speech_prob": 4.364754568086937e-05}, {"id": 28, "seek": 13788, "start": 137.88, "end": 142.48, "text": " There are some places, like, there are a couple of solutions where it works kind of okay for", "tokens": [821, 366, 512, 3190, 11, 411, 11, 456, 366, 257, 1916, 295, 6547, 689, 309, 1985, 733, 295, 1392, 337], "temperature": 0.0, "avg_logprob": -0.19496044480657004, "compression_ratio": 1.6074766355140186, "no_speech_prob": 6.115292489994317e-05}, {"id": 29, "seek": 13788, "start": 142.48, "end": 149.96, "text": " simple pipelines, but for more complex cases it's just a nightmare.", "tokens": [2199, 40168, 11, 457, 337, 544, 3997, 3331, 309, 311, 445, 257, 18724, 13], "temperature": 0.0, "avg_logprob": -0.19496044480657004, "compression_ratio": 1.6074766355140186, "no_speech_prob": 6.115292489994317e-05}, {"id": 30, "seek": 13788, "start": 149.96, "end": 158.96, "text": " Then CI has this tendency to break for no obvious reason, like, one day the pipeline", "tokens": [1396, 37777, 575, 341, 18187, 281, 1821, 337, 572, 6322, 1778, 11, 411, 11, 472, 786, 264, 15517], "temperature": 0.0, "avg_logprob": -0.19496044480657004, "compression_ratio": 1.6074766355140186, "no_speech_prob": 6.115292489994317e-05}, {"id": 31, "seek": 13788, "start": 158.96, "end": 166.92, "text": " works and the other way it just doesn't, and, well, for operations, for deploying your application", "tokens": [1985, 293, 264, 661, 636, 309, 445, 1177, 380, 11, 293, 11, 731, 11, 337, 7705, 11, 337, 34198, 428, 3861], "temperature": 0.0, "avg_logprob": -0.19496044480657004, "compression_ratio": 1.6074766355140186, "no_speech_prob": 6.115292489994317e-05}, {"id": 32, "seek": 16692, "start": 166.92, "end": 172.48, "text": " to a production environment, you can always say, okay, OAPS problem, let them solve, but", "tokens": [281, 257, 4265, 2823, 11, 291, 393, 1009, 584, 11, 1392, 11, 422, 4715, 50, 1154, 11, 718, 552, 5039, 11, 457], "temperature": 0.0, "avg_logprob": -0.18661485276780687, "compression_ratio": 1.689922480620155, "no_speech_prob": 1.0438073331897613e-05}, {"id": 33, "seek": 16692, "start": 172.48, "end": 178.67999999999998, "text": " for CI that's not really a case, like, developers have to interface with interactive CI, and", "tokens": [337, 37777, 300, 311, 406, 534, 257, 1389, 11, 411, 11, 8849, 362, 281, 9226, 365, 15141, 37777, 11, 293], "temperature": 0.0, "avg_logprob": -0.18661485276780687, "compression_ratio": 1.689922480620155, "no_speech_prob": 1.0438073331897613e-05}, {"id": 34, "seek": 16692, "start": 178.67999999999998, "end": 184.44, "text": " if the CI is breaking, then it's probably the developers who have to fix it.", "tokens": [498, 264, 37777, 307, 7697, 11, 550, 309, 311, 1391, 264, 8849, 567, 362, 281, 3191, 309, 13], "temperature": 0.0, "avg_logprob": -0.18661485276780687, "compression_ratio": 1.689922480620155, "no_speech_prob": 1.0438073331897613e-05}, {"id": 35, "seek": 16692, "start": 184.44, "end": 189.2, "text": " And the problem with current CI solutions today is that we don't really have, like,", "tokens": [400, 264, 1154, 365, 2190, 37777, 6547, 965, 307, 300, 321, 500, 380, 534, 362, 11, 411, 11], "temperature": 0.0, "avg_logprob": -0.18661485276780687, "compression_ratio": 1.689922480620155, "no_speech_prob": 1.0438073331897613e-05}, {"id": 36, "seek": 16692, "start": 189.2, "end": 195.11999999999998, "text": " an easy way to debug CI issues, like, if there is something wrong, you probably have to guess", "tokens": [364, 1858, 636, 281, 24083, 37777, 2663, 11, 411, 11, 498, 456, 307, 746, 2085, 11, 291, 1391, 362, 281, 2041], "temperature": 0.0, "avg_logprob": -0.18661485276780687, "compression_ratio": 1.689922480620155, "no_speech_prob": 1.0438073331897613e-05}, {"id": 37, "seek": 19512, "start": 195.12, "end": 200.72, "text": " where the problem is, maybe add a few echo lines to the YAML file, push the whole thing", "tokens": [689, 264, 1154, 307, 11, 1310, 909, 257, 1326, 14300, 3876, 281, 264, 398, 2865, 43, 3991, 11, 2944, 264, 1379, 551], "temperature": 0.0, "avg_logprob": -0.14216747283935546, "compression_ratio": 1.6283185840707965, "no_speech_prob": 4.781199459102936e-05}, {"id": 38, "seek": 19512, "start": 200.72, "end": 206.84, "text": " to the repository, wait for the CI to get triggered by the repository, and then go through", "tokens": [281, 264, 25841, 11, 1699, 337, 264, 37777, 281, 483, 21710, 538, 264, 25841, 11, 293, 550, 352, 807], "temperature": 0.0, "avg_logprob": -0.14216747283935546, "compression_ratio": 1.6283185840707965, "no_speech_prob": 4.781199459102936e-05}, {"id": 39, "seek": 19512, "start": 206.84, "end": 215.04000000000002, "text": " this whole long feedback loop over and over again, you see a lot of people nodding.", "tokens": [341, 1379, 938, 5824, 6367, 670, 293, 670, 797, 11, 291, 536, 257, 688, 295, 561, 15224, 3584, 13], "temperature": 0.0, "avg_logprob": -0.14216747283935546, "compression_ratio": 1.6283185840707965, "no_speech_prob": 4.781199459102936e-05}, {"id": 40, "seek": 19512, "start": 215.04000000000002, "end": 221.28, "text": " So when something goes wrong, it's your job to fix it, and it takes a lot of time, and", "tokens": [407, 562, 746, 1709, 2085, 11, 309, 311, 428, 1691, 281, 3191, 309, 11, 293, 309, 2516, 257, 688, 295, 565, 11, 293], "temperature": 0.0, "avg_logprob": -0.14216747283935546, "compression_ratio": 1.6283185840707965, "no_speech_prob": 4.781199459102936e-05}, {"id": 41, "seek": 19512, "start": 221.28, "end": 223.12, "text": " it's just painful.", "tokens": [309, 311, 445, 11697, 13], "temperature": 0.0, "avg_logprob": -0.14216747283935546, "compression_ratio": 1.6283185840707965, "no_speech_prob": 4.781199459102936e-05}, {"id": 42, "seek": 22312, "start": 223.12, "end": 231.88, "text": " And sometimes, sometimes, it's actually not the CI's fault, but your fault.", "tokens": [400, 2171, 11, 2171, 11, 309, 311, 767, 406, 264, 37777, 311, 7441, 11, 457, 428, 7441, 13], "temperature": 0.0, "avg_logprob": -0.15492620139286437, "compression_ratio": 1.766497461928934, "no_speech_prob": 3.298759111203253e-05}, {"id": 43, "seek": 22312, "start": 231.88, "end": 239.8, "text": " The code doesn't, or the test doesn't pass, the linter doesn't pass, and that's often", "tokens": [440, 3089, 1177, 380, 11, 420, 264, 1500, 1177, 380, 1320, 11, 264, 287, 5106, 1177, 380, 1320, 11, 293, 300, 311, 2049], "temperature": 0.0, "avg_logprob": -0.15492620139286437, "compression_ratio": 1.766497461928934, "no_speech_prob": 3.298759111203253e-05}, {"id": 44, "seek": 22312, "start": 239.8, "end": 245.52, "text": " caused by things like having different versions in the CI and different versions in your development", "tokens": [7008, 538, 721, 411, 1419, 819, 9606, 294, 264, 37777, 293, 819, 9606, 294, 428, 3250], "temperature": 0.0, "avg_logprob": -0.15492620139286437, "compression_ratio": 1.766497461928934, "no_speech_prob": 3.298759111203253e-05}, {"id": 45, "seek": 22312, "start": 245.52, "end": 252.28, "text": " environment, and there are tools, and there are, like, ways to make those as close to", "tokens": [2823, 11, 293, 456, 366, 3873, 11, 293, 456, 366, 11, 411, 11, 2098, 281, 652, 729, 382, 1998, 281], "temperature": 0.0, "avg_logprob": -0.15492620139286437, "compression_ratio": 1.766497461928934, "no_speech_prob": 3.298759111203253e-05}, {"id": 46, "seek": 25228, "start": 252.28, "end": 258.32, "text": " each other as possible, but still this is happening very often, like, I don't know, I had this", "tokens": [1184, 661, 382, 1944, 11, 457, 920, 341, 307, 2737, 588, 2049, 11, 411, 11, 286, 500, 380, 458, 11, 286, 632, 341], "temperature": 0.0, "avg_logprob": -0.16139732874356782, "compression_ratio": 1.6016260162601625, "no_speech_prob": 2.8328668122412637e-05}, {"id": 47, "seek": 25228, "start": 258.32, "end": 261.0, "text": " problem like a week ago.", "tokens": [1154, 411, 257, 1243, 2057, 13], "temperature": 0.0, "avg_logprob": -0.16139732874356782, "compression_ratio": 1.6016260162601625, "no_speech_prob": 2.8328668122412637e-05}, {"id": 48, "seek": 25228, "start": 261.0, "end": 266.92, "text": " So sometimes it's just your code that's not working with the CI, and you have to go through", "tokens": [407, 2171, 309, 311, 445, 428, 3089, 300, 311, 406, 1364, 365, 264, 37777, 11, 293, 291, 362, 281, 352, 807], "temperature": 0.0, "avg_logprob": -0.16139732874356782, "compression_ratio": 1.6016260162601625, "no_speech_prob": 2.8328668122412637e-05}, {"id": 49, "seek": 25228, "start": 266.92, "end": 273.12, "text": " the same feedback loop trying to push a change, hoping that it will fix your problem, and", "tokens": [264, 912, 5824, 6367, 1382, 281, 2944, 257, 1319, 11, 7159, 300, 309, 486, 3191, 428, 1154, 11, 293], "temperature": 0.0, "avg_logprob": -0.16139732874356782, "compression_ratio": 1.6016260162601625, "no_speech_prob": 2.8328668122412637e-05}, {"id": 50, "seek": 25228, "start": 273.12, "end": 279.28, "text": " of course, it doesn't work for the first time, so you do it over and over again, until after", "tokens": [295, 1164, 11, 309, 1177, 380, 589, 337, 264, 700, 565, 11, 370, 291, 360, 309, 670, 293, 670, 797, 11, 1826, 934], "temperature": 0.0, "avg_logprob": -0.16139732874356782, "compression_ratio": 1.6016260162601625, "no_speech_prob": 2.8328668122412637e-05}, {"id": 51, "seek": 27928, "start": 279.28, "end": 286.88, "text": " an hour, maybe, maybe if you sacrifice something to the CI gods, it works.", "tokens": [364, 1773, 11, 1310, 11, 1310, 498, 291, 11521, 746, 281, 264, 37777, 14049, 11, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.1588460665482741, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.0269794984196778e-05}, {"id": 52, "seek": 27928, "start": 286.88, "end": 293.64, "text": " So I'm pretty sure there are other challenges with CI, but let's see how Dagger may be able", "tokens": [407, 286, 478, 1238, 988, 456, 366, 661, 4759, 365, 37777, 11, 457, 718, 311, 536, 577, 413, 11062, 815, 312, 1075], "temperature": 0.0, "avg_logprob": -0.1588460665482741, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.0269794984196778e-05}, {"id": 53, "seek": 27928, "start": 293.64, "end": 296.11999999999995, "text": " to solve some of these challenges here.", "tokens": [281, 5039, 512, 295, 613, 4759, 510, 13], "temperature": 0.0, "avg_logprob": -0.1588460665482741, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.0269794984196778e-05}, {"id": 54, "seek": 27928, "start": 296.11999999999995, "end": 299.2, "text": " So first of all, who has heard about Dagger?", "tokens": [407, 700, 295, 439, 11, 567, 575, 2198, 466, 413, 11062, 30], "temperature": 0.0, "avg_logprob": -0.1588460665482741, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.0269794984196778e-05}, {"id": 55, "seek": 27928, "start": 299.2, "end": 302.96, "text": " Who knows what Dagger is, oh, cool.", "tokens": [2102, 3255, 437, 413, 11062, 307, 11, 1954, 11, 1627, 13], "temperature": 0.0, "avg_logprob": -0.1588460665482741, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.0269794984196778e-05}, {"id": 56, "seek": 27928, "start": 302.96, "end": 308.96, "text": " So Dagger is a program about portable CI-CD solution, and portable is a pretty great", "tokens": [407, 413, 11062, 307, 257, 1461, 466, 21800, 37777, 12, 16508, 3827, 11, 293, 21800, 307, 257, 1238, 869], "temperature": 0.0, "avg_logprob": -0.1588460665482741, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.0269794984196778e-05}, {"id": 57, "seek": 30896, "start": 308.96, "end": 314.12, "text": " feature here, because instead of going through the total feedback loop I was talking about,", "tokens": [4111, 510, 11, 570, 2602, 295, 516, 807, 264, 3217, 5824, 6367, 286, 390, 1417, 466, 11], "temperature": 0.0, "avg_logprob": -0.10873896220944962, "compression_ratio": 1.664179104477612, "no_speech_prob": 9.330033208243549e-05}, {"id": 58, "seek": 30896, "start": 314.12, "end": 320.03999999999996, "text": " you can run your CI-CD pipeline on your own local machine and figure out what's wrong", "tokens": [291, 393, 1190, 428, 37777, 12, 16508, 15517, 322, 428, 1065, 2654, 3479, 293, 2573, 484, 437, 311, 2085], "temperature": 0.0, "avg_logprob": -0.10873896220944962, "compression_ratio": 1.664179104477612, "no_speech_prob": 9.330033208243549e-05}, {"id": 59, "seek": 30896, "start": 320.03999999999996, "end": 325.35999999999996, "text": " much sooner than by pushing to the Git repository and waiting for the CI over and over again.", "tokens": [709, 15324, 813, 538, 7380, 281, 264, 16939, 25841, 293, 3806, 337, 264, 37777, 670, 293, 670, 797, 13], "temperature": 0.0, "avg_logprob": -0.10873896220944962, "compression_ratio": 1.664179104477612, "no_speech_prob": 9.330033208243549e-05}, {"id": 60, "seek": 30896, "start": 325.35999999999996, "end": 332.08, "text": " So it's much quicker that way to debug issues either related to the CI or your code, and", "tokens": [407, 309, 311, 709, 16255, 300, 636, 281, 24083, 2663, 2139, 4077, 281, 264, 37777, 420, 428, 3089, 11, 293], "temperature": 0.0, "avg_logprob": -0.10873896220944962, "compression_ratio": 1.664179104477612, "no_speech_prob": 9.330033208243549e-05}, {"id": 61, "seek": 30896, "start": 332.08, "end": 335.84, "text": " it's also much easier to build the pipeline in the first place, like when you build a", "tokens": [309, 311, 611, 709, 3571, 281, 1322, 264, 15517, 294, 264, 700, 1081, 11, 411, 562, 291, 1322, 257], "temperature": 0.0, "avg_logprob": -0.10873896220944962, "compression_ratio": 1.664179104477612, "no_speech_prob": 9.330033208243549e-05}, {"id": 62, "seek": 33584, "start": 335.84, "end": 340.79999999999995, "text": " new CI pipeline for a new project, you have to go through the same feedback loop, because", "tokens": [777, 37777, 15517, 337, 257, 777, 1716, 11, 291, 362, 281, 352, 807, 264, 912, 5824, 6367, 11, 570], "temperature": 0.0, "avg_logprob": -0.11857547574830286, "compression_ratio": 1.7302904564315353, "no_speech_prob": 3.959356035920791e-05}, {"id": 63, "seek": 33584, "start": 340.79999999999995, "end": 346.0, "text": " you have to add new steps and figure out if it works or not, and if it doesn't, then you", "tokens": [291, 362, 281, 909, 777, 4439, 293, 2573, 484, 498, 309, 1985, 420, 406, 11, 293, 498, 309, 1177, 380, 11, 550, 291], "temperature": 0.0, "avg_logprob": -0.11857547574830286, "compression_ratio": 1.7302904564315353, "no_speech_prob": 3.959356035920791e-05}, {"id": 64, "seek": 33584, "start": 346.0, "end": 348.64, "text": " have to figure out how to add the right parameters.", "tokens": [362, 281, 2573, 484, 577, 281, 909, 264, 558, 9834, 13], "temperature": 0.0, "avg_logprob": -0.11857547574830286, "compression_ratio": 1.7302904564315353, "no_speech_prob": 3.959356035920791e-05}, {"id": 65, "seek": 33584, "start": 348.64, "end": 355.03999999999996, "text": " So even building new pipelines is way easier, because the whole thing is portable.", "tokens": [407, 754, 2390, 777, 40168, 307, 636, 3571, 11, 570, 264, 1379, 551, 307, 21800, 13], "temperature": 0.0, "avg_logprob": -0.11857547574830286, "compression_ratio": 1.7302904564315353, "no_speech_prob": 3.959356035920791e-05}, {"id": 66, "seek": 33584, "start": 355.03999999999996, "end": 360.59999999999997, "text": " The other thing that makes Dagger great is that you can basically write your pipelines", "tokens": [440, 661, 551, 300, 1669, 413, 11062, 869, 307, 300, 291, 393, 1936, 2464, 428, 40168], "temperature": 0.0, "avg_logprob": -0.11857547574830286, "compression_ratio": 1.7302904564315353, "no_speech_prob": 3.959356035920791e-05}, {"id": 67, "seek": 33584, "start": 360.59999999999997, "end": 363.28, "text": " in any language.", "tokens": [294, 604, 2856, 13], "temperature": 0.0, "avg_logprob": -0.11857547574830286, "compression_ratio": 1.7302904564315353, "no_speech_prob": 3.959356035920791e-05}, {"id": 68, "seek": 36328, "start": 363.28, "end": 370.64, "text": " Dagger officially supports a couple of languages, like Go, Python, TypeScript, and Q, but basically", "tokens": [413, 11062, 12053, 9346, 257, 1916, 295, 8650, 11, 411, 1037, 11, 15329, 11, 15576, 14237, 11, 293, 1249, 11, 457, 1936], "temperature": 0.0, "avg_logprob": -0.13492501035649726, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.00012066909403074533}, {"id": 69, "seek": 36328, "start": 370.64, "end": 376.23999999999995, "text": " any language that can talk to a GraphQL API, because that's what's under the hood, any", "tokens": [604, 2856, 300, 393, 751, 281, 257, 21884, 13695, 9362, 11, 570, 300, 311, 437, 311, 833, 264, 13376, 11, 604], "temperature": 0.0, "avg_logprob": -0.13492501035649726, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.00012066909403074533}, {"id": 70, "seek": 36328, "start": 376.23999999999995, "end": 382.08, "text": " language that can talk to a GraphQL API basically can be used to build your own pipelines with", "tokens": [2856, 300, 393, 751, 281, 257, 21884, 13695, 9362, 1936, 393, 312, 1143, 281, 1322, 428, 1065, 40168, 365], "temperature": 0.0, "avg_logprob": -0.13492501035649726, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.00012066909403074533}, {"id": 71, "seek": 36328, "start": 382.08, "end": 383.55999999999995, "text": " Dagger.", "tokens": [413, 11062, 13], "temperature": 0.0, "avg_logprob": -0.13492501035649726, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.00012066909403074533}, {"id": 72, "seek": 36328, "start": 383.55999999999995, "end": 388.67999999999995, "text": " And if you combine these two traits, like being portable and being able to write pipelines", "tokens": [400, 498, 291, 10432, 613, 732, 19526, 11, 411, 885, 21800, 293, 885, 1075, 281, 2464, 40168], "temperature": 0.0, "avg_logprob": -0.13492501035649726, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.00012066909403074533}, {"id": 73, "seek": 38868, "start": 388.68, "end": 393.44, "text": " in your own language, it also points to the fact that you can completely avoid vendor", "tokens": [294, 428, 1065, 2856, 11, 309, 611, 2793, 281, 264, 1186, 300, 291, 393, 2584, 5042, 24321], "temperature": 0.0, "avg_logprob": -0.17607147288772296, "compression_ratio": 1.7325581395348837, "no_speech_prob": 3.982704583904706e-05}, {"id": 74, "seek": 38868, "start": 393.44, "end": 399.36, "text": " locking, like you can, obviously you would still need some sort of CI service and you", "tokens": [23954, 11, 411, 291, 393, 11, 2745, 291, 576, 920, 643, 512, 1333, 295, 37777, 2643, 293, 291], "temperature": 0.0, "avg_logprob": -0.17607147288772296, "compression_ratio": 1.7325581395348837, "no_speech_prob": 3.982704583904706e-05}, {"id": 75, "seek": 38868, "start": 399.36, "end": 405.12, "text": " would need like a thin layer of integration that would run Dagger itself, but once you", "tokens": [576, 643, 411, 257, 5862, 4583, 295, 10980, 300, 576, 1190, 413, 11062, 2564, 11, 457, 1564, 291], "temperature": 0.0, "avg_logprob": -0.17607147288772296, "compression_ratio": 1.7325581395348837, "no_speech_prob": 3.982704583904706e-05}, {"id": 76, "seek": 38868, "start": 405.12, "end": 410.96000000000004, "text": " have a portable pipeline written in your own format in your own language, not in a proprietary", "tokens": [362, 257, 21800, 15517, 3720, 294, 428, 1065, 7877, 294, 428, 1065, 2856, 11, 406, 294, 257, 38992], "temperature": 0.0, "avg_logprob": -0.17607147288772296, "compression_ratio": 1.7325581395348837, "no_speech_prob": 3.982704583904706e-05}, {"id": 77, "seek": 38868, "start": 410.96000000000004, "end": 417.28000000000003, "text": " or CI-specific general format, you are not logged into the CI vendor you are using right", "tokens": [420, 37777, 12, 29258, 2674, 7877, 11, 291, 366, 406, 27231, 666, 264, 37777, 24321, 291, 366, 1228, 558], "temperature": 0.0, "avg_logprob": -0.17607147288772296, "compression_ratio": 1.7325581395348837, "no_speech_prob": 3.982704583904706e-05}, {"id": 78, "seek": 38868, "start": 417.28000000000003, "end": 418.28000000000003, "text": " now.", "tokens": [586, 13], "temperature": 0.0, "avg_logprob": -0.17607147288772296, "compression_ratio": 1.7325581395348837, "no_speech_prob": 3.982704583904706e-05}, {"id": 79, "seek": 41828, "start": 418.28, "end": 424.76, "text": " And that you don't really switch like CI providers often, but that happens, like when they buy", "tokens": [400, 300, 291, 500, 380, 534, 3679, 411, 37777, 11330, 2049, 11, 457, 300, 2314, 11, 411, 562, 436, 2256], "temperature": 0.0, "avg_logprob": -0.17463082533616286, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.3527503546793014e-05}, {"id": 80, "seek": 41828, "start": 424.76, "end": 428.52, "text": " your company and then you have to move from one provider to another and then you have", "tokens": [428, 2237, 293, 550, 291, 362, 281, 1286, 490, 472, 12398, 281, 1071, 293, 550, 291, 362], "temperature": 0.0, "avg_logprob": -0.17463082533616286, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.3527503546793014e-05}, {"id": 81, "seek": 41828, "start": 428.52, "end": 433.4, "text": " to move again because reasons.", "tokens": [281, 1286, 797, 570, 4112, 13], "temperature": 0.0, "avg_logprob": -0.17463082533616286, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.3527503546793014e-05}, {"id": 82, "seek": 41828, "start": 433.4, "end": 440.03999999999996, "text": " And the fourth reason or the fourth thing that makes Dagger great is caching.", "tokens": [400, 264, 6409, 1778, 420, 264, 6409, 551, 300, 1669, 413, 11062, 869, 307, 269, 2834, 13], "temperature": 0.0, "avg_logprob": -0.17463082533616286, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.3527503546793014e-05}, {"id": 83, "seek": 41828, "start": 440.03999999999996, "end": 447.15999999999997, "text": " Now, most CI services already have some sort of caching solution that you can use to cache", "tokens": [823, 11, 881, 37777, 3328, 1217, 362, 512, 1333, 295, 269, 2834, 3827, 300, 291, 393, 764, 281, 19459], "temperature": 0.0, "avg_logprob": -0.17463082533616286, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.3527503546793014e-05}, {"id": 84, "seek": 44716, "start": 447.16, "end": 452.56, "text": " like intermediary artifacts or dependencies or whatever you want to store in a cache that", "tokens": [411, 15184, 822, 24617, 420, 36606, 420, 2035, 291, 528, 281, 3531, 294, 257, 19459, 300], "temperature": 0.0, "avg_logprob": -0.13757970256190147, "compression_ratio": 1.8863636363636365, "no_speech_prob": 0.00012142412015236914}, {"id": 85, "seek": 44716, "start": 452.56, "end": 457.76000000000005, "text": " you don't want to download or compile every single time when your CI pipeline runs.", "tokens": [291, 500, 380, 528, 281, 5484, 420, 31413, 633, 2167, 565, 562, 428, 37777, 15517, 6676, 13], "temperature": 0.0, "avg_logprob": -0.13757970256190147, "compression_ratio": 1.8863636363636365, "no_speech_prob": 0.00012142412015236914}, {"id": 86, "seek": 44716, "start": 457.76000000000005, "end": 461.8, "text": " But you still have to configure it properly and you have to make sure that you have the", "tokens": [583, 291, 920, 362, 281, 22162, 309, 6108, 293, 291, 362, 281, 652, 988, 300, 291, 362, 264], "temperature": 0.0, "avg_logprob": -0.13757970256190147, "compression_ratio": 1.8863636363636365, "no_speech_prob": 0.00012142412015236914}, {"id": 87, "seek": 44716, "start": 461.8, "end": 467.40000000000003, "text": " right caching keys, you have to add the right paths to the caching configuration.", "tokens": [558, 269, 2834, 9317, 11, 291, 362, 281, 909, 264, 558, 14518, 281, 264, 269, 2834, 11694, 13], "temperature": 0.0, "avg_logprob": -0.13757970256190147, "compression_ratio": 1.8863636363636365, "no_speech_prob": 0.00012142412015236914}, {"id": 88, "seek": 44716, "start": 467.40000000000003, "end": 472.44000000000005, "text": " And you may either end up with a huge cache at the end of the day or you may not use cache", "tokens": [400, 291, 815, 2139, 917, 493, 365, 257, 2603, 19459, 412, 264, 917, 295, 264, 786, 420, 291, 815, 406, 764, 19459], "temperature": 0.0, "avg_logprob": -0.13757970256190147, "compression_ratio": 1.8863636363636365, "no_speech_prob": 0.00012142412015236914}, {"id": 89, "seek": 44716, "start": 472.44000000000005, "end": 473.44000000000005, "text": " at all.", "tokens": [412, 439, 13], "temperature": 0.0, "avg_logprob": -0.13757970256190147, "compression_ratio": 1.8863636363636365, "no_speech_prob": 0.00012142412015236914}, {"id": 90, "seek": 44716, "start": 473.44000000000005, "end": 476.36, "text": " So if you don't configure it properly, it may not work.", "tokens": [407, 498, 291, 500, 380, 22162, 309, 6108, 11, 309, 815, 406, 589, 13], "temperature": 0.0, "avg_logprob": -0.13757970256190147, "compression_ratio": 1.8863636363636365, "no_speech_prob": 0.00012142412015236914}, {"id": 91, "seek": 47636, "start": 476.36, "end": 479.96000000000004, "text": " With Dagger, you get caching by default.", "tokens": [2022, 413, 11062, 11, 291, 483, 269, 2834, 538, 7576, 13], "temperature": 0.0, "avg_logprob": -0.1512030283610026, "compression_ratio": 1.6629834254143647, "no_speech_prob": 6.562760245287791e-05}, {"id": 92, "seek": 47636, "start": 479.96000000000004, "end": 486.28000000000003, "text": " And by default, I mean every single step in your pipeline, the result of that run will", "tokens": [400, 538, 7576, 11, 286, 914, 633, 2167, 1823, 294, 428, 15517, 11, 264, 1874, 295, 300, 1190, 486], "temperature": 0.0, "avg_logprob": -0.1512030283610026, "compression_ratio": 1.6629834254143647, "no_speech_prob": 6.562760245287791e-05}, {"id": 93, "seek": 47636, "start": 486.28000000000003, "end": 491.88, "text": " be cached similarly how a Docker file works, like every single instruction, the result", "tokens": [312, 269, 15095, 14138, 577, 257, 33772, 3991, 1985, 11, 411, 633, 2167, 10951, 11, 264, 1874], "temperature": 0.0, "avg_logprob": -0.1512030283610026, "compression_ratio": 1.6629834254143647, "no_speech_prob": 6.562760245287791e-05}, {"id": 94, "seek": 47636, "start": 491.88, "end": 499.08000000000004, "text": " of that instruction will be cached if there are no changes before that step, actually.", "tokens": [295, 300, 10951, 486, 312, 269, 15095, 498, 456, 366, 572, 2962, 949, 300, 1823, 11, 767, 13], "temperature": 0.0, "avg_logprob": -0.1512030283610026, "compression_ratio": 1.6629834254143647, "no_speech_prob": 6.562760245287791e-05}, {"id": 95, "seek": 49908, "start": 499.08, "end": 506.64, "text": " So similarly to that, Dagger caches every step in your CI pipeline.", "tokens": [407, 14138, 281, 300, 11, 413, 11062, 269, 13272, 633, 1823, 294, 428, 37777, 15517, 13], "temperature": 0.0, "avg_logprob": -0.15960389917547052, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.6226193110924214e-05}, {"id": 96, "seek": 49908, "start": 506.64, "end": 507.64, "text": " Now how does it do that?", "tokens": [823, 577, 775, 309, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.15960389917547052, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.6226193110924214e-05}, {"id": 97, "seek": 49908, "start": 507.64, "end": 508.64, "text": " How does it work?", "tokens": [1012, 775, 309, 589, 30], "temperature": 0.0, "avg_logprob": -0.15960389917547052, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.6226193110924214e-05}, {"id": 98, "seek": 49908, "start": 508.64, "end": 509.64, "text": " How is this portable?", "tokens": [1012, 307, 341, 21800, 30], "temperature": 0.0, "avg_logprob": -0.15960389917547052, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.6226193110924214e-05}, {"id": 99, "seek": 49908, "start": 509.64, "end": 510.64, "text": " Any guesses?", "tokens": [2639, 42703, 30], "temperature": 0.0, "avg_logprob": -0.15960389917547052, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.6226193110924214e-05}, {"id": 100, "seek": 49908, "start": 510.64, "end": 511.64, "text": " One word?", "tokens": [1485, 1349, 30], "temperature": 0.0, "avg_logprob": -0.15960389917547052, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.6226193110924214e-05}, {"id": 101, "seek": 49908, "start": 511.64, "end": 516.0799999999999, "text": " Yeah, Docker, yeah, that's right.", "tokens": [865, 11, 33772, 11, 1338, 11, 300, 311, 558, 13], "temperature": 0.0, "avg_logprob": -0.15960389917547052, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.6226193110924214e-05}, {"id": 102, "seek": 49908, "start": 516.0799999999999, "end": 518.24, "text": " So containers, of course, containers.", "tokens": [407, 17089, 11, 295, 1164, 11, 17089, 13], "temperature": 0.0, "avg_logprob": -0.15960389917547052, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.6226193110924214e-05}, {"id": 103, "seek": 49908, "start": 518.24, "end": 523.6, "text": " So in order to be portable and to do all this magic that Dagger does, it needs to have like", "tokens": [407, 294, 1668, 281, 312, 21800, 293, 281, 360, 439, 341, 5585, 300, 413, 11062, 775, 11, 309, 2203, 281, 362, 411], "temperature": 0.0, "avg_logprob": -0.15960389917547052, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.6226193110924214e-05}, {"id": 104, "seek": 52360, "start": 523.6, "end": 529.28, "text": " a reasonable level of isolation so that you can be confident that it will run on your local", "tokens": [257, 10585, 1496, 295, 16001, 370, 300, 291, 393, 312, 6679, 300, 309, 486, 1190, 322, 428, 2654], "temperature": 0.0, "avg_logprob": -0.1289016487672157, "compression_ratio": 1.6502242152466369, "no_speech_prob": 3.299916352261789e-05}, {"id": 105, "seek": 52360, "start": 529.28, "end": 531.16, "text": " machine and on your CI the same way.", "tokens": [3479, 293, 322, 428, 37777, 264, 912, 636, 13], "temperature": 0.0, "avg_logprob": -0.1289016487672157, "compression_ratio": 1.6502242152466369, "no_speech_prob": 3.299916352261789e-05}, {"id": 106, "seek": 52360, "start": 531.16, "end": 536.48, "text": " So it runs your builds in containers.", "tokens": [407, 309, 6676, 428, 15182, 294, 17089, 13], "temperature": 0.0, "avg_logprob": -0.1289016487672157, "compression_ratio": 1.6502242152466369, "no_speech_prob": 3.299916352261789e-05}, {"id": 107, "seek": 52360, "start": 536.48, "end": 542.24, "text": " And I already mentioned that Dagger has a few official SDKs that you can use to build", "tokens": [400, 286, 1217, 2835, 300, 413, 11062, 575, 257, 1326, 4783, 37135, 82, 300, 291, 393, 764, 281, 1322], "temperature": 0.0, "avg_logprob": -0.1289016487672157, "compression_ratio": 1.6502242152466369, "no_speech_prob": 3.299916352261789e-05}, {"id": 108, "seek": 52360, "start": 542.24, "end": 545.28, "text": " the pipeline in your own code.", "tokens": [264, 15517, 294, 428, 1065, 3089, 13], "temperature": 0.0, "avg_logprob": -0.1289016487672157, "compression_ratio": 1.6502242152466369, "no_speech_prob": 3.299916352261789e-05}, {"id": 109, "seek": 52360, "start": 545.28, "end": 551.36, "text": " Using that Dagger SDK, you can talk to the so-called Dagger engine, which is the API", "tokens": [11142, 300, 413, 11062, 37135, 11, 291, 393, 751, 281, 264, 370, 12, 11880, 413, 11062, 2848, 11, 597, 307, 264, 9362], "temperature": 0.0, "avg_logprob": -0.1289016487672157, "compression_ratio": 1.6502242152466369, "no_speech_prob": 3.299916352261789e-05}, {"id": 110, "seek": 55136, "start": 551.36, "end": 555.24, "text": " that implements the GraphQL specification.", "tokens": [300, 704, 17988, 264, 21884, 13695, 31256, 13], "temperature": 0.0, "avg_logprob": -0.14463223693191365, "compression_ratio": 1.7110091743119267, "no_speech_prob": 2.3124850486055948e-05}, {"id": 111, "seek": 55136, "start": 555.24, "end": 561.0, "text": " And the Dagger SDK will call this API with the steps in your pipeline.", "tokens": [400, 264, 413, 11062, 37135, 486, 818, 341, 9362, 365, 264, 4439, 294, 428, 15517, 13], "temperature": 0.0, "avg_logprob": -0.14463223693191365, "compression_ratio": 1.7110091743119267, "no_speech_prob": 2.3124850486055948e-05}, {"id": 112, "seek": 55136, "start": 561.0, "end": 566.32, "text": " And the Dagger engine will build a DAG from these steps.", "tokens": [400, 264, 413, 11062, 2848, 486, 1322, 257, 9578, 38, 490, 613, 4439, 13], "temperature": 0.0, "avg_logprob": -0.14463223693191365, "compression_ratio": 1.7110091743119267, "no_speech_prob": 2.3124850486055948e-05}, {"id": 113, "seek": 55136, "start": 566.32, "end": 571.0, "text": " And then we'll pass that to basically to run through a container runtime.", "tokens": [400, 550, 321, 603, 1320, 300, 281, 1936, 281, 1190, 807, 257, 10129, 34474, 13], "temperature": 0.0, "avg_logprob": -0.14463223693191365, "compression_ratio": 1.7110091743119267, "no_speech_prob": 2.3124850486055948e-05}, {"id": 114, "seek": 55136, "start": 571.0, "end": 575.08, "text": " And that's how your pipeline will run.", "tokens": [400, 300, 311, 577, 428, 15517, 486, 1190, 13], "temperature": 0.0, "avg_logprob": -0.14463223693191365, "compression_ratio": 1.7110091743119267, "no_speech_prob": 2.3124850486055948e-05}, {"id": 115, "seek": 55136, "start": 575.08, "end": 579.72, "text": " And the good thing about this is that you can actually change this pipeline so the output", "tokens": [400, 264, 665, 551, 466, 341, 307, 300, 291, 393, 767, 1319, 341, 15517, 370, 264, 5598], "temperature": 0.0, "avg_logprob": -0.14463223693191365, "compression_ratio": 1.7110091743119267, "no_speech_prob": 2.3124850486055948e-05}, {"id": 116, "seek": 57972, "start": 579.72, "end": 582.72, "text": " of one pipeline can be the input of another.", "tokens": [295, 472, 15517, 393, 312, 264, 4846, 295, 1071, 13], "temperature": 0.0, "avg_logprob": -0.12458824907612596, "compression_ratio": 1.6798561151079137, "no_speech_prob": 3.0004883228684776e-05}, {"id": 117, "seek": 57972, "start": 582.72, "end": 586.44, "text": " And this whole thing goes through a single thing called session.", "tokens": [400, 341, 1379, 551, 1709, 807, 257, 2167, 551, 1219, 5481, 13], "temperature": 0.0, "avg_logprob": -0.12458824907612596, "compression_ratio": 1.6798561151079137, "no_speech_prob": 3.0004883228684776e-05}, {"id": 118, "seek": 57972, "start": 586.44, "end": 591.28, "text": " So in a single session, you can have multiple, like, these container executions.", "tokens": [407, 294, 257, 2167, 5481, 11, 291, 393, 362, 3866, 11, 411, 11, 613, 10129, 4454, 3666, 13], "temperature": 0.0, "avg_logprob": -0.12458824907612596, "compression_ratio": 1.6798561151079137, "no_speech_prob": 3.0004883228684776e-05}, {"id": 119, "seek": 57972, "start": 591.28, "end": 595.36, "text": " And you can change the results into each other if you want to.", "tokens": [400, 291, 393, 1319, 264, 3542, 666, 1184, 661, 498, 291, 528, 281, 13], "temperature": 0.0, "avg_logprob": -0.12458824907612596, "compression_ratio": 1.6798561151079137, "no_speech_prob": 3.0004883228684776e-05}, {"id": 120, "seek": 57972, "start": 595.36, "end": 600.6800000000001, "text": " Now let's actually take a look at how these things run.", "tokens": [823, 718, 311, 767, 747, 257, 574, 412, 577, 613, 721, 1190, 13], "temperature": 0.0, "avg_logprob": -0.12458824907612596, "compression_ratio": 1.6798561151079137, "no_speech_prob": 3.0004883228684776e-05}, {"id": 121, "seek": 57972, "start": 600.6800000000001, "end": 604.88, "text": " And the reason why I asked if there is anyone here who was in my presentation in the morning", "tokens": [400, 264, 1778, 983, 286, 2351, 498, 456, 307, 2878, 510, 567, 390, 294, 452, 5860, 294, 264, 2446], "temperature": 0.0, "avg_logprob": -0.12458824907612596, "compression_ratio": 1.6798561151079137, "no_speech_prob": 3.0004883228684776e-05}, {"id": 122, "seek": 57972, "start": 604.88, "end": 608.9200000000001, "text": " because I completely botched the demo and it didn't work at all.", "tokens": [570, 286, 2584, 10592, 19318, 264, 10723, 293, 309, 994, 380, 589, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.12458824907612596, "compression_ratio": 1.6798561151079137, "no_speech_prob": 3.0004883228684776e-05}, {"id": 123, "seek": 60892, "start": 608.92, "end": 612.88, "text": " So let's hope it works this time.", "tokens": [407, 718, 311, 1454, 309, 1985, 341, 565, 13], "temperature": 0.0, "avg_logprob": -0.18803436972878196, "compression_ratio": 1.452991452991453, "no_speech_prob": 4.8194291593972594e-05}, {"id": 124, "seek": 60892, "start": 612.88, "end": 615.12, "text": " So the example is in Go.", "tokens": [407, 264, 1365, 307, 294, 1037, 13], "temperature": 0.0, "avg_logprob": -0.18803436972878196, "compression_ratio": 1.452991452991453, "no_speech_prob": 4.8194291593972594e-05}, {"id": 125, "seek": 60892, "start": 615.12, "end": 621.64, "text": " But again, it could be, like, TypeScript and Python or even Q. And I'm not going to go", "tokens": [583, 797, 11, 309, 727, 312, 11, 411, 11, 15576, 14237, 293, 15329, 420, 754, 1249, 13, 400, 286, 478, 406, 516, 281, 352], "temperature": 0.0, "avg_logprob": -0.18803436972878196, "compression_ratio": 1.452991452991453, "no_speech_prob": 4.8194291593972594e-05}, {"id": 126, "seek": 60892, "start": 621.64, "end": 625.0799999999999, "text": " into that much detail about the Go specific here.", "tokens": [666, 300, 709, 2607, 466, 264, 1037, 2685, 510, 13], "temperature": 0.0, "avg_logprob": -0.18803436972878196, "compression_ratio": 1.452991452991453, "no_speech_prob": 4.8194291593972594e-05}, {"id": 127, "seek": 60892, "start": 625.0799999999999, "end": 631.1999999999999, "text": " But basically, you need to import this Dagger SDK in order to, by the way, can you see the", "tokens": [583, 1936, 11, 291, 643, 281, 974, 341, 413, 11062, 37135, 294, 1668, 281, 11, 538, 264, 636, 11, 393, 291, 536, 264], "temperature": 0.0, "avg_logprob": -0.18803436972878196, "compression_ratio": 1.452991452991453, "no_speech_prob": 4.8194291593972594e-05}, {"id": 128, "seek": 60892, "start": 631.1999999999999, "end": 634.7199999999999, "text": " screen or the code?", "tokens": [2568, 420, 264, 3089, 30], "temperature": 0.0, "avg_logprob": -0.18803436972878196, "compression_ratio": 1.452991452991453, "no_speech_prob": 4.8194291593972594e-05}, {"id": 129, "seek": 60892, "start": 634.7199999999999, "end": 635.7199999999999, "text": " Make it bigger?", "tokens": [4387, 309, 3801, 30], "temperature": 0.0, "avg_logprob": -0.18803436972878196, "compression_ratio": 1.452991452991453, "no_speech_prob": 4.8194291593972594e-05}, {"id": 130, "seek": 60892, "start": 635.7199999999999, "end": 636.7199999999999, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.18803436972878196, "compression_ratio": 1.452991452991453, "no_speech_prob": 4.8194291593972594e-05}, {"id": 131, "seek": 60892, "start": 636.7199999999999, "end": 637.7199999999999, "text": " Better?", "tokens": [15753, 30], "temperature": 0.0, "avg_logprob": -0.18803436972878196, "compression_ratio": 1.452991452991453, "no_speech_prob": 4.8194291593972594e-05}, {"id": 132, "seek": 60892, "start": 637.7199999999999, "end": 638.88, "text": " Cool.", "tokens": [8561, 13], "temperature": 0.0, "avg_logprob": -0.18803436972878196, "compression_ratio": 1.452991452991453, "no_speech_prob": 4.8194291593972594e-05}, {"id": 133, "seek": 63888, "start": 638.88, "end": 642.72, "text": " So you have to import this Dagger SDK if you want to use Go.", "tokens": [407, 291, 362, 281, 974, 341, 413, 11062, 37135, 498, 291, 528, 281, 764, 1037, 13], "temperature": 0.0, "avg_logprob": -0.14257519916423317, "compression_ratio": 1.892018779342723, "no_speech_prob": 1.868356775958091e-05}, {"id": 134, "seek": 63888, "start": 642.72, "end": 646.32, "text": " And then the first thing you need to do is connect to the Dagger Engine.", "tokens": [400, 550, 264, 700, 551, 291, 643, 281, 360, 307, 1745, 281, 264, 413, 11062, 7659, 13], "temperature": 0.0, "avg_logprob": -0.14257519916423317, "compression_ratio": 1.892018779342723, "no_speech_prob": 1.868356775958091e-05}, {"id": 135, "seek": 63888, "start": 646.32, "end": 653.0, "text": " Now if the Dagger Engine doesn't run locally, then the SDK will actually run it using, as", "tokens": [823, 498, 264, 413, 11062, 7659, 1177, 380, 1190, 16143, 11, 550, 264, 37135, 486, 767, 1190, 309, 1228, 11, 382], "temperature": 0.0, "avg_logprob": -0.14257519916423317, "compression_ratio": 1.892018779342723, "no_speech_prob": 1.868356775958091e-05}, {"id": 136, "seek": 63888, "start": 653.0, "end": 655.32, "text": " a simple Docker container.", "tokens": [257, 2199, 33772, 10129, 13], "temperature": 0.0, "avg_logprob": -0.14257519916423317, "compression_ratio": 1.892018779342723, "no_speech_prob": 1.868356775958091e-05}, {"id": 137, "seek": 63888, "start": 655.32, "end": 660.96, "text": " So the first thing you need to do is connect to this Dagger Engine.", "tokens": [407, 264, 700, 551, 291, 643, 281, 360, 307, 1745, 281, 341, 413, 11062, 7659, 13], "temperature": 0.0, "avg_logprob": -0.14257519916423317, "compression_ratio": 1.892018779342723, "no_speech_prob": 1.868356775958091e-05}, {"id": 138, "seek": 63888, "start": 660.96, "end": 664.96, "text": " And then you can start launching these containers and start building your pipelines.", "tokens": [400, 550, 291, 393, 722, 18354, 613, 17089, 293, 722, 2390, 428, 40168, 13], "temperature": 0.0, "avg_logprob": -0.14257519916423317, "compression_ratio": 1.892018779342723, "no_speech_prob": 1.868356775958091e-05}, {"id": 139, "seek": 66496, "start": 664.96, "end": 670.24, "text": " And if it looks very similar, it's because it's basically the same, uses the same language", "tokens": [400, 498, 309, 1542, 588, 2531, 11, 309, 311, 570, 309, 311, 1936, 264, 912, 11, 4960, 264, 912, 2856], "temperature": 0.0, "avg_logprob": -0.09474690471376691, "compression_ratio": 1.7466666666666666, "no_speech_prob": 2.2566757252207026e-05}, {"id": 140, "seek": 66496, "start": 670.24, "end": 674.84, "text": " and it looks very similar to Docker files.", "tokens": [293, 309, 1542, 588, 2531, 281, 33772, 7098, 13], "temperature": 0.0, "avg_logprob": -0.09474690471376691, "compression_ratio": 1.7466666666666666, "no_speech_prob": 2.2566757252207026e-05}, {"id": 141, "seek": 66496, "start": 674.84, "end": 678.64, "text": " And it works basically the same way.", "tokens": [400, 309, 1985, 1936, 264, 912, 636, 13], "temperature": 0.0, "avg_logprob": -0.09474690471376691, "compression_ratio": 1.7466666666666666, "no_speech_prob": 2.2566757252207026e-05}, {"id": 142, "seek": 66496, "start": 678.64, "end": 680.96, "text": " So you have, like, a base image.", "tokens": [407, 291, 362, 11, 411, 11, 257, 3096, 3256, 13], "temperature": 0.0, "avg_logprob": -0.09474690471376691, "compression_ratio": 1.7466666666666666, "no_speech_prob": 2.2566757252207026e-05}, {"id": 143, "seek": 66496, "start": 680.96, "end": 684.48, "text": " You have a bunch of mounted volumes for caching.", "tokens": [509, 362, 257, 3840, 295, 19138, 22219, 337, 269, 2834, 13], "temperature": 0.0, "avg_logprob": -0.09474690471376691, "compression_ratio": 1.7466666666666666, "no_speech_prob": 2.2566757252207026e-05}, {"id": 144, "seek": 66496, "start": 684.48, "end": 690.2800000000001, "text": " And then you mount the source code and you run some sort of command.", "tokens": [400, 550, 291, 3746, 264, 4009, 3089, 293, 291, 1190, 512, 1333, 295, 5622, 13], "temperature": 0.0, "avg_logprob": -0.09474690471376691, "compression_ratio": 1.7466666666666666, "no_speech_prob": 2.2566757252207026e-05}, {"id": 145, "seek": 66496, "start": 690.2800000000001, "end": 691.2800000000001, "text": " And that's it.", "tokens": [400, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.09474690471376691, "compression_ratio": 1.7466666666666666, "no_speech_prob": 2.2566757252207026e-05}, {"id": 146, "seek": 66496, "start": 691.2800000000001, "end": 692.2800000000001, "text": " That's your pipeline.", "tokens": [663, 311, 428, 15517, 13], "temperature": 0.0, "avg_logprob": -0.09474690471376691, "compression_ratio": 1.7466666666666666, "no_speech_prob": 2.2566757252207026e-05}, {"id": 147, "seek": 66496, "start": 692.2800000000001, "end": 693.76, "text": " Now let's see if it actually runs.", "tokens": [823, 718, 311, 536, 498, 309, 767, 6676, 13], "temperature": 0.0, "avg_logprob": -0.09474690471376691, "compression_ratio": 1.7466666666666666, "no_speech_prob": 2.2566757252207026e-05}, {"id": 148, "seek": 69376, "start": 693.76, "end": 699.88, "text": " So I use this make file authority for Go called mage.", "tokens": [407, 286, 764, 341, 652, 3991, 8281, 337, 1037, 1219, 463, 432, 13], "temperature": 0.0, "avg_logprob": -0.30176550766517374, "compression_ratio": 1.2835820895522387, "no_speech_prob": 0.00016741521540097892}, {"id": 149, "seek": 69376, "start": 699.88, "end": 705.24, "text": " So this is how I have this whole code implemented in a test function.", "tokens": [407, 341, 307, 577, 286, 362, 341, 1379, 3089, 12270, 294, 257, 1500, 2445, 13], "temperature": 0.0, "avg_logprob": -0.30176550766517374, "compression_ratio": 1.2835820895522387, "no_speech_prob": 0.00016741521540097892}, {"id": 150, "seek": 69376, "start": 705.24, "end": 710.4399999999999, "text": " Let's see if it runs.", "tokens": [961, 311, 536, 498, 309, 6676, 13], "temperature": 0.0, "avg_logprob": -0.30176550766517374, "compression_ratio": 1.2835820895522387, "no_speech_prob": 0.00016741521540097892}, {"id": 151, "seek": 69376, "start": 710.4399999999999, "end": 715.2, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.30176550766517374, "compression_ratio": 1.2835820895522387, "no_speech_prob": 0.00016741521540097892}, {"id": 152, "seek": 69376, "start": 715.2, "end": 717.2, "text": " So it did run.", "tokens": [407, 309, 630, 1190, 13], "temperature": 0.0, "avg_logprob": -0.30176550766517374, "compression_ratio": 1.2835820895522387, "no_speech_prob": 0.00016741521540097892}, {"id": 153, "seek": 69376, "start": 717.2, "end": 718.2, "text": " Cool.", "tokens": [8561, 13], "temperature": 0.0, "avg_logprob": -0.30176550766517374, "compression_ratio": 1.2835820895522387, "no_speech_prob": 0.00016741521540097892}, {"id": 154, "seek": 71820, "start": 718.2, "end": 726.76, "text": " Let's try just for, let's debug to see what's happening in the background.", "tokens": [961, 311, 853, 445, 337, 11, 718, 311, 24083, 281, 536, 437, 311, 2737, 294, 264, 3678, 13], "temperature": 0.0, "avg_logprob": -0.26987681850310297, "compression_ratio": 1.4539007092198581, "no_speech_prob": 9.107127698371187e-05}, {"id": 155, "seek": 71820, "start": 726.76, "end": 732.2, "text": " So it pulls an image, the goal and image.", "tokens": [407, 309, 16982, 364, 3256, 11, 264, 3387, 293, 3256, 13], "temperature": 0.0, "avg_logprob": -0.26987681850310297, "compression_ratio": 1.4539007092198581, "no_speech_prob": 9.107127698371187e-05}, {"id": 156, "seek": 71820, "start": 732.2, "end": 733.2, "text": " It mounts the code.", "tokens": [467, 40982, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.26987681850310297, "compression_ratio": 1.4539007092198581, "no_speech_prob": 9.107127698371187e-05}, {"id": 157, "seek": 71820, "start": 733.2, "end": 735.4000000000001, "text": " It mounts the volumes.", "tokens": [467, 40982, 264, 22219, 13], "temperature": 0.0, "avg_logprob": -0.26987681850310297, "compression_ratio": 1.4539007092198581, "no_speech_prob": 9.107127698371187e-05}, {"id": 158, "seek": 71820, "start": 735.4000000000001, "end": 742.36, "text": " And then runs my Go test on the mounted code.", "tokens": [400, 550, 6676, 452, 1037, 1500, 322, 264, 19138, 3089, 13], "temperature": 0.0, "avg_logprob": -0.26987681850310297, "compression_ratio": 1.4539007092198581, "no_speech_prob": 9.107127698371187e-05}, {"id": 159, "seek": 74236, "start": 742.36, "end": 749.4, "text": " And then basically exits and outputs the result of the test.", "tokens": [400, 550, 1936, 44183, 293, 23930, 264, 1874, 295, 264, 1500, 13], "temperature": 0.0, "avg_logprob": -0.16604813822993525, "compression_ratio": 1.648, "no_speech_prob": 8.616113518655766e-06}, {"id": 160, "seek": 74236, "start": 749.4, "end": 753.8000000000001, "text": " So, well, that's it.", "tokens": [407, 11, 731, 11, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.16604813822993525, "compression_ratio": 1.648, "no_speech_prob": 8.616113518655766e-06}, {"id": 161, "seek": 74236, "start": 753.8000000000001, "end": 757.04, "text": " If you want to get started with Dagger, check out the documentation.", "tokens": [759, 291, 528, 281, 483, 1409, 365, 413, 11062, 11, 1520, 484, 264, 14333, 13], "temperature": 0.0, "avg_logprob": -0.16604813822993525, "compression_ratio": 1.648, "no_speech_prob": 8.616113518655766e-06}, {"id": 162, "seek": 74236, "start": 757.04, "end": 758.72, "text": " It's getting better by the day.", "tokens": [467, 311, 1242, 1101, 538, 264, 786, 13], "temperature": 0.0, "avg_logprob": -0.16604813822993525, "compression_ratio": 1.648, "no_speech_prob": 8.616113518655766e-06}, {"id": 163, "seek": 74236, "start": 758.72, "end": 763.4, "text": " They actually released, well, either today or yesterday, a new quick start guide, which", "tokens": [814, 767, 4736, 11, 731, 11, 2139, 965, 420, 5186, 11, 257, 777, 1702, 722, 5934, 11, 597], "temperature": 0.0, "avg_logprob": -0.16604813822993525, "compression_ratio": 1.648, "no_speech_prob": 8.616113518655766e-06}, {"id": 164, "seek": 74236, "start": 763.4, "end": 764.88, "text": " is pretty awesome.", "tokens": [307, 1238, 3476, 13], "temperature": 0.0, "avg_logprob": -0.16604813822993525, "compression_ratio": 1.648, "no_speech_prob": 8.616113518655766e-06}, {"id": 165, "seek": 74236, "start": 764.88, "end": 769.84, "text": " It has all the three or four supported languages in a single document.", "tokens": [467, 575, 439, 264, 1045, 420, 1451, 8104, 8650, 294, 257, 2167, 4166, 13], "temperature": 0.0, "avg_logprob": -0.16604813822993525, "compression_ratio": 1.648, "no_speech_prob": 8.616113518655766e-06}, {"id": 166, "seek": 74236, "start": 769.84, "end": 772.32, "text": " So you can switch between languages if you want to.", "tokens": [407, 291, 393, 3679, 1296, 8650, 498, 291, 528, 281, 13], "temperature": 0.0, "avg_logprob": -0.16604813822993525, "compression_ratio": 1.648, "no_speech_prob": 8.616113518655766e-06}, {"id": 167, "seek": 77232, "start": 772.32, "end": 775.24, "text": " There is even a playground for the lower level GraphQL API.", "tokens": [821, 307, 754, 257, 24646, 337, 264, 3126, 1496, 21884, 13695, 9362, 13], "temperature": 0.0, "avg_logprob": -0.21834182739257812, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.00021082251623738557}, {"id": 168, "seek": 77232, "start": 775.24, "end": 779.6800000000001, "text": " So if you don't want to start a new project, you can play directly with the GraphQL API", "tokens": [407, 498, 291, 500, 380, 528, 281, 722, 257, 777, 1716, 11, 291, 393, 862, 3838, 365, 264, 21884, 13695, 9362], "temperature": 0.0, "avg_logprob": -0.21834182739257812, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.00021082251623738557}, {"id": 169, "seek": 77232, "start": 779.6800000000001, "end": 783.0400000000001, "text": " with the hosted version of the Dagger engine.", "tokens": [365, 264, 19204, 3037, 295, 264, 413, 11062, 2848, 13], "temperature": 0.0, "avg_logprob": -0.21834182739257812, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.00021082251623738557}, {"id": 170, "seek": 77232, "start": 783.0400000000001, "end": 784.1600000000001, "text": " So thank you for your attention.", "tokens": [407, 1309, 291, 337, 428, 3202, 13], "temperature": 0.0, "avg_logprob": -0.21834182739257812, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.00021082251623738557}, {"id": 171, "seek": 77232, "start": 784.1600000000001, "end": 797.9200000000001, "text": " If you have any questions, I'm happy to answer if you have time for that.", "tokens": [759, 291, 362, 604, 1651, 11, 286, 478, 2055, 281, 1867, 498, 291, 362, 565, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.21834182739257812, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.00021082251623738557}, {"id": 172, "seek": 77232, "start": 797.9200000000001, "end": 798.9200000000001, "text": " Thank you so much.", "tokens": [1044, 291, 370, 709, 13], "temperature": 0.0, "avg_logprob": -0.21834182739257812, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.00021082251623738557}, {"id": 173, "seek": 77232, "start": 798.9200000000001, "end": 799.9200000000001, "text": " Awesome.", "tokens": [10391, 13], "temperature": 0.0, "avg_logprob": -0.21834182739257812, "compression_ratio": 1.5471698113207548, "no_speech_prob": 0.00021082251623738557}, {"id": 174, "seek": 79992, "start": 799.92, "end": 803.8, "text": " I have a question with regards to implementation, so let's say that you roll your pipeline,", "tokens": [286, 362, 257, 1168, 365, 14258, 281, 11420, 11, 370, 718, 311, 584, 300, 291, 3373, 428, 15517, 11], "temperature": 0.0, "avg_logprob": -0.26264970198921533, "compression_ratio": 1.8090277777777777, "no_speech_prob": 0.0030620177276432514}, {"id": 175, "seek": 79992, "start": 803.8, "end": 808.0, "text": " you commit that you want to run that somewhere like in a CI environment.", "tokens": [291, 5599, 300, 291, 528, 281, 1190, 300, 4079, 411, 294, 257, 37777, 2823, 13], "temperature": 0.0, "avg_logprob": -0.26264970198921533, "compression_ratio": 1.8090277777777777, "no_speech_prob": 0.0030620177276432514}, {"id": 176, "seek": 79992, "start": 808.0, "end": 812.16, "text": " GitHub Actions, or Gaffer Bids, Jenkins, or whatever.", "tokens": [23331, 3251, 626, 11, 420, 460, 2518, 260, 363, 3742, 11, 41273, 11, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.26264970198921533, "compression_ratio": 1.8090277777777777, "no_speech_prob": 0.0030620177276432514}, {"id": 177, "seek": 79992, "start": 812.16, "end": 813.4799999999999, "text": " How do you go about that?", "tokens": [1012, 360, 291, 352, 466, 300, 30], "temperature": 0.0, "avg_logprob": -0.26264970198921533, "compression_ratio": 1.8090277777777777, "no_speech_prob": 0.0030620177276432514}, {"id": 178, "seek": 79992, "start": 813.4799999999999, "end": 818.4799999999999, "text": " I can imagine that you need to expose the Docker socket to the pipeline, or how does", "tokens": [286, 393, 3811, 300, 291, 643, 281, 19219, 264, 33772, 19741, 281, 264, 15517, 11, 420, 577, 775], "temperature": 0.0, "avg_logprob": -0.26264970198921533, "compression_ratio": 1.8090277777777777, "no_speech_prob": 0.0030620177276432514}, {"id": 179, "seek": 79992, "start": 818.4799999999999, "end": 819.4799999999999, "text": " it work?", "tokens": [309, 589, 30], "temperature": 0.0, "avg_logprob": -0.26264970198921533, "compression_ratio": 1.8090277777777777, "no_speech_prob": 0.0030620177276432514}, {"id": 180, "seek": 79992, "start": 819.4799999999999, "end": 823.4399999999999, "text": " Yeah, so basically, if you have Docker running in your environment, you can run this pipeline.", "tokens": [865, 11, 370, 1936, 11, 498, 291, 362, 33772, 2614, 294, 428, 2823, 11, 291, 393, 1190, 341, 15517, 13], "temperature": 0.0, "avg_logprob": -0.26264970198921533, "compression_ratio": 1.8090277777777777, "no_speech_prob": 0.0030620177276432514}, {"id": 181, "seek": 79992, "start": 823.4399999999999, "end": 826.16, "text": " And you can run Docker anywhere, basically, today.", "tokens": [400, 291, 393, 1190, 33772, 4992, 11, 1936, 11, 965, 13], "temperature": 0.0, "avg_logprob": -0.26264970198921533, "compression_ratio": 1.8090277777777777, "no_speech_prob": 0.0030620177276432514}, {"id": 182, "seek": 79992, "start": 826.16, "end": 828.56, "text": " You can run it in Jenkins or GitHub.", "tokens": [509, 393, 1190, 309, 294, 41273, 420, 23331, 13], "temperature": 0.0, "avg_logprob": -0.26264970198921533, "compression_ratio": 1.8090277777777777, "no_speech_prob": 0.0030620177276432514}, {"id": 183, "seek": 82856, "start": 828.56, "end": 830.92, "text": " You have it in GitHub Actions, actually.", "tokens": [509, 362, 309, 294, 23331, 3251, 626, 11, 767, 13], "temperature": 0.0, "avg_logprob": -0.23599947293599446, "compression_ratio": 1.664092664092664, "no_speech_prob": 4.261664435034618e-05}, {"id": 184, "seek": 82856, "start": 830.92, "end": 833.4399999999999, "text": " And you probably have it on your machine, as well.", "tokens": [400, 291, 1391, 362, 309, 322, 428, 3479, 11, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.23599947293599446, "compression_ratio": 1.664092664092664, "no_speech_prob": 4.261664435034618e-05}, {"id": 185, "seek": 82856, "start": 833.4399999999999, "end": 838.1199999999999, "text": " So wherever you have Docker running today, this pipeline will run.", "tokens": [407, 8660, 291, 362, 33772, 2614, 965, 11, 341, 15517, 486, 1190, 13], "temperature": 0.0, "avg_logprob": -0.23599947293599446, "compression_ratio": 1.664092664092664, "no_speech_prob": 4.261664435034618e-05}, {"id": 186, "seek": 82856, "start": 838.1199999999999, "end": 842.7199999999999, "text": " So you just invoke the Dagger command, that command that you just showed us.", "tokens": [407, 291, 445, 41117, 264, 413, 11062, 5622, 11, 300, 5622, 300, 291, 445, 4712, 505, 13], "temperature": 0.0, "avg_logprob": -0.23599947293599446, "compression_ratio": 1.664092664092664, "no_speech_prob": 4.261664435034618e-05}, {"id": 187, "seek": 82856, "start": 842.7199999999999, "end": 844.56, "text": " Yeah, it's not even a Dagger command.", "tokens": [865, 11, 309, 311, 406, 754, 257, 413, 11062, 5622, 13], "temperature": 0.0, "avg_logprob": -0.23599947293599446, "compression_ratio": 1.664092664092664, "no_speech_prob": 4.261664435034618e-05}, {"id": 188, "seek": 82856, "start": 844.56, "end": 845.8, "text": " This is entirely my code.", "tokens": [639, 307, 7696, 452, 3089, 13], "temperature": 0.0, "avg_logprob": -0.23599947293599446, "compression_ratio": 1.664092664092664, "no_speech_prob": 4.261664435034618e-05}, {"id": 189, "seek": 82856, "start": 845.8, "end": 847.8, "text": " This is my go binary, basically.", "tokens": [639, 307, 452, 352, 17434, 11, 1936, 13], "temperature": 0.0, "avg_logprob": -0.23599947293599446, "compression_ratio": 1.664092664092664, "no_speech_prob": 4.261664435034618e-05}, {"id": 190, "seek": 82856, "start": 847.8, "end": 848.8, "text": " Right, okay.", "tokens": [1779, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.23599947293599446, "compression_ratio": 1.664092664092664, "no_speech_prob": 4.261664435034618e-05}, {"id": 191, "seek": 82856, "start": 848.8, "end": 857.68, "text": " And it will find the Docker API socket, and if you just start containers there, yeah.", "tokens": [400, 309, 486, 915, 264, 33772, 9362, 19741, 11, 293, 498, 291, 445, 722, 17089, 456, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.23599947293599446, "compression_ratio": 1.664092664092664, "no_speech_prob": 4.261664435034618e-05}, {"id": 192, "seek": 85768, "start": 857.68, "end": 861.52, "text": " Before I, very cool stuff.", "tokens": [4546, 286, 11, 588, 1627, 1507, 13], "temperature": 0.0, "avg_logprob": -0.14251968886826064, "compression_ratio": 1.6394230769230769, "no_speech_prob": 0.0016934392042458057}, {"id": 193, "seek": 85768, "start": 861.52, "end": 868.5999999999999, "text": " So before I switch all my CI to Dagger, let's frame it like this.", "tokens": [407, 949, 286, 3679, 439, 452, 37777, 281, 413, 11062, 11, 718, 311, 3920, 309, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.14251968886826064, "compression_ratio": 1.6394230769230769, "no_speech_prob": 0.0016934392042458057}, {"id": 194, "seek": 85768, "start": 868.5999999999999, "end": 874.4399999999999, "text": " What would be the two things that you would really love to see an improved implementation", "tokens": [708, 576, 312, 264, 732, 721, 300, 291, 576, 534, 959, 281, 536, 364, 9689, 11420], "temperature": 0.0, "avg_logprob": -0.14251968886826064, "compression_ratio": 1.6394230769230769, "no_speech_prob": 0.0016934392042458057}, {"id": 195, "seek": 85768, "start": 874.4399999999999, "end": 877.8, "text": " of in the next version?", "tokens": [295, 294, 264, 958, 3037, 30], "temperature": 0.0, "avg_logprob": -0.14251968886826064, "compression_ratio": 1.6394230769230769, "no_speech_prob": 0.0016934392042458057}, {"id": 196, "seek": 85768, "start": 877.8, "end": 878.9599999999999, "text": " Can you repeat the question?", "tokens": [1664, 291, 7149, 264, 1168, 30], "temperature": 0.0, "avg_logprob": -0.14251968886826064, "compression_ratio": 1.6394230769230769, "no_speech_prob": 0.0016934392042458057}, {"id": 197, "seek": 85768, "start": 878.9599999999999, "end": 884.4799999999999, "text": " What are the two things that really need to be improved about the current state of Dagger?", "tokens": [708, 366, 264, 732, 721, 300, 534, 643, 281, 312, 9689, 466, 264, 2190, 1785, 295, 413, 11062, 30], "temperature": 0.0, "avg_logprob": -0.14251968886826064, "compression_ratio": 1.6394230769230769, "no_speech_prob": 0.0016934392042458057}, {"id": 198, "seek": 85768, "start": 884.4799999999999, "end": 886.16, "text": " What improves?", "tokens": [708, 24771, 30], "temperature": 0.0, "avg_logprob": -0.14251968886826064, "compression_ratio": 1.6394230769230769, "no_speech_prob": 0.0016934392042458057}, {"id": 199, "seek": 88616, "start": 886.16, "end": 892.6, "text": " To improve Dagger, in your opinion, what are the two things that need the most improvement?", "tokens": [1407, 3470, 413, 11062, 11, 294, 428, 4800, 11, 437, 366, 264, 732, 721, 300, 643, 264, 881, 10444, 30], "temperature": 0.0, "avg_logprob": -0.15932242075602213, "compression_ratio": 1.7224334600760456, "no_speech_prob": 0.00028394657419994473}, {"id": 200, "seek": 88616, "start": 892.6, "end": 896.24, "text": " Okay, so one thing is secret management.", "tokens": [1033, 11, 370, 472, 551, 307, 4054, 4592, 13], "temperature": 0.0, "avg_logprob": -0.15932242075602213, "compression_ratio": 1.7224334600760456, "no_speech_prob": 0.00028394657419994473}, {"id": 201, "seek": 88616, "start": 896.24, "end": 901.92, "text": " Right now, Dagger, it's not that easy to work with secrets, so that needs to be improved,", "tokens": [1779, 586, 11, 413, 11062, 11, 309, 311, 406, 300, 1858, 281, 589, 365, 14093, 11, 370, 300, 2203, 281, 312, 9689, 11], "temperature": 0.0, "avg_logprob": -0.15932242075602213, "compression_ratio": 1.7224334600760456, "no_speech_prob": 0.00028394657419994473}, {"id": 202, "seek": 88616, "start": 901.92, "end": 904.0799999999999, "text": " and they are actually working on it, so that's great.", "tokens": [293, 436, 366, 767, 1364, 322, 309, 11, 370, 300, 311, 869, 13], "temperature": 0.0, "avg_logprob": -0.15932242075602213, "compression_ratio": 1.7224334600760456, "no_speech_prob": 0.00028394657419994473}, {"id": 203, "seek": 88616, "start": 904.0799999999999, "end": 908.36, "text": " The other thing is that right now, if you build something in one language, for example,", "tokens": [440, 661, 551, 307, 300, 558, 586, 11, 498, 291, 1322, 746, 294, 472, 2856, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.15932242075602213, "compression_ratio": 1.7224334600760456, "no_speech_prob": 0.00028394657419994473}, {"id": 204, "seek": 88616, "start": 908.36, "end": 915.48, "text": " if I build a reusable library in Go to run my pipelines, I can't reuse it in TypeScript,", "tokens": [498, 286, 1322, 257, 41807, 6405, 294, 1037, 281, 1190, 452, 40168, 11, 286, 393, 380, 26225, 309, 294, 15576, 14237, 11], "temperature": 0.0, "avg_logprob": -0.15932242075602213, "compression_ratio": 1.7224334600760456, "no_speech_prob": 0.00028394657419994473}, {"id": 205, "seek": 91548, "start": 915.48, "end": 916.6800000000001, "text": " for example, today.", "tokens": [337, 1365, 11, 965, 13], "temperature": 0.0, "avg_logprob": -0.21586120880401885, "compression_ratio": 1.7896825396825398, "no_speech_prob": 3.652232408057898e-05}, {"id": 206, "seek": 91548, "start": 916.6800000000001, "end": 923.32, "text": " And for that, there is actually a feature called extensions, so they are working on a", "tokens": [400, 337, 300, 11, 456, 307, 767, 257, 4111, 1219, 25129, 11, 370, 436, 366, 1364, 322, 257], "temperature": 0.0, "avg_logprob": -0.21586120880401885, "compression_ratio": 1.7896825396825398, "no_speech_prob": 3.652232408057898e-05}, {"id": 207, "seek": 91548, "start": 923.32, "end": 928.6, "text": " feature so you can build extensions to the Dagger engine, so you can build these reliable", "tokens": [4111, 370, 291, 393, 1322, 25129, 281, 264, 413, 11062, 2848, 11, 370, 291, 393, 1322, 613, 12924], "temperature": 0.0, "avg_logprob": -0.21586120880401885, "compression_ratio": 1.7896825396825398, "no_speech_prob": 3.652232408057898e-05}, {"id": 208, "seek": 91548, "start": 928.6, "end": 935.0, "text": " or reusable pipeline PCs, like running liters and stuff like that, so you don't have to", "tokens": [420, 41807, 15517, 46913, 11, 411, 2614, 32323, 293, 1507, 411, 300, 11, 370, 291, 500, 380, 362, 281], "temperature": 0.0, "avg_logprob": -0.21586120880401885, "compression_ratio": 1.7896825396825398, "no_speech_prob": 3.652232408057898e-05}, {"id": 209, "seek": 91548, "start": 935.0, "end": 938.5600000000001, "text": " build that in your own code, you just have to build the extensions, and you can call", "tokens": [1322, 300, 294, 428, 1065, 3089, 11, 291, 445, 362, 281, 1322, 264, 25129, 11, 293, 291, 393, 818], "temperature": 0.0, "avg_logprob": -0.21586120880401885, "compression_ratio": 1.7896825396825398, "no_speech_prob": 3.652232408057898e-05}, {"id": 210, "seek": 91548, "start": 938.5600000000001, "end": 941.16, "text": " it from whatever language you want to call them.", "tokens": [309, 490, 2035, 2856, 291, 528, 281, 818, 552, 13], "temperature": 0.0, "avg_logprob": -0.21586120880401885, "compression_ratio": 1.7896825396825398, "no_speech_prob": 3.652232408057898e-05}, {"id": 211, "seek": 91548, "start": 941.16, "end": 943.76, "text": " Basically GraphQL API extensions.", "tokens": [8537, 21884, 13695, 9362, 25129, 13], "temperature": 0.0, "avg_logprob": -0.21586120880401885, "compression_ratio": 1.7896825396825398, "no_speech_prob": 3.652232408057898e-05}, {"id": 212, "seek": 94376, "start": 943.76, "end": 946.08, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.327965669228997, "compression_ratio": 1.4098360655737705, "no_speech_prob": 0.0005299891927279532}, {"id": 213, "seek": 94376, "start": 946.08, "end": 951.48, "text": " Last question.", "tokens": [5264, 1168, 13], "temperature": 0.0, "avg_logprob": -0.327965669228997, "compression_ratio": 1.4098360655737705, "no_speech_prob": 0.0005299891927279532}, {"id": 214, "seek": 94376, "start": 951.48, "end": 960.4399999999999, "text": " Hi, does Dagger support spinning up Service A concurrently with Service B, because the", "tokens": [2421, 11, 775, 413, 11062, 1406, 15640, 493, 9561, 316, 37702, 356, 365, 9561, 363, 11, 570, 264], "temperature": 0.0, "avg_logprob": -0.327965669228997, "compression_ratio": 1.4098360655737705, "no_speech_prob": 0.0005299891927279532}, {"id": 215, "seek": 94376, "start": 960.4399999999999, "end": 964.3199999999999, "text": " tests need something else to run while the test is running, and then afterward you can", "tokens": [6921, 643, 746, 1646, 281, 1190, 1339, 264, 1500, 307, 2614, 11, 293, 550, 934, 1007, 291, 393], "temperature": 0.0, "avg_logprob": -0.327965669228997, "compression_ratio": 1.4098360655737705, "no_speech_prob": 0.0005299891927279532}, {"id": 216, "seek": 94376, "start": 964.3199999999999, "end": 966.0, "text": " continue to other stuff?", "tokens": [2354, 281, 661, 1507, 30], "temperature": 0.0, "avg_logprob": -0.327965669228997, "compression_ratio": 1.4098360655737705, "no_speech_prob": 0.0005299891927279532}, {"id": 217, "seek": 94376, "start": 966.0, "end": 967.88, "text": " Right now, I don't think it does.", "tokens": [1779, 586, 11, 286, 500, 380, 519, 309, 775, 13], "temperature": 0.0, "avg_logprob": -0.327965669228997, "compression_ratio": 1.4098360655737705, "no_speech_prob": 0.0005299891927279532}, {"id": 218, "seek": 96788, "start": 967.88, "end": 974.48, "text": " Again, this is something that they are thinking about, but it's not a trivial thing to do,", "tokens": [3764, 11, 341, 307, 746, 300, 436, 366, 1953, 466, 11, 457, 309, 311, 406, 257, 26703, 551, 281, 360, 11], "temperature": 0.0, "avg_logprob": -0.23414557530329777, "compression_ratio": 1.4057971014492754, "no_speech_prob": 0.00020529561152216047}, {"id": 219, "seek": 96788, "start": 974.48, "end": 975.48, "text": " so no.", "tokens": [370, 572, 13], "temperature": 0.0, "avg_logprob": -0.23414557530329777, "compression_ratio": 1.4057971014492754, "no_speech_prob": 0.00020529561152216047}, {"id": 220, "seek": 96788, "start": 975.48, "end": 976.48, "text": " Currently.", "tokens": [19964, 13], "temperature": 0.0, "avg_logprob": -0.23414557530329777, "compression_ratio": 1.4057971014492754, "no_speech_prob": 0.00020529561152216047}, {"id": 221, "seek": 96788, "start": 976.48, "end": 981.48, "text": " Okay, someone is working on it.", "tokens": [1033, 11, 1580, 307, 1364, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.23414557530329777, "compression_ratio": 1.4057971014492754, "no_speech_prob": 0.00020529561152216047}, {"id": 222, "seek": 96788, "start": 981.48, "end": 982.48, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.23414557530329777, "compression_ratio": 1.4057971014492754, "no_speech_prob": 0.00020529561152216047}, {"id": 223, "seek": 96788, "start": 982.48, "end": 983.48, "text": " Of course.", "tokens": [2720, 1164, 13], "temperature": 0.0, "avg_logprob": -0.23414557530329777, "compression_ratio": 1.4057971014492754, "no_speech_prob": 0.00020529561152216047}, {"id": 224, "seek": 96788, "start": 983.48, "end": 984.48, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.23414557530329777, "compression_ratio": 1.4057971014492754, "no_speech_prob": 0.00020529561152216047}, {"id": 225, "seek": 98448, "start": 984.48, "end": 999.44, "text": " Thank you very much.", "tokens": [50364, 1044, 291, 588, 709, 13, 51112], "temperature": 0.0, "avg_logprob": -0.5767360925674438, "compression_ratio": 0.7142857142857143, "no_speech_prob": 0.00022553002054337412}], "language": "en"}