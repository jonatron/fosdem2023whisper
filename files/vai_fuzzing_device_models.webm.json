{"text": " Hello everyone, my name is Andrea, and today I'm going to tell you about what I think about fuzzing in Brazil. This presentation is not about fuzzing itself, but rather how we've failed at it. So before I start with the big pose of fuzzing, I will tell you a bit about fuzzing itself. I hope some of you already know about it, I don't have a lot of time. So fuzzing is basically an automated testing technique. The idea is to just send random input to a program to see how it behaves in that case, and how it works is that you use typically a tool, like a buzzer, that is going to generate random input for you, and then you're going to close some functions with that random input, and the buzzer is going to report some findings, and if it finds any interesting input files, it's going to write them to a course. Findings in this case can have crashes, can be hands, but can also be timeouts. So for fuzzing, when you first do it, you typically start from an empty corpus, but as you run fuzzing, you're going to generate some interesting inputs, which is helpful because in the next ones, you can just reuse those inputs and start from scratch. This helps with finding interesting things faster. So in this VMM, we implemented fuzzing for VM RTIO. We have three fast targets, one for the RTIO queue, one for the serialization of the RTIO queue, and one for the RTIO WISO in the Rescue Memo project. We only have implementation for the packet, so that's what we fuzzed. During fuzzing, we discovered three crashes, and only one of them is triggerable by any quotation malicious driver, and what we have now is that we are able to run fuzzing for every request that you're submitting to Rescue Memo to the VM RTIO repository. The fuzzing is apparently using link fuzzer, and besides the fuzzing that is happening in Rescue Memo itself, the folks from Cloud Hypervisor are also running fuzzing, and we also discovered a timeout theme in the package. So this actually brings me to our first report. So what is it you want? It should actually be... It's a people, and that is me. The first people is that you actually have to run the timeout, just in the code field for what you're in fuzzing for. Because the default, for example, for the fuzzing that we were using, is actually 20 minutes, and since we are just working with what I always would have used, and there's nothing that can possibly take 20 minutes to process, so we have to adjust the timeout to 60 seconds in our case, and this is something that was recommended by the folks from Cloud Hypervisor. Now, how we're running fuzzing in Rescue Memo is at the library level. The advantage of this is that it's easier to set up. So it's really important that it's easy to set up. It is a good thing. People are like, oh, but you're running fuzzing at the library level, so you don't have to have the kernel that's like so easy, so simple. So they're like, yeah, it's great, right? I mean, like, this is a good thing. And yeah, it's a good thing because you can also run on almost any host. You just have to have a fuzzing install in the repository, and then you just run fuzzing. And it also runs in a user space. There's also disadvantages, of course. The first one being that you cannot cover the whole repair setup, so that means that you're going to have some things that are great to be fuzzed. And then because you are fuzzing in user space, we need to do some more things for the driver, and this tends to be a bit complicated. And also you can find false positives. With the false positives, the idea is that you will find crashes that otherwise would not be triggered by a driver, because maybe you have some other chase in place. I would say that it's still important to fix these ones as well, because you never know how you're going to change your code and how it might end up actually triggering those findings in the future. And for the mocking of the driver, how it works, we've already simplified here, but the idea is that the driver is writing something in memory, and then the device reads what the driver wrote in memory, and it does stuff with the data. We want to fuzze in the human, and part of the piece we're doing in the human is this side of the device, and then what we need to mock is actually the driver's side of the communication. And in fuzzing the human, what we did is that we started this mocking of the driver from the beginning, so we needed it anyway to run some unit tests, we needed it for other kind of testing as well. So we had an initial mock interface from the beginning, and when we wanted to do fuzzing, we just evolved the mock driver in order to support that as well. Okay, so at the high level, how it happens right now in Rasmussen, is that we parsed the random bytes, we initialized the mock driver with the data that was parsed by fuzzer. At the high level, it ends up with some descriptors and some key functions that have some random input that they need to process, and then we create the queue, and we call these random functions with random input. And yeah, the second before is that if you are trying to do fuzzing and you just start when the project is already mature, what is going to happen is that it's going to be a bit difficult, you might find it very complicated to retrofit it. So instead, I know that it's not necessarily viable to start fuzzing when you start the project, but what you can do instead is that you can keep fuzzing in the back of your head, and then when you create some mock objects or some unit tests, you can think about how you can actually reuse them in fuzzing as well. Which is what we did but not very well. So one of the crashes that we actually found was that the mock driver was crashing on invalid input. So we had to adapt these actually to return errors, even though it was just one test, we couldn't just crash on invalid input anymore. So the idea is to return errors at the level where you want to do fuzzing, that can be processed at higher levels and so the fuzzing can crash. And now for structural interfuzzing. So with those structural interfuzzing, how it works is that the fuzzer is going to generate some random bytes, and then you have to interpret these as the bytes that you have to use for your library. So with structural interfuzzing, it's really nice because there are some tools that are just going to basically interpret the random bytes as a structure that you actually need. So it's super nice what it does is that it significantly reduces the code that you need to write, and even the rest of the information is arbitrary. Now, we had to change it unfortunately, before we knew that we had only 270 lines of code, and now we have around 740 lines of code for the fuzzer. And unfortunately, it came with some problems, so that's why we have to actually basically... The most important part is that it's not really produceable. So you can't really use the corpus that you had in previous runs, which was a big problem for us, because basically what happens is that arbitrary is introducing some... some randomness in one terminal so that it goes right into the input. And that basically means that you cannot use the corpus from previous runs. The thing for here is that we would like that we can do incremental improvements for the fuzzer, and we didn't check that what we want to increment can actually be implemented through N. So, yeah, instead, a better point would be to make sure that we can actually reuse the corpus that we generated. Okay, and now about when fuzzing actually fails. So, we had a PR in U.S. Human, at this point we were already running fuzzing for cool requests, and there was a PR that was introducing actually an overflow. So here the overflow is that the packet header size addition to the packet length can actually overflow, because the packet length is set up by the joint. This bug, well, I actually found it during code review, so it was a bit unexpected because I was hoping that the fuzzer was going to find it, which was not the case. So after some dive deep, I realized that running fuzzing for just 15 minutes might not actually be enough, because this bug was triggered with fuzzing about 40 minutes instead. So, how we fixed that is that we added a fuzzing session that is optional and that fuzzed for 24 hours. This one is to be started manually by the U.S. Human Maintainers, and should only be started when there are cool requests that actually impact the fuzzing relation. This is because we're also consuming a lot of resources when doing fuzzing, and also you don't want to block all the cool requests for 24 hours. So typically the last one on the slide is a page that quickly needs to be executed, so blocking it for one day might not be reasonable for all the cool requests. So the people here was not trying to fuzze for long enough, and, yeah, instead we had to work our way to find a way to not block cool requests, but at the same time to provide a way to fuzze. Fourth coverage for rust. So in rust you can actually get coverage information by running LLVM. In rust you only get light coverage. So basically this was the starting point of the presentation. I was thinking I would come here and I'm going to show you how great it is to run fuzzing for 15 minutes and then more minutes and then the coverage and all these really extravagant things. And so we ended up with fuzzing for 15 minutes generating 10 for these regions and the coverage of around 82%. So it's like, well, it's okay, that's good. So then let's just run with some minimal coverage as well. So this is some coverage that we generated from unit tests. Let's just feed through the fuzzer and see how this changes. There was no change, actually. So I was like, okay, it's not bad, not bad. Let's just run for two weeks. So what do you think this might happen now? So actually, it's working. At this point I was like, you press, I have to change my presentation, so it's not what I expected. But instead I learned something, right? So you can't actually use coverage to decide where to stop fuzzing. So instead what you can do is that you can use coverage information to see when, what the parts of your code are not actually covered. And yeah, well, that's about it, actually. We see the summary of the people that we read into. And I think now we have a lot of different questions. Did you look at how the fuzzer works and then what areas were not covered and try to figure out why it wasn't found in those areas? Yeah, so the question was if we looked at how the fuzzer works and what areas were not covered. Yes, we did, and I have a slide for that. Thanks for the question. Okay, so actually, I have two slides for that. There were some functions that we were not calling on purpose. So because on the virtual queue, for example, we have some functions that are just iterating over the script chain and then they're doing something with the data. And at the virtual queue level, you can't do something with the data. So it's like, okay, this needs to be fast at the high level, like at the device implementation level. So it's like, okay, we're not going to call these functions, which is a bit hilarious because that's where Proc Hypervisor actually found the timeout problem, which we were not able to reproduce with the virtual queue, but still. And we actually did this one function that shouldn't be called during fuzzing. And then I rerun the fuzzing and, yeah, it's a bit better, but it's still not great. And then I looked into what, well, actually, you can't see very well. That's unfortunate. Yeah, so I looked into what actually is not covered and you're not seeing there, so you have to trust me. These are actually errors. So the printing of errors to files. So since in the fuzzing we're not actually initializing a logger, these things cannot be triggered by fuzzing. So there's lots of error in this printing to a file that's not happening through fuzzing. Yeah. What's subsequently taken to actually make sure it covers everywhere which needs to be covered? And so discovering certain areas which clearly aren't covered. I didn't understand the question. Which areas of view? Well, what's subsequently taken to make sure the areas which weren't covered in the fuzzing are going to be covered in the future? Oh, okay, so the question was what measures are we taking? In order to make sure that all that was covered before is going to be covered in the next generations? Yeah. None? So right now we're not doing anything. This whole coverage thing is just something that I need for the presentation and it's not automatic in any way. But this is actually a good point for future investment to make sure that we're covering code because what we help with as well is that we make sure that new functions that we are adding to the code are also covered. So it's a great point to make that way. We're talking about the structure of our fuzzing. Yeah. And you mentioned that we cannot reuse the code to explain a bit more about that. Okay, so the question was how structured are we fuzzing and just that we cannot reuse the code. Let me see if I actually have a question here. No. Okay, so the idea is that what we were using, which is arbitrary, when it was taking the uniformed fuzzer was also adding some randomness to it. So because it was random, basically, every time it was writing the purpose to the file, it was introducing some randomness to it. So when the same people get to read again, then it would not have been the same. So where does the randomness come from? Where does the randomness come from? This is just how arbitrary it decided to implement it. There's actually an issue in arbitrary that they are aware of the problem with their not actually... It doesn't seem like they are just fixing it for some reason. So what we ended up doing is that we ended up doing some custom serialization with info, which is also a very well-known Rust package. It's not much more digital than it is arbitrary. And it doesn't matter. When you discover a bug with this puzzle, does it transform into a unit that's not the one? The question is, when we discover a bug, does it try to... Yeah, the way that we are fixing this kind of problem is that we are always adding a regression guess for them just to make sure that they don't... I was wondering about the computation requirements. So how many cores are you using? How many cores we are using? So when we read for two weeks, we actually used 96 cores. In uniqueness, I do not... So when you are running on coding, because I don't know exactly how many... One? Maybe? I don't know. But I think we have been running on 96 cores as well. That was another one. Can I see that guy? One minute. We found a color case that we are trying to shrink the crazy smaller color steps. Oh, this is... I don't know. Let's shut up afterwards. Thanks.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 11.0, "text": " Hello everyone, my name is Andrea, and today I'm going to tell you about what I think", "tokens": [2425, 1518, 11, 452, 1315, 307, 24215, 11, 293, 965, 286, 478, 516, 281, 980, 291, 466, 437, 286, 519], "temperature": 0.0, "avg_logprob": -0.46023102748541184, "compression_ratio": 1.569060773480663, "no_speech_prob": 0.6406915783882141}, {"id": 1, "seek": 0, "start": 11.0, "end": 12.0, "text": " about fuzzing in Brazil.", "tokens": [466, 283, 3334, 8781, 294, 9435, 13], "temperature": 0.0, "avg_logprob": -0.46023102748541184, "compression_ratio": 1.569060773480663, "no_speech_prob": 0.6406915783882141}, {"id": 2, "seek": 0, "start": 12.0, "end": 18.0, "text": " This presentation is not about fuzzing itself, but rather how we've failed at it.", "tokens": [639, 5860, 307, 406, 466, 283, 3334, 8781, 2564, 11, 457, 2831, 577, 321, 600, 7612, 412, 309, 13], "temperature": 0.0, "avg_logprob": -0.46023102748541184, "compression_ratio": 1.569060773480663, "no_speech_prob": 0.6406915783882141}, {"id": 3, "seek": 0, "start": 18.0, "end": 26.0, "text": " So before I start with the big pose of fuzzing, I will tell you a bit about fuzzing itself.", "tokens": [407, 949, 286, 722, 365, 264, 955, 10774, 295, 283, 3334, 8781, 11, 286, 486, 980, 291, 257, 857, 466, 283, 3334, 8781, 2564, 13], "temperature": 0.0, "avg_logprob": -0.46023102748541184, "compression_ratio": 1.569060773480663, "no_speech_prob": 0.6406915783882141}, {"id": 4, "seek": 2600, "start": 26.0, "end": 30.0, "text": " I hope some of you already know about it, I don't have a lot of time.", "tokens": [286, 1454, 512, 295, 291, 1217, 458, 466, 309, 11, 286, 500, 380, 362, 257, 688, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.16993498528140716, "compression_ratio": 1.5673076923076923, "no_speech_prob": 0.000986756756901741}, {"id": 5, "seek": 2600, "start": 30.0, "end": 35.0, "text": " So fuzzing is basically an automated testing technique.", "tokens": [407, 283, 3334, 8781, 307, 1936, 364, 18473, 4997, 6532, 13], "temperature": 0.0, "avg_logprob": -0.16993498528140716, "compression_ratio": 1.5673076923076923, "no_speech_prob": 0.000986756756901741}, {"id": 6, "seek": 2600, "start": 35.0, "end": 44.0, "text": " The idea is to just send random input to a program to see how it behaves in that case,", "tokens": [440, 1558, 307, 281, 445, 2845, 4974, 4846, 281, 257, 1461, 281, 536, 577, 309, 36896, 294, 300, 1389, 11], "temperature": 0.0, "avg_logprob": -0.16993498528140716, "compression_ratio": 1.5673076923076923, "no_speech_prob": 0.000986756756901741}, {"id": 7, "seek": 2600, "start": 44.0, "end": 53.0, "text": " and how it works is that you use typically a tool, like a buzzer, that is going to generate random input for you,", "tokens": [293, 577, 309, 1985, 307, 300, 291, 764, 5850, 257, 2290, 11, 411, 257, 13036, 260, 11, 300, 307, 516, 281, 8460, 4974, 4846, 337, 291, 11], "temperature": 0.0, "avg_logprob": -0.16993498528140716, "compression_ratio": 1.5673076923076923, "no_speech_prob": 0.000986756756901741}, {"id": 8, "seek": 5300, "start": 53.0, "end": 58.0, "text": " and then you're going to close some functions with that random input,", "tokens": [293, 550, 291, 434, 516, 281, 1998, 512, 6828, 365, 300, 4974, 4846, 11], "temperature": 0.0, "avg_logprob": -0.2272702639864892, "compression_ratio": 1.6409090909090909, "no_speech_prob": 0.00012750575842801481}, {"id": 9, "seek": 5300, "start": 58.0, "end": 66.0, "text": " and the buzzer is going to report some findings, and if it finds any interesting input files,", "tokens": [293, 264, 13036, 260, 307, 516, 281, 2275, 512, 16483, 11, 293, 498, 309, 10704, 604, 1880, 4846, 7098, 11], "temperature": 0.0, "avg_logprob": -0.2272702639864892, "compression_ratio": 1.6409090909090909, "no_speech_prob": 0.00012750575842801481}, {"id": 10, "seek": 5300, "start": 66.0, "end": 69.0, "text": " it's going to write them to a course.", "tokens": [309, 311, 516, 281, 2464, 552, 281, 257, 1164, 13], "temperature": 0.0, "avg_logprob": -0.2272702639864892, "compression_ratio": 1.6409090909090909, "no_speech_prob": 0.00012750575842801481}, {"id": 11, "seek": 5300, "start": 69.0, "end": 75.0, "text": " Findings in this case can have crashes, can be hands, but can also be timeouts.", "tokens": [11809, 1109, 294, 341, 1389, 393, 362, 28642, 11, 393, 312, 2377, 11, 457, 393, 611, 312, 565, 7711, 13], "temperature": 0.0, "avg_logprob": -0.2272702639864892, "compression_ratio": 1.6409090909090909, "no_speech_prob": 0.00012750575842801481}, {"id": 12, "seek": 5300, "start": 75.0, "end": 80.0, "text": " So for fuzzing, when you first do it, you typically start from an empty corpus,", "tokens": [407, 337, 283, 3334, 8781, 11, 562, 291, 700, 360, 309, 11, 291, 5850, 722, 490, 364, 6707, 1181, 31624, 11], "temperature": 0.0, "avg_logprob": -0.2272702639864892, "compression_ratio": 1.6409090909090909, "no_speech_prob": 0.00012750575842801481}, {"id": 13, "seek": 8000, "start": 80.0, "end": 84.0, "text": " but as you run fuzzing, you're going to generate some interesting inputs,", "tokens": [457, 382, 291, 1190, 283, 3334, 8781, 11, 291, 434, 516, 281, 8460, 512, 1880, 15743, 11], "temperature": 0.0, "avg_logprob": -0.22048625946044922, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.00024169692187570035}, {"id": 14, "seek": 8000, "start": 84.0, "end": 91.0, "text": " which is helpful because in the next ones, you can just reuse those inputs and start from scratch.", "tokens": [597, 307, 4961, 570, 294, 264, 958, 2306, 11, 291, 393, 445, 26225, 729, 15743, 293, 722, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.22048625946044922, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.00024169692187570035}, {"id": 15, "seek": 8000, "start": 91.0, "end": 94.0, "text": " This helps with finding interesting things faster.", "tokens": [639, 3665, 365, 5006, 1880, 721, 4663, 13], "temperature": 0.0, "avg_logprob": -0.22048625946044922, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.00024169692187570035}, {"id": 16, "seek": 8000, "start": 94.0, "end": 101.0, "text": " So in this VMM, we implemented fuzzing for VM RTIO.", "tokens": [407, 294, 341, 18038, 44, 11, 321, 12270, 283, 3334, 8781, 337, 18038, 497, 5422, 46, 13], "temperature": 0.0, "avg_logprob": -0.22048625946044922, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.00024169692187570035}, {"id": 17, "seek": 8000, "start": 101.0, "end": 107.0, "text": " We have three fast targets, one for the RTIO queue, one for the serialization of the RTIO queue,", "tokens": [492, 362, 1045, 2370, 12911, 11, 472, 337, 264, 497, 5422, 46, 18639, 11, 472, 337, 264, 17436, 2144, 295, 264, 497, 5422, 46, 18639, 11], "temperature": 0.0, "avg_logprob": -0.22048625946044922, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.00024169692187570035}, {"id": 18, "seek": 10700, "start": 107.0, "end": 113.0, "text": " and one for the RTIO WISO in the Rescue Memo project.", "tokens": [293, 472, 337, 264, 497, 5422, 46, 343, 2343, 46, 294, 264, 39379, 8731, 78, 1716, 13], "temperature": 0.0, "avg_logprob": -0.32273202572228776, "compression_ratio": 1.5949367088607596, "no_speech_prob": 7.931206346256658e-05}, {"id": 19, "seek": 10700, "start": 113.0, "end": 116.0, "text": " We only have implementation for the packet, so that's what we fuzzed.", "tokens": [492, 787, 362, 11420, 337, 264, 20300, 11, 370, 300, 311, 437, 321, 283, 3334, 11312, 13], "temperature": 0.0, "avg_logprob": -0.32273202572228776, "compression_ratio": 1.5949367088607596, "no_speech_prob": 7.931206346256658e-05}, {"id": 20, "seek": 10700, "start": 116.0, "end": 119.0, "text": " During fuzzing, we discovered three crashes,", "tokens": [6842, 283, 3334, 8781, 11, 321, 6941, 1045, 28642, 11], "temperature": 0.0, "avg_logprob": -0.32273202572228776, "compression_ratio": 1.5949367088607596, "no_speech_prob": 7.931206346256658e-05}, {"id": 21, "seek": 10700, "start": 119.0, "end": 125.0, "text": " and only one of them is triggerable by any quotation malicious driver,", "tokens": [293, 787, 472, 295, 552, 307, 7875, 712, 538, 604, 47312, 33496, 6787, 11], "temperature": 0.0, "avg_logprob": -0.32273202572228776, "compression_ratio": 1.5949367088607596, "no_speech_prob": 7.931206346256658e-05}, {"id": 22, "seek": 10700, "start": 125.0, "end": 128.0, "text": " and what we have now is that we are able to run fuzzing", "tokens": [293, 437, 321, 362, 586, 307, 300, 321, 366, 1075, 281, 1190, 283, 3334, 8781], "temperature": 0.0, "avg_logprob": -0.32273202572228776, "compression_ratio": 1.5949367088607596, "no_speech_prob": 7.931206346256658e-05}, {"id": 23, "seek": 10700, "start": 128.0, "end": 134.0, "text": " for every request that you're submitting to Rescue Memo to the VM RTIO repository.", "tokens": [337, 633, 5308, 300, 291, 434, 31836, 281, 39379, 8731, 78, 281, 264, 18038, 497, 5422, 46, 25841, 13], "temperature": 0.0, "avg_logprob": -0.32273202572228776, "compression_ratio": 1.5949367088607596, "no_speech_prob": 7.931206346256658e-05}, {"id": 24, "seek": 13400, "start": 134.0, "end": 138.0, "text": " The fuzzing is apparently using link fuzzer,", "tokens": [440, 283, 3334, 8781, 307, 7970, 1228, 2113, 283, 3334, 4527, 11], "temperature": 0.0, "avg_logprob": -0.419516202565786, "compression_ratio": 1.4917127071823204, "no_speech_prob": 0.0010492816800251603}, {"id": 25, "seek": 13400, "start": 138.0, "end": 144.0, "text": " and besides the fuzzing that is happening in Rescue Memo itself,", "tokens": [293, 11868, 264, 283, 3334, 8781, 300, 307, 2737, 294, 39379, 8731, 78, 2564, 11], "temperature": 0.0, "avg_logprob": -0.419516202565786, "compression_ratio": 1.4917127071823204, "no_speech_prob": 0.0010492816800251603}, {"id": 26, "seek": 13400, "start": 144.0, "end": 147.0, "text": " the folks from Cloud Hypervisor are also running fuzzing,", "tokens": [264, 4024, 490, 8061, 29592, 16457, 366, 611, 2614, 283, 3334, 8781, 11], "temperature": 0.0, "avg_logprob": -0.419516202565786, "compression_ratio": 1.4917127071823204, "no_speech_prob": 0.0010492816800251603}, {"id": 27, "seek": 13400, "start": 147.0, "end": 152.0, "text": " and we also discovered a timeout theme in the package.", "tokens": [293, 321, 611, 6941, 257, 565, 346, 6314, 294, 264, 7372, 13], "temperature": 0.0, "avg_logprob": -0.419516202565786, "compression_ratio": 1.4917127071823204, "no_speech_prob": 0.0010492816800251603}, {"id": 28, "seek": 13400, "start": 152.0, "end": 156.0, "text": " So this actually brings me to our first report.", "tokens": [407, 341, 767, 5607, 385, 281, 527, 700, 2275, 13], "temperature": 0.0, "avg_logprob": -0.419516202565786, "compression_ratio": 1.4917127071823204, "no_speech_prob": 0.0010492816800251603}, {"id": 29, "seek": 15600, "start": 156.0, "end": 164.0, "text": " So what is it you want? It should actually be...", "tokens": [407, 437, 307, 309, 291, 528, 30, 467, 820, 767, 312, 485], "temperature": 0.0, "avg_logprob": -0.4255025269555264, "compression_ratio": 1.4172661870503598, "no_speech_prob": 0.0006176070310175419}, {"id": 30, "seek": 15600, "start": 164.0, "end": 173.0, "text": " It's a people, and that is me.", "tokens": [467, 311, 257, 561, 11, 293, 300, 307, 385, 13], "temperature": 0.0, "avg_logprob": -0.4255025269555264, "compression_ratio": 1.4172661870503598, "no_speech_prob": 0.0006176070310175419}, {"id": 31, "seek": 15600, "start": 173.0, "end": 178.0, "text": " The first people is that you actually have to run the timeout,", "tokens": [440, 700, 561, 307, 300, 291, 767, 362, 281, 1190, 264, 565, 346, 11], "temperature": 0.0, "avg_logprob": -0.4255025269555264, "compression_ratio": 1.4172661870503598, "no_speech_prob": 0.0006176070310175419}, {"id": 32, "seek": 15600, "start": 178.0, "end": 182.0, "text": " just in the code field for what you're in fuzzing for.", "tokens": [445, 294, 264, 3089, 2519, 337, 437, 291, 434, 294, 283, 3334, 8781, 337, 13], "temperature": 0.0, "avg_logprob": -0.4255025269555264, "compression_ratio": 1.4172661870503598, "no_speech_prob": 0.0006176070310175419}, {"id": 33, "seek": 18200, "start": 182.0, "end": 186.0, "text": " Because the default, for example, for the fuzzing that we were using,", "tokens": [1436, 264, 7576, 11, 337, 1365, 11, 337, 264, 283, 3334, 8781, 300, 321, 645, 1228, 11], "temperature": 0.0, "avg_logprob": -0.2767629099416209, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.0006486561615020037}, {"id": 34, "seek": 18200, "start": 186.0, "end": 191.0, "text": " is actually 20 minutes, and since we are just working with what I always would have used,", "tokens": [307, 767, 945, 2077, 11, 293, 1670, 321, 366, 445, 1364, 365, 437, 286, 1009, 576, 362, 1143, 11], "temperature": 0.0, "avg_logprob": -0.2767629099416209, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.0006486561615020037}, {"id": 35, "seek": 18200, "start": 191.0, "end": 196.0, "text": " and there's nothing that can possibly take 20 minutes to process,", "tokens": [293, 456, 311, 1825, 300, 393, 6264, 747, 945, 2077, 281, 1399, 11], "temperature": 0.0, "avg_logprob": -0.2767629099416209, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.0006486561615020037}, {"id": 36, "seek": 18200, "start": 196.0, "end": 202.0, "text": " so we have to adjust the timeout to 60 seconds in our case,", "tokens": [370, 321, 362, 281, 4369, 264, 565, 346, 281, 4060, 3949, 294, 527, 1389, 11], "temperature": 0.0, "avg_logprob": -0.2767629099416209, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.0006486561615020037}, {"id": 37, "seek": 18200, "start": 202.0, "end": 208.0, "text": " and this is something that was recommended by the folks from Cloud Hypervisor.", "tokens": [293, 341, 307, 746, 300, 390, 9628, 538, 264, 4024, 490, 8061, 29592, 16457, 13], "temperature": 0.0, "avg_logprob": -0.2767629099416209, "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.0006486561615020037}, {"id": 38, "seek": 20800, "start": 208.0, "end": 213.0, "text": " Now, how we're running fuzzing in Rescue Memo is at the library level.", "tokens": [823, 11, 577, 321, 434, 2614, 283, 3334, 8781, 294, 39379, 8731, 78, 307, 412, 264, 6405, 1496, 13], "temperature": 0.0, "avg_logprob": -0.17657631866691648, "compression_ratio": 1.7792207792207793, "no_speech_prob": 0.000594235782045871}, {"id": 39, "seek": 20800, "start": 213.0, "end": 217.0, "text": " The advantage of this is that it's easier to set up.", "tokens": [440, 5002, 295, 341, 307, 300, 309, 311, 3571, 281, 992, 493, 13], "temperature": 0.0, "avg_logprob": -0.17657631866691648, "compression_ratio": 1.7792207792207793, "no_speech_prob": 0.000594235782045871}, {"id": 40, "seek": 20800, "start": 217.0, "end": 220.0, "text": " So it's really important that it's easy to set up.", "tokens": [407, 309, 311, 534, 1021, 300, 309, 311, 1858, 281, 992, 493, 13], "temperature": 0.0, "avg_logprob": -0.17657631866691648, "compression_ratio": 1.7792207792207793, "no_speech_prob": 0.000594235782045871}, {"id": 41, "seek": 20800, "start": 220.0, "end": 222.0, "text": " It is a good thing.", "tokens": [467, 307, 257, 665, 551, 13], "temperature": 0.0, "avg_logprob": -0.17657631866691648, "compression_ratio": 1.7792207792207793, "no_speech_prob": 0.000594235782045871}, {"id": 42, "seek": 20800, "start": 222.0, "end": 225.0, "text": " People are like, oh, but you're running fuzzing at the library level,", "tokens": [3432, 366, 411, 11, 1954, 11, 457, 291, 434, 2614, 283, 3334, 8781, 412, 264, 6405, 1496, 11], "temperature": 0.0, "avg_logprob": -0.17657631866691648, "compression_ratio": 1.7792207792207793, "no_speech_prob": 0.000594235782045871}, {"id": 43, "seek": 20800, "start": 225.0, "end": 229.0, "text": " so you don't have to have the kernel that's like so easy, so simple.", "tokens": [370, 291, 500, 380, 362, 281, 362, 264, 28256, 300, 311, 411, 370, 1858, 11, 370, 2199, 13], "temperature": 0.0, "avg_logprob": -0.17657631866691648, "compression_ratio": 1.7792207792207793, "no_speech_prob": 0.000594235782045871}, {"id": 44, "seek": 20800, "start": 229.0, "end": 231.0, "text": " So they're like, yeah, it's great, right?", "tokens": [407, 436, 434, 411, 11, 1338, 11, 309, 311, 869, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17657631866691648, "compression_ratio": 1.7792207792207793, "no_speech_prob": 0.000594235782045871}, {"id": 45, "seek": 20800, "start": 231.0, "end": 233.0, "text": " I mean, like, this is a good thing.", "tokens": [286, 914, 11, 411, 11, 341, 307, 257, 665, 551, 13], "temperature": 0.0, "avg_logprob": -0.17657631866691648, "compression_ratio": 1.7792207792207793, "no_speech_prob": 0.000594235782045871}, {"id": 46, "seek": 23300, "start": 233.0, "end": 238.0, "text": " And yeah, it's a good thing because you can also run on almost any host.", "tokens": [400, 1338, 11, 309, 311, 257, 665, 551, 570, 291, 393, 611, 1190, 322, 1920, 604, 3975, 13], "temperature": 0.0, "avg_logprob": -0.23064001921181368, "compression_ratio": 1.6754385964912282, "no_speech_prob": 9.557540761306882e-05}, {"id": 47, "seek": 23300, "start": 238.0, "end": 242.0, "text": " You just have to have a fuzzing install in the repository,", "tokens": [509, 445, 362, 281, 362, 257, 283, 3334, 8781, 3625, 294, 264, 25841, 11], "temperature": 0.0, "avg_logprob": -0.23064001921181368, "compression_ratio": 1.6754385964912282, "no_speech_prob": 9.557540761306882e-05}, {"id": 48, "seek": 23300, "start": 242.0, "end": 244.0, "text": " and then you just run fuzzing.", "tokens": [293, 550, 291, 445, 1190, 283, 3334, 8781, 13], "temperature": 0.0, "avg_logprob": -0.23064001921181368, "compression_ratio": 1.6754385964912282, "no_speech_prob": 9.557540761306882e-05}, {"id": 49, "seek": 23300, "start": 244.0, "end": 247.0, "text": " And it also runs in a user space.", "tokens": [400, 309, 611, 6676, 294, 257, 4195, 1901, 13], "temperature": 0.0, "avg_logprob": -0.23064001921181368, "compression_ratio": 1.6754385964912282, "no_speech_prob": 9.557540761306882e-05}, {"id": 50, "seek": 23300, "start": 247.0, "end": 249.0, "text": " There's also disadvantages, of course.", "tokens": [821, 311, 611, 37431, 11, 295, 1164, 13], "temperature": 0.0, "avg_logprob": -0.23064001921181368, "compression_ratio": 1.6754385964912282, "no_speech_prob": 9.557540761306882e-05}, {"id": 51, "seek": 23300, "start": 249.0, "end": 254.0, "text": " The first one being that you cannot cover the whole repair setup,", "tokens": [440, 700, 472, 885, 300, 291, 2644, 2060, 264, 1379, 10535, 8657, 11], "temperature": 0.0, "avg_logprob": -0.23064001921181368, "compression_ratio": 1.6754385964912282, "no_speech_prob": 9.557540761306882e-05}, {"id": 52, "seek": 23300, "start": 254.0, "end": 259.0, "text": " so that means that you're going to have some things that are great to be fuzzed.", "tokens": [370, 300, 1355, 300, 291, 434, 516, 281, 362, 512, 721, 300, 366, 869, 281, 312, 283, 3334, 11312, 13], "temperature": 0.0, "avg_logprob": -0.23064001921181368, "compression_ratio": 1.6754385964912282, "no_speech_prob": 9.557540761306882e-05}, {"id": 53, "seek": 25900, "start": 259.0, "end": 263.0, "text": " And then because you are fuzzing in user space,", "tokens": [400, 550, 570, 291, 366, 283, 3334, 8781, 294, 4195, 1901, 11], "temperature": 0.0, "avg_logprob": -0.18090120351539468, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.0003135238657705486}, {"id": 54, "seek": 25900, "start": 263.0, "end": 266.0, "text": " we need to do some more things for the driver,", "tokens": [321, 643, 281, 360, 512, 544, 721, 337, 264, 6787, 11], "temperature": 0.0, "avg_logprob": -0.18090120351539468, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.0003135238657705486}, {"id": 55, "seek": 25900, "start": 266.0, "end": 270.0, "text": " and this tends to be a bit complicated.", "tokens": [293, 341, 12258, 281, 312, 257, 857, 6179, 13], "temperature": 0.0, "avg_logprob": -0.18090120351539468, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.0003135238657705486}, {"id": 56, "seek": 25900, "start": 270.0, "end": 274.0, "text": " And also you can find false positives.", "tokens": [400, 611, 291, 393, 915, 7908, 35127, 13], "temperature": 0.0, "avg_logprob": -0.18090120351539468, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.0003135238657705486}, {"id": 57, "seek": 25900, "start": 274.0, "end": 278.0, "text": " With the false positives, the idea is that you will find crashes", "tokens": [2022, 264, 7908, 35127, 11, 264, 1558, 307, 300, 291, 486, 915, 28642], "temperature": 0.0, "avg_logprob": -0.18090120351539468, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.0003135238657705486}, {"id": 58, "seek": 25900, "start": 278.0, "end": 280.0, "text": " that otherwise would not be triggered by a driver,", "tokens": [300, 5911, 576, 406, 312, 21710, 538, 257, 6787, 11], "temperature": 0.0, "avg_logprob": -0.18090120351539468, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.0003135238657705486}, {"id": 59, "seek": 25900, "start": 280.0, "end": 283.0, "text": " because maybe you have some other chase in place.", "tokens": [570, 1310, 291, 362, 512, 661, 15359, 294, 1081, 13], "temperature": 0.0, "avg_logprob": -0.18090120351539468, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.0003135238657705486}, {"id": 60, "seek": 25900, "start": 283.0, "end": 287.0, "text": " I would say that it's still important to fix these ones as well,", "tokens": [286, 576, 584, 300, 309, 311, 920, 1021, 281, 3191, 613, 2306, 382, 731, 11], "temperature": 0.0, "avg_logprob": -0.18090120351539468, "compression_ratio": 1.6694214876033058, "no_speech_prob": 0.0003135238657705486}, {"id": 61, "seek": 28700, "start": 287.0, "end": 290.0, "text": " because you never know how you're going to change your code", "tokens": [570, 291, 1128, 458, 577, 291, 434, 516, 281, 1319, 428, 3089], "temperature": 0.0, "avg_logprob": -0.17984326680501303, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.00021387965534813702}, {"id": 62, "seek": 28700, "start": 290.0, "end": 297.0, "text": " and how it might end up actually triggering those findings in the future.", "tokens": [293, 577, 309, 1062, 917, 493, 767, 40406, 729, 16483, 294, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.17984326680501303, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.00021387965534813702}, {"id": 63, "seek": 28700, "start": 297.0, "end": 300.0, "text": " And for the mocking of the driver, how it works,", "tokens": [400, 337, 264, 49792, 295, 264, 6787, 11, 577, 309, 1985, 11], "temperature": 0.0, "avg_logprob": -0.17984326680501303, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.00021387965534813702}, {"id": 64, "seek": 28700, "start": 300.0, "end": 304.0, "text": " we've already simplified here,", "tokens": [321, 600, 1217, 26335, 510, 11], "temperature": 0.0, "avg_logprob": -0.17984326680501303, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.00021387965534813702}, {"id": 65, "seek": 28700, "start": 304.0, "end": 307.0, "text": " but the idea is that the driver is writing something in memory,", "tokens": [457, 264, 1558, 307, 300, 264, 6787, 307, 3579, 746, 294, 4675, 11], "temperature": 0.0, "avg_logprob": -0.17984326680501303, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.00021387965534813702}, {"id": 66, "seek": 28700, "start": 307.0, "end": 310.0, "text": " and then the device reads what the driver wrote in memory,", "tokens": [293, 550, 264, 4302, 15700, 437, 264, 6787, 4114, 294, 4675, 11], "temperature": 0.0, "avg_logprob": -0.17984326680501303, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.00021387965534813702}, {"id": 67, "seek": 28700, "start": 310.0, "end": 314.0, "text": " and it does stuff with the data.", "tokens": [293, 309, 775, 1507, 365, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.17984326680501303, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.00021387965534813702}, {"id": 68, "seek": 31400, "start": 314.0, "end": 317.0, "text": " We want to fuzze in the human, and part of the piece we're doing in the human", "tokens": [492, 528, 281, 283, 3334, 1381, 294, 264, 1952, 11, 293, 644, 295, 264, 2522, 321, 434, 884, 294, 264, 1952], "temperature": 0.0, "avg_logprob": -0.2874400774637858, "compression_ratio": 1.8728813559322033, "no_speech_prob": 0.0002795214531943202}, {"id": 69, "seek": 31400, "start": 317.0, "end": 320.0, "text": " is this side of the device,", "tokens": [307, 341, 1252, 295, 264, 4302, 11], "temperature": 0.0, "avg_logprob": -0.2874400774637858, "compression_ratio": 1.8728813559322033, "no_speech_prob": 0.0002795214531943202}, {"id": 70, "seek": 31400, "start": 320.0, "end": 324.0, "text": " and then what we need to mock is actually the driver's side of the communication.", "tokens": [293, 550, 437, 321, 643, 281, 17362, 307, 767, 264, 6787, 311, 1252, 295, 264, 6101, 13], "temperature": 0.0, "avg_logprob": -0.2874400774637858, "compression_ratio": 1.8728813559322033, "no_speech_prob": 0.0002795214531943202}, {"id": 71, "seek": 31400, "start": 324.0, "end": 328.0, "text": " And in fuzzing the human, what we did is that we started this mocking", "tokens": [400, 294, 283, 3334, 8781, 264, 1952, 11, 437, 321, 630, 307, 300, 321, 1409, 341, 49792], "temperature": 0.0, "avg_logprob": -0.2874400774637858, "compression_ratio": 1.8728813559322033, "no_speech_prob": 0.0002795214531943202}, {"id": 72, "seek": 31400, "start": 328.0, "end": 330.0, "text": " of the driver from the beginning,", "tokens": [295, 264, 6787, 490, 264, 2863, 11], "temperature": 0.0, "avg_logprob": -0.2874400774637858, "compression_ratio": 1.8728813559322033, "no_speech_prob": 0.0002795214531943202}, {"id": 73, "seek": 31400, "start": 330.0, "end": 333.0, "text": " so we needed it anyway to run some unit tests,", "tokens": [370, 321, 2978, 309, 4033, 281, 1190, 512, 4985, 6921, 11], "temperature": 0.0, "avg_logprob": -0.2874400774637858, "compression_ratio": 1.8728813559322033, "no_speech_prob": 0.0002795214531943202}, {"id": 74, "seek": 31400, "start": 333.0, "end": 336.0, "text": " we needed it for other kind of testing as well.", "tokens": [321, 2978, 309, 337, 661, 733, 295, 4997, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.2874400774637858, "compression_ratio": 1.8728813559322033, "no_speech_prob": 0.0002795214531943202}, {"id": 75, "seek": 31400, "start": 336.0, "end": 340.0, "text": " So we had an initial mock interface from the beginning,", "tokens": [407, 321, 632, 364, 5883, 17362, 9226, 490, 264, 2863, 11], "temperature": 0.0, "avg_logprob": -0.2874400774637858, "compression_ratio": 1.8728813559322033, "no_speech_prob": 0.0002795214531943202}, {"id": 76, "seek": 34000, "start": 340.0, "end": 345.0, "text": " and when we wanted to do fuzzing, we just evolved the mock driver", "tokens": [293, 562, 321, 1415, 281, 360, 283, 3334, 8781, 11, 321, 445, 14178, 264, 17362, 6787], "temperature": 0.0, "avg_logprob": -0.18757471048607016, "compression_ratio": 1.737327188940092, "no_speech_prob": 0.00010927004041150212}, {"id": 77, "seek": 34000, "start": 345.0, "end": 350.0, "text": " in order to support that as well.", "tokens": [294, 1668, 281, 1406, 300, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.18757471048607016, "compression_ratio": 1.737327188940092, "no_speech_prob": 0.00010927004041150212}, {"id": 78, "seek": 34000, "start": 350.0, "end": 354.0, "text": " Okay, so at the high level, how it happens right now in Rasmussen,", "tokens": [1033, 11, 370, 412, 264, 1090, 1496, 11, 577, 309, 2314, 558, 586, 294, 497, 14774, 29202, 11], "temperature": 0.0, "avg_logprob": -0.18757471048607016, "compression_ratio": 1.737327188940092, "no_speech_prob": 0.00010927004041150212}, {"id": 79, "seek": 34000, "start": 354.0, "end": 356.0, "text": " is that we parsed the random bytes,", "tokens": [307, 300, 321, 21156, 292, 264, 4974, 36088, 11], "temperature": 0.0, "avg_logprob": -0.18757471048607016, "compression_ratio": 1.737327188940092, "no_speech_prob": 0.00010927004041150212}, {"id": 80, "seek": 34000, "start": 356.0, "end": 361.0, "text": " we initialized the mock driver with the data that was parsed by fuzzer.", "tokens": [321, 5883, 1602, 264, 17362, 6787, 365, 264, 1412, 300, 390, 21156, 292, 538, 283, 3334, 4527, 13], "temperature": 0.0, "avg_logprob": -0.18757471048607016, "compression_ratio": 1.737327188940092, "no_speech_prob": 0.00010927004041150212}, {"id": 81, "seek": 34000, "start": 361.0, "end": 364.0, "text": " At the high level, it ends up with some descriptors", "tokens": [1711, 264, 1090, 1496, 11, 309, 5314, 493, 365, 512, 31280, 830], "temperature": 0.0, "avg_logprob": -0.18757471048607016, "compression_ratio": 1.737327188940092, "no_speech_prob": 0.00010927004041150212}, {"id": 82, "seek": 34000, "start": 364.0, "end": 367.0, "text": " and some key functions that have some random input", "tokens": [293, 512, 2141, 6828, 300, 362, 512, 4974, 4846], "temperature": 0.0, "avg_logprob": -0.18757471048607016, "compression_ratio": 1.737327188940092, "no_speech_prob": 0.00010927004041150212}, {"id": 83, "seek": 36700, "start": 367.0, "end": 370.0, "text": " that they need to process,", "tokens": [300, 436, 643, 281, 1399, 11], "temperature": 0.0, "avg_logprob": -0.19214079115125868, "compression_ratio": 1.60752688172043, "no_speech_prob": 0.0003714606864377856}, {"id": 84, "seek": 36700, "start": 370.0, "end": 373.0, "text": " and then we create the queue,", "tokens": [293, 550, 321, 1884, 264, 18639, 11], "temperature": 0.0, "avg_logprob": -0.19214079115125868, "compression_ratio": 1.60752688172043, "no_speech_prob": 0.0003714606864377856}, {"id": 85, "seek": 36700, "start": 373.0, "end": 378.0, "text": " and we call these random functions with random input.", "tokens": [293, 321, 818, 613, 4974, 6828, 365, 4974, 4846, 13], "temperature": 0.0, "avg_logprob": -0.19214079115125868, "compression_ratio": 1.60752688172043, "no_speech_prob": 0.0003714606864377856}, {"id": 86, "seek": 36700, "start": 378.0, "end": 386.0, "text": " And yeah, the second before is that if you are trying to do fuzzing", "tokens": [400, 1338, 11, 264, 1150, 949, 307, 300, 498, 291, 366, 1382, 281, 360, 283, 3334, 8781], "temperature": 0.0, "avg_logprob": -0.19214079115125868, "compression_ratio": 1.60752688172043, "no_speech_prob": 0.0003714606864377856}, {"id": 87, "seek": 36700, "start": 386.0, "end": 389.0, "text": " and you just start when the project is already mature,", "tokens": [293, 291, 445, 722, 562, 264, 1716, 307, 1217, 14442, 11], "temperature": 0.0, "avg_logprob": -0.19214079115125868, "compression_ratio": 1.60752688172043, "no_speech_prob": 0.0003714606864377856}, {"id": 88, "seek": 36700, "start": 389.0, "end": 394.0, "text": " what is going to happen is that it's going to be a bit difficult,", "tokens": [437, 307, 516, 281, 1051, 307, 300, 309, 311, 516, 281, 312, 257, 857, 2252, 11], "temperature": 0.0, "avg_logprob": -0.19214079115125868, "compression_ratio": 1.60752688172043, "no_speech_prob": 0.0003714606864377856}, {"id": 89, "seek": 39400, "start": 394.0, "end": 398.0, "text": " you might find it very complicated to retrofit it.", "tokens": [291, 1062, 915, 309, 588, 6179, 281, 18820, 6845, 309, 13], "temperature": 0.0, "avg_logprob": -0.11579930305480957, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0001772870891727507}, {"id": 90, "seek": 39400, "start": 398.0, "end": 403.0, "text": " So instead, I know that it's not necessarily viable to start fuzzing", "tokens": [407, 2602, 11, 286, 458, 300, 309, 311, 406, 4725, 22024, 281, 722, 283, 3334, 8781], "temperature": 0.0, "avg_logprob": -0.11579930305480957, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0001772870891727507}, {"id": 91, "seek": 39400, "start": 403.0, "end": 405.0, "text": " when you start the project,", "tokens": [562, 291, 722, 264, 1716, 11], "temperature": 0.0, "avg_logprob": -0.11579930305480957, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0001772870891727507}, {"id": 92, "seek": 39400, "start": 405.0, "end": 408.0, "text": " but what you can do instead is that you can keep fuzzing", "tokens": [457, 437, 291, 393, 360, 2602, 307, 300, 291, 393, 1066, 283, 3334, 8781], "temperature": 0.0, "avg_logprob": -0.11579930305480957, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0001772870891727507}, {"id": 93, "seek": 39400, "start": 408.0, "end": 410.0, "text": " in the back of your head,", "tokens": [294, 264, 646, 295, 428, 1378, 11], "temperature": 0.0, "avg_logprob": -0.11579930305480957, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0001772870891727507}, {"id": 94, "seek": 39400, "start": 410.0, "end": 415.0, "text": " and then when you create some mock objects or some unit tests,", "tokens": [293, 550, 562, 291, 1884, 512, 17362, 6565, 420, 512, 4985, 6921, 11], "temperature": 0.0, "avg_logprob": -0.11579930305480957, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0001772870891727507}, {"id": 95, "seek": 39400, "start": 415.0, "end": 423.0, "text": " you can think about how you can actually reuse them in fuzzing as well.", "tokens": [291, 393, 519, 466, 577, 291, 393, 767, 26225, 552, 294, 283, 3334, 8781, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.11579930305480957, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0001772870891727507}, {"id": 96, "seek": 42300, "start": 423.0, "end": 427.0, "text": " Which is what we did but not very well.", "tokens": [3013, 307, 437, 321, 630, 457, 406, 588, 731, 13], "temperature": 0.0, "avg_logprob": -0.16518304703083445, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.00017055930220521986}, {"id": 97, "seek": 42300, "start": 427.0, "end": 430.0, "text": " So one of the crashes that we actually found", "tokens": [407, 472, 295, 264, 28642, 300, 321, 767, 1352], "temperature": 0.0, "avg_logprob": -0.16518304703083445, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.00017055930220521986}, {"id": 98, "seek": 42300, "start": 430.0, "end": 434.0, "text": " was that the mock driver was crashing on invalid input.", "tokens": [390, 300, 264, 17362, 6787, 390, 26900, 322, 34702, 4846, 13], "temperature": 0.0, "avg_logprob": -0.16518304703083445, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.00017055930220521986}, {"id": 99, "seek": 42300, "start": 434.0, "end": 439.0, "text": " So we had to adapt these actually to return errors,", "tokens": [407, 321, 632, 281, 6231, 613, 767, 281, 2736, 13603, 11], "temperature": 0.0, "avg_logprob": -0.16518304703083445, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.00017055930220521986}, {"id": 100, "seek": 42300, "start": 439.0, "end": 441.0, "text": " even though it was just one test,", "tokens": [754, 1673, 309, 390, 445, 472, 1500, 11], "temperature": 0.0, "avg_logprob": -0.16518304703083445, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.00017055930220521986}, {"id": 101, "seek": 42300, "start": 441.0, "end": 443.0, "text": " we couldn't just crash on invalid input anymore.", "tokens": [321, 2809, 380, 445, 8252, 322, 34702, 4846, 3602, 13], "temperature": 0.0, "avg_logprob": -0.16518304703083445, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.00017055930220521986}, {"id": 102, "seek": 42300, "start": 443.0, "end": 448.0, "text": " So the idea is to return errors at the level where you want to do fuzzing,", "tokens": [407, 264, 1558, 307, 281, 2736, 13603, 412, 264, 1496, 689, 291, 528, 281, 360, 283, 3334, 8781, 11], "temperature": 0.0, "avg_logprob": -0.16518304703083445, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.00017055930220521986}, {"id": 103, "seek": 44800, "start": 448.0, "end": 458.0, "text": " that can be processed at higher levels and so the fuzzing can crash.", "tokens": [300, 393, 312, 18846, 412, 2946, 4358, 293, 370, 264, 283, 3334, 8781, 393, 8252, 13], "temperature": 0.0, "avg_logprob": -0.2547137795425043, "compression_ratio": 1.6758241758241759, "no_speech_prob": 6.256059714360163e-05}, {"id": 104, "seek": 44800, "start": 458.0, "end": 463.0, "text": " And now for structural interfuzzing.", "tokens": [400, 586, 337, 15067, 14510, 3334, 8781, 13], "temperature": 0.0, "avg_logprob": -0.2547137795425043, "compression_ratio": 1.6758241758241759, "no_speech_prob": 6.256059714360163e-05}, {"id": 105, "seek": 44800, "start": 463.0, "end": 465.0, "text": " So with those structural interfuzzing,", "tokens": [407, 365, 729, 15067, 14510, 3334, 8781, 11], "temperature": 0.0, "avg_logprob": -0.2547137795425043, "compression_ratio": 1.6758241758241759, "no_speech_prob": 6.256059714360163e-05}, {"id": 106, "seek": 44800, "start": 465.0, "end": 468.0, "text": " how it works is that the fuzzer is going to generate some random bytes,", "tokens": [577, 309, 1985, 307, 300, 264, 283, 3334, 4527, 307, 516, 281, 8460, 512, 4974, 36088, 11], "temperature": 0.0, "avg_logprob": -0.2547137795425043, "compression_ratio": 1.6758241758241759, "no_speech_prob": 6.256059714360163e-05}, {"id": 107, "seek": 44800, "start": 468.0, "end": 473.0, "text": " and then you have to interpret these as the bytes that you have to use", "tokens": [293, 550, 291, 362, 281, 7302, 613, 382, 264, 36088, 300, 291, 362, 281, 764], "temperature": 0.0, "avg_logprob": -0.2547137795425043, "compression_ratio": 1.6758241758241759, "no_speech_prob": 6.256059714360163e-05}, {"id": 108, "seek": 44800, "start": 473.0, "end": 475.0, "text": " for your library.", "tokens": [337, 428, 6405, 13], "temperature": 0.0, "avg_logprob": -0.2547137795425043, "compression_ratio": 1.6758241758241759, "no_speech_prob": 6.256059714360163e-05}, {"id": 109, "seek": 47500, "start": 475.0, "end": 480.0, "text": " So with structural interfuzzing, it's really nice because there are some tools", "tokens": [407, 365, 15067, 14510, 3334, 8781, 11, 309, 311, 534, 1481, 570, 456, 366, 512, 3873], "temperature": 0.0, "avg_logprob": -0.26788080003526477, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.0001454767188988626}, {"id": 110, "seek": 47500, "start": 480.0, "end": 485.0, "text": " that are just going to basically interpret the random bytes as a structure", "tokens": [300, 366, 445, 516, 281, 1936, 7302, 264, 4974, 36088, 382, 257, 3877], "temperature": 0.0, "avg_logprob": -0.26788080003526477, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.0001454767188988626}, {"id": 111, "seek": 47500, "start": 485.0, "end": 486.0, "text": " that you actually need.", "tokens": [300, 291, 767, 643, 13], "temperature": 0.0, "avg_logprob": -0.26788080003526477, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.0001454767188988626}, {"id": 112, "seek": 47500, "start": 486.0, "end": 491.0, "text": " So it's super nice what it does is that it significantly reduces the code", "tokens": [407, 309, 311, 1687, 1481, 437, 309, 775, 307, 300, 309, 10591, 18081, 264, 3089], "temperature": 0.0, "avg_logprob": -0.26788080003526477, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.0001454767188988626}, {"id": 113, "seek": 47500, "start": 491.0, "end": 493.0, "text": " that you need to write,", "tokens": [300, 291, 643, 281, 2464, 11], "temperature": 0.0, "avg_logprob": -0.26788080003526477, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.0001454767188988626}, {"id": 114, "seek": 47500, "start": 493.0, "end": 498.0, "text": " and even the rest of the information is arbitrary.", "tokens": [293, 754, 264, 1472, 295, 264, 1589, 307, 23211, 13], "temperature": 0.0, "avg_logprob": -0.26788080003526477, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.0001454767188988626}, {"id": 115, "seek": 47500, "start": 498.0, "end": 502.0, "text": " Now, we had to change it unfortunately,", "tokens": [823, 11, 321, 632, 281, 1319, 309, 7015, 11], "temperature": 0.0, "avg_logprob": -0.26788080003526477, "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.0001454767188988626}, {"id": 116, "seek": 50200, "start": 502.0, "end": 505.0, "text": " before we knew that we had only 270 lines of code,", "tokens": [949, 321, 2586, 300, 321, 632, 787, 40774, 3876, 295, 3089, 11], "temperature": 0.0, "avg_logprob": -0.23945877075195313, "compression_ratio": 1.592920353982301, "no_speech_prob": 7.942367665236816e-05}, {"id": 117, "seek": 50200, "start": 505.0, "end": 510.0, "text": " and now we have around 740 lines of code for the fuzzer.", "tokens": [293, 586, 321, 362, 926, 1614, 5254, 3876, 295, 3089, 337, 264, 283, 3334, 4527, 13], "temperature": 0.0, "avg_logprob": -0.23945877075195313, "compression_ratio": 1.592920353982301, "no_speech_prob": 7.942367665236816e-05}, {"id": 118, "seek": 50200, "start": 510.0, "end": 514.0, "text": " And unfortunately, it came with some problems,", "tokens": [400, 7015, 11, 309, 1361, 365, 512, 2740, 11], "temperature": 0.0, "avg_logprob": -0.23945877075195313, "compression_ratio": 1.592920353982301, "no_speech_prob": 7.942367665236816e-05}, {"id": 119, "seek": 50200, "start": 514.0, "end": 518.0, "text": " so that's why we have to actually basically...", "tokens": [370, 300, 311, 983, 321, 362, 281, 767, 1936, 485], "temperature": 0.0, "avg_logprob": -0.23945877075195313, "compression_ratio": 1.592920353982301, "no_speech_prob": 7.942367665236816e-05}, {"id": 120, "seek": 50200, "start": 518.0, "end": 522.0, "text": " The most important part is that it's not really produceable.", "tokens": [440, 881, 1021, 644, 307, 300, 309, 311, 406, 534, 5258, 712, 13], "temperature": 0.0, "avg_logprob": -0.23945877075195313, "compression_ratio": 1.592920353982301, "no_speech_prob": 7.942367665236816e-05}, {"id": 121, "seek": 50200, "start": 522.0, "end": 526.0, "text": " So you can't really use the corpus that you had in previous runs,", "tokens": [407, 291, 393, 380, 534, 764, 264, 1181, 31624, 300, 291, 632, 294, 3894, 6676, 11], "temperature": 0.0, "avg_logprob": -0.23945877075195313, "compression_ratio": 1.592920353982301, "no_speech_prob": 7.942367665236816e-05}, {"id": 122, "seek": 50200, "start": 526.0, "end": 528.0, "text": " which was a big problem for us,", "tokens": [597, 390, 257, 955, 1154, 337, 505, 11], "temperature": 0.0, "avg_logprob": -0.23945877075195313, "compression_ratio": 1.592920353982301, "no_speech_prob": 7.942367665236816e-05}, {"id": 123, "seek": 52800, "start": 528.0, "end": 534.0, "text": " because basically what happens is that arbitrary is introducing some...", "tokens": [570, 1936, 437, 2314, 307, 300, 23211, 307, 15424, 512, 485], "temperature": 0.0, "avg_logprob": -0.3487392736940968, "compression_ratio": 1.5174825174825175, "no_speech_prob": 0.001027965103276074}, {"id": 124, "seek": 52800, "start": 534.0, "end": 540.0, "text": " some randomness in one terminal so that it goes right into the input.", "tokens": [512, 4974, 1287, 294, 472, 14709, 370, 300, 309, 1709, 558, 666, 264, 4846, 13], "temperature": 0.0, "avg_logprob": -0.3487392736940968, "compression_ratio": 1.5174825174825175, "no_speech_prob": 0.001027965103276074}, {"id": 125, "seek": 52800, "start": 540.0, "end": 551.0, "text": " And that basically means that you cannot use the corpus from previous runs.", "tokens": [400, 300, 1936, 1355, 300, 291, 2644, 764, 264, 1181, 31624, 490, 3894, 6676, 13], "temperature": 0.0, "avg_logprob": -0.3487392736940968, "compression_ratio": 1.5174825174825175, "no_speech_prob": 0.001027965103276074}, {"id": 126, "seek": 55100, "start": 551.0, "end": 559.0, "text": " The thing for here is that we would like that we can do incremental improvements", "tokens": [440, 551, 337, 510, 307, 300, 321, 576, 411, 300, 321, 393, 360, 35759, 13797], "temperature": 0.0, "avg_logprob": -0.3267215294174001, "compression_ratio": 1.6519337016574585, "no_speech_prob": 0.0003369241894688457}, {"id": 127, "seek": 55100, "start": 559.0, "end": 560.0, "text": " for the fuzzer,", "tokens": [337, 264, 283, 3334, 4527, 11], "temperature": 0.0, "avg_logprob": -0.3267215294174001, "compression_ratio": 1.6519337016574585, "no_speech_prob": 0.0003369241894688457}, {"id": 128, "seek": 55100, "start": 560.0, "end": 564.0, "text": " and we didn't check that what we want to increment", "tokens": [293, 321, 994, 380, 1520, 300, 437, 321, 528, 281, 26200], "temperature": 0.0, "avg_logprob": -0.3267215294174001, "compression_ratio": 1.6519337016574585, "no_speech_prob": 0.0003369241894688457}, {"id": 129, "seek": 55100, "start": 564.0, "end": 566.0, "text": " can actually be implemented through N.", "tokens": [393, 767, 312, 12270, 807, 426, 13], "temperature": 0.0, "avg_logprob": -0.3267215294174001, "compression_ratio": 1.6519337016574585, "no_speech_prob": 0.0003369241894688457}, {"id": 130, "seek": 55100, "start": 566.0, "end": 571.0, "text": " So, yeah, instead, a better point would be to make sure", "tokens": [407, 11, 1338, 11, 2602, 11, 257, 1101, 935, 576, 312, 281, 652, 988], "temperature": 0.0, "avg_logprob": -0.3267215294174001, "compression_ratio": 1.6519337016574585, "no_speech_prob": 0.0003369241894688457}, {"id": 131, "seek": 55100, "start": 571.0, "end": 578.0, "text": " that we can actually reuse the corpus that we generated.", "tokens": [300, 321, 393, 767, 26225, 264, 1181, 31624, 300, 321, 10833, 13], "temperature": 0.0, "avg_logprob": -0.3267215294174001, "compression_ratio": 1.6519337016574585, "no_speech_prob": 0.0003369241894688457}, {"id": 132, "seek": 57800, "start": 578.0, "end": 582.0, "text": " Okay, and now about when fuzzing actually fails.", "tokens": [1033, 11, 293, 586, 466, 562, 283, 3334, 8781, 767, 18199, 13], "temperature": 0.0, "avg_logprob": -0.22912445522490002, "compression_ratio": 1.6197916666666667, "no_speech_prob": 0.0002449607418384403}, {"id": 133, "seek": 57800, "start": 582.0, "end": 586.0, "text": " So, we had a PR in U.S. Human,", "tokens": [407, 11, 321, 632, 257, 11568, 294, 624, 13, 50, 13, 10294, 11], "temperature": 0.0, "avg_logprob": -0.22912445522490002, "compression_ratio": 1.6197916666666667, "no_speech_prob": 0.0002449607418384403}, {"id": 134, "seek": 57800, "start": 586.0, "end": 590.0, "text": " at this point we were already running fuzzing for cool requests,", "tokens": [412, 341, 935, 321, 645, 1217, 2614, 283, 3334, 8781, 337, 1627, 12475, 11], "temperature": 0.0, "avg_logprob": -0.22912445522490002, "compression_ratio": 1.6197916666666667, "no_speech_prob": 0.0002449607418384403}, {"id": 135, "seek": 57800, "start": 590.0, "end": 597.0, "text": " and there was a PR that was introducing actually an overflow.", "tokens": [293, 456, 390, 257, 11568, 300, 390, 15424, 767, 364, 37772, 13], "temperature": 0.0, "avg_logprob": -0.22912445522490002, "compression_ratio": 1.6197916666666667, "no_speech_prob": 0.0002449607418384403}, {"id": 136, "seek": 57800, "start": 597.0, "end": 603.0, "text": " So here the overflow is that the packet header size addition to the packet length", "tokens": [407, 510, 264, 37772, 307, 300, 264, 20300, 23117, 2744, 4500, 281, 264, 20300, 4641], "temperature": 0.0, "avg_logprob": -0.22912445522490002, "compression_ratio": 1.6197916666666667, "no_speech_prob": 0.0002449607418384403}, {"id": 137, "seek": 57800, "start": 603.0, "end": 605.0, "text": " can actually overflow,", "tokens": [393, 767, 37772, 11], "temperature": 0.0, "avg_logprob": -0.22912445522490002, "compression_ratio": 1.6197916666666667, "no_speech_prob": 0.0002449607418384403}, {"id": 138, "seek": 60500, "start": 605.0, "end": 610.0, "text": " because the packet length is set up by the joint.", "tokens": [570, 264, 20300, 4641, 307, 992, 493, 538, 264, 7225, 13], "temperature": 0.0, "avg_logprob": -0.20516267189612755, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0002090116759063676}, {"id": 139, "seek": 60500, "start": 610.0, "end": 614.0, "text": " This bug, well, I actually found it during code review,", "tokens": [639, 7426, 11, 731, 11, 286, 767, 1352, 309, 1830, 3089, 3131, 11], "temperature": 0.0, "avg_logprob": -0.20516267189612755, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0002090116759063676}, {"id": 140, "seek": 60500, "start": 614.0, "end": 619.0, "text": " so it was a bit unexpected because I was hoping that the fuzzer was going to find it,", "tokens": [370, 309, 390, 257, 857, 13106, 570, 286, 390, 7159, 300, 264, 283, 3334, 4527, 390, 516, 281, 915, 309, 11], "temperature": 0.0, "avg_logprob": -0.20516267189612755, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0002090116759063676}, {"id": 141, "seek": 60500, "start": 619.0, "end": 621.0, "text": " which was not the case.", "tokens": [597, 390, 406, 264, 1389, 13], "temperature": 0.0, "avg_logprob": -0.20516267189612755, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0002090116759063676}, {"id": 142, "seek": 60500, "start": 621.0, "end": 623.0, "text": " So after some dive deep,", "tokens": [407, 934, 512, 9192, 2452, 11], "temperature": 0.0, "avg_logprob": -0.20516267189612755, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0002090116759063676}, {"id": 143, "seek": 60500, "start": 623.0, "end": 629.0, "text": " I realized that running fuzzing for just 15 minutes might not actually be enough,", "tokens": [286, 5334, 300, 2614, 283, 3334, 8781, 337, 445, 2119, 2077, 1062, 406, 767, 312, 1547, 11], "temperature": 0.0, "avg_logprob": -0.20516267189612755, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0002090116759063676}, {"id": 144, "seek": 60500, "start": 629.0, "end": 634.0, "text": " because this bug was triggered with fuzzing about 40 minutes instead.", "tokens": [570, 341, 7426, 390, 21710, 365, 283, 3334, 8781, 466, 3356, 2077, 2602, 13], "temperature": 0.0, "avg_logprob": -0.20516267189612755, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0002090116759063676}, {"id": 145, "seek": 63400, "start": 634.0, "end": 639.0, "text": " So, how we fixed that is that we added a fuzzing session that is optional", "tokens": [407, 11, 577, 321, 6806, 300, 307, 300, 321, 3869, 257, 283, 3334, 8781, 5481, 300, 307, 17312], "temperature": 0.0, "avg_logprob": -0.20181816948784723, "compression_ratio": 1.5804878048780489, "no_speech_prob": 0.00030347908614203334}, {"id": 146, "seek": 63400, "start": 639.0, "end": 641.0, "text": " and that fuzzed for 24 hours.", "tokens": [293, 300, 283, 3334, 11312, 337, 4022, 2496, 13], "temperature": 0.0, "avg_logprob": -0.20181816948784723, "compression_ratio": 1.5804878048780489, "no_speech_prob": 0.00030347908614203334}, {"id": 147, "seek": 63400, "start": 641.0, "end": 646.0, "text": " This one is to be started manually by the U.S. Human Maintainers,", "tokens": [639, 472, 307, 281, 312, 1409, 16945, 538, 264, 624, 13, 50, 13, 10294, 376, 5114, 491, 433, 11], "temperature": 0.0, "avg_logprob": -0.20181816948784723, "compression_ratio": 1.5804878048780489, "no_speech_prob": 0.00030347908614203334}, {"id": 148, "seek": 63400, "start": 646.0, "end": 650.0, "text": " and should only be started when there are cool requests", "tokens": [293, 820, 787, 312, 1409, 562, 456, 366, 1627, 12475], "temperature": 0.0, "avg_logprob": -0.20181816948784723, "compression_ratio": 1.5804878048780489, "no_speech_prob": 0.00030347908614203334}, {"id": 149, "seek": 63400, "start": 650.0, "end": 654.0, "text": " that actually impact the fuzzing relation.", "tokens": [300, 767, 2712, 264, 283, 3334, 8781, 9721, 13], "temperature": 0.0, "avg_logprob": -0.20181816948784723, "compression_ratio": 1.5804878048780489, "no_speech_prob": 0.00030347908614203334}, {"id": 150, "seek": 63400, "start": 654.0, "end": 659.0, "text": " This is because we're also consuming a lot of resources", "tokens": [639, 307, 570, 321, 434, 611, 19867, 257, 688, 295, 3593], "temperature": 0.0, "avg_logprob": -0.20181816948784723, "compression_ratio": 1.5804878048780489, "no_speech_prob": 0.00030347908614203334}, {"id": 151, "seek": 65900, "start": 659.0, "end": 665.0, "text": " when doing fuzzing, and also you don't want to block all the cool requests for 24 hours.", "tokens": [562, 884, 283, 3334, 8781, 11, 293, 611, 291, 500, 380, 528, 281, 3461, 439, 264, 1627, 12475, 337, 4022, 2496, 13], "temperature": 0.0, "avg_logprob": -0.2454200707949125, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.0008639196748845279}, {"id": 152, "seek": 65900, "start": 665.0, "end": 670.0, "text": " So typically the last one on the slide is a page that quickly needs to be executed,", "tokens": [407, 5850, 264, 1036, 472, 322, 264, 4137, 307, 257, 3028, 300, 2661, 2203, 281, 312, 17577, 11], "temperature": 0.0, "avg_logprob": -0.2454200707949125, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.0008639196748845279}, {"id": 153, "seek": 65900, "start": 670.0, "end": 675.0, "text": " so blocking it for one day might not be reasonable for all the cool requests.", "tokens": [370, 17776, 309, 337, 472, 786, 1062, 406, 312, 10585, 337, 439, 264, 1627, 12475, 13], "temperature": 0.0, "avg_logprob": -0.2454200707949125, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.0008639196748845279}, {"id": 154, "seek": 65900, "start": 675.0, "end": 680.0, "text": " So the people here was not trying to fuzze for long enough,", "tokens": [407, 264, 561, 510, 390, 406, 1382, 281, 283, 3334, 1381, 337, 938, 1547, 11], "temperature": 0.0, "avg_logprob": -0.2454200707949125, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.0008639196748845279}, {"id": 155, "seek": 65900, "start": 680.0, "end": 687.0, "text": " and, yeah, instead we had to work our way to find a way to not block cool requests,", "tokens": [293, 11, 1338, 11, 2602, 321, 632, 281, 589, 527, 636, 281, 915, 257, 636, 281, 406, 3461, 1627, 12475, 11], "temperature": 0.0, "avg_logprob": -0.2454200707949125, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.0008639196748845279}, {"id": 156, "seek": 68700, "start": 687.0, "end": 691.0, "text": " but at the same time to provide a way to fuzze.", "tokens": [457, 412, 264, 912, 565, 281, 2893, 257, 636, 281, 283, 3334, 1381, 13], "temperature": 0.0, "avg_logprob": -0.2953479808309804, "compression_ratio": 1.6076555023923444, "no_speech_prob": 0.00024217800819315016}, {"id": 157, "seek": 68700, "start": 691.0, "end": 694.0, "text": " Fourth coverage for rust.", "tokens": [23773, 9645, 337, 15259, 13], "temperature": 0.0, "avg_logprob": -0.2953479808309804, "compression_ratio": 1.6076555023923444, "no_speech_prob": 0.00024217800819315016}, {"id": 158, "seek": 68700, "start": 694.0, "end": 699.0, "text": " So in rust you can actually get coverage information by running LLVM.", "tokens": [407, 294, 15259, 291, 393, 767, 483, 9645, 1589, 538, 2614, 441, 43, 53, 44, 13], "temperature": 0.0, "avg_logprob": -0.2953479808309804, "compression_ratio": 1.6076555023923444, "no_speech_prob": 0.00024217800819315016}, {"id": 159, "seek": 68700, "start": 699.0, "end": 704.0, "text": " In rust you only get light coverage.", "tokens": [682, 15259, 291, 787, 483, 1442, 9645, 13], "temperature": 0.0, "avg_logprob": -0.2953479808309804, "compression_ratio": 1.6076555023923444, "no_speech_prob": 0.00024217800819315016}, {"id": 160, "seek": 68700, "start": 704.0, "end": 708.0, "text": " So basically this was the starting point of the presentation.", "tokens": [407, 1936, 341, 390, 264, 2891, 935, 295, 264, 5860, 13], "temperature": 0.0, "avg_logprob": -0.2953479808309804, "compression_ratio": 1.6076555023923444, "no_speech_prob": 0.00024217800819315016}, {"id": 161, "seek": 68700, "start": 708.0, "end": 714.0, "text": " I was thinking I would come here and I'm going to show you how great it is to run fuzzing for", "tokens": [286, 390, 1953, 286, 576, 808, 510, 293, 286, 478, 516, 281, 855, 291, 577, 869, 309, 307, 281, 1190, 283, 3334, 8781, 337], "temperature": 0.0, "avg_logprob": -0.2953479808309804, "compression_ratio": 1.6076555023923444, "no_speech_prob": 0.00024217800819315016}, {"id": 162, "seek": 71400, "start": 714.0, "end": 720.0, "text": " 15 minutes and then more minutes and then the coverage and all these really extravagant things.", "tokens": [2119, 2077, 293, 550, 544, 2077, 293, 550, 264, 9645, 293, 439, 613, 534, 2857, 42586, 394, 721, 13], "temperature": 0.0, "avg_logprob": -0.24556598466696197, "compression_ratio": 1.7242990654205608, "no_speech_prob": 0.0002756101603154093}, {"id": 163, "seek": 71400, "start": 720.0, "end": 726.0, "text": " And so we ended up with fuzzing for 15 minutes generating 10 for these regions", "tokens": [400, 370, 321, 4590, 493, 365, 283, 3334, 8781, 337, 2119, 2077, 17746, 1266, 337, 613, 10682], "temperature": 0.0, "avg_logprob": -0.24556598466696197, "compression_ratio": 1.7242990654205608, "no_speech_prob": 0.0002756101603154093}, {"id": 164, "seek": 71400, "start": 726.0, "end": 730.0, "text": " and the coverage of around 82%.", "tokens": [293, 264, 9645, 295, 926, 29097, 6856], "temperature": 0.0, "avg_logprob": -0.24556598466696197, "compression_ratio": 1.7242990654205608, "no_speech_prob": 0.0002756101603154093}, {"id": 165, "seek": 71400, "start": 730.0, "end": 734.0, "text": " So it's like, well, it's okay, that's good.", "tokens": [407, 309, 311, 411, 11, 731, 11, 309, 311, 1392, 11, 300, 311, 665, 13], "temperature": 0.0, "avg_logprob": -0.24556598466696197, "compression_ratio": 1.7242990654205608, "no_speech_prob": 0.0002756101603154093}, {"id": 166, "seek": 71400, "start": 734.0, "end": 737.0, "text": " So then let's just run with some minimal coverage as well.", "tokens": [407, 550, 718, 311, 445, 1190, 365, 512, 13206, 9645, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.24556598466696197, "compression_ratio": 1.7242990654205608, "no_speech_prob": 0.0002756101603154093}, {"id": 167, "seek": 71400, "start": 737.0, "end": 740.0, "text": " So this is some coverage that we generated from unit tests.", "tokens": [407, 341, 307, 512, 9645, 300, 321, 10833, 490, 4985, 6921, 13], "temperature": 0.0, "avg_logprob": -0.24556598466696197, "compression_ratio": 1.7242990654205608, "no_speech_prob": 0.0002756101603154093}, {"id": 168, "seek": 74000, "start": 740.0, "end": 745.0, "text": " Let's just feed through the fuzzer and see how this changes.", "tokens": [961, 311, 445, 3154, 807, 264, 283, 3334, 4527, 293, 536, 577, 341, 2962, 13], "temperature": 0.0, "avg_logprob": -0.21318367907875463, "compression_ratio": 1.4567901234567902, "no_speech_prob": 0.00017354135343339294}, {"id": 169, "seek": 74000, "start": 745.0, "end": 747.0, "text": " There was no change, actually.", "tokens": [821, 390, 572, 1319, 11, 767, 13], "temperature": 0.0, "avg_logprob": -0.21318367907875463, "compression_ratio": 1.4567901234567902, "no_speech_prob": 0.00017354135343339294}, {"id": 170, "seek": 74000, "start": 747.0, "end": 750.0, "text": " So I was like, okay, it's not bad, not bad.", "tokens": [407, 286, 390, 411, 11, 1392, 11, 309, 311, 406, 1578, 11, 406, 1578, 13], "temperature": 0.0, "avg_logprob": -0.21318367907875463, "compression_ratio": 1.4567901234567902, "no_speech_prob": 0.00017354135343339294}, {"id": 171, "seek": 74000, "start": 750.0, "end": 752.0, "text": " Let's just run for two weeks.", "tokens": [961, 311, 445, 1190, 337, 732, 3259, 13], "temperature": 0.0, "avg_logprob": -0.21318367907875463, "compression_ratio": 1.4567901234567902, "no_speech_prob": 0.00017354135343339294}, {"id": 172, "seek": 74000, "start": 752.0, "end": 758.0, "text": " So what do you think this might happen now?", "tokens": [407, 437, 360, 291, 519, 341, 1062, 1051, 586, 30], "temperature": 0.0, "avg_logprob": -0.21318367907875463, "compression_ratio": 1.4567901234567902, "no_speech_prob": 0.00017354135343339294}, {"id": 173, "seek": 74000, "start": 758.0, "end": 764.0, "text": " So actually, it's working.", "tokens": [407, 767, 11, 309, 311, 1364, 13], "temperature": 0.0, "avg_logprob": -0.21318367907875463, "compression_ratio": 1.4567901234567902, "no_speech_prob": 0.00017354135343339294}, {"id": 174, "seek": 76400, "start": 764.0, "end": 771.0, "text": " At this point I was like, you press, I have to change my presentation, so it's not what I expected.", "tokens": [1711, 341, 935, 286, 390, 411, 11, 291, 1886, 11, 286, 362, 281, 1319, 452, 5860, 11, 370, 309, 311, 406, 437, 286, 5176, 13], "temperature": 0.0, "avg_logprob": -0.1742145452606544, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.702720904489979e-05}, {"id": 175, "seek": 76400, "start": 771.0, "end": 773.0, "text": " But instead I learned something, right?", "tokens": [583, 2602, 286, 3264, 746, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1742145452606544, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.702720904489979e-05}, {"id": 176, "seek": 76400, "start": 773.0, "end": 778.0, "text": " So you can't actually use coverage to decide where to stop fuzzing.", "tokens": [407, 291, 393, 380, 767, 764, 9645, 281, 4536, 689, 281, 1590, 283, 3334, 8781, 13], "temperature": 0.0, "avg_logprob": -0.1742145452606544, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.702720904489979e-05}, {"id": 177, "seek": 76400, "start": 778.0, "end": 784.0, "text": " So instead what you can do is that you can use coverage information to see when,", "tokens": [407, 2602, 437, 291, 393, 360, 307, 300, 291, 393, 764, 9645, 1589, 281, 536, 562, 11], "temperature": 0.0, "avg_logprob": -0.1742145452606544, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.702720904489979e-05}, {"id": 178, "seek": 76400, "start": 784.0, "end": 789.0, "text": " what the parts of your code are not actually covered.", "tokens": [437, 264, 3166, 295, 428, 3089, 366, 406, 767, 5343, 13], "temperature": 0.0, "avg_logprob": -0.1742145452606544, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.702720904489979e-05}, {"id": 179, "seek": 78900, "start": 789.0, "end": 794.0, "text": " And yeah, well, that's about it, actually.", "tokens": [400, 1338, 11, 731, 11, 300, 311, 466, 309, 11, 767, 13], "temperature": 0.0, "avg_logprob": -0.17950951136075532, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0001247694599442184}, {"id": 180, "seek": 78900, "start": 794.0, "end": 798.0, "text": " We see the summary of the people that we read into.", "tokens": [492, 536, 264, 12691, 295, 264, 561, 300, 321, 1401, 666, 13], "temperature": 0.0, "avg_logprob": -0.17950951136075532, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0001247694599442184}, {"id": 181, "seek": 78900, "start": 798.0, "end": 804.0, "text": " And I think now we have a lot of different questions.", "tokens": [400, 286, 519, 586, 321, 362, 257, 688, 295, 819, 1651, 13], "temperature": 0.0, "avg_logprob": -0.17950951136075532, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0001247694599442184}, {"id": 182, "seek": 78900, "start": 804.0, "end": 810.0, "text": " Did you look at how the fuzzer works and then what areas were not covered", "tokens": [2589, 291, 574, 412, 577, 264, 283, 3334, 4527, 1985, 293, 550, 437, 3179, 645, 406, 5343], "temperature": 0.0, "avg_logprob": -0.17950951136075532, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0001247694599442184}, {"id": 183, "seek": 78900, "start": 810.0, "end": 813.0, "text": " and try to figure out why it wasn't found in those areas?", "tokens": [293, 853, 281, 2573, 484, 983, 309, 2067, 380, 1352, 294, 729, 3179, 30], "temperature": 0.0, "avg_logprob": -0.17950951136075532, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0001247694599442184}, {"id": 184, "seek": 78900, "start": 813.0, "end": 818.0, "text": " Yeah, so the question was if we looked at how the fuzzer works and what areas were not covered.", "tokens": [865, 11, 370, 264, 1168, 390, 498, 321, 2956, 412, 577, 264, 283, 3334, 4527, 1985, 293, 437, 3179, 645, 406, 5343, 13], "temperature": 0.0, "avg_logprob": -0.17950951136075532, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0001247694599442184}, {"id": 185, "seek": 81800, "start": 818.0, "end": 821.0, "text": " Yes, we did, and I have a slide for that.", "tokens": [1079, 11, 321, 630, 11, 293, 286, 362, 257, 4137, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.2332714380842916, "compression_ratio": 1.661764705882353, "no_speech_prob": 3.875161200994626e-05}, {"id": 186, "seek": 81800, "start": 821.0, "end": 824.0, "text": " Thanks for the question.", "tokens": [2561, 337, 264, 1168, 13], "temperature": 0.0, "avg_logprob": -0.2332714380842916, "compression_ratio": 1.661764705882353, "no_speech_prob": 3.875161200994626e-05}, {"id": 187, "seek": 81800, "start": 824.0, "end": 829.0, "text": " Okay, so actually, I have two slides for that.", "tokens": [1033, 11, 370, 767, 11, 286, 362, 732, 9788, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.2332714380842916, "compression_ratio": 1.661764705882353, "no_speech_prob": 3.875161200994626e-05}, {"id": 188, "seek": 81800, "start": 829.0, "end": 833.0, "text": " There were some functions that we were not calling on purpose.", "tokens": [821, 645, 512, 6828, 300, 321, 645, 406, 5141, 322, 4334, 13], "temperature": 0.0, "avg_logprob": -0.2332714380842916, "compression_ratio": 1.661764705882353, "no_speech_prob": 3.875161200994626e-05}, {"id": 189, "seek": 81800, "start": 833.0, "end": 838.0, "text": " So because on the virtual queue, for example, we have some functions", "tokens": [407, 570, 322, 264, 6374, 18639, 11, 337, 1365, 11, 321, 362, 512, 6828], "temperature": 0.0, "avg_logprob": -0.2332714380842916, "compression_ratio": 1.661764705882353, "no_speech_prob": 3.875161200994626e-05}, {"id": 190, "seek": 81800, "start": 838.0, "end": 843.0, "text": " that are just iterating over the script chain and then they're doing something with the data.", "tokens": [300, 366, 445, 17138, 990, 670, 264, 5755, 5021, 293, 550, 436, 434, 884, 746, 365, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.2332714380842916, "compression_ratio": 1.661764705882353, "no_speech_prob": 3.875161200994626e-05}, {"id": 191, "seek": 84300, "start": 843.0, "end": 848.0, "text": " And at the virtual queue level, you can't do something with the data.", "tokens": [400, 412, 264, 6374, 18639, 1496, 11, 291, 393, 380, 360, 746, 365, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1732536797384614, "compression_ratio": 1.684873949579832, "no_speech_prob": 6.249677971936762e-05}, {"id": 192, "seek": 84300, "start": 848.0, "end": 854.0, "text": " So it's like, okay, this needs to be fast at the high level, like at the device implementation level.", "tokens": [407, 309, 311, 411, 11, 1392, 11, 341, 2203, 281, 312, 2370, 412, 264, 1090, 1496, 11, 411, 412, 264, 4302, 11420, 1496, 13], "temperature": 0.0, "avg_logprob": -0.1732536797384614, "compression_ratio": 1.684873949579832, "no_speech_prob": 6.249677971936762e-05}, {"id": 193, "seek": 84300, "start": 854.0, "end": 858.0, "text": " So it's like, okay, we're not going to call these functions, which is a bit hilarious", "tokens": [407, 309, 311, 411, 11, 1392, 11, 321, 434, 406, 516, 281, 818, 613, 6828, 11, 597, 307, 257, 857, 19796], "temperature": 0.0, "avg_logprob": -0.1732536797384614, "compression_ratio": 1.684873949579832, "no_speech_prob": 6.249677971936762e-05}, {"id": 194, "seek": 84300, "start": 858.0, "end": 863.0, "text": " because that's where Proc Hypervisor actually found the timeout problem,", "tokens": [570, 300, 311, 689, 1705, 66, 29592, 16457, 767, 1352, 264, 565, 346, 1154, 11], "temperature": 0.0, "avg_logprob": -0.1732536797384614, "compression_ratio": 1.684873949579832, "no_speech_prob": 6.249677971936762e-05}, {"id": 195, "seek": 84300, "start": 863.0, "end": 867.0, "text": " which we were not able to reproduce with the virtual queue, but still.", "tokens": [597, 321, 645, 406, 1075, 281, 29501, 365, 264, 6374, 18639, 11, 457, 920, 13], "temperature": 0.0, "avg_logprob": -0.1732536797384614, "compression_ratio": 1.684873949579832, "no_speech_prob": 6.249677971936762e-05}, {"id": 196, "seek": 86700, "start": 867.0, "end": 873.0, "text": " And we actually did this one function that shouldn't be called during fuzzing.", "tokens": [400, 321, 767, 630, 341, 472, 2445, 300, 4659, 380, 312, 1219, 1830, 283, 3334, 8781, 13], "temperature": 0.0, "avg_logprob": -0.19725771511302276, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.00018799824465531856}, {"id": 197, "seek": 86700, "start": 873.0, "end": 879.0, "text": " And then I rerun the fuzzing and, yeah, it's a bit better, but it's still not great.", "tokens": [400, 550, 286, 43819, 409, 264, 283, 3334, 8781, 293, 11, 1338, 11, 309, 311, 257, 857, 1101, 11, 457, 309, 311, 920, 406, 869, 13], "temperature": 0.0, "avg_logprob": -0.19725771511302276, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.00018799824465531856}, {"id": 198, "seek": 86700, "start": 879.0, "end": 886.0, "text": " And then I looked into what, well, actually, you can't see very well.", "tokens": [400, 550, 286, 2956, 666, 437, 11, 731, 11, 767, 11, 291, 393, 380, 536, 588, 731, 13], "temperature": 0.0, "avg_logprob": -0.19725771511302276, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.00018799824465531856}, {"id": 199, "seek": 86700, "start": 886.0, "end": 888.0, "text": " That's unfortunate.", "tokens": [663, 311, 17843, 13], "temperature": 0.0, "avg_logprob": -0.19725771511302276, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.00018799824465531856}, {"id": 200, "seek": 86700, "start": 888.0, "end": 895.0, "text": " Yeah, so I looked into what actually is not covered and you're not seeing there, so you have to trust me.", "tokens": [865, 11, 370, 286, 2956, 666, 437, 767, 307, 406, 5343, 293, 291, 434, 406, 2577, 456, 11, 370, 291, 362, 281, 3361, 385, 13], "temperature": 0.0, "avg_logprob": -0.19725771511302276, "compression_ratio": 1.669767441860465, "no_speech_prob": 0.00018799824465531856}, {"id": 201, "seek": 89500, "start": 895.0, "end": 900.0, "text": " These are actually errors.", "tokens": [1981, 366, 767, 13603, 13], "temperature": 0.0, "avg_logprob": -0.2138207753499349, "compression_ratio": 1.7025316455696202, "no_speech_prob": 0.0002618255093693733}, {"id": 202, "seek": 89500, "start": 900.0, "end": 904.0, "text": " So the printing of errors to files.", "tokens": [407, 264, 14699, 295, 13603, 281, 7098, 13], "temperature": 0.0, "avg_logprob": -0.2138207753499349, "compression_ratio": 1.7025316455696202, "no_speech_prob": 0.0002618255093693733}, {"id": 203, "seek": 89500, "start": 904.0, "end": 912.0, "text": " So since in the fuzzing we're not actually initializing a logger, these things cannot be triggered by fuzzing.", "tokens": [407, 1670, 294, 264, 283, 3334, 8781, 321, 434, 406, 767, 5883, 3319, 257, 3565, 1321, 11, 613, 721, 2644, 312, 21710, 538, 283, 3334, 8781, 13], "temperature": 0.0, "avg_logprob": -0.2138207753499349, "compression_ratio": 1.7025316455696202, "no_speech_prob": 0.0002618255093693733}, {"id": 204, "seek": 89500, "start": 912.0, "end": 919.0, "text": " So there's lots of error in this printing to a file that's not happening through fuzzing.", "tokens": [407, 456, 311, 3195, 295, 6713, 294, 341, 14699, 281, 257, 3991, 300, 311, 406, 2737, 807, 283, 3334, 8781, 13], "temperature": 0.0, "avg_logprob": -0.2138207753499349, "compression_ratio": 1.7025316455696202, "no_speech_prob": 0.0002618255093693733}, {"id": 205, "seek": 89500, "start": 919.0, "end": 922.0, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2138207753499349, "compression_ratio": 1.7025316455696202, "no_speech_prob": 0.0002618255093693733}, {"id": 206, "seek": 92200, "start": 922.0, "end": 928.0, "text": " What's subsequently taken to actually make sure it covers everywhere which needs to be covered?", "tokens": [708, 311, 26514, 2726, 281, 767, 652, 988, 309, 10538, 5315, 597, 2203, 281, 312, 5343, 30], "temperature": 0.0, "avg_logprob": -0.3018926640146786, "compression_ratio": 1.8401826484018264, "no_speech_prob": 0.0017982340650632977}, {"id": 207, "seek": 92200, "start": 928.0, "end": 933.0, "text": " And so discovering certain areas which clearly aren't covered.", "tokens": [400, 370, 24773, 1629, 3179, 597, 4448, 3212, 380, 5343, 13], "temperature": 0.0, "avg_logprob": -0.3018926640146786, "compression_ratio": 1.8401826484018264, "no_speech_prob": 0.0017982340650632977}, {"id": 208, "seek": 92200, "start": 933.0, "end": 935.0, "text": " I didn't understand the question.", "tokens": [286, 994, 380, 1223, 264, 1168, 13], "temperature": 0.0, "avg_logprob": -0.3018926640146786, "compression_ratio": 1.8401826484018264, "no_speech_prob": 0.0017982340650632977}, {"id": 209, "seek": 92200, "start": 935.0, "end": 936.0, "text": " Which areas of view?", "tokens": [3013, 3179, 295, 1910, 30], "temperature": 0.0, "avg_logprob": -0.3018926640146786, "compression_ratio": 1.8401826484018264, "no_speech_prob": 0.0017982340650632977}, {"id": 210, "seek": 92200, "start": 936.0, "end": 944.0, "text": " Well, what's subsequently taken to make sure the areas which weren't covered in the fuzzing are going to be covered in the future?", "tokens": [1042, 11, 437, 311, 26514, 2726, 281, 652, 988, 264, 3179, 597, 4999, 380, 5343, 294, 264, 283, 3334, 8781, 366, 516, 281, 312, 5343, 294, 264, 2027, 30], "temperature": 0.0, "avg_logprob": -0.3018926640146786, "compression_ratio": 1.8401826484018264, "no_speech_prob": 0.0017982340650632977}, {"id": 211, "seek": 92200, "start": 944.0, "end": 951.0, "text": " Oh, okay, so the question was what measures are we taking?", "tokens": [876, 11, 1392, 11, 370, 264, 1168, 390, 437, 8000, 366, 321, 1940, 30], "temperature": 0.0, "avg_logprob": -0.3018926640146786, "compression_ratio": 1.8401826484018264, "no_speech_prob": 0.0017982340650632977}, {"id": 212, "seek": 95100, "start": 951.0, "end": 958.0, "text": " In order to make sure that all that was covered before is going to be covered in the next generations?", "tokens": [682, 1668, 281, 652, 988, 300, 439, 300, 390, 5343, 949, 307, 516, 281, 312, 5343, 294, 264, 958, 10593, 30], "temperature": 0.0, "avg_logprob": -0.18138627444996552, "compression_ratio": 1.4887640449438202, "no_speech_prob": 0.000465528282802552}, {"id": 213, "seek": 95100, "start": 958.0, "end": 960.0, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.18138627444996552, "compression_ratio": 1.4887640449438202, "no_speech_prob": 0.000465528282802552}, {"id": 214, "seek": 95100, "start": 960.0, "end": 963.0, "text": " None?", "tokens": [14492, 30], "temperature": 0.0, "avg_logprob": -0.18138627444996552, "compression_ratio": 1.4887640449438202, "no_speech_prob": 0.000465528282802552}, {"id": 215, "seek": 95100, "start": 963.0, "end": 965.0, "text": " So right now we're not doing anything.", "tokens": [407, 558, 586, 321, 434, 406, 884, 1340, 13], "temperature": 0.0, "avg_logprob": -0.18138627444996552, "compression_ratio": 1.4887640449438202, "no_speech_prob": 0.000465528282802552}, {"id": 216, "seek": 95100, "start": 965.0, "end": 972.0, "text": " This whole coverage thing is just something that I need for the presentation and it's not automatic in any way.", "tokens": [639, 1379, 9645, 551, 307, 445, 746, 300, 286, 643, 337, 264, 5860, 293, 309, 311, 406, 12509, 294, 604, 636, 13], "temperature": 0.0, "avg_logprob": -0.18138627444996552, "compression_ratio": 1.4887640449438202, "no_speech_prob": 0.000465528282802552}, {"id": 217, "seek": 97200, "start": 972.0, "end": 987.0, "text": " But this is actually a good point for future investment to make sure that we're covering code because what we help with as well is that we make sure that new functions that we are adding to the code are also covered.", "tokens": [583, 341, 307, 767, 257, 665, 935, 337, 2027, 6078, 281, 652, 988, 300, 321, 434, 10322, 3089, 570, 437, 321, 854, 365, 382, 731, 307, 300, 321, 652, 988, 300, 777, 6828, 300, 321, 366, 5127, 281, 264, 3089, 366, 611, 5343, 13], "temperature": 0.0, "avg_logprob": -0.27410925651083184, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.0017472546314820647}, {"id": 218, "seek": 97200, "start": 987.0, "end": 993.0, "text": " So it's a great point to make that way.", "tokens": [407, 309, 311, 257, 869, 935, 281, 652, 300, 636, 13], "temperature": 0.0, "avg_logprob": -0.27410925651083184, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.0017472546314820647}, {"id": 219, "seek": 97200, "start": 993.0, "end": 995.0, "text": " We're talking about the structure of our fuzzing.", "tokens": [492, 434, 1417, 466, 264, 3877, 295, 527, 283, 3334, 8781, 13], "temperature": 0.0, "avg_logprob": -0.27410925651083184, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.0017472546314820647}, {"id": 220, "seek": 97200, "start": 995.0, "end": 996.0, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.27410925651083184, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.0017472546314820647}, {"id": 221, "seek": 97200, "start": 996.0, "end": 1001.0, "text": " And you mentioned that we cannot reuse the code to explain a bit more about that.", "tokens": [400, 291, 2835, 300, 321, 2644, 26225, 264, 3089, 281, 2903, 257, 857, 544, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.27410925651083184, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.0017472546314820647}, {"id": 222, "seek": 100100, "start": 1001.0, "end": 1007.0, "text": " Okay, so the question was how structured are we fuzzing and just that we cannot reuse the code.", "tokens": [1033, 11, 370, 264, 1168, 390, 577, 18519, 366, 321, 283, 3334, 8781, 293, 445, 300, 321, 2644, 26225, 264, 3089, 13], "temperature": 0.0, "avg_logprob": -0.30676584243774413, "compression_ratio": 1.5815217391304348, "no_speech_prob": 0.0004111311864107847}, {"id": 223, "seek": 100100, "start": 1007.0, "end": 1016.0, "text": " Let me see if I actually have a question here.", "tokens": [961, 385, 536, 498, 286, 767, 362, 257, 1168, 510, 13], "temperature": 0.0, "avg_logprob": -0.30676584243774413, "compression_ratio": 1.5815217391304348, "no_speech_prob": 0.0004111311864107847}, {"id": 224, "seek": 100100, "start": 1016.0, "end": 1017.0, "text": " No.", "tokens": [883, 13], "temperature": 0.0, "avg_logprob": -0.30676584243774413, "compression_ratio": 1.5815217391304348, "no_speech_prob": 0.0004111311864107847}, {"id": 225, "seek": 100100, "start": 1017.0, "end": 1028.0, "text": " Okay, so the idea is that what we were using, which is arbitrary, when it was taking the uniformed fuzzer was also adding some randomness to it.", "tokens": [1033, 11, 370, 264, 1558, 307, 300, 437, 321, 645, 1228, 11, 597, 307, 23211, 11, 562, 309, 390, 1940, 264, 9452, 292, 283, 3334, 4527, 390, 611, 5127, 512, 4974, 1287, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.30676584243774413, "compression_ratio": 1.5815217391304348, "no_speech_prob": 0.0004111311864107847}, {"id": 226, "seek": 102800, "start": 1028.0, "end": 1036.0, "text": " So because it was random, basically, every time it was writing the purpose to the file, it was introducing some randomness to it.", "tokens": [407, 570, 309, 390, 4974, 11, 1936, 11, 633, 565, 309, 390, 3579, 264, 4334, 281, 264, 3991, 11, 309, 390, 15424, 512, 4974, 1287, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.2689391728994009, "compression_ratio": 1.771604938271605, "no_speech_prob": 0.00040830502985045314}, {"id": 227, "seek": 102800, "start": 1036.0, "end": 1044.0, "text": " So when the same people get to read again, then it would not have been the same.", "tokens": [407, 562, 264, 912, 561, 483, 281, 1401, 797, 11, 550, 309, 576, 406, 362, 668, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.2689391728994009, "compression_ratio": 1.771604938271605, "no_speech_prob": 0.00040830502985045314}, {"id": 228, "seek": 102800, "start": 1044.0, "end": 1046.0, "text": " So where does the randomness come from?", "tokens": [407, 689, 775, 264, 4974, 1287, 808, 490, 30], "temperature": 0.0, "avg_logprob": -0.2689391728994009, "compression_ratio": 1.771604938271605, "no_speech_prob": 0.00040830502985045314}, {"id": 229, "seek": 102800, "start": 1046.0, "end": 1048.0, "text": " Where does the randomness come from?", "tokens": [2305, 775, 264, 4974, 1287, 808, 490, 30], "temperature": 0.0, "avg_logprob": -0.2689391728994009, "compression_ratio": 1.771604938271605, "no_speech_prob": 0.00040830502985045314}, {"id": 230, "seek": 104800, "start": 1048.0, "end": 1058.0, "text": " This is just how arbitrary it decided to implement it. There's actually an issue in arbitrary that they are aware of the problem with their not actually...", "tokens": [639, 307, 445, 577, 23211, 309, 3047, 281, 4445, 309, 13, 821, 311, 767, 364, 2734, 294, 23211, 300, 436, 366, 3650, 295, 264, 1154, 365, 641, 406, 767, 485], "temperature": 0.0, "avg_logprob": -0.2740699721545708, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.00089417421258986}, {"id": 231, "seek": 104800, "start": 1058.0, "end": 1062.0, "text": " It doesn't seem like they are just fixing it for some reason.", "tokens": [467, 1177, 380, 1643, 411, 436, 366, 445, 19442, 309, 337, 512, 1778, 13], "temperature": 0.0, "avg_logprob": -0.2740699721545708, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.00089417421258986}, {"id": 232, "seek": 104800, "start": 1062.0, "end": 1075.0, "text": " So what we ended up doing is that we ended up doing some custom serialization with info, which is also a very well-known Rust package.", "tokens": [407, 437, 321, 4590, 493, 884, 307, 300, 321, 4590, 493, 884, 512, 2375, 17436, 2144, 365, 13614, 11, 597, 307, 611, 257, 588, 731, 12, 6861, 34952, 7372, 13], "temperature": 0.0, "avg_logprob": -0.2740699721545708, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.00089417421258986}, {"id": 233, "seek": 107500, "start": 1075.0, "end": 1083.0, "text": " It's not much more digital than it is arbitrary. And it doesn't matter.", "tokens": [467, 311, 406, 709, 544, 4562, 813, 309, 307, 23211, 13, 400, 309, 1177, 380, 1871, 13], "temperature": 0.0, "avg_logprob": -0.43451775097456136, "compression_ratio": 1.4415584415584415, "no_speech_prob": 0.006721198093146086}, {"id": 234, "seek": 107500, "start": 1083.0, "end": 1091.0, "text": " When you discover a bug with this puzzle, does it transform into a unit that's not the one?", "tokens": [1133, 291, 4411, 257, 7426, 365, 341, 12805, 11, 775, 309, 4088, 666, 257, 4985, 300, 311, 406, 264, 472, 30], "temperature": 0.0, "avg_logprob": -0.43451775097456136, "compression_ratio": 1.4415584415584415, "no_speech_prob": 0.006721198093146086}, {"id": 235, "seek": 107500, "start": 1091.0, "end": 1097.0, "text": " The question is, when we discover a bug, does it try to...", "tokens": [440, 1168, 307, 11, 562, 321, 4411, 257, 7426, 11, 775, 309, 853, 281, 485], "temperature": 0.0, "avg_logprob": -0.43451775097456136, "compression_ratio": 1.4415584415584415, "no_speech_prob": 0.006721198093146086}, {"id": 236, "seek": 109700, "start": 1097.0, "end": 1107.0, "text": " Yeah, the way that we are fixing this kind of problem is that we are always adding a regression guess for them just to make sure that they don't...", "tokens": [865, 11, 264, 636, 300, 321, 366, 19442, 341, 733, 295, 1154, 307, 300, 321, 366, 1009, 5127, 257, 24590, 2041, 337, 552, 445, 281, 652, 988, 300, 436, 500, 380, 485], "temperature": 0.0, "avg_logprob": -0.35268979132929934, "compression_ratio": 1.5792079207920793, "no_speech_prob": 0.002140824683010578}, {"id": 237, "seek": 109700, "start": 1107.0, "end": 1113.0, "text": " I was wondering about the computation requirements. So how many cores are you using?", "tokens": [286, 390, 6359, 466, 264, 24903, 7728, 13, 407, 577, 867, 24826, 366, 291, 1228, 30], "temperature": 0.0, "avg_logprob": -0.35268979132929934, "compression_ratio": 1.5792079207920793, "no_speech_prob": 0.002140824683010578}, {"id": 238, "seek": 109700, "start": 1113.0, "end": 1117.0, "text": " How many cores we are using?", "tokens": [1012, 867, 24826, 321, 366, 1228, 30], "temperature": 0.0, "avg_logprob": -0.35268979132929934, "compression_ratio": 1.5792079207920793, "no_speech_prob": 0.002140824683010578}, {"id": 239, "seek": 109700, "start": 1117.0, "end": 1125.0, "text": " So when we read for two weeks, we actually used 96 cores.", "tokens": [407, 562, 321, 1401, 337, 732, 3259, 11, 321, 767, 1143, 24124, 24826, 13], "temperature": 0.0, "avg_logprob": -0.35268979132929934, "compression_ratio": 1.5792079207920793, "no_speech_prob": 0.002140824683010578}, {"id": 240, "seek": 112500, "start": 1125.0, "end": 1132.0, "text": " In uniqueness, I do not... So when you are running on coding, because I don't know exactly how many...", "tokens": [682, 48294, 11, 286, 360, 406, 485, 407, 562, 291, 366, 2614, 322, 17720, 11, 570, 286, 500, 380, 458, 2293, 577, 867, 485], "temperature": 0.0, "avg_logprob": -0.40421837397984095, "compression_ratio": 1.3964497041420119, "no_speech_prob": 0.011521932668983936}, {"id": 241, "seek": 112500, "start": 1132.0, "end": 1138.0, "text": " One? Maybe? I don't know. But I think we have been running on 96 cores as well.", "tokens": [1485, 30, 2704, 30, 286, 500, 380, 458, 13, 583, 286, 519, 321, 362, 668, 2614, 322, 24124, 24826, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.40421837397984095, "compression_ratio": 1.3964497041420119, "no_speech_prob": 0.011521932668983936}, {"id": 242, "seek": 112500, "start": 1138.0, "end": 1141.0, "text": " That was another one. Can I see that guy?", "tokens": [663, 390, 1071, 472, 13, 1664, 286, 536, 300, 2146, 30], "temperature": 0.0, "avg_logprob": -0.40421837397984095, "compression_ratio": 1.3964497041420119, "no_speech_prob": 0.011521932668983936}, {"id": 243, "seek": 112500, "start": 1141.0, "end": 1144.0, "text": " One minute.", "tokens": [1485, 3456, 13], "temperature": 0.0, "avg_logprob": -0.40421837397984095, "compression_ratio": 1.3964497041420119, "no_speech_prob": 0.011521932668983936}, {"id": 244, "seek": 114400, "start": 1144.0, "end": 1155.0, "text": " We found a color case that we are trying to shrink the crazy smaller color steps.", "tokens": [492, 1352, 257, 2017, 1389, 300, 321, 366, 1382, 281, 23060, 264, 3219, 4356, 2017, 4439, 13], "temperature": 0.0, "avg_logprob": -0.5764438538324266, "compression_ratio": 1.2, "no_speech_prob": 0.02520022541284561}, {"id": 245, "seek": 114400, "start": 1155.0, "end": 1160.0, "text": " Oh, this is... I don't know. Let's shut up afterwards.", "tokens": [876, 11, 341, 307, 485, 286, 500, 380, 458, 13, 961, 311, 5309, 493, 10543, 13], "temperature": 0.0, "avg_logprob": -0.5764438538324266, "compression_ratio": 1.2, "no_speech_prob": 0.02520022541284561}, {"id": 246, "seek": 116000, "start": 1160.0, "end": 1175.0, "text": " Thanks.", "tokens": [50364, 2561, 13, 51114], "temperature": 0.0, "avg_logprob": -0.6074437618255615, "compression_ratio": 0.4666666666666667, "no_speech_prob": 0.0023842090740799904}], "language": "en"}