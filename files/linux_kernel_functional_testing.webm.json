{"text": " Welcome to this session about LKFT, the Linux Can Help Functional Testing Project. My name is R\u00e9mi Durafor, I'm a Principal Technic at Linao. I've been working on open source projects since 2007 and I've been the Lava Architect and Main Developer since for eight years now, so quite some time now. So I will speak today about LKFT because it's a project I'm working with. So what is LKFT? So the goal of LKFT is to improve the Linux kernel quality on the ARM architecture by performing regression testing and reporting on selective Linux kernel branches and the Android common kernel in real time. That's what is written on the website. So it's a project that is led by Linao. The goal is to build and test a set of Linux kernel trees. So we care mainly about LTS trees, mainline and next. For LTS in particular, we have a 48 hour SLA, which means that we have to provide a full report in less than 48 hours for any change on LTS. If you look at the numbers for 2023, we tested 465 RCs. As we test mainline and next, we also built and tested 2,628 different commit versions, which means that we built 1.6 million kernels and ran 200 million tests in a year. That's only for Linux. If you look at Android common kernel, only for the test, that's 58 million tests, 580 million tests, so VTS and CTS mainly. And this is all done by only three people. So the question is how do we do to build that many kernels and test that many kernels with only three people, obviously automation. So my goal today is to show you the architecture of LKFT and to also show you the different tools that we created and maintained to make that possible. Because I'm sure that you can go back home with some of these tools and might be useful for you. So let's look at the architecture now. So this is a really simple view. We have a set of trees in GitLab that are just simple mirrors in GitLab of the official trees. We just use GitLab for a scheduling mechanism. So it will pull the new changes and it will run a GitLab CI pipeline. But we won't do anything specific in GitLab CI pipeline. We won't do build or test inside it. It's too slow and costly. So we just use it for submitting a plan to our system that will do the build and test and reporting. And at the end, we will just get a report that three engineers will look at and decide if we have to report something to the main developers or if we can just find a commit ourselves and send a patch. Let's dig in a bit now. So as I said, we don't use GitLab CI for building. We submit only from GitLab CI a build request to our system. So for building, we created a tool which is called text make. I will explain the different tools later on. I'm just showing the architecture right now. So we use a tool called text make that allows for building the system with different combinations of options. And we created a software as a service that allows to use text make at a large scale in the cloud. So we can build something like 5,000 kernels in parallel in the cloud in some minutes. When one build is finished, so when text make finishes build, they are sent to a storage. It's an S-free like bucket somewhere. And a result is sent to Squad, which is a second project that we also maintain. That would be what that I like where everything is stored. As we send results really early, if there is a build failure, a build regression, you will notice that in some minutes or hours depending on how long the build takes. Because for example, if you do an old mod config build with Clang, it will take up to one or two hours easily. But this way we can have early regression that we can send immediately to the main list saying that it's failing to build on this architecture for this tool chain. That's for building. I will explain text make a bit later on. So as I said, when a text make build finish, we send the result to Squad, we store in the storage and we also submit multiple run test runs that will be done in the cloud. So we do a test in the cloud and on physical devices. For the cloud, we have a product called text run that will allow to test on virtual devices, so QMU and a VP. And the same, we have a system that allows to scale in the cloud the text run processes. So you can spawn the same thousands of processes of text run processes in parallel in the cloud. And they will send the results to Squad also. Testing in virtualization is nice. You find a lot of bugs because you can test a lot of different combinations. But that's not enough. So I have to test on real devices. That's where a second software come in, which is Lava, that will allow to test on real devices. So the same when text make finishes to build, it will submit a set of test requests to Lava that will run on real hardware, this case. So obviously, we run less test on real devices and on virtual devices because we don't have enough board. It's always the single point that you're missing. The same results are sent to Squad and when everything is finished, we have a full report that we can provide to the developers that we run something like thousands of tests, thousands of builds, and everything is working or we find some regressions. That's the overall architecture. I will now look at the different projects so you can know if something can be useful for you. So let's look at the build parts. So as I said before, we use text make. It's a project that we created to make building easy and reproducible. So it's an open source command application. It allows for portable and repeatable Linux kind of builds. So for that, we use containers. We provide a set of containers with all the tools you need inside and everything is done inside a container. So it can be reproducible from one machine to another. So because that's often a problem when you report a build failure, it's always a nightmare to know the exact toolchain that you're using, everything. So as everything is inside a container, you can just reproduce it in another machine. So we support multiple toolchains from GCC from A to 12, client from 10 to 15. In fact, 16 has been added this week. We also have a Clang Android version and a Clang Nightly. Clang Nightly is specific because we rebuild the nightly Clang toolchain every night and we push it to our system so we can just test with the latest Clang. We also support multiple target architectures, all the ARM versions, Intel EMDs, and then some MIPS, PowerPC, RISV5, and some exotic one like S390, SH4, things like that. So building is really simple. You just specify the target architectures, so X8664 in this case. You specify the toolchain, so I want to use GCC12. You just need to have text-making installed on your computer because everything will then be done inside a container where you will have GCC12 to chain for X8664. If you want to build with GCC13, just change toolchain to GCC13 and it will use another container to build it. As I said before, we have a private software that allows to run text-making at a large scale in the cloud, but I'm not presenting that it's a close-up software. So just to explain how it's working, text-making will pull the right container for you. So for this specific target-arched toolchain couple, it will be X8664 GCC12 container. We have thousands of containers, hundreds of containers. It will create a unique build directory, so it's reproducible from one build to another. And then we just start a podman container, jump into it, and just build. We advise to use podman, obviously, and not docker because it will be a rootless container, so you can at least don't run asboot your build. And then it will invoke a set of different make comments depending on what you want to build. And then it will move everything to a specific directory that will be kept on the machine. And you will have all the artifacts, kernel, headers, et cetera. And you also have metadata.json file that will include a lot of metadata about your build, like version of your toolchain, of different utilities on the machine, the time taken by different steps, the size of everything, et cetera. And it will be useful for debugging also what's going on, if something breaks. And yeah, we provide multiple containers that you can reuse. And it's an open source project, so you can contribute to it a few months, and you can just use it right now. And some kind of developer use it for reproducing builds, build failures. And in fact, as I said, we have a client-nightly toolchain that is rebuilt nightly. It's in fact because the client project asked us to do that because they use Tuxmake with client-nightly for validating their client version against different kernel versions to see if clang is not regression. That's for building. So now, how do we test? So as I said, we test on virtual devices with Tuxrun and on physical devices with Lava. So for Tuxrun, it's the same. It's an open source common line application. It's the same for Tuxmake, but for running. It allows for portable and repeatable kernel tests. We support multiple devices, MVP MVA, which is an ARM V9.3 emulator, a simulator. That's the latest version that you can try for ARM. And then multiple ARM versions with multiple QEMU devices. Many ARM Intel MIPS in many different versions and PPC, et cetera, and multiple tests with LTP, K-Unit, K-Self tests, et cetera, et cetera. Adding one is not quite easy to do. The same, the common line is quite simple. We also use Sponman for containerizing everything. You specify the device that you want to use, the kernel that you want. It can be your URL, obviously, and a root file system also if you want. And again, we have a SAS that allows to run that at large scale in the cloud. When you call that, that common line, Tuxrun will download all the artifacts that you need. So kernel, DTB, root file system modules. It will inject the modules inside the root file system for you, so that it will be used at a good time. And start the container, start QEMU system, so R64 in this case. Look at the outputs, et cetera, et cetera, all the classical things, and store the results. As I said, we provide a lot of root file systems because we know it's painful to build your root file system for multiple architectures. So we do the work for that. We use billroot and debian. Billroot allows us to have the 19 supported architectures, one root file system for each. And for the main one, the one supported by debian, we do provide the debian root file system that we build. And obviously, if you build your own one, you can use it if you want. And we will do the job of rebuilding the billroot and debian file systems regularly. And in fact, it's a fun thing, we actually found bugs in QEMU before pushing the new file systems. We test in our system with the new root file systems. And the last time we did that, we found issues in QEMU 7.2 that are currently being fixed by QEMU developers. Something fun because Tux-Mech and Tux-Run has been done by the same team. So we make the work to combine the two tools together. So obviously, you can, doing a bisection of a build failure is quite easy. You just need a lot of CPU time. Same for a runtime issue, which is you find a regression where a test fail on a specific architecture. For example, when you run a LTP test suite on QEMU ARM64, it's failing. And you want to bisect that. So find the faulty commit. You have a good commit and a bad commit. And you want to find the faulty commit. Git allows you to help you on that. But thanks to Tux-Mech and Tux-Run, we can automate all that job of testing. So with this common line, Git will call Tux-Mech on different commits to try to find the 41. And Tux-Mech will just build. And at the end of the build, thanks to minus minus result hook, it will exec the command that is behind that will run Tux-Run with the kernel that has been just built. So it will build with Tux-Mech, and at the end, run with Tux-Run, the exact LTP test suite that fails. And if it's passing, it will return zero. If it's failing, it will return one. So based on that, Git will be able to find the faulty commit for you, which is quite... We find a lot of regression or test regression and find the faulty commit thanks to just that command line, which is really cool. Thanks to Anders for the idea. So that was all virtual build, building containers, test on virtual devices, but as I said before, we have to test on physical devices because multiple bugs are only found on physical devices because they are based on drivers failing and things like that. So for that, we use Lava, like many, many, some people in this room. So Lava stands for linear automated validation architecture. It's a text execution system. So it will allow for testing software on real hardware automatically for you. So it will automatically deploy, boot, and test your software on your hardware. So it's used by Canon CI a lot, by LKFT, obviously. And for... You can do system level testing, boot level testing. You can do boot loader also testing. You can test directly, directly, your boot loader and the firmware. And it currently supports 356 different device types. So from IoT to phones, Raspberry Pi-like boards, and servers. So multiple different device types. So for example, if you want to test on a Raspberry Pi, without Lava, you will have to pour on the board, download the artifacts, so kernel, rootFS, files, DTBs, place them on a specific directory, like NFS or TFT directory, connect to the serial, type a lot of commands, boot the board, watch the boot outputs, type the logging prompt, et cetera, et cetera. So it's really painful to do that manually. Lava will just do exactly what I just listed, automatically for you. It will just provide a job definition, which is a YAML file, with links to all the artifacts that you want to test. You specify the kind of board that you have. So it's a Raspberry Pi 4B, and Lava will know then how to interact with that board. And you will say that you boot install on it, and you have a TFTP server. Just use that, and test what I want to test on it. And Lava will do that automatically for you. Obviously, you can have multiple boards attached to the same worker, and you can have multiple workers on a Lava instance. So as a user, it's really an abstraction of the hardware, and you just send a YAML file and you get results, and all the hardware part is done automatically by Lava for you. So as I said, maybe you remember the first LKFT diagram. I'm sure you don't. That was a small box called KeysCache. So when we submit jobs to Lava, we submit multiple jobs for the same artifacts at the same time. We have multiple devices. So the scheduler will start the job for the same artifacts all at the same time. So it will download multiple times the same artifact at the same time, so we just should be able to catch that and decrease network usage. So we tried squid, and the short answer is squid is not working for that use case for different reasons. The first one is that, as I said before, all the artifacts are stored in an S3 like bucket. So it's somewhere on internet. So obviously we use SSL, HTTPS, to download it. And squid and HTTPS are not really working well together. You have to fake SSL certificates. It's all creepy things to do. And also a thing that, as I said, with download, Lava will start all the jobs at the same time. So they will more or less download all the same artifacts at exactly the same time. And if you do that with squid, squid will download, if you ask for n times the same file to squid, if it's not already cached, squid will download it n times. And only when one is finished, or when download is finished, the next one will use a cache version. So it's just pointless for us, just not working. So we created a tool called keyscache, the keys is for keep it simple, stupid. It's a simple and stupid caching server. It's not a proxy, it's a service, which means that it can handle HTTPS, and it will only download once when you have multiple clients, and it will stream to the clients while downloading. It's not transparent because it's not a proxy, and because it's not transparent, it can do HTTPS, because you will have to prefix your URL by the keyscache instance that you have. And you will talk to keyscache directly. It also automatically retries on failures, because we've found multiple failures that all the HTTP code that you can have when you request on an S3 like bucket, just insane. And sometimes also you will get, the connection will finish like if everything was done correctly. And in fact, the file is not complete, it's a partial download, and you don't get any errors. So keyscache will detect that for you. It will detect that it's a partial download, and it will retry and download only the remaining things for you. And it's fully transparent as a user. It will do that in the background and still stream your data to you. So thanks to that, we've been using it for 2.5 years now. In the graph, in green is what we serve locally from keyscache, and in red is what we download from Internet. So we downloaded 25 terabytes of data from Internet, and we serve 1.3 petabytes of data in the local network, which is the 52 times expansion ratio. So it's quite useful, and it improves stability also. So it's really cool. It's a good tool for your CI if you don't use it already. And last but not the least, we store all the job results in Squad. So it's software quality dashboard. It will store, it's a data lake. It will store all the results for you in different categories, and it will allow you to create reports, so failures, regressions, et cetera. Everything is stored in this one, and then we extract data and make report based on Squad. And that's all. That's what I just explained. If you have any questions, I have some time for questions. Five minutes. Perfect. Oh, yeah, that's good. Testing methods? We use LTP, KUNIT, KSELF-SES, all the kernel test suites that we don't, we are not creating new test suites. We are using test suites that does exist, and we build for the community, and we test for the community, and then we provide reports. We obviously interact a lot with the test suite maintainers, because we found bugs in the test suite, too. We have to report to them, and there's reporting a lot to them. And one of our projects is to test KSELF-SES in advance, test KSELF-SES master, to find bugs in KSELF-SES before they are actually running in production after. If you find any problems and report them, are current developers actually looking at them, or do you have to ping them and make sure they take care of the problem? Okay, so we have an SLA with Greg Croatman, so he's waiting for our results. So they will look at it for LTS. And for Mainline and Next, we are used to reports. We report a lot of issues, so they know us. If you look at LWN articles, about they classify the different contributions to the kernel, and Linaro is in the tested-by top in the tested-by, so they know us a lot, so they know that we provide good results. And when we provide a mail, there is everything that, every tool they need for reproducible. They are reproducing a build, so we provide all the binaries that they need for reproducing it. If it's a big failure, we provide a tux-make command line that they can use, and they are now used to use tux-make for rebuilding things. And if it's a test failure, we provide the logs, obviously, the job definition, and all the binaries they need for reproducing it. Do you actually check that every problem you found is actually fixed? And those are all the bugs that we found fixed? Not all of them? Yeah, if you found some bugs on SH4, no one will care, for example. The QMU 7.2 has been released recently, just not working on SH4. I couldn't answer that. We use the WS. No, it's not that bad. We build a dynamic system, which means that we do not rent 5,000 machines in parallel. Obviously not. It's just impossible for us. We are a small company. Everything is dynamic, so from one second to another, if you look at the graph of usage, when Anders submits a plan for testing, in one minute, we'll book 5,000 machines for building it, likely more 1.5,000 machines to build it. They will build and they will just stop at the end. So no, we don't have 5,000 machines. How many devices do you have in your lava test brick? So for the LKFT, we have multiple lava instances in Linauro, in LKFT, how many devices? About 20. 20, yeah. And about 5 different device types, like Rolls-Royce, Dragon Balls, Junos, X8, X15. But yeah, you can have really large labs in lava. We have another one for just Linauro usage, where we have something like 100 balls, I think, the main one. Thanks. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.92, "text": " Welcome to this session about LKFT, the Linux Can Help Functional Testing Project.", "tokens": [4027, 281, 341, 5481, 466, 441, 42, 25469, 11, 264, 18734, 1664, 10773, 11166, 41048, 45517, 9849, 13], "temperature": 0.0, "avg_logprob": -0.3705729042611471, "compression_ratio": 1.3669724770642202, "no_speech_prob": 0.08739719539880753}, {"id": 1, "seek": 0, "start": 10.92, "end": 14.52, "text": " My name is R\u00e9mi Durafor, I'm a Principal Technic at Linao.", "tokens": [1222, 1315, 307, 497, 526, 3057, 413, 2991, 2994, 11, 286, 478, 257, 38575, 8337, 299, 412, 441, 1426, 78, 13], "temperature": 0.0, "avg_logprob": -0.3705729042611471, "compression_ratio": 1.3669724770642202, "no_speech_prob": 0.08739719539880753}, {"id": 2, "seek": 0, "start": 14.52, "end": 20.12, "text": " I've been working on open source projects since 2007 and I've been the Lava Architect", "tokens": [286, 600, 668, 1364, 322, 1269, 4009, 4455, 1670, 12656, 293, 286, 600, 668, 264, 441, 4061, 29306], "temperature": 0.0, "avg_logprob": -0.3705729042611471, "compression_ratio": 1.3669724770642202, "no_speech_prob": 0.08739719539880753}, {"id": 3, "seek": 0, "start": 20.12, "end": 26.48, "text": " and Main Developer since for eight years now, so quite some time now.", "tokens": [293, 12383, 44915, 1670, 337, 3180, 924, 586, 11, 370, 1596, 512, 565, 586, 13], "temperature": 0.0, "avg_logprob": -0.3705729042611471, "compression_ratio": 1.3669724770642202, "no_speech_prob": 0.08739719539880753}, {"id": 4, "seek": 2648, "start": 26.48, "end": 32.68, "text": " So I will speak today about LKFT because it's a project I'm working with.", "tokens": [407, 286, 486, 1710, 965, 466, 441, 42, 25469, 570, 309, 311, 257, 1716, 286, 478, 1364, 365, 13], "temperature": 0.0, "avg_logprob": -0.1390804032147941, "compression_ratio": 1.70703125, "no_speech_prob": 6.393317744368687e-05}, {"id": 5, "seek": 2648, "start": 32.68, "end": 34.04, "text": " So what is LKFT?", "tokens": [407, 437, 307, 441, 42, 25469, 30], "temperature": 0.0, "avg_logprob": -0.1390804032147941, "compression_ratio": 1.70703125, "no_speech_prob": 6.393317744368687e-05}, {"id": 6, "seek": 2648, "start": 34.04, "end": 39.0, "text": " So the goal of LKFT is to improve the Linux kernel quality on the ARM architecture by", "tokens": [407, 264, 3387, 295, 441, 42, 25469, 307, 281, 3470, 264, 18734, 28256, 3125, 322, 264, 45209, 9482, 538], "temperature": 0.0, "avg_logprob": -0.1390804032147941, "compression_ratio": 1.70703125, "no_speech_prob": 6.393317744368687e-05}, {"id": 7, "seek": 2648, "start": 39.0, "end": 43.400000000000006, "text": " performing regression testing and reporting on selective Linux kernel branches and the", "tokens": [10205, 24590, 4997, 293, 10031, 322, 33930, 18734, 28256, 14770, 293, 264], "temperature": 0.0, "avg_logprob": -0.1390804032147941, "compression_ratio": 1.70703125, "no_speech_prob": 6.393317744368687e-05}, {"id": 8, "seek": 2648, "start": 43.400000000000006, "end": 46.72, "text": " Android common kernel in real time.", "tokens": [8853, 2689, 28256, 294, 957, 565, 13], "temperature": 0.0, "avg_logprob": -0.1390804032147941, "compression_ratio": 1.70703125, "no_speech_prob": 6.393317744368687e-05}, {"id": 9, "seek": 2648, "start": 46.72, "end": 49.28, "text": " That's what is written on the website.", "tokens": [663, 311, 437, 307, 3720, 322, 264, 3144, 13], "temperature": 0.0, "avg_logprob": -0.1390804032147941, "compression_ratio": 1.70703125, "no_speech_prob": 6.393317744368687e-05}, {"id": 10, "seek": 2648, "start": 49.28, "end": 52.36, "text": " So it's a project that is led by Linao.", "tokens": [407, 309, 311, 257, 1716, 300, 307, 4684, 538, 441, 1426, 78, 13], "temperature": 0.0, "avg_logprob": -0.1390804032147941, "compression_ratio": 1.70703125, "no_speech_prob": 6.393317744368687e-05}, {"id": 11, "seek": 2648, "start": 52.36, "end": 56.44, "text": " The goal is to build and test a set of Linux kernel trees.", "tokens": [440, 3387, 307, 281, 1322, 293, 1500, 257, 992, 295, 18734, 28256, 5852, 13], "temperature": 0.0, "avg_logprob": -0.1390804032147941, "compression_ratio": 1.70703125, "no_speech_prob": 6.393317744368687e-05}, {"id": 12, "seek": 5644, "start": 56.44, "end": 60.76, "text": " So we care mainly about LTS trees, mainline and next.", "tokens": [407, 321, 1127, 8704, 466, 441, 7327, 5852, 11, 2135, 1889, 293, 958, 13], "temperature": 0.0, "avg_logprob": -0.1302344799041748, "compression_ratio": 1.3812154696132597, "no_speech_prob": 8.816710760584101e-05}, {"id": 13, "seek": 5644, "start": 60.76, "end": 66.32, "text": " For LTS in particular, we have a 48 hour SLA, which means that we have to provide a full", "tokens": [1171, 441, 7327, 294, 1729, 11, 321, 362, 257, 11174, 1773, 318, 11435, 11, 597, 1355, 300, 321, 362, 281, 2893, 257, 1577], "temperature": 0.0, "avg_logprob": -0.1302344799041748, "compression_ratio": 1.3812154696132597, "no_speech_prob": 8.816710760584101e-05}, {"id": 14, "seek": 5644, "start": 66.32, "end": 71.67999999999999, "text": " report in less than 48 hours for any change on LTS.", "tokens": [2275, 294, 1570, 813, 11174, 2496, 337, 604, 1319, 322, 441, 7327, 13], "temperature": 0.0, "avg_logprob": -0.1302344799041748, "compression_ratio": 1.3812154696132597, "no_speech_prob": 8.816710760584101e-05}, {"id": 15, "seek": 5644, "start": 71.67999999999999, "end": 79.56, "text": " If you look at the numbers for 2023, we tested 465 RCs.", "tokens": [759, 291, 574, 412, 264, 3547, 337, 44377, 11, 321, 8246, 1017, 16824, 28987, 82, 13], "temperature": 0.0, "avg_logprob": -0.1302344799041748, "compression_ratio": 1.3812154696132597, "no_speech_prob": 8.816710760584101e-05}, {"id": 16, "seek": 7956, "start": 79.56, "end": 87.32000000000001, "text": " As we test mainline and next, we also built and tested 2,628 different commit versions,", "tokens": [1018, 321, 1500, 2135, 1889, 293, 958, 11, 321, 611, 3094, 293, 8246, 568, 11, 21, 11205, 819, 5599, 9606, 11], "temperature": 0.0, "avg_logprob": -0.13458624163877617, "compression_ratio": 1.592920353982301, "no_speech_prob": 3.467550777713768e-05}, {"id": 17, "seek": 7956, "start": 87.32000000000001, "end": 93.0, "text": " which means that we built 1.6 million kernels and ran 200 million tests in a year.", "tokens": [597, 1355, 300, 321, 3094, 502, 13, 21, 2459, 23434, 1625, 293, 5872, 2331, 2459, 6921, 294, 257, 1064, 13], "temperature": 0.0, "avg_logprob": -0.13458624163877617, "compression_ratio": 1.592920353982301, "no_speech_prob": 3.467550777713768e-05}, {"id": 18, "seek": 7956, "start": 93.0, "end": 94.0, "text": " That's only for Linux.", "tokens": [663, 311, 787, 337, 18734, 13], "temperature": 0.0, "avg_logprob": -0.13458624163877617, "compression_ratio": 1.592920353982301, "no_speech_prob": 3.467550777713768e-05}, {"id": 19, "seek": 7956, "start": 94.0, "end": 101.2, "text": " If you look at Android common kernel, only for the test, that's 58 million tests, 580", "tokens": [759, 291, 574, 412, 8853, 2689, 28256, 11, 787, 337, 264, 1500, 11, 300, 311, 21786, 2459, 6921, 11, 1025, 4702], "temperature": 0.0, "avg_logprob": -0.13458624163877617, "compression_ratio": 1.592920353982301, "no_speech_prob": 3.467550777713768e-05}, {"id": 20, "seek": 7956, "start": 101.2, "end": 104.04, "text": " million tests, so VTS and CTS mainly.", "tokens": [2459, 6921, 11, 370, 691, 7327, 293, 383, 7327, 8704, 13], "temperature": 0.0, "avg_logprob": -0.13458624163877617, "compression_ratio": 1.592920353982301, "no_speech_prob": 3.467550777713768e-05}, {"id": 21, "seek": 7956, "start": 104.04, "end": 107.88, "text": " And this is all done by only three people.", "tokens": [400, 341, 307, 439, 1096, 538, 787, 1045, 561, 13], "temperature": 0.0, "avg_logprob": -0.13458624163877617, "compression_ratio": 1.592920353982301, "no_speech_prob": 3.467550777713768e-05}, {"id": 22, "seek": 10788, "start": 107.88, "end": 112.75999999999999, "text": " So the question is how do we do to build that many kernels and test that many kernels with", "tokens": [407, 264, 1168, 307, 577, 360, 321, 360, 281, 1322, 300, 867, 23434, 1625, 293, 1500, 300, 867, 23434, 1625, 365], "temperature": 0.0, "avg_logprob": -0.14024105235042736, "compression_ratio": 1.7121212121212122, "no_speech_prob": 6.751658656867221e-05}, {"id": 23, "seek": 10788, "start": 112.75999999999999, "end": 115.75999999999999, "text": " only three people, obviously automation.", "tokens": [787, 1045, 561, 11, 2745, 17769, 13], "temperature": 0.0, "avg_logprob": -0.14024105235042736, "compression_ratio": 1.7121212121212122, "no_speech_prob": 6.751658656867221e-05}, {"id": 24, "seek": 10788, "start": 115.75999999999999, "end": 120.75999999999999, "text": " So my goal today is to show you the architecture of LKFT and to also show you the different", "tokens": [407, 452, 3387, 965, 307, 281, 855, 291, 264, 9482, 295, 441, 42, 25469, 293, 281, 611, 855, 291, 264, 819], "temperature": 0.0, "avg_logprob": -0.14024105235042736, "compression_ratio": 1.7121212121212122, "no_speech_prob": 6.751658656867221e-05}, {"id": 25, "seek": 10788, "start": 120.75999999999999, "end": 124.75999999999999, "text": " tools that we created and maintained to make that possible.", "tokens": [3873, 300, 321, 2942, 293, 17578, 281, 652, 300, 1944, 13], "temperature": 0.0, "avg_logprob": -0.14024105235042736, "compression_ratio": 1.7121212121212122, "no_speech_prob": 6.751658656867221e-05}, {"id": 26, "seek": 10788, "start": 124.75999999999999, "end": 128.76, "text": " Because I'm sure that you can go back home with some of these tools and might be useful", "tokens": [1436, 286, 478, 988, 300, 291, 393, 352, 646, 1280, 365, 512, 295, 613, 3873, 293, 1062, 312, 4420], "temperature": 0.0, "avg_logprob": -0.14024105235042736, "compression_ratio": 1.7121212121212122, "no_speech_prob": 6.751658656867221e-05}, {"id": 27, "seek": 10788, "start": 128.76, "end": 130.48, "text": " for you.", "tokens": [337, 291, 13], "temperature": 0.0, "avg_logprob": -0.14024105235042736, "compression_ratio": 1.7121212121212122, "no_speech_prob": 6.751658656867221e-05}, {"id": 28, "seek": 10788, "start": 130.48, "end": 132.72, "text": " So let's look at the architecture now.", "tokens": [407, 718, 311, 574, 412, 264, 9482, 586, 13], "temperature": 0.0, "avg_logprob": -0.14024105235042736, "compression_ratio": 1.7121212121212122, "no_speech_prob": 6.751658656867221e-05}, {"id": 29, "seek": 10788, "start": 132.72, "end": 134.04, "text": " So this is a really simple view.", "tokens": [407, 341, 307, 257, 534, 2199, 1910, 13], "temperature": 0.0, "avg_logprob": -0.14024105235042736, "compression_ratio": 1.7121212121212122, "no_speech_prob": 6.751658656867221e-05}, {"id": 30, "seek": 13404, "start": 134.04, "end": 139.76, "text": " We have a set of trees in GitLab that are just simple mirrors in GitLab of the official", "tokens": [492, 362, 257, 992, 295, 5852, 294, 16939, 37880, 300, 366, 445, 2199, 24238, 294, 16939, 37880, 295, 264, 4783], "temperature": 0.0, "avg_logprob": -0.1346241028840877, "compression_ratio": 1.763265306122449, "no_speech_prob": 0.00011742063361452892}, {"id": 31, "seek": 13404, "start": 139.76, "end": 141.23999999999998, "text": " trees.", "tokens": [5852, 13], "temperature": 0.0, "avg_logprob": -0.1346241028840877, "compression_ratio": 1.763265306122449, "no_speech_prob": 0.00011742063361452892}, {"id": 32, "seek": 13404, "start": 141.23999999999998, "end": 143.95999999999998, "text": " We just use GitLab for a scheduling mechanism.", "tokens": [492, 445, 764, 16939, 37880, 337, 257, 29055, 7513, 13], "temperature": 0.0, "avg_logprob": -0.1346241028840877, "compression_ratio": 1.763265306122449, "no_speech_prob": 0.00011742063361452892}, {"id": 33, "seek": 13404, "start": 143.95999999999998, "end": 148.64, "text": " So it will pull the new changes and it will run a GitLab CI pipeline.", "tokens": [407, 309, 486, 2235, 264, 777, 2962, 293, 309, 486, 1190, 257, 16939, 37880, 37777, 15517, 13], "temperature": 0.0, "avg_logprob": -0.1346241028840877, "compression_ratio": 1.763265306122449, "no_speech_prob": 0.00011742063361452892}, {"id": 34, "seek": 13404, "start": 148.64, "end": 151.39999999999998, "text": " But we won't do anything specific in GitLab CI pipeline.", "tokens": [583, 321, 1582, 380, 360, 1340, 2685, 294, 16939, 37880, 37777, 15517, 13], "temperature": 0.0, "avg_logprob": -0.1346241028840877, "compression_ratio": 1.763265306122449, "no_speech_prob": 0.00011742063361452892}, {"id": 35, "seek": 13404, "start": 151.39999999999998, "end": 153.28, "text": " We won't do build or test inside it.", "tokens": [492, 1582, 380, 360, 1322, 420, 1500, 1854, 309, 13], "temperature": 0.0, "avg_logprob": -0.1346241028840877, "compression_ratio": 1.763265306122449, "no_speech_prob": 0.00011742063361452892}, {"id": 36, "seek": 13404, "start": 153.28, "end": 155.6, "text": " It's too slow and costly.", "tokens": [467, 311, 886, 2964, 293, 28328, 13], "temperature": 0.0, "avg_logprob": -0.1346241028840877, "compression_ratio": 1.763265306122449, "no_speech_prob": 0.00011742063361452892}, {"id": 37, "seek": 13404, "start": 155.6, "end": 161.2, "text": " So we just use it for submitting a plan to our system that will do the build and test", "tokens": [407, 321, 445, 764, 309, 337, 31836, 257, 1393, 281, 527, 1185, 300, 486, 360, 264, 1322, 293, 1500], "temperature": 0.0, "avg_logprob": -0.1346241028840877, "compression_ratio": 1.763265306122449, "no_speech_prob": 0.00011742063361452892}, {"id": 38, "seek": 13404, "start": 161.2, "end": 162.84, "text": " and reporting.", "tokens": [293, 10031, 13], "temperature": 0.0, "avg_logprob": -0.1346241028840877, "compression_ratio": 1.763265306122449, "no_speech_prob": 0.00011742063361452892}, {"id": 39, "seek": 16284, "start": 162.84, "end": 166.92000000000002, "text": " And at the end, we will just get a report that three engineers will look at and decide", "tokens": [400, 412, 264, 917, 11, 321, 486, 445, 483, 257, 2275, 300, 1045, 11955, 486, 574, 412, 293, 4536], "temperature": 0.0, "avg_logprob": -0.13952141097097687, "compression_ratio": 1.6827586206896552, "no_speech_prob": 3.987749732914381e-05}, {"id": 40, "seek": 16284, "start": 166.92000000000002, "end": 172.12, "text": " if we have to report something to the main developers or if we can just find a commit", "tokens": [498, 321, 362, 281, 2275, 746, 281, 264, 2135, 8849, 420, 498, 321, 393, 445, 915, 257, 5599], "temperature": 0.0, "avg_logprob": -0.13952141097097687, "compression_ratio": 1.6827586206896552, "no_speech_prob": 3.987749732914381e-05}, {"id": 41, "seek": 16284, "start": 172.12, "end": 174.8, "text": " ourselves and send a patch.", "tokens": [4175, 293, 2845, 257, 9972, 13], "temperature": 0.0, "avg_logprob": -0.13952141097097687, "compression_ratio": 1.6827586206896552, "no_speech_prob": 3.987749732914381e-05}, {"id": 42, "seek": 16284, "start": 174.8, "end": 176.0, "text": " Let's dig in a bit now.", "tokens": [961, 311, 2528, 294, 257, 857, 586, 13], "temperature": 0.0, "avg_logprob": -0.13952141097097687, "compression_ratio": 1.6827586206896552, "no_speech_prob": 3.987749732914381e-05}, {"id": 43, "seek": 16284, "start": 176.0, "end": 179.08, "text": " So as I said, we don't use GitLab CI for building.", "tokens": [407, 382, 286, 848, 11, 321, 500, 380, 764, 16939, 37880, 37777, 337, 2390, 13], "temperature": 0.0, "avg_logprob": -0.13952141097097687, "compression_ratio": 1.6827586206896552, "no_speech_prob": 3.987749732914381e-05}, {"id": 44, "seek": 16284, "start": 179.08, "end": 183.04, "text": " We submit only from GitLab CI a build request to our system.", "tokens": [492, 10315, 787, 490, 16939, 37880, 37777, 257, 1322, 5308, 281, 527, 1185, 13], "temperature": 0.0, "avg_logprob": -0.13952141097097687, "compression_ratio": 1.6827586206896552, "no_speech_prob": 3.987749732914381e-05}, {"id": 45, "seek": 16284, "start": 183.04, "end": 186.2, "text": " So for building, we created a tool which is called text make.", "tokens": [407, 337, 2390, 11, 321, 2942, 257, 2290, 597, 307, 1219, 2487, 652, 13], "temperature": 0.0, "avg_logprob": -0.13952141097097687, "compression_ratio": 1.6827586206896552, "no_speech_prob": 3.987749732914381e-05}, {"id": 46, "seek": 16284, "start": 186.2, "end": 189.28, "text": " I will explain the different tools later on.", "tokens": [286, 486, 2903, 264, 819, 3873, 1780, 322, 13], "temperature": 0.0, "avg_logprob": -0.13952141097097687, "compression_ratio": 1.6827586206896552, "no_speech_prob": 3.987749732914381e-05}, {"id": 47, "seek": 16284, "start": 189.28, "end": 191.28, "text": " I'm just showing the architecture right now.", "tokens": [286, 478, 445, 4099, 264, 9482, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.13952141097097687, "compression_ratio": 1.6827586206896552, "no_speech_prob": 3.987749732914381e-05}, {"id": 48, "seek": 19128, "start": 191.28, "end": 197.64000000000001, "text": " So we use a tool called text make that allows for building the system with different combinations", "tokens": [407, 321, 764, 257, 2290, 1219, 2487, 652, 300, 4045, 337, 2390, 264, 1185, 365, 819, 21267], "temperature": 0.0, "avg_logprob": -0.20369227727254233, "compression_ratio": 1.6852589641434264, "no_speech_prob": 4.542764509096742e-05}, {"id": 49, "seek": 19128, "start": 197.64000000000001, "end": 198.64000000000001, "text": " of options.", "tokens": [295, 3956, 13], "temperature": 0.0, "avg_logprob": -0.20369227727254233, "compression_ratio": 1.6852589641434264, "no_speech_prob": 4.542764509096742e-05}, {"id": 50, "seek": 19128, "start": 198.64000000000001, "end": 206.04, "text": " And we created a software as a service that allows to use text make at a large scale in", "tokens": [400, 321, 2942, 257, 4722, 382, 257, 2643, 300, 4045, 281, 764, 2487, 652, 412, 257, 2416, 4373, 294], "temperature": 0.0, "avg_logprob": -0.20369227727254233, "compression_ratio": 1.6852589641434264, "no_speech_prob": 4.542764509096742e-05}, {"id": 51, "seek": 19128, "start": 206.04, "end": 207.04, "text": " the cloud.", "tokens": [264, 4588, 13], "temperature": 0.0, "avg_logprob": -0.20369227727254233, "compression_ratio": 1.6852589641434264, "no_speech_prob": 4.542764509096742e-05}, {"id": 52, "seek": 19128, "start": 207.04, "end": 211.32, "text": " So we can build something like 5,000 kernels in parallel in the cloud in some minutes.", "tokens": [407, 321, 393, 1322, 746, 411, 1025, 11, 1360, 23434, 1625, 294, 8952, 294, 264, 4588, 294, 512, 2077, 13], "temperature": 0.0, "avg_logprob": -0.20369227727254233, "compression_ratio": 1.6852589641434264, "no_speech_prob": 4.542764509096742e-05}, {"id": 53, "seek": 19128, "start": 211.32, "end": 217.28, "text": " When one build is finished, so when text make finishes build, they are sent to a storage.", "tokens": [1133, 472, 1322, 307, 4335, 11, 370, 562, 2487, 652, 23615, 1322, 11, 436, 366, 2279, 281, 257, 6725, 13], "temperature": 0.0, "avg_logprob": -0.20369227727254233, "compression_ratio": 1.6852589641434264, "no_speech_prob": 4.542764509096742e-05}, {"id": 54, "seek": 19128, "start": 217.28, "end": 220.68, "text": " It's an S-free like bucket somewhere.", "tokens": [467, 311, 364, 318, 12, 10792, 411, 13058, 4079, 13], "temperature": 0.0, "avg_logprob": -0.20369227727254233, "compression_ratio": 1.6852589641434264, "no_speech_prob": 4.542764509096742e-05}, {"id": 55, "seek": 22068, "start": 220.68, "end": 226.52, "text": " And a result is sent to Squad, which is a second project that we also maintain.", "tokens": [400, 257, 1874, 307, 2279, 281, 26596, 11, 597, 307, 257, 1150, 1716, 300, 321, 611, 6909, 13], "temperature": 0.0, "avg_logprob": -0.2095184326171875, "compression_ratio": 1.7089041095890412, "no_speech_prob": 5.3476542234420776e-05}, {"id": 56, "seek": 22068, "start": 226.52, "end": 229.52, "text": " That would be what that I like where everything is stored.", "tokens": [663, 576, 312, 437, 300, 286, 411, 689, 1203, 307, 12187, 13], "temperature": 0.0, "avg_logprob": -0.2095184326171875, "compression_ratio": 1.7089041095890412, "no_speech_prob": 5.3476542234420776e-05}, {"id": 57, "seek": 22068, "start": 229.52, "end": 234.16, "text": " As we send results really early, if there is a build failure, a build regression, you", "tokens": [1018, 321, 2845, 3542, 534, 2440, 11, 498, 456, 307, 257, 1322, 7763, 11, 257, 1322, 24590, 11, 291], "temperature": 0.0, "avg_logprob": -0.2095184326171875, "compression_ratio": 1.7089041095890412, "no_speech_prob": 5.3476542234420776e-05}, {"id": 58, "seek": 22068, "start": 234.16, "end": 238.60000000000002, "text": " will notice that in some minutes or hours depending on how long the build takes.", "tokens": [486, 3449, 300, 294, 512, 2077, 420, 2496, 5413, 322, 577, 938, 264, 1322, 2516, 13], "temperature": 0.0, "avg_logprob": -0.2095184326171875, "compression_ratio": 1.7089041095890412, "no_speech_prob": 5.3476542234420776e-05}, {"id": 59, "seek": 22068, "start": 238.60000000000002, "end": 242.96, "text": " Because for example, if you do an old mod config build with Clang, it will take up to", "tokens": [1436, 337, 1365, 11, 498, 291, 360, 364, 1331, 1072, 6662, 1322, 365, 2033, 656, 11, 309, 486, 747, 493, 281], "temperature": 0.0, "avg_logprob": -0.2095184326171875, "compression_ratio": 1.7089041095890412, "no_speech_prob": 5.3476542234420776e-05}, {"id": 60, "seek": 22068, "start": 242.96, "end": 245.24, "text": " one or two hours easily.", "tokens": [472, 420, 732, 2496, 3612, 13], "temperature": 0.0, "avg_logprob": -0.2095184326171875, "compression_ratio": 1.7089041095890412, "no_speech_prob": 5.3476542234420776e-05}, {"id": 61, "seek": 22068, "start": 245.24, "end": 250.32, "text": " But this way we can have early regression that we can send immediately to the main", "tokens": [583, 341, 636, 321, 393, 362, 2440, 24590, 300, 321, 393, 2845, 4258, 281, 264, 2135], "temperature": 0.0, "avg_logprob": -0.2095184326171875, "compression_ratio": 1.7089041095890412, "no_speech_prob": 5.3476542234420776e-05}, {"id": 62, "seek": 25032, "start": 250.32, "end": 257.28, "text": " list saying that it's failing to build on this architecture for this tool chain.", "tokens": [1329, 1566, 300, 309, 311, 18223, 281, 1322, 322, 341, 9482, 337, 341, 2290, 5021, 13], "temperature": 0.0, "avg_logprob": -0.20250629892154615, "compression_ratio": 1.665158371040724, "no_speech_prob": 5.4235504649113864e-05}, {"id": 63, "seek": 25032, "start": 257.28, "end": 258.28, "text": " That's for building.", "tokens": [663, 311, 337, 2390, 13], "temperature": 0.0, "avg_logprob": -0.20250629892154615, "compression_ratio": 1.665158371040724, "no_speech_prob": 5.4235504649113864e-05}, {"id": 64, "seek": 25032, "start": 258.28, "end": 261.52, "text": " I will explain text make a bit later on.", "tokens": [286, 486, 2903, 2487, 652, 257, 857, 1780, 322, 13], "temperature": 0.0, "avg_logprob": -0.20250629892154615, "compression_ratio": 1.665158371040724, "no_speech_prob": 5.4235504649113864e-05}, {"id": 65, "seek": 25032, "start": 261.52, "end": 267.44, "text": " So as I said, when a text make build finish, we send the result to Squad, we store in the", "tokens": [407, 382, 286, 848, 11, 562, 257, 2487, 652, 1322, 2413, 11, 321, 2845, 264, 1874, 281, 26596, 11, 321, 3531, 294, 264], "temperature": 0.0, "avg_logprob": -0.20250629892154615, "compression_ratio": 1.665158371040724, "no_speech_prob": 5.4235504649113864e-05}, {"id": 66, "seek": 25032, "start": 267.44, "end": 273.0, "text": " storage and we also submit multiple run test runs that will be done in the cloud.", "tokens": [6725, 293, 321, 611, 10315, 3866, 1190, 1500, 6676, 300, 486, 312, 1096, 294, 264, 4588, 13], "temperature": 0.0, "avg_logprob": -0.20250629892154615, "compression_ratio": 1.665158371040724, "no_speech_prob": 5.4235504649113864e-05}, {"id": 67, "seek": 25032, "start": 273.0, "end": 276.56, "text": " So we do a test in the cloud and on physical devices.", "tokens": [407, 321, 360, 257, 1500, 294, 264, 4588, 293, 322, 4001, 5759, 13], "temperature": 0.0, "avg_logprob": -0.20250629892154615, "compression_ratio": 1.665158371040724, "no_speech_prob": 5.4235504649113864e-05}, {"id": 68, "seek": 27656, "start": 276.56, "end": 282.0, "text": " For the cloud, we have a product called text run that will allow to test on virtual devices,", "tokens": [1171, 264, 4588, 11, 321, 362, 257, 1674, 1219, 2487, 1190, 300, 486, 2089, 281, 1500, 322, 6374, 5759, 11], "temperature": 0.0, "avg_logprob": -0.17707641663089876, "compression_ratio": 1.7827715355805243, "no_speech_prob": 3.414317325223237e-05}, {"id": 69, "seek": 27656, "start": 282.0, "end": 283.84, "text": " so QMU and a VP.", "tokens": [370, 1249, 44, 52, 293, 257, 35812, 13], "temperature": 0.0, "avg_logprob": -0.17707641663089876, "compression_ratio": 1.7827715355805243, "no_speech_prob": 3.414317325223237e-05}, {"id": 70, "seek": 27656, "start": 283.84, "end": 289.36, "text": " And the same, we have a system that allows to scale in the cloud the text run processes.", "tokens": [400, 264, 912, 11, 321, 362, 257, 1185, 300, 4045, 281, 4373, 294, 264, 4588, 264, 2487, 1190, 7555, 13], "temperature": 0.0, "avg_logprob": -0.17707641663089876, "compression_ratio": 1.7827715355805243, "no_speech_prob": 3.414317325223237e-05}, {"id": 71, "seek": 27656, "start": 289.36, "end": 295.92, "text": " So you can spawn the same thousands of processes of text run processes in parallel in the cloud.", "tokens": [407, 291, 393, 17088, 264, 912, 5383, 295, 7555, 295, 2487, 1190, 7555, 294, 8952, 294, 264, 4588, 13], "temperature": 0.0, "avg_logprob": -0.17707641663089876, "compression_ratio": 1.7827715355805243, "no_speech_prob": 3.414317325223237e-05}, {"id": 72, "seek": 27656, "start": 295.92, "end": 300.16, "text": " And they will send the results to Squad also.", "tokens": [400, 436, 486, 2845, 264, 3542, 281, 26596, 611, 13], "temperature": 0.0, "avg_logprob": -0.17707641663089876, "compression_ratio": 1.7827715355805243, "no_speech_prob": 3.414317325223237e-05}, {"id": 73, "seek": 27656, "start": 300.16, "end": 301.56, "text": " Testing in virtualization is nice.", "tokens": [45517, 294, 6374, 2144, 307, 1481, 13], "temperature": 0.0, "avg_logprob": -0.17707641663089876, "compression_ratio": 1.7827715355805243, "no_speech_prob": 3.414317325223237e-05}, {"id": 74, "seek": 27656, "start": 301.56, "end": 305.16, "text": " You find a lot of bugs because you can test a lot of different combinations.", "tokens": [509, 915, 257, 688, 295, 15120, 570, 291, 393, 1500, 257, 688, 295, 819, 21267, 13], "temperature": 0.0, "avg_logprob": -0.17707641663089876, "compression_ratio": 1.7827715355805243, "no_speech_prob": 3.414317325223237e-05}, {"id": 75, "seek": 27656, "start": 305.16, "end": 306.16, "text": " But that's not enough.", "tokens": [583, 300, 311, 406, 1547, 13], "temperature": 0.0, "avg_logprob": -0.17707641663089876, "compression_ratio": 1.7827715355805243, "no_speech_prob": 3.414317325223237e-05}, {"id": 76, "seek": 30616, "start": 306.16, "end": 308.52000000000004, "text": " So I have to test on real devices.", "tokens": [407, 286, 362, 281, 1500, 322, 957, 5759, 13], "temperature": 0.0, "avg_logprob": -0.16578197479248047, "compression_ratio": 1.7615062761506277, "no_speech_prob": 0.00010428368841530755}, {"id": 77, "seek": 30616, "start": 308.52000000000004, "end": 314.88000000000005, "text": " That's where a second software come in, which is Lava, that will allow to test on real devices.", "tokens": [663, 311, 689, 257, 1150, 4722, 808, 294, 11, 597, 307, 441, 4061, 11, 300, 486, 2089, 281, 1500, 322, 957, 5759, 13], "temperature": 0.0, "avg_logprob": -0.16578197479248047, "compression_ratio": 1.7615062761506277, "no_speech_prob": 0.00010428368841530755}, {"id": 78, "seek": 30616, "start": 314.88000000000005, "end": 320.6, "text": " So the same when text make finishes to build, it will submit a set of test requests to Lava", "tokens": [407, 264, 912, 562, 2487, 652, 23615, 281, 1322, 11, 309, 486, 10315, 257, 992, 295, 1500, 12475, 281, 441, 4061], "temperature": 0.0, "avg_logprob": -0.16578197479248047, "compression_ratio": 1.7615062761506277, "no_speech_prob": 0.00010428368841530755}, {"id": 79, "seek": 30616, "start": 320.6, "end": 323.04, "text": " that will run on real hardware, this case.", "tokens": [300, 486, 1190, 322, 957, 8837, 11, 341, 1389, 13], "temperature": 0.0, "avg_logprob": -0.16578197479248047, "compression_ratio": 1.7615062761506277, "no_speech_prob": 0.00010428368841530755}, {"id": 80, "seek": 30616, "start": 323.04, "end": 328.0, "text": " So obviously, we run less test on real devices and on virtual devices because we don't have", "tokens": [407, 2745, 11, 321, 1190, 1570, 1500, 322, 957, 5759, 293, 322, 6374, 5759, 570, 321, 500, 380, 362], "temperature": 0.0, "avg_logprob": -0.16578197479248047, "compression_ratio": 1.7615062761506277, "no_speech_prob": 0.00010428368841530755}, {"id": 81, "seek": 30616, "start": 328.0, "end": 329.0, "text": " enough board.", "tokens": [1547, 3150, 13], "temperature": 0.0, "avg_logprob": -0.16578197479248047, "compression_ratio": 1.7615062761506277, "no_speech_prob": 0.00010428368841530755}, {"id": 82, "seek": 30616, "start": 329.0, "end": 332.6, "text": " It's always the single point that you're missing.", "tokens": [467, 311, 1009, 264, 2167, 935, 300, 291, 434, 5361, 13], "temperature": 0.0, "avg_logprob": -0.16578197479248047, "compression_ratio": 1.7615062761506277, "no_speech_prob": 0.00010428368841530755}, {"id": 83, "seek": 33260, "start": 332.6, "end": 337.8, "text": " The same results are sent to Squad and when everything is finished, we have a full report", "tokens": [440, 912, 3542, 366, 2279, 281, 26596, 293, 562, 1203, 307, 4335, 11, 321, 362, 257, 1577, 2275], "temperature": 0.0, "avg_logprob": -0.14996152175100227, "compression_ratio": 1.7241379310344827, "no_speech_prob": 5.093938671052456e-05}, {"id": 84, "seek": 33260, "start": 337.8, "end": 343.04, "text": " that we can provide to the developers that we run something like thousands of tests,", "tokens": [300, 321, 393, 2893, 281, 264, 8849, 300, 321, 1190, 746, 411, 5383, 295, 6921, 11], "temperature": 0.0, "avg_logprob": -0.14996152175100227, "compression_ratio": 1.7241379310344827, "no_speech_prob": 5.093938671052456e-05}, {"id": 85, "seek": 33260, "start": 343.04, "end": 347.84000000000003, "text": " thousands of builds, and everything is working or we find some regressions.", "tokens": [5383, 295, 15182, 11, 293, 1203, 307, 1364, 420, 321, 915, 512, 1121, 735, 626, 13], "temperature": 0.0, "avg_logprob": -0.14996152175100227, "compression_ratio": 1.7241379310344827, "no_speech_prob": 5.093938671052456e-05}, {"id": 86, "seek": 33260, "start": 347.84000000000003, "end": 349.44, "text": " That's the overall architecture.", "tokens": [663, 311, 264, 4787, 9482, 13], "temperature": 0.0, "avg_logprob": -0.14996152175100227, "compression_ratio": 1.7241379310344827, "no_speech_prob": 5.093938671052456e-05}, {"id": 87, "seek": 33260, "start": 349.44, "end": 353.44, "text": " I will now look at the different projects so you can know if something can be useful", "tokens": [286, 486, 586, 574, 412, 264, 819, 4455, 370, 291, 393, 458, 498, 746, 393, 312, 4420], "temperature": 0.0, "avg_logprob": -0.14996152175100227, "compression_ratio": 1.7241379310344827, "no_speech_prob": 5.093938671052456e-05}, {"id": 88, "seek": 33260, "start": 353.44, "end": 355.24, "text": " for you.", "tokens": [337, 291, 13], "temperature": 0.0, "avg_logprob": -0.14996152175100227, "compression_ratio": 1.7241379310344827, "no_speech_prob": 5.093938671052456e-05}, {"id": 89, "seek": 33260, "start": 355.24, "end": 357.52000000000004, "text": " So let's look at the build parts.", "tokens": [407, 718, 311, 574, 412, 264, 1322, 3166, 13], "temperature": 0.0, "avg_logprob": -0.14996152175100227, "compression_ratio": 1.7241379310344827, "no_speech_prob": 5.093938671052456e-05}, {"id": 90, "seek": 33260, "start": 357.52000000000004, "end": 359.96000000000004, "text": " So as I said before, we use text make.", "tokens": [407, 382, 286, 848, 949, 11, 321, 764, 2487, 652, 13], "temperature": 0.0, "avg_logprob": -0.14996152175100227, "compression_ratio": 1.7241379310344827, "no_speech_prob": 5.093938671052456e-05}, {"id": 91, "seek": 35996, "start": 359.96, "end": 364.79999999999995, "text": " It's a project that we created to make building easy and reproducible.", "tokens": [467, 311, 257, 1716, 300, 321, 2942, 281, 652, 2390, 1858, 293, 11408, 32128, 13], "temperature": 0.0, "avg_logprob": -0.155691383420959, "compression_ratio": 1.7627118644067796, "no_speech_prob": 2.5853185434243642e-05}, {"id": 92, "seek": 35996, "start": 364.79999999999995, "end": 366.59999999999997, "text": " So it's an open source command application.", "tokens": [407, 309, 311, 364, 1269, 4009, 5622, 3861, 13], "temperature": 0.0, "avg_logprob": -0.155691383420959, "compression_ratio": 1.7627118644067796, "no_speech_prob": 2.5853185434243642e-05}, {"id": 93, "seek": 35996, "start": 366.59999999999997, "end": 370.64, "text": " It allows for portable and repeatable Linux kind of builds.", "tokens": [467, 4045, 337, 21800, 293, 7149, 712, 18734, 733, 295, 15182, 13], "temperature": 0.0, "avg_logprob": -0.155691383420959, "compression_ratio": 1.7627118644067796, "no_speech_prob": 2.5853185434243642e-05}, {"id": 94, "seek": 35996, "start": 370.64, "end": 372.44, "text": " So for that, we use containers.", "tokens": [407, 337, 300, 11, 321, 764, 17089, 13], "temperature": 0.0, "avg_logprob": -0.155691383420959, "compression_ratio": 1.7627118644067796, "no_speech_prob": 2.5853185434243642e-05}, {"id": 95, "seek": 35996, "start": 372.44, "end": 376.15999999999997, "text": " We provide a set of containers with all the tools you need inside and everything is done", "tokens": [492, 2893, 257, 992, 295, 17089, 365, 439, 264, 3873, 291, 643, 1854, 293, 1203, 307, 1096], "temperature": 0.0, "avg_logprob": -0.155691383420959, "compression_ratio": 1.7627118644067796, "no_speech_prob": 2.5853185434243642e-05}, {"id": 96, "seek": 35996, "start": 376.15999999999997, "end": 377.44, "text": " inside a container.", "tokens": [1854, 257, 10129, 13], "temperature": 0.0, "avg_logprob": -0.155691383420959, "compression_ratio": 1.7627118644067796, "no_speech_prob": 2.5853185434243642e-05}, {"id": 97, "seek": 35996, "start": 377.44, "end": 380.15999999999997, "text": " So it can be reproducible from one machine to another.", "tokens": [407, 309, 393, 312, 11408, 32128, 490, 472, 3479, 281, 1071, 13], "temperature": 0.0, "avg_logprob": -0.155691383420959, "compression_ratio": 1.7627118644067796, "no_speech_prob": 2.5853185434243642e-05}, {"id": 98, "seek": 35996, "start": 380.15999999999997, "end": 384.4, "text": " So because that's often a problem when you report a build failure, it's always a nightmare", "tokens": [407, 570, 300, 311, 2049, 257, 1154, 562, 291, 2275, 257, 1322, 7763, 11, 309, 311, 1009, 257, 18724], "temperature": 0.0, "avg_logprob": -0.155691383420959, "compression_ratio": 1.7627118644067796, "no_speech_prob": 2.5853185434243642e-05}, {"id": 99, "seek": 35996, "start": 384.4, "end": 387.12, "text": " to know the exact toolchain that you're using, everything.", "tokens": [281, 458, 264, 1900, 2290, 11509, 300, 291, 434, 1228, 11, 1203, 13], "temperature": 0.0, "avg_logprob": -0.155691383420959, "compression_ratio": 1.7627118644067796, "no_speech_prob": 2.5853185434243642e-05}, {"id": 100, "seek": 38712, "start": 387.12, "end": 392.8, "text": " So as everything is inside a container, you can just reproduce it in another machine.", "tokens": [407, 382, 1203, 307, 1854, 257, 10129, 11, 291, 393, 445, 29501, 309, 294, 1071, 3479, 13], "temperature": 0.0, "avg_logprob": -0.1855016725253215, "compression_ratio": 1.62890625, "no_speech_prob": 9.021653386298567e-05}, {"id": 101, "seek": 38712, "start": 392.8, "end": 399.08, "text": " So we support multiple toolchains from GCC from A to 12, client from 10 to 15.", "tokens": [407, 321, 1406, 3866, 2290, 339, 2315, 490, 460, 11717, 490, 316, 281, 2272, 11, 6423, 490, 1266, 281, 2119, 13], "temperature": 0.0, "avg_logprob": -0.1855016725253215, "compression_ratio": 1.62890625, "no_speech_prob": 9.021653386298567e-05}, {"id": 102, "seek": 38712, "start": 399.08, "end": 401.6, "text": " In fact, 16 has been added this week.", "tokens": [682, 1186, 11, 3165, 575, 668, 3869, 341, 1243, 13], "temperature": 0.0, "avg_logprob": -0.1855016725253215, "compression_ratio": 1.62890625, "no_speech_prob": 9.021653386298567e-05}, {"id": 103, "seek": 38712, "start": 401.6, "end": 405.92, "text": " We also have a Clang Android version and a Clang Nightly.", "tokens": [492, 611, 362, 257, 2033, 656, 8853, 3037, 293, 257, 2033, 656, 10190, 356, 13], "temperature": 0.0, "avg_logprob": -0.1855016725253215, "compression_ratio": 1.62890625, "no_speech_prob": 9.021653386298567e-05}, {"id": 104, "seek": 38712, "start": 405.92, "end": 411.8, "text": " Clang Nightly is specific because we rebuild the nightly Clang toolchain every night and", "tokens": [2033, 656, 10190, 356, 307, 2685, 570, 321, 16877, 264, 1818, 356, 2033, 656, 2290, 11509, 633, 1818, 293], "temperature": 0.0, "avg_logprob": -0.1855016725253215, "compression_ratio": 1.62890625, "no_speech_prob": 9.021653386298567e-05}, {"id": 105, "seek": 38712, "start": 411.8, "end": 416.64, "text": " we push it to our system so we can just test with the latest Clang.", "tokens": [321, 2944, 309, 281, 527, 1185, 370, 321, 393, 445, 1500, 365, 264, 6792, 2033, 656, 13], "temperature": 0.0, "avg_logprob": -0.1855016725253215, "compression_ratio": 1.62890625, "no_speech_prob": 9.021653386298567e-05}, {"id": 106, "seek": 41664, "start": 416.64, "end": 422.15999999999997, "text": " We also support multiple target architectures, all the ARM versions, Intel EMDs, and then", "tokens": [492, 611, 1406, 3866, 3779, 6331, 1303, 11, 439, 264, 45209, 9606, 11, 19762, 16237, 35, 82, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.2491738858430282, "compression_ratio": 1.5416666666666667, "no_speech_prob": 4.0717655792832375e-05}, {"id": 107, "seek": 41664, "start": 422.15999999999997, "end": 430.03999999999996, "text": " some MIPS, PowerPC, RISV5, and some exotic one like S390, SH4, things like that.", "tokens": [512, 13696, 6273, 11, 7086, 12986, 11, 497, 2343, 53, 20, 11, 293, 512, 27063, 472, 411, 318, 18, 7771, 11, 7405, 19, 11, 721, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.2491738858430282, "compression_ratio": 1.5416666666666667, "no_speech_prob": 4.0717655792832375e-05}, {"id": 108, "seek": 41664, "start": 430.03999999999996, "end": 431.91999999999996, "text": " So building is really simple.", "tokens": [407, 2390, 307, 534, 2199, 13], "temperature": 0.0, "avg_logprob": -0.2491738858430282, "compression_ratio": 1.5416666666666667, "no_speech_prob": 4.0717655792832375e-05}, {"id": 109, "seek": 41664, "start": 431.91999999999996, "end": 436.64, "text": " You just specify the target architectures, so X8664 in this case.", "tokens": [509, 445, 16500, 264, 3779, 6331, 1303, 11, 370, 1783, 22193, 19395, 294, 341, 1389, 13], "temperature": 0.0, "avg_logprob": -0.2491738858430282, "compression_ratio": 1.5416666666666667, "no_speech_prob": 4.0717655792832375e-05}, {"id": 110, "seek": 41664, "start": 436.64, "end": 439.84, "text": " You specify the toolchain, so I want to use GCC12.", "tokens": [509, 16500, 264, 2290, 11509, 11, 370, 286, 528, 281, 764, 460, 11717, 4762, 13], "temperature": 0.0, "avg_logprob": -0.2491738858430282, "compression_ratio": 1.5416666666666667, "no_speech_prob": 4.0717655792832375e-05}, {"id": 111, "seek": 41664, "start": 439.84, "end": 443.44, "text": " You just need to have text-making installed on your computer because everything will then", "tokens": [509, 445, 643, 281, 362, 2487, 12, 12402, 8899, 322, 428, 3820, 570, 1203, 486, 550], "temperature": 0.0, "avg_logprob": -0.2491738858430282, "compression_ratio": 1.5416666666666667, "no_speech_prob": 4.0717655792832375e-05}, {"id": 112, "seek": 44344, "start": 443.44, "end": 449.6, "text": " be done inside a container where you will have GCC12 to chain for X8664.", "tokens": [312, 1096, 1854, 257, 10129, 689, 291, 486, 362, 460, 11717, 4762, 281, 5021, 337, 1783, 22193, 19395, 13], "temperature": 0.0, "avg_logprob": -0.15664469401041667, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.00010516141628613696}, {"id": 113, "seek": 44344, "start": 449.6, "end": 454.64, "text": " If you want to build with GCC13, just change toolchain to GCC13 and it will use another", "tokens": [759, 291, 528, 281, 1322, 365, 460, 11717, 7668, 11, 445, 1319, 2290, 11509, 281, 460, 11717, 7668, 293, 309, 486, 764, 1071], "temperature": 0.0, "avg_logprob": -0.15664469401041667, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.00010516141628613696}, {"id": 114, "seek": 44344, "start": 454.64, "end": 457.44, "text": " container to build it.", "tokens": [10129, 281, 1322, 309, 13], "temperature": 0.0, "avg_logprob": -0.15664469401041667, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.00010516141628613696}, {"id": 115, "seek": 44344, "start": 457.44, "end": 461.24, "text": " As I said before, we have a private software that allows to run text-making at a large", "tokens": [1018, 286, 848, 949, 11, 321, 362, 257, 4551, 4722, 300, 4045, 281, 1190, 2487, 12, 12402, 412, 257, 2416], "temperature": 0.0, "avg_logprob": -0.15664469401041667, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.00010516141628613696}, {"id": 116, "seek": 44344, "start": 461.24, "end": 467.08, "text": " scale in the cloud, but I'm not presenting that it's a close-up software.", "tokens": [4373, 294, 264, 4588, 11, 457, 286, 478, 406, 15578, 300, 309, 311, 257, 1998, 12, 1010, 4722, 13], "temperature": 0.0, "avg_logprob": -0.15664469401041667, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.00010516141628613696}, {"id": 117, "seek": 44344, "start": 467.08, "end": 471.56, "text": " So just to explain how it's working, text-making will pull the right container for you.", "tokens": [407, 445, 281, 2903, 577, 309, 311, 1364, 11, 2487, 12, 12402, 486, 2235, 264, 558, 10129, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.15664469401041667, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.00010516141628613696}, {"id": 118, "seek": 47156, "start": 471.56, "end": 479.36, "text": " So for this specific target-arched toolchain couple, it will be X8664 GCC12 container.", "tokens": [407, 337, 341, 2685, 3779, 12, 1178, 292, 2290, 11509, 1916, 11, 309, 486, 312, 1783, 22193, 19395, 460, 11717, 4762, 10129, 13], "temperature": 0.0, "avg_logprob": -0.19314438501993816, "compression_ratio": 1.657992565055762, "no_speech_prob": 6.989103712840006e-05}, {"id": 119, "seek": 47156, "start": 479.36, "end": 481.88, "text": " We have thousands of containers, hundreds of containers.", "tokens": [492, 362, 5383, 295, 17089, 11, 6779, 295, 17089, 13], "temperature": 0.0, "avg_logprob": -0.19314438501993816, "compression_ratio": 1.657992565055762, "no_speech_prob": 6.989103712840006e-05}, {"id": 120, "seek": 47156, "start": 481.88, "end": 487.48, "text": " It will create a unique build directory, so it's reproducible from one build to another.", "tokens": [467, 486, 1884, 257, 3845, 1322, 21120, 11, 370, 309, 311, 11408, 32128, 490, 472, 1322, 281, 1071, 13], "temperature": 0.0, "avg_logprob": -0.19314438501993816, "compression_ratio": 1.657992565055762, "no_speech_prob": 6.989103712840006e-05}, {"id": 121, "seek": 47156, "start": 487.48, "end": 491.32, "text": " And then we just start a podman container, jump into it, and just build.", "tokens": [400, 550, 321, 445, 722, 257, 2497, 1601, 10129, 11, 3012, 666, 309, 11, 293, 445, 1322, 13], "temperature": 0.0, "avg_logprob": -0.19314438501993816, "compression_ratio": 1.657992565055762, "no_speech_prob": 6.989103712840006e-05}, {"id": 122, "seek": 47156, "start": 491.32, "end": 496.24, "text": " We advise to use podman, obviously, and not docker because it will be a rootless container,", "tokens": [492, 18312, 281, 764, 2497, 1601, 11, 2745, 11, 293, 406, 360, 9178, 570, 309, 486, 312, 257, 5593, 1832, 10129, 11], "temperature": 0.0, "avg_logprob": -0.19314438501993816, "compression_ratio": 1.657992565055762, "no_speech_prob": 6.989103712840006e-05}, {"id": 123, "seek": 47156, "start": 496.24, "end": 500.48, "text": " so you can at least don't run asboot your build.", "tokens": [370, 291, 393, 412, 1935, 500, 380, 1190, 382, 1763, 310, 428, 1322, 13], "temperature": 0.0, "avg_logprob": -0.19314438501993816, "compression_ratio": 1.657992565055762, "no_speech_prob": 6.989103712840006e-05}, {"id": 124, "seek": 50048, "start": 500.48, "end": 505.68, "text": " And then it will invoke a set of different make comments depending on what you want to", "tokens": [400, 550, 309, 486, 41117, 257, 992, 295, 819, 652, 3053, 5413, 322, 437, 291, 528, 281], "temperature": 0.0, "avg_logprob": -0.1845517117752988, "compression_ratio": 1.8937007874015748, "no_speech_prob": 5.3255385864758864e-05}, {"id": 125, "seek": 50048, "start": 505.68, "end": 506.68, "text": " build.", "tokens": [1322, 13], "temperature": 0.0, "avg_logprob": -0.1845517117752988, "compression_ratio": 1.8937007874015748, "no_speech_prob": 5.3255385864758864e-05}, {"id": 126, "seek": 50048, "start": 506.68, "end": 511.96000000000004, "text": " And then it will move everything to a specific directory that will be kept on the machine.", "tokens": [400, 550, 309, 486, 1286, 1203, 281, 257, 2685, 21120, 300, 486, 312, 4305, 322, 264, 3479, 13], "temperature": 0.0, "avg_logprob": -0.1845517117752988, "compression_ratio": 1.8937007874015748, "no_speech_prob": 5.3255385864758864e-05}, {"id": 127, "seek": 50048, "start": 511.96000000000004, "end": 514.6800000000001, "text": " And you will have all the artifacts, kernel, headers, et cetera.", "tokens": [400, 291, 486, 362, 439, 264, 24617, 11, 28256, 11, 45101, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.1845517117752988, "compression_ratio": 1.8937007874015748, "no_speech_prob": 5.3255385864758864e-05}, {"id": 128, "seek": 50048, "start": 514.6800000000001, "end": 518.84, "text": " And you also have metadata.json file that will include a lot of metadata about your", "tokens": [400, 291, 611, 362, 26603, 13, 73, 3015, 3991, 300, 486, 4090, 257, 688, 295, 26603, 466, 428], "temperature": 0.0, "avg_logprob": -0.1845517117752988, "compression_ratio": 1.8937007874015748, "no_speech_prob": 5.3255385864758864e-05}, {"id": 129, "seek": 50048, "start": 518.84, "end": 524.84, "text": " build, like version of your toolchain, of different utilities on the machine, the time", "tokens": [1322, 11, 411, 3037, 295, 428, 2290, 11509, 11, 295, 819, 30482, 322, 264, 3479, 11, 264, 565], "temperature": 0.0, "avg_logprob": -0.1845517117752988, "compression_ratio": 1.8937007874015748, "no_speech_prob": 5.3255385864758864e-05}, {"id": 130, "seek": 50048, "start": 524.84, "end": 528.44, "text": " taken by different steps, the size of everything, et cetera.", "tokens": [2726, 538, 819, 4439, 11, 264, 2744, 295, 1203, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.1845517117752988, "compression_ratio": 1.8937007874015748, "no_speech_prob": 5.3255385864758864e-05}, {"id": 131, "seek": 52844, "start": 528.44, "end": 533.9200000000001, "text": " And it will be useful for debugging also what's going on, if something breaks.", "tokens": [400, 309, 486, 312, 4420, 337, 45592, 611, 437, 311, 516, 322, 11, 498, 746, 9857, 13], "temperature": 0.0, "avg_logprob": -0.23428638164813703, "compression_ratio": 1.7256944444444444, "no_speech_prob": 0.00018265165272168815}, {"id": 132, "seek": 52844, "start": 533.9200000000001, "end": 537.84, "text": " And yeah, we provide multiple containers that you can reuse.", "tokens": [400, 1338, 11, 321, 2893, 3866, 17089, 300, 291, 393, 26225, 13], "temperature": 0.0, "avg_logprob": -0.23428638164813703, "compression_ratio": 1.7256944444444444, "no_speech_prob": 0.00018265165272168815}, {"id": 133, "seek": 52844, "start": 537.84, "end": 541.24, "text": " And it's an open source project, so you can contribute to it a few months, and you can", "tokens": [400, 309, 311, 364, 1269, 4009, 1716, 11, 370, 291, 393, 10586, 281, 309, 257, 1326, 2493, 11, 293, 291, 393], "temperature": 0.0, "avg_logprob": -0.23428638164813703, "compression_ratio": 1.7256944444444444, "no_speech_prob": 0.00018265165272168815}, {"id": 134, "seek": 52844, "start": 541.24, "end": 542.44, "text": " just use it right now.", "tokens": [445, 764, 309, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.23428638164813703, "compression_ratio": 1.7256944444444444, "no_speech_prob": 0.00018265165272168815}, {"id": 135, "seek": 52844, "start": 542.44, "end": 548.08, "text": " And some kind of developer use it for reproducing builds, build failures.", "tokens": [400, 512, 733, 295, 10754, 764, 309, 337, 11408, 2175, 15182, 11, 1322, 20774, 13], "temperature": 0.0, "avg_logprob": -0.23428638164813703, "compression_ratio": 1.7256944444444444, "no_speech_prob": 0.00018265165272168815}, {"id": 136, "seek": 52844, "start": 548.08, "end": 552.44, "text": " And in fact, as I said, we have a client-nightly toolchain that is rebuilt nightly.", "tokens": [400, 294, 1186, 11, 382, 286, 848, 11, 321, 362, 257, 6423, 12, 6402, 356, 2290, 11509, 300, 307, 38532, 1818, 356, 13], "temperature": 0.0, "avg_logprob": -0.23428638164813703, "compression_ratio": 1.7256944444444444, "no_speech_prob": 0.00018265165272168815}, {"id": 137, "seek": 52844, "start": 552.44, "end": 558.2800000000001, "text": " It's in fact because the client project asked us to do that because they use Tuxmake with", "tokens": [467, 311, 294, 1186, 570, 264, 6423, 1716, 2351, 505, 281, 360, 300, 570, 436, 764, 314, 2449, 38705, 365], "temperature": 0.0, "avg_logprob": -0.23428638164813703, "compression_ratio": 1.7256944444444444, "no_speech_prob": 0.00018265165272168815}, {"id": 138, "seek": 55828, "start": 558.28, "end": 564.3199999999999, "text": " client-nightly for validating their client version against different kernel versions", "tokens": [6423, 12, 6402, 356, 337, 7363, 990, 641, 6423, 3037, 1970, 819, 28256, 9606], "temperature": 0.0, "avg_logprob": -0.19447531581910188, "compression_ratio": 1.6987951807228916, "no_speech_prob": 6.437668344005942e-05}, {"id": 139, "seek": 55828, "start": 564.3199999999999, "end": 568.0, "text": " to see if clang is not regression.", "tokens": [281, 536, 498, 596, 656, 307, 406, 24590, 13], "temperature": 0.0, "avg_logprob": -0.19447531581910188, "compression_ratio": 1.6987951807228916, "no_speech_prob": 6.437668344005942e-05}, {"id": 140, "seek": 55828, "start": 568.0, "end": 569.28, "text": " That's for building.", "tokens": [663, 311, 337, 2390, 13], "temperature": 0.0, "avg_logprob": -0.19447531581910188, "compression_ratio": 1.6987951807228916, "no_speech_prob": 6.437668344005942e-05}, {"id": 141, "seek": 55828, "start": 569.28, "end": 570.72, "text": " So now, how do we test?", "tokens": [407, 586, 11, 577, 360, 321, 1500, 30], "temperature": 0.0, "avg_logprob": -0.19447531581910188, "compression_ratio": 1.6987951807228916, "no_speech_prob": 6.437668344005942e-05}, {"id": 142, "seek": 55828, "start": 570.72, "end": 576.68, "text": " So as I said, we test on virtual devices with Tuxrun and on physical devices with Lava.", "tokens": [407, 382, 286, 848, 11, 321, 1500, 322, 6374, 5759, 365, 314, 2449, 12997, 293, 322, 4001, 5759, 365, 441, 4061, 13], "temperature": 0.0, "avg_logprob": -0.19447531581910188, "compression_ratio": 1.6987951807228916, "no_speech_prob": 6.437668344005942e-05}, {"id": 143, "seek": 55828, "start": 576.68, "end": 578.8399999999999, "text": " So for Tuxrun, it's the same.", "tokens": [407, 337, 314, 2449, 12997, 11, 309, 311, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.19447531581910188, "compression_ratio": 1.6987951807228916, "no_speech_prob": 6.437668344005942e-05}, {"id": 144, "seek": 55828, "start": 578.8399999999999, "end": 581.6, "text": " It's an open source common line application.", "tokens": [467, 311, 364, 1269, 4009, 2689, 1622, 3861, 13], "temperature": 0.0, "avg_logprob": -0.19447531581910188, "compression_ratio": 1.6987951807228916, "no_speech_prob": 6.437668344005942e-05}, {"id": 145, "seek": 55828, "start": 581.6, "end": 584.16, "text": " It's the same for Tuxmake, but for running.", "tokens": [467, 311, 264, 912, 337, 314, 2449, 38705, 11, 457, 337, 2614, 13], "temperature": 0.0, "avg_logprob": -0.19447531581910188, "compression_ratio": 1.6987951807228916, "no_speech_prob": 6.437668344005942e-05}, {"id": 146, "seek": 55828, "start": 584.16, "end": 587.72, "text": " It allows for portable and repeatable kernel tests.", "tokens": [467, 4045, 337, 21800, 293, 7149, 712, 28256, 6921, 13], "temperature": 0.0, "avg_logprob": -0.19447531581910188, "compression_ratio": 1.6987951807228916, "no_speech_prob": 6.437668344005942e-05}, {"id": 147, "seek": 58772, "start": 587.72, "end": 596.48, "text": " We support multiple devices, MVP MVA, which is an ARM V9.3 emulator, a simulator.", "tokens": [492, 1406, 3866, 5759, 11, 37151, 17663, 32, 11, 597, 307, 364, 45209, 691, 24, 13, 18, 846, 16381, 11, 257, 32974, 13], "temperature": 0.0, "avg_logprob": -0.31203857576004185, "compression_ratio": 1.603864734299517, "no_speech_prob": 4.693207301897928e-05}, {"id": 148, "seek": 58772, "start": 596.48, "end": 599.64, "text": " That's the latest version that you can try for ARM.", "tokens": [663, 311, 264, 6792, 3037, 300, 291, 393, 853, 337, 45209, 13], "temperature": 0.0, "avg_logprob": -0.31203857576004185, "compression_ratio": 1.603864734299517, "no_speech_prob": 4.693207301897928e-05}, {"id": 149, "seek": 58772, "start": 599.64, "end": 605.36, "text": " And then multiple ARM versions with multiple QEMU devices.", "tokens": [400, 550, 3866, 45209, 9606, 365, 3866, 1249, 6683, 52, 5759, 13], "temperature": 0.0, "avg_logprob": -0.31203857576004185, "compression_ratio": 1.603864734299517, "no_speech_prob": 4.693207301897928e-05}, {"id": 150, "seek": 58772, "start": 605.36, "end": 611.44, "text": " Many ARM Intel MIPS in many different versions and PPC, et cetera, and multiple tests with", "tokens": [5126, 45209, 19762, 13696, 6273, 294, 867, 819, 9606, 293, 430, 12986, 11, 1030, 11458, 11, 293, 3866, 6921, 365], "temperature": 0.0, "avg_logprob": -0.31203857576004185, "compression_ratio": 1.603864734299517, "no_speech_prob": 4.693207301897928e-05}, {"id": 151, "seek": 58772, "start": 611.44, "end": 614.9200000000001, "text": " LTP, K-Unit, K-Self tests, et cetera, et cetera.", "tokens": [441, 16804, 11, 591, 12, 12405, 270, 11, 591, 12, 50, 1967, 6921, 11, 1030, 11458, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.31203857576004185, "compression_ratio": 1.603864734299517, "no_speech_prob": 4.693207301897928e-05}, {"id": 152, "seek": 61492, "start": 614.92, "end": 618.1999999999999, "text": " Adding one is not quite easy to do.", "tokens": [31204, 472, 307, 406, 1596, 1858, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.18207206726074218, "compression_ratio": 1.6870229007633588, "no_speech_prob": 6.845026655355468e-05}, {"id": 153, "seek": 61492, "start": 618.1999999999999, "end": 620.12, "text": " The same, the common line is quite simple.", "tokens": [440, 912, 11, 264, 2689, 1622, 307, 1596, 2199, 13], "temperature": 0.0, "avg_logprob": -0.18207206726074218, "compression_ratio": 1.6870229007633588, "no_speech_prob": 6.845026655355468e-05}, {"id": 154, "seek": 61492, "start": 620.12, "end": 624.88, "text": " We also use Sponman for containerizing everything.", "tokens": [492, 611, 764, 1738, 266, 1601, 337, 10129, 3319, 1203, 13], "temperature": 0.0, "avg_logprob": -0.18207206726074218, "compression_ratio": 1.6870229007633588, "no_speech_prob": 6.845026655355468e-05}, {"id": 155, "seek": 61492, "start": 624.88, "end": 627.8, "text": " You specify the device that you want to use, the kernel that you want.", "tokens": [509, 16500, 264, 4302, 300, 291, 528, 281, 764, 11, 264, 28256, 300, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.18207206726074218, "compression_ratio": 1.6870229007633588, "no_speech_prob": 6.845026655355468e-05}, {"id": 156, "seek": 61492, "start": 627.8, "end": 632.1999999999999, "text": " It can be your URL, obviously, and a root file system also if you want.", "tokens": [467, 393, 312, 428, 12905, 11, 2745, 11, 293, 257, 5593, 3991, 1185, 611, 498, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.18207206726074218, "compression_ratio": 1.6870229007633588, "no_speech_prob": 6.845026655355468e-05}, {"id": 157, "seek": 61492, "start": 632.1999999999999, "end": 638.28, "text": " And again, we have a SAS that allows to run that at large scale in the cloud.", "tokens": [400, 797, 11, 321, 362, 257, 33441, 300, 4045, 281, 1190, 300, 412, 2416, 4373, 294, 264, 4588, 13], "temperature": 0.0, "avg_logprob": -0.18207206726074218, "compression_ratio": 1.6870229007633588, "no_speech_prob": 6.845026655355468e-05}, {"id": 158, "seek": 61492, "start": 638.28, "end": 642.92, "text": " When you call that, that common line, Tuxrun will download all the artifacts that you need.", "tokens": [1133, 291, 818, 300, 11, 300, 2689, 1622, 11, 314, 2449, 12997, 486, 5484, 439, 264, 24617, 300, 291, 643, 13], "temperature": 0.0, "avg_logprob": -0.18207206726074218, "compression_ratio": 1.6870229007633588, "no_speech_prob": 6.845026655355468e-05}, {"id": 159, "seek": 64292, "start": 642.92, "end": 646.0799999999999, "text": " So kernel, DTB, root file system modules.", "tokens": [407, 28256, 11, 413, 51, 33, 11, 5593, 3991, 1185, 16679, 13], "temperature": 0.0, "avg_logprob": -0.205567995707194, "compression_ratio": 1.7215686274509805, "no_speech_prob": 5.234329364611767e-05}, {"id": 160, "seek": 64292, "start": 646.0799999999999, "end": 650.76, "text": " It will inject the modules inside the root file system for you, so that it will be used", "tokens": [467, 486, 10711, 264, 16679, 1854, 264, 5593, 3991, 1185, 337, 291, 11, 370, 300, 309, 486, 312, 1143], "temperature": 0.0, "avg_logprob": -0.205567995707194, "compression_ratio": 1.7215686274509805, "no_speech_prob": 5.234329364611767e-05}, {"id": 161, "seek": 64292, "start": 650.76, "end": 652.4399999999999, "text": " at a good time.", "tokens": [412, 257, 665, 565, 13], "temperature": 0.0, "avg_logprob": -0.205567995707194, "compression_ratio": 1.7215686274509805, "no_speech_prob": 5.234329364611767e-05}, {"id": 162, "seek": 64292, "start": 652.4399999999999, "end": 656.88, "text": " And start the container, start QEMU system, so R64 in this case.", "tokens": [400, 722, 264, 10129, 11, 722, 1249, 6683, 52, 1185, 11, 370, 497, 19395, 294, 341, 1389, 13], "temperature": 0.0, "avg_logprob": -0.205567995707194, "compression_ratio": 1.7215686274509805, "no_speech_prob": 5.234329364611767e-05}, {"id": 163, "seek": 64292, "start": 656.88, "end": 663.12, "text": " Look at the outputs, et cetera, et cetera, all the classical things, and store the results.", "tokens": [2053, 412, 264, 23930, 11, 1030, 11458, 11, 1030, 11458, 11, 439, 264, 13735, 721, 11, 293, 3531, 264, 3542, 13], "temperature": 0.0, "avg_logprob": -0.205567995707194, "compression_ratio": 1.7215686274509805, "no_speech_prob": 5.234329364611767e-05}, {"id": 164, "seek": 64292, "start": 663.12, "end": 668.76, "text": " As I said, we provide a lot of root file systems because we know it's painful to build your", "tokens": [1018, 286, 848, 11, 321, 2893, 257, 688, 295, 5593, 3991, 3652, 570, 321, 458, 309, 311, 11697, 281, 1322, 428], "temperature": 0.0, "avg_logprob": -0.205567995707194, "compression_ratio": 1.7215686274509805, "no_speech_prob": 5.234329364611767e-05}, {"id": 165, "seek": 64292, "start": 668.76, "end": 671.3199999999999, "text": " root file system for multiple architectures.", "tokens": [5593, 3991, 1185, 337, 3866, 6331, 1303, 13], "temperature": 0.0, "avg_logprob": -0.205567995707194, "compression_ratio": 1.7215686274509805, "no_speech_prob": 5.234329364611767e-05}, {"id": 166, "seek": 67132, "start": 671.32, "end": 673.1600000000001, "text": " So we do the work for that.", "tokens": [407, 321, 360, 264, 589, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.15768761761420597, "compression_ratio": 1.8461538461538463, "no_speech_prob": 5.347804835764691e-05}, {"id": 167, "seek": 67132, "start": 673.1600000000001, "end": 675.6400000000001, "text": " We use billroot and debian.", "tokens": [492, 764, 2961, 44147, 293, 3001, 952, 13], "temperature": 0.0, "avg_logprob": -0.15768761761420597, "compression_ratio": 1.8461538461538463, "no_speech_prob": 5.347804835764691e-05}, {"id": 168, "seek": 67132, "start": 675.6400000000001, "end": 681.12, "text": " Billroot allows us to have the 19 supported architectures, one root file system for each.", "tokens": [5477, 44147, 4045, 505, 281, 362, 264, 1294, 8104, 6331, 1303, 11, 472, 5593, 3991, 1185, 337, 1184, 13], "temperature": 0.0, "avg_logprob": -0.15768761761420597, "compression_ratio": 1.8461538461538463, "no_speech_prob": 5.347804835764691e-05}, {"id": 169, "seek": 67132, "start": 681.12, "end": 684.88, "text": " And for the main one, the one supported by debian, we do provide the debian root file", "tokens": [400, 337, 264, 2135, 472, 11, 264, 472, 8104, 538, 3001, 952, 11, 321, 360, 2893, 264, 3001, 952, 5593, 3991], "temperature": 0.0, "avg_logprob": -0.15768761761420597, "compression_ratio": 1.8461538461538463, "no_speech_prob": 5.347804835764691e-05}, {"id": 170, "seek": 67132, "start": 684.88, "end": 686.6400000000001, "text": " system that we build.", "tokens": [1185, 300, 321, 1322, 13], "temperature": 0.0, "avg_logprob": -0.15768761761420597, "compression_ratio": 1.8461538461538463, "no_speech_prob": 5.347804835764691e-05}, {"id": 171, "seek": 67132, "start": 686.6400000000001, "end": 690.72, "text": " And obviously, if you build your own one, you can use it if you want.", "tokens": [400, 2745, 11, 498, 291, 1322, 428, 1065, 472, 11, 291, 393, 764, 309, 498, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.15768761761420597, "compression_ratio": 1.8461538461538463, "no_speech_prob": 5.347804835764691e-05}, {"id": 172, "seek": 67132, "start": 690.72, "end": 696.44, "text": " And we will do the job of rebuilding the billroot and debian file systems regularly.", "tokens": [400, 321, 486, 360, 264, 1691, 295, 36717, 264, 2961, 44147, 293, 3001, 952, 3991, 3652, 11672, 13], "temperature": 0.0, "avg_logprob": -0.15768761761420597, "compression_ratio": 1.8461538461538463, "no_speech_prob": 5.347804835764691e-05}, {"id": 173, "seek": 69644, "start": 696.44, "end": 702.7600000000001, "text": " And in fact, it's a fun thing, we actually found bugs in QEMU before pushing the new", "tokens": [400, 294, 1186, 11, 309, 311, 257, 1019, 551, 11, 321, 767, 1352, 15120, 294, 1249, 6683, 52, 949, 7380, 264, 777], "temperature": 0.0, "avg_logprob": -0.19969270125679348, "compression_ratio": 1.6065573770491803, "no_speech_prob": 9.550909453537315e-05}, {"id": 174, "seek": 69644, "start": 702.7600000000001, "end": 703.7600000000001, "text": " file systems.", "tokens": [3991, 3652, 13], "temperature": 0.0, "avg_logprob": -0.19969270125679348, "compression_ratio": 1.6065573770491803, "no_speech_prob": 9.550909453537315e-05}, {"id": 175, "seek": 69644, "start": 703.7600000000001, "end": 707.2800000000001, "text": " We test in our system with the new root file systems.", "tokens": [492, 1500, 294, 527, 1185, 365, 264, 777, 5593, 3991, 3652, 13], "temperature": 0.0, "avg_logprob": -0.19969270125679348, "compression_ratio": 1.6065573770491803, "no_speech_prob": 9.550909453537315e-05}, {"id": 176, "seek": 69644, "start": 707.2800000000001, "end": 712.6400000000001, "text": " And the last time we did that, we found issues in QEMU 7.2 that are currently being fixed", "tokens": [400, 264, 1036, 565, 321, 630, 300, 11, 321, 1352, 2663, 294, 1249, 6683, 52, 1614, 13, 17, 300, 366, 4362, 885, 6806], "temperature": 0.0, "avg_logprob": -0.19969270125679348, "compression_ratio": 1.6065573770491803, "no_speech_prob": 9.550909453537315e-05}, {"id": 177, "seek": 69644, "start": 712.6400000000001, "end": 717.4000000000001, "text": " by QEMU developers.", "tokens": [538, 1249, 6683, 52, 8849, 13], "temperature": 0.0, "avg_logprob": -0.19969270125679348, "compression_ratio": 1.6065573770491803, "no_speech_prob": 9.550909453537315e-05}, {"id": 178, "seek": 69644, "start": 717.4000000000001, "end": 720.5200000000001, "text": " Something fun because Tux-Mech and Tux-Run has been done by the same team.", "tokens": [6595, 1019, 570, 314, 2449, 12, 44, 5023, 293, 314, 2449, 12, 42639, 575, 668, 1096, 538, 264, 912, 1469, 13], "temperature": 0.0, "avg_logprob": -0.19969270125679348, "compression_ratio": 1.6065573770491803, "no_speech_prob": 9.550909453537315e-05}, {"id": 179, "seek": 69644, "start": 720.5200000000001, "end": 725.0400000000001, "text": " So we make the work to combine the two tools together.", "tokens": [407, 321, 652, 264, 589, 281, 10432, 264, 732, 3873, 1214, 13], "temperature": 0.0, "avg_logprob": -0.19969270125679348, "compression_ratio": 1.6065573770491803, "no_speech_prob": 9.550909453537315e-05}, {"id": 180, "seek": 72504, "start": 725.04, "end": 730.48, "text": " So obviously, you can, doing a bisection of a build failure is quite easy.", "tokens": [407, 2745, 11, 291, 393, 11, 884, 257, 7393, 10183, 295, 257, 1322, 7763, 307, 1596, 1858, 13], "temperature": 0.0, "avg_logprob": -0.18000779724121094, "compression_ratio": 1.6303501945525292, "no_speech_prob": 7.993486360646784e-05}, {"id": 181, "seek": 72504, "start": 730.48, "end": 733.92, "text": " You just need a lot of CPU time.", "tokens": [509, 445, 643, 257, 688, 295, 13199, 565, 13], "temperature": 0.0, "avg_logprob": -0.18000779724121094, "compression_ratio": 1.6303501945525292, "no_speech_prob": 7.993486360646784e-05}, {"id": 182, "seek": 72504, "start": 733.92, "end": 739.7199999999999, "text": " Same for a runtime issue, which is you find a regression where a test fail on a specific", "tokens": [10635, 337, 257, 34474, 2734, 11, 597, 307, 291, 915, 257, 24590, 689, 257, 1500, 3061, 322, 257, 2685], "temperature": 0.0, "avg_logprob": -0.18000779724121094, "compression_ratio": 1.6303501945525292, "no_speech_prob": 7.993486360646784e-05}, {"id": 183, "seek": 72504, "start": 739.7199999999999, "end": 740.7199999999999, "text": " architecture.", "tokens": [9482, 13], "temperature": 0.0, "avg_logprob": -0.18000779724121094, "compression_ratio": 1.6303501945525292, "no_speech_prob": 7.993486360646784e-05}, {"id": 184, "seek": 72504, "start": 740.7199999999999, "end": 745.92, "text": " For example, when you run a LTP test suite on QEMU ARM64, it's failing.", "tokens": [1171, 1365, 11, 562, 291, 1190, 257, 441, 16804, 1500, 14205, 322, 1249, 6683, 52, 45209, 19395, 11, 309, 311, 18223, 13], "temperature": 0.0, "avg_logprob": -0.18000779724121094, "compression_ratio": 1.6303501945525292, "no_speech_prob": 7.993486360646784e-05}, {"id": 185, "seek": 72504, "start": 745.92, "end": 747.4399999999999, "text": " And you want to bisect that.", "tokens": [400, 291, 528, 281, 7393, 557, 300, 13], "temperature": 0.0, "avg_logprob": -0.18000779724121094, "compression_ratio": 1.6303501945525292, "no_speech_prob": 7.993486360646784e-05}, {"id": 186, "seek": 72504, "start": 747.4399999999999, "end": 748.64, "text": " So find the faulty commit.", "tokens": [407, 915, 264, 2050, 5773, 5599, 13], "temperature": 0.0, "avg_logprob": -0.18000779724121094, "compression_ratio": 1.6303501945525292, "no_speech_prob": 7.993486360646784e-05}, {"id": 187, "seek": 72504, "start": 748.64, "end": 750.4399999999999, "text": " You have a good commit and a bad commit.", "tokens": [509, 362, 257, 665, 5599, 293, 257, 1578, 5599, 13], "temperature": 0.0, "avg_logprob": -0.18000779724121094, "compression_ratio": 1.6303501945525292, "no_speech_prob": 7.993486360646784e-05}, {"id": 188, "seek": 72504, "start": 750.4399999999999, "end": 753.0799999999999, "text": " And you want to find the faulty commit.", "tokens": [400, 291, 528, 281, 915, 264, 2050, 5773, 5599, 13], "temperature": 0.0, "avg_logprob": -0.18000779724121094, "compression_ratio": 1.6303501945525292, "no_speech_prob": 7.993486360646784e-05}, {"id": 189, "seek": 75308, "start": 753.08, "end": 756.1600000000001, "text": " Git allows you to help you on that.", "tokens": [16939, 4045, 291, 281, 854, 291, 322, 300, 13], "temperature": 0.0, "avg_logprob": -0.1722385021818786, "compression_ratio": 1.7370689655172413, "no_speech_prob": 8.18911285023205e-05}, {"id": 190, "seek": 75308, "start": 756.1600000000001, "end": 760.84, "text": " But thanks to Tux-Mech and Tux-Run, we can automate all that job of testing.", "tokens": [583, 3231, 281, 314, 2449, 12, 44, 5023, 293, 314, 2449, 12, 42639, 11, 321, 393, 31605, 439, 300, 1691, 295, 4997, 13], "temperature": 0.0, "avg_logprob": -0.1722385021818786, "compression_ratio": 1.7370689655172413, "no_speech_prob": 8.18911285023205e-05}, {"id": 191, "seek": 75308, "start": 760.84, "end": 768.5200000000001, "text": " So with this common line, Git will call Tux-Mech on different commits to try to find the 41.", "tokens": [407, 365, 341, 2689, 1622, 11, 16939, 486, 818, 314, 2449, 12, 44, 5023, 322, 819, 48311, 281, 853, 281, 915, 264, 18173, 13], "temperature": 0.0, "avg_logprob": -0.1722385021818786, "compression_ratio": 1.7370689655172413, "no_speech_prob": 8.18911285023205e-05}, {"id": 192, "seek": 75308, "start": 768.5200000000001, "end": 770.5200000000001, "text": " And Tux-Mech will just build.", "tokens": [400, 314, 2449, 12, 44, 5023, 486, 445, 1322, 13], "temperature": 0.0, "avg_logprob": -0.1722385021818786, "compression_ratio": 1.7370689655172413, "no_speech_prob": 8.18911285023205e-05}, {"id": 193, "seek": 75308, "start": 770.5200000000001, "end": 775.08, "text": " And at the end of the build, thanks to minus minus result hook, it will exec the command", "tokens": [400, 412, 264, 917, 295, 264, 1322, 11, 3231, 281, 3175, 3175, 1874, 6328, 11, 309, 486, 4454, 264, 5622], "temperature": 0.0, "avg_logprob": -0.1722385021818786, "compression_ratio": 1.7370689655172413, "no_speech_prob": 8.18911285023205e-05}, {"id": 194, "seek": 75308, "start": 775.08, "end": 780.08, "text": " that is behind that will run Tux-Run with the kernel that has been just built.", "tokens": [300, 307, 2261, 300, 486, 1190, 314, 2449, 12, 42639, 365, 264, 28256, 300, 575, 668, 445, 3094, 13], "temperature": 0.0, "avg_logprob": -0.1722385021818786, "compression_ratio": 1.7370689655172413, "no_speech_prob": 8.18911285023205e-05}, {"id": 195, "seek": 78008, "start": 780.08, "end": 786.2, "text": " So it will build with Tux-Mech, and at the end, run with Tux-Run, the exact LTP test", "tokens": [407, 309, 486, 1322, 365, 314, 2449, 12, 44, 5023, 11, 293, 412, 264, 917, 11, 1190, 365, 314, 2449, 12, 42639, 11, 264, 1900, 441, 16804, 1500], "temperature": 0.0, "avg_logprob": -0.1710632053885873, "compression_ratio": 1.7695473251028806, "no_speech_prob": 3.902967000612989e-05}, {"id": 196, "seek": 78008, "start": 786.2, "end": 787.6, "text": " suite that fails.", "tokens": [14205, 300, 18199, 13], "temperature": 0.0, "avg_logprob": -0.1710632053885873, "compression_ratio": 1.7695473251028806, "no_speech_prob": 3.902967000612989e-05}, {"id": 197, "seek": 78008, "start": 787.6, "end": 789.9200000000001, "text": " And if it's passing, it will return zero.", "tokens": [400, 498, 309, 311, 8437, 11, 309, 486, 2736, 4018, 13], "temperature": 0.0, "avg_logprob": -0.1710632053885873, "compression_ratio": 1.7695473251028806, "no_speech_prob": 3.902967000612989e-05}, {"id": 198, "seek": 78008, "start": 789.9200000000001, "end": 791.5600000000001, "text": " If it's failing, it will return one.", "tokens": [759, 309, 311, 18223, 11, 309, 486, 2736, 472, 13], "temperature": 0.0, "avg_logprob": -0.1710632053885873, "compression_ratio": 1.7695473251028806, "no_speech_prob": 3.902967000612989e-05}, {"id": 199, "seek": 78008, "start": 791.5600000000001, "end": 796.0400000000001, "text": " So based on that, Git will be able to find the faulty commit for you, which is quite...", "tokens": [407, 2361, 322, 300, 11, 16939, 486, 312, 1075, 281, 915, 264, 2050, 5773, 5599, 337, 291, 11, 597, 307, 1596, 485], "temperature": 0.0, "avg_logprob": -0.1710632053885873, "compression_ratio": 1.7695473251028806, "no_speech_prob": 3.902967000612989e-05}, {"id": 200, "seek": 78008, "start": 796.0400000000001, "end": 799.84, "text": " We find a lot of regression or test regression and find the faulty commit thanks to just", "tokens": [492, 915, 257, 688, 295, 24590, 420, 1500, 24590, 293, 915, 264, 2050, 5773, 5599, 3231, 281, 445], "temperature": 0.0, "avg_logprob": -0.1710632053885873, "compression_ratio": 1.7695473251028806, "no_speech_prob": 3.902967000612989e-05}, {"id": 201, "seek": 78008, "start": 799.84, "end": 803.2800000000001, "text": " that command line, which is really cool.", "tokens": [300, 5622, 1622, 11, 597, 307, 534, 1627, 13], "temperature": 0.0, "avg_logprob": -0.1710632053885873, "compression_ratio": 1.7695473251028806, "no_speech_prob": 3.902967000612989e-05}, {"id": 202, "seek": 78008, "start": 803.2800000000001, "end": 807.12, "text": " Thanks to Anders for the idea.", "tokens": [2561, 281, 33988, 337, 264, 1558, 13], "temperature": 0.0, "avg_logprob": -0.1710632053885873, "compression_ratio": 1.7695473251028806, "no_speech_prob": 3.902967000612989e-05}, {"id": 203, "seek": 80712, "start": 807.12, "end": 813.76, "text": " So that was all virtual build, building containers, test on virtual devices, but as I said before,", "tokens": [407, 300, 390, 439, 6374, 1322, 11, 2390, 17089, 11, 1500, 322, 6374, 5759, 11, 457, 382, 286, 848, 949, 11], "temperature": 0.0, "avg_logprob": -0.2215726089477539, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.0002895748766604811}, {"id": 204, "seek": 80712, "start": 813.76, "end": 819.44, "text": " we have to test on physical devices because multiple bugs are only found on physical devices", "tokens": [321, 362, 281, 1500, 322, 4001, 5759, 570, 3866, 15120, 366, 787, 1352, 322, 4001, 5759], "temperature": 0.0, "avg_logprob": -0.2215726089477539, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.0002895748766604811}, {"id": 205, "seek": 80712, "start": 819.44, "end": 824.04, "text": " because they are based on drivers failing and things like that.", "tokens": [570, 436, 366, 2361, 322, 11590, 18223, 293, 721, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.2215726089477539, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.0002895748766604811}, {"id": 206, "seek": 80712, "start": 824.04, "end": 828.5600000000001, "text": " So for that, we use Lava, like many, many, some people in this room.", "tokens": [407, 337, 300, 11, 321, 764, 441, 4061, 11, 411, 867, 11, 867, 11, 512, 561, 294, 341, 1808, 13], "temperature": 0.0, "avg_logprob": -0.2215726089477539, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.0002895748766604811}, {"id": 207, "seek": 80712, "start": 828.5600000000001, "end": 832.36, "text": " So Lava stands for linear automated validation architecture.", "tokens": [407, 441, 4061, 7382, 337, 8213, 18473, 24071, 9482, 13], "temperature": 0.0, "avg_logprob": -0.2215726089477539, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.0002895748766604811}, {"id": 208, "seek": 80712, "start": 832.36, "end": 834.24, "text": " It's a text execution system.", "tokens": [467, 311, 257, 2487, 15058, 1185, 13], "temperature": 0.0, "avg_logprob": -0.2215726089477539, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.0002895748766604811}, {"id": 209, "seek": 83424, "start": 834.24, "end": 838.76, "text": " So it will allow for testing software on real hardware automatically for you.", "tokens": [407, 309, 486, 2089, 337, 4997, 4722, 322, 957, 8837, 6772, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.20427546880941475, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.00019266374874860048}, {"id": 210, "seek": 83424, "start": 838.76, "end": 843.76, "text": " So it will automatically deploy, boot, and test your software on your hardware.", "tokens": [407, 309, 486, 6772, 7274, 11, 11450, 11, 293, 1500, 428, 4722, 322, 428, 8837, 13], "temperature": 0.0, "avg_logprob": -0.20427546880941475, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.00019266374874860048}, {"id": 211, "seek": 83424, "start": 843.76, "end": 848.08, "text": " So it's used by Canon CI a lot, by LKFT, obviously.", "tokens": [407, 309, 311, 1143, 538, 27666, 37777, 257, 688, 11, 538, 441, 42, 25469, 11, 2745, 13], "temperature": 0.0, "avg_logprob": -0.20427546880941475, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.00019266374874860048}, {"id": 212, "seek": 83424, "start": 848.08, "end": 849.08, "text": " And for...", "tokens": [400, 337, 485], "temperature": 0.0, "avg_logprob": -0.20427546880941475, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.00019266374874860048}, {"id": 213, "seek": 83424, "start": 849.08, "end": 851.4, "text": " You can do system level testing, boot level testing.", "tokens": [509, 393, 360, 1185, 1496, 4997, 11, 11450, 1496, 4997, 13], "temperature": 0.0, "avg_logprob": -0.20427546880941475, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.00019266374874860048}, {"id": 214, "seek": 83424, "start": 851.4, "end": 853.16, "text": " You can do boot loader also testing.", "tokens": [509, 393, 360, 11450, 3677, 260, 611, 4997, 13], "temperature": 0.0, "avg_logprob": -0.20427546880941475, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.00019266374874860048}, {"id": 215, "seek": 83424, "start": 853.16, "end": 856.84, "text": " You can test directly, directly, your boot loader and the firmware.", "tokens": [509, 393, 1500, 3838, 11, 3838, 11, 428, 11450, 3677, 260, 293, 264, 30289, 13], "temperature": 0.0, "avg_logprob": -0.20427546880941475, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.00019266374874860048}, {"id": 216, "seek": 83424, "start": 856.84, "end": 860.0, "text": " And it currently supports 356 different device types.", "tokens": [400, 309, 4362, 9346, 6976, 21, 819, 4302, 3467, 13], "temperature": 0.0, "avg_logprob": -0.20427546880941475, "compression_ratio": 1.8227848101265822, "no_speech_prob": 0.00019266374874860048}, {"id": 217, "seek": 86000, "start": 860.0, "end": 865.08, "text": " So from IoT to phones, Raspberry Pi-like boards, and servers.", "tokens": [407, 490, 30112, 281, 10216, 11, 41154, 17741, 12, 4092, 13293, 11, 293, 15909, 13], "temperature": 0.0, "avg_logprob": -0.2561147212982178, "compression_ratio": 1.5349794238683128, "no_speech_prob": 0.00016992833116091788}, {"id": 218, "seek": 86000, "start": 865.08, "end": 867.92, "text": " So multiple different device types.", "tokens": [407, 3866, 819, 4302, 3467, 13], "temperature": 0.0, "avg_logprob": -0.2561147212982178, "compression_ratio": 1.5349794238683128, "no_speech_prob": 0.00016992833116091788}, {"id": 219, "seek": 86000, "start": 867.92, "end": 872.36, "text": " So for example, if you want to test on a Raspberry Pi, without Lava, you will have to pour on", "tokens": [407, 337, 1365, 11, 498, 291, 528, 281, 1500, 322, 257, 41154, 17741, 11, 1553, 441, 4061, 11, 291, 486, 362, 281, 2016, 322], "temperature": 0.0, "avg_logprob": -0.2561147212982178, "compression_ratio": 1.5349794238683128, "no_speech_prob": 0.00016992833116091788}, {"id": 220, "seek": 86000, "start": 872.36, "end": 879.24, "text": " the board, download the artifacts, so kernel, rootFS, files, DTBs, place them on a specific", "tokens": [264, 3150, 11, 5484, 264, 24617, 11, 370, 28256, 11, 5593, 29318, 11, 7098, 11, 413, 51, 33, 82, 11, 1081, 552, 322, 257, 2685], "temperature": 0.0, "avg_logprob": -0.2561147212982178, "compression_ratio": 1.5349794238683128, "no_speech_prob": 0.00016992833116091788}, {"id": 221, "seek": 86000, "start": 879.24, "end": 889.32, "text": " directory, like NFS or TFT directory, connect to the serial, type a lot of commands, boot", "tokens": [21120, 11, 411, 13576, 50, 420, 314, 25469, 21120, 11, 1745, 281, 264, 17436, 11, 2010, 257, 688, 295, 16901, 11, 11450], "temperature": 0.0, "avg_logprob": -0.2561147212982178, "compression_ratio": 1.5349794238683128, "no_speech_prob": 0.00016992833116091788}, {"id": 222, "seek": 88932, "start": 889.32, "end": 893.5600000000001, "text": " the board, watch the boot outputs, type the logging prompt, et cetera, et cetera.", "tokens": [264, 3150, 11, 1159, 264, 11450, 23930, 11, 2010, 264, 27991, 12391, 11, 1030, 11458, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.22820348739624025, "compression_ratio": 1.650375939849624, "no_speech_prob": 5.838553261128254e-05}, {"id": 223, "seek": 88932, "start": 893.5600000000001, "end": 896.9200000000001, "text": " So it's really painful to do that manually.", "tokens": [407, 309, 311, 534, 11697, 281, 360, 300, 16945, 13], "temperature": 0.0, "avg_logprob": -0.22820348739624025, "compression_ratio": 1.650375939849624, "no_speech_prob": 5.838553261128254e-05}, {"id": 224, "seek": 88932, "start": 896.9200000000001, "end": 900.5600000000001, "text": " Lava will just do exactly what I just listed, automatically for you.", "tokens": [441, 4061, 486, 445, 360, 2293, 437, 286, 445, 10052, 11, 6772, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.22820348739624025, "compression_ratio": 1.650375939849624, "no_speech_prob": 5.838553261128254e-05}, {"id": 225, "seek": 88932, "start": 900.5600000000001, "end": 905.2800000000001, "text": " It will just provide a job definition, which is a YAML file, with links to all the artifacts", "tokens": [467, 486, 445, 2893, 257, 1691, 7123, 11, 597, 307, 257, 398, 2865, 43, 3991, 11, 365, 6123, 281, 439, 264, 24617], "temperature": 0.0, "avg_logprob": -0.22820348739624025, "compression_ratio": 1.650375939849624, "no_speech_prob": 5.838553261128254e-05}, {"id": 226, "seek": 88932, "start": 905.2800000000001, "end": 907.5200000000001, "text": " that you want to test.", "tokens": [300, 291, 528, 281, 1500, 13], "temperature": 0.0, "avg_logprob": -0.22820348739624025, "compression_ratio": 1.650375939849624, "no_speech_prob": 5.838553261128254e-05}, {"id": 227, "seek": 88932, "start": 907.5200000000001, "end": 909.2800000000001, "text": " You specify the kind of board that you have.", "tokens": [509, 16500, 264, 733, 295, 3150, 300, 291, 362, 13], "temperature": 0.0, "avg_logprob": -0.22820348739624025, "compression_ratio": 1.650375939849624, "no_speech_prob": 5.838553261128254e-05}, {"id": 228, "seek": 88932, "start": 909.2800000000001, "end": 914.96, "text": " So it's a Raspberry Pi 4B, and Lava will know then how to interact with that board.", "tokens": [407, 309, 311, 257, 41154, 17741, 1017, 33, 11, 293, 441, 4061, 486, 458, 550, 577, 281, 4648, 365, 300, 3150, 13], "temperature": 0.0, "avg_logprob": -0.22820348739624025, "compression_ratio": 1.650375939849624, "no_speech_prob": 5.838553261128254e-05}, {"id": 229, "seek": 91496, "start": 914.96, "end": 920.1600000000001, "text": " And you will say that you boot install on it, and you have a TFTP server.", "tokens": [400, 291, 486, 584, 300, 291, 11450, 3625, 322, 309, 11, 293, 291, 362, 257, 40964, 16804, 7154, 13], "temperature": 0.0, "avg_logprob": -0.20560909452892484, "compression_ratio": 1.6593886462882097, "no_speech_prob": 1.7441681848140433e-05}, {"id": 230, "seek": 91496, "start": 920.1600000000001, "end": 923.32, "text": " Just use that, and test what I want to test on it.", "tokens": [1449, 764, 300, 11, 293, 1500, 437, 286, 528, 281, 1500, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.20560909452892484, "compression_ratio": 1.6593886462882097, "no_speech_prob": 1.7441681848140433e-05}, {"id": 231, "seek": 91496, "start": 923.32, "end": 926.72, "text": " And Lava will do that automatically for you.", "tokens": [400, 441, 4061, 486, 360, 300, 6772, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.20560909452892484, "compression_ratio": 1.6593886462882097, "no_speech_prob": 1.7441681848140433e-05}, {"id": 232, "seek": 91496, "start": 926.72, "end": 931.48, "text": " Obviously, you can have multiple boards attached to the same worker, and you can have multiple", "tokens": [7580, 11, 291, 393, 362, 3866, 13293, 8570, 281, 264, 912, 11346, 11, 293, 291, 393, 362, 3866], "temperature": 0.0, "avg_logprob": -0.20560909452892484, "compression_ratio": 1.6593886462882097, "no_speech_prob": 1.7441681848140433e-05}, {"id": 233, "seek": 91496, "start": 931.48, "end": 933.8000000000001, "text": " workers on a Lava instance.", "tokens": [5600, 322, 257, 441, 4061, 5197, 13], "temperature": 0.0, "avg_logprob": -0.20560909452892484, "compression_ratio": 1.6593886462882097, "no_speech_prob": 1.7441681848140433e-05}, {"id": 234, "seek": 91496, "start": 933.8000000000001, "end": 939.5600000000001, "text": " So as a user, it's really an abstraction of the hardware, and you just send a YAML file", "tokens": [407, 382, 257, 4195, 11, 309, 311, 534, 364, 37765, 295, 264, 8837, 11, 293, 291, 445, 2845, 257, 398, 2865, 43, 3991], "temperature": 0.0, "avg_logprob": -0.20560909452892484, "compression_ratio": 1.6593886462882097, "no_speech_prob": 1.7441681848140433e-05}, {"id": 235, "seek": 93956, "start": 939.56, "end": 946.52, "text": " and you get results, and all the hardware part is done automatically by Lava for you.", "tokens": [293, 291, 483, 3542, 11, 293, 439, 264, 8837, 644, 307, 1096, 6772, 538, 441, 4061, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.15029402996631377, "compression_ratio": 1.528301886792453, "no_speech_prob": 1.4432164789468516e-05}, {"id": 236, "seek": 93956, "start": 946.52, "end": 952.64, "text": " So as I said, maybe you remember the first LKFT diagram.", "tokens": [407, 382, 286, 848, 11, 1310, 291, 1604, 264, 700, 441, 42, 25469, 10686, 13], "temperature": 0.0, "avg_logprob": -0.15029402996631377, "compression_ratio": 1.528301886792453, "no_speech_prob": 1.4432164789468516e-05}, {"id": 237, "seek": 93956, "start": 952.64, "end": 953.64, "text": " I'm sure you don't.", "tokens": [286, 478, 988, 291, 500, 380, 13], "temperature": 0.0, "avg_logprob": -0.15029402996631377, "compression_ratio": 1.528301886792453, "no_speech_prob": 1.4432164789468516e-05}, {"id": 238, "seek": 93956, "start": 953.64, "end": 956.7199999999999, "text": " That was a small box called KeysCache.", "tokens": [663, 390, 257, 1359, 2424, 1219, 43733, 34, 6000, 13], "temperature": 0.0, "avg_logprob": -0.15029402996631377, "compression_ratio": 1.528301886792453, "no_speech_prob": 1.4432164789468516e-05}, {"id": 239, "seek": 93956, "start": 956.7199999999999, "end": 965.5999999999999, "text": " So when we submit jobs to Lava, we submit multiple jobs for the same artifacts at the", "tokens": [407, 562, 321, 10315, 4782, 281, 441, 4061, 11, 321, 10315, 3866, 4782, 337, 264, 912, 24617, 412, 264], "temperature": 0.0, "avg_logprob": -0.15029402996631377, "compression_ratio": 1.528301886792453, "no_speech_prob": 1.4432164789468516e-05}, {"id": 240, "seek": 93956, "start": 965.5999999999999, "end": 966.5999999999999, "text": " same time.", "tokens": [912, 565, 13], "temperature": 0.0, "avg_logprob": -0.15029402996631377, "compression_ratio": 1.528301886792453, "no_speech_prob": 1.4432164789468516e-05}, {"id": 241, "seek": 93956, "start": 966.5999999999999, "end": 968.4, "text": " We have multiple devices.", "tokens": [492, 362, 3866, 5759, 13], "temperature": 0.0, "avg_logprob": -0.15029402996631377, "compression_ratio": 1.528301886792453, "no_speech_prob": 1.4432164789468516e-05}, {"id": 242, "seek": 96840, "start": 968.4, "end": 974.68, "text": " So the scheduler will start the job for the same artifacts all at the same time.", "tokens": [407, 264, 12000, 260, 486, 722, 264, 1691, 337, 264, 912, 24617, 439, 412, 264, 912, 565, 13], "temperature": 0.0, "avg_logprob": -0.17388300310101426, "compression_ratio": 1.8024193548387097, "no_speech_prob": 4.1877316107274964e-05}, {"id": 243, "seek": 96840, "start": 974.68, "end": 979.16, "text": " So it will download multiple times the same artifact at the same time, so we just should", "tokens": [407, 309, 486, 5484, 3866, 1413, 264, 912, 34806, 412, 264, 912, 565, 11, 370, 321, 445, 820], "temperature": 0.0, "avg_logprob": -0.17388300310101426, "compression_ratio": 1.8024193548387097, "no_speech_prob": 4.1877316107274964e-05}, {"id": 244, "seek": 96840, "start": 979.16, "end": 982.76, "text": " be able to catch that and decrease network usage.", "tokens": [312, 1075, 281, 3745, 300, 293, 11514, 3209, 14924, 13], "temperature": 0.0, "avg_logprob": -0.17388300310101426, "compression_ratio": 1.8024193548387097, "no_speech_prob": 4.1877316107274964e-05}, {"id": 245, "seek": 96840, "start": 982.76, "end": 986.8, "text": " So we tried squid, and the short answer is squid is not working for that use case for", "tokens": [407, 321, 3031, 28015, 11, 293, 264, 2099, 1867, 307, 28015, 307, 406, 1364, 337, 300, 764, 1389, 337], "temperature": 0.0, "avg_logprob": -0.17388300310101426, "compression_ratio": 1.8024193548387097, "no_speech_prob": 4.1877316107274964e-05}, {"id": 246, "seek": 96840, "start": 986.8, "end": 988.24, "text": " different reasons.", "tokens": [819, 4112, 13], "temperature": 0.0, "avg_logprob": -0.17388300310101426, "compression_ratio": 1.8024193548387097, "no_speech_prob": 4.1877316107274964e-05}, {"id": 247, "seek": 96840, "start": 988.24, "end": 993.8, "text": " The first one is that, as I said before, all the artifacts are stored in an S3 like bucket.", "tokens": [440, 700, 472, 307, 300, 11, 382, 286, 848, 949, 11, 439, 264, 24617, 366, 12187, 294, 364, 318, 18, 411, 13058, 13], "temperature": 0.0, "avg_logprob": -0.17388300310101426, "compression_ratio": 1.8024193548387097, "no_speech_prob": 4.1877316107274964e-05}, {"id": 248, "seek": 96840, "start": 993.8, "end": 995.24, "text": " So it's somewhere on internet.", "tokens": [407, 309, 311, 4079, 322, 4705, 13], "temperature": 0.0, "avg_logprob": -0.17388300310101426, "compression_ratio": 1.8024193548387097, "no_speech_prob": 4.1877316107274964e-05}, {"id": 249, "seek": 99524, "start": 995.24, "end": 1000.0, "text": " So obviously we use SSL, HTTPS, to download it.", "tokens": [407, 2745, 321, 764, 12238, 43, 11, 11751, 51, 6273, 11, 281, 5484, 309, 13], "temperature": 0.0, "avg_logprob": -0.17958168905289446, "compression_ratio": 1.736, "no_speech_prob": 5.136388062965125e-05}, {"id": 250, "seek": 99524, "start": 1000.0, "end": 1005.6800000000001, "text": " And squid and HTTPS are not really working well together.", "tokens": [400, 28015, 293, 11751, 51, 6273, 366, 406, 534, 1364, 731, 1214, 13], "temperature": 0.0, "avg_logprob": -0.17958168905289446, "compression_ratio": 1.736, "no_speech_prob": 5.136388062965125e-05}, {"id": 251, "seek": 99524, "start": 1005.6800000000001, "end": 1008.12, "text": " You have to fake SSL certificates.", "tokens": [509, 362, 281, 7592, 12238, 43, 32941, 13], "temperature": 0.0, "avg_logprob": -0.17958168905289446, "compression_ratio": 1.736, "no_speech_prob": 5.136388062965125e-05}, {"id": 252, "seek": 99524, "start": 1008.12, "end": 1010.96, "text": " It's all creepy things to do.", "tokens": [467, 311, 439, 14717, 721, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.17958168905289446, "compression_ratio": 1.736, "no_speech_prob": 5.136388062965125e-05}, {"id": 253, "seek": 99524, "start": 1010.96, "end": 1015.64, "text": " And also a thing that, as I said, with download, Lava will start all the jobs at the same time.", "tokens": [400, 611, 257, 551, 300, 11, 382, 286, 848, 11, 365, 5484, 11, 441, 4061, 486, 722, 439, 264, 4782, 412, 264, 912, 565, 13], "temperature": 0.0, "avg_logprob": -0.17958168905289446, "compression_ratio": 1.736, "no_speech_prob": 5.136388062965125e-05}, {"id": 254, "seek": 99524, "start": 1015.64, "end": 1020.08, "text": " So they will more or less download all the same artifacts at exactly the same time.", "tokens": [407, 436, 486, 544, 420, 1570, 5484, 439, 264, 912, 24617, 412, 2293, 264, 912, 565, 13], "temperature": 0.0, "avg_logprob": -0.17958168905289446, "compression_ratio": 1.736, "no_speech_prob": 5.136388062965125e-05}, {"id": 255, "seek": 99524, "start": 1020.08, "end": 1024.88, "text": " And if you do that with squid, squid will download, if you ask for n times the same", "tokens": [400, 498, 291, 360, 300, 365, 28015, 11, 28015, 486, 5484, 11, 498, 291, 1029, 337, 297, 1413, 264, 912], "temperature": 0.0, "avg_logprob": -0.17958168905289446, "compression_ratio": 1.736, "no_speech_prob": 5.136388062965125e-05}, {"id": 256, "seek": 102488, "start": 1024.88, "end": 1030.2800000000002, "text": " file to squid, if it's not already cached, squid will download it n times.", "tokens": [3991, 281, 28015, 11, 498, 309, 311, 406, 1217, 269, 15095, 11, 28015, 486, 5484, 309, 297, 1413, 13], "temperature": 0.0, "avg_logprob": -0.1694540669841151, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.7884416087763384e-05}, {"id": 257, "seek": 102488, "start": 1030.2800000000002, "end": 1034.8400000000001, "text": " And only when one is finished, or when download is finished, the next one will use a cache", "tokens": [400, 787, 562, 472, 307, 4335, 11, 420, 562, 5484, 307, 4335, 11, 264, 958, 472, 486, 764, 257, 19459], "temperature": 0.0, "avg_logprob": -0.1694540669841151, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.7884416087763384e-05}, {"id": 258, "seek": 102488, "start": 1034.8400000000001, "end": 1035.8400000000001, "text": " version.", "tokens": [3037, 13], "temperature": 0.0, "avg_logprob": -0.1694540669841151, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.7884416087763384e-05}, {"id": 259, "seek": 102488, "start": 1035.8400000000001, "end": 1038.6000000000001, "text": " So it's just pointless for us, just not working.", "tokens": [407, 309, 311, 445, 32824, 337, 505, 11, 445, 406, 1364, 13], "temperature": 0.0, "avg_logprob": -0.1694540669841151, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.7884416087763384e-05}, {"id": 260, "seek": 102488, "start": 1038.6000000000001, "end": 1044.4, "text": " So we created a tool called keyscache, the keys is for keep it simple, stupid.", "tokens": [407, 321, 2942, 257, 2290, 1219, 9317, 66, 6000, 11, 264, 9317, 307, 337, 1066, 309, 2199, 11, 6631, 13], "temperature": 0.0, "avg_logprob": -0.1694540669841151, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.7884416087763384e-05}, {"id": 261, "seek": 102488, "start": 1044.4, "end": 1046.2, "text": " It's a simple and stupid caching server.", "tokens": [467, 311, 257, 2199, 293, 6631, 269, 2834, 7154, 13], "temperature": 0.0, "avg_logprob": -0.1694540669841151, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.7884416087763384e-05}, {"id": 262, "seek": 102488, "start": 1046.2, "end": 1051.8400000000001, "text": " It's not a proxy, it's a service, which means that it can handle HTTPS, and it will only", "tokens": [467, 311, 406, 257, 29690, 11, 309, 311, 257, 2643, 11, 597, 1355, 300, 309, 393, 4813, 11751, 51, 6273, 11, 293, 309, 486, 787], "temperature": 0.0, "avg_logprob": -0.1694540669841151, "compression_ratio": 1.7142857142857142, "no_speech_prob": 3.7884416087763384e-05}, {"id": 263, "seek": 105184, "start": 1051.84, "end": 1058.1999999999998, "text": " download once when you have multiple clients, and it will stream to the clients while downloading.", "tokens": [5484, 1564, 562, 291, 362, 3866, 6982, 11, 293, 309, 486, 4309, 281, 264, 6982, 1339, 32529, 13], "temperature": 0.0, "avg_logprob": -0.15937251460795498, "compression_ratio": 1.738197424892704, "no_speech_prob": 5.566017352975905e-05}, {"id": 264, "seek": 105184, "start": 1058.1999999999998, "end": 1062.6, "text": " It's not transparent because it's not a proxy, and because it's not transparent, it can do", "tokens": [467, 311, 406, 12737, 570, 309, 311, 406, 257, 29690, 11, 293, 570, 309, 311, 406, 12737, 11, 309, 393, 360], "temperature": 0.0, "avg_logprob": -0.15937251460795498, "compression_ratio": 1.738197424892704, "no_speech_prob": 5.566017352975905e-05}, {"id": 265, "seek": 105184, "start": 1062.6, "end": 1068.08, "text": " HTTPS, because you will have to prefix your URL by the keyscache instance that you have.", "tokens": [11751, 51, 6273, 11, 570, 291, 486, 362, 281, 46969, 428, 12905, 538, 264, 9317, 66, 6000, 5197, 300, 291, 362, 13], "temperature": 0.0, "avg_logprob": -0.15937251460795498, "compression_ratio": 1.738197424892704, "no_speech_prob": 5.566017352975905e-05}, {"id": 266, "seek": 105184, "start": 1068.08, "end": 1072.6799999999998, "text": " And you will talk to keyscache directly.", "tokens": [400, 291, 486, 751, 281, 9317, 66, 6000, 3838, 13], "temperature": 0.0, "avg_logprob": -0.15937251460795498, "compression_ratio": 1.738197424892704, "no_speech_prob": 5.566017352975905e-05}, {"id": 267, "seek": 105184, "start": 1072.6799999999998, "end": 1077.52, "text": " It also automatically retries on failures, because we've found multiple failures that", "tokens": [467, 611, 6772, 1533, 2244, 322, 20774, 11, 570, 321, 600, 1352, 3866, 20774, 300], "temperature": 0.0, "avg_logprob": -0.15937251460795498, "compression_ratio": 1.738197424892704, "no_speech_prob": 5.566017352975905e-05}, {"id": 268, "seek": 107752, "start": 1077.52, "end": 1083.24, "text": " all the HTTP code that you can have when you request on an S3 like bucket, just insane.", "tokens": [439, 264, 33283, 3089, 300, 291, 393, 362, 562, 291, 5308, 322, 364, 318, 18, 411, 13058, 11, 445, 10838, 13], "temperature": 0.0, "avg_logprob": -0.16718261123549008, "compression_ratio": 1.8080808080808082, "no_speech_prob": 3.4470594982849434e-05}, {"id": 269, "seek": 107752, "start": 1083.24, "end": 1088.12, "text": " And sometimes also you will get, the connection will finish like if everything was done correctly.", "tokens": [400, 2171, 611, 291, 486, 483, 11, 264, 4984, 486, 2413, 411, 498, 1203, 390, 1096, 8944, 13], "temperature": 0.0, "avg_logprob": -0.16718261123549008, "compression_ratio": 1.8080808080808082, "no_speech_prob": 3.4470594982849434e-05}, {"id": 270, "seek": 107752, "start": 1088.12, "end": 1091.28, "text": " And in fact, the file is not complete, it's a partial download, and you don't get any", "tokens": [400, 294, 1186, 11, 264, 3991, 307, 406, 3566, 11, 309, 311, 257, 14641, 5484, 11, 293, 291, 500, 380, 483, 604], "temperature": 0.0, "avg_logprob": -0.16718261123549008, "compression_ratio": 1.8080808080808082, "no_speech_prob": 3.4470594982849434e-05}, {"id": 271, "seek": 107752, "start": 1091.28, "end": 1092.28, "text": " errors.", "tokens": [13603, 13], "temperature": 0.0, "avg_logprob": -0.16718261123549008, "compression_ratio": 1.8080808080808082, "no_speech_prob": 3.4470594982849434e-05}, {"id": 272, "seek": 107752, "start": 1092.28, "end": 1094.0, "text": " So keyscache will detect that for you.", "tokens": [407, 9317, 66, 6000, 486, 5531, 300, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.16718261123549008, "compression_ratio": 1.8080808080808082, "no_speech_prob": 3.4470594982849434e-05}, {"id": 273, "seek": 107752, "start": 1094.0, "end": 1097.68, "text": " It will detect that it's a partial download, and it will retry and download only the remaining", "tokens": [467, 486, 5531, 300, 309, 311, 257, 14641, 5484, 11, 293, 309, 486, 1533, 627, 293, 5484, 787, 264, 8877], "temperature": 0.0, "avg_logprob": -0.16718261123549008, "compression_ratio": 1.8080808080808082, "no_speech_prob": 3.4470594982849434e-05}, {"id": 274, "seek": 107752, "start": 1097.68, "end": 1099.0, "text": " things for you.", "tokens": [721, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.16718261123549008, "compression_ratio": 1.8080808080808082, "no_speech_prob": 3.4470594982849434e-05}, {"id": 275, "seek": 107752, "start": 1099.0, "end": 1100.96, "text": " And it's fully transparent as a user.", "tokens": [400, 309, 311, 4498, 12737, 382, 257, 4195, 13], "temperature": 0.0, "avg_logprob": -0.16718261123549008, "compression_ratio": 1.8080808080808082, "no_speech_prob": 3.4470594982849434e-05}, {"id": 276, "seek": 107752, "start": 1100.96, "end": 1105.2, "text": " It will do that in the background and still stream your data to you.", "tokens": [467, 486, 360, 300, 294, 264, 3678, 293, 920, 4309, 428, 1412, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.16718261123549008, "compression_ratio": 1.8080808080808082, "no_speech_prob": 3.4470594982849434e-05}, {"id": 277, "seek": 110520, "start": 1105.2, "end": 1110.04, "text": " So thanks to that, we've been using it for 2.5 years now.", "tokens": [407, 3231, 281, 300, 11, 321, 600, 668, 1228, 309, 337, 568, 13, 20, 924, 586, 13], "temperature": 0.0, "avg_logprob": -0.16506696821333053, "compression_ratio": 1.7074235807860263, "no_speech_prob": 6.233443855307996e-05}, {"id": 278, "seek": 110520, "start": 1110.04, "end": 1114.48, "text": " In the graph, in green is what we serve locally from keyscache, and in red is what we download", "tokens": [682, 264, 4295, 11, 294, 3092, 307, 437, 321, 4596, 16143, 490, 9317, 66, 6000, 11, 293, 294, 2182, 307, 437, 321, 5484], "temperature": 0.0, "avg_logprob": -0.16506696821333053, "compression_ratio": 1.7074235807860263, "no_speech_prob": 6.233443855307996e-05}, {"id": 279, "seek": 110520, "start": 1114.48, "end": 1116.48, "text": " from Internet.", "tokens": [490, 7703, 13], "temperature": 0.0, "avg_logprob": -0.16506696821333053, "compression_ratio": 1.7074235807860263, "no_speech_prob": 6.233443855307996e-05}, {"id": 280, "seek": 110520, "start": 1116.48, "end": 1123.0800000000002, "text": " So we downloaded 25 terabytes of data from Internet, and we serve 1.3 petabytes of data", "tokens": [407, 321, 21748, 3552, 1796, 24538, 295, 1412, 490, 7703, 11, 293, 321, 4596, 502, 13, 18, 3817, 24538, 295, 1412], "temperature": 0.0, "avg_logprob": -0.16506696821333053, "compression_ratio": 1.7074235807860263, "no_speech_prob": 6.233443855307996e-05}, {"id": 281, "seek": 110520, "start": 1123.0800000000002, "end": 1129.2, "text": " in the local network, which is the 52 times expansion ratio.", "tokens": [294, 264, 2654, 3209, 11, 597, 307, 264, 18079, 1413, 11260, 8509, 13], "temperature": 0.0, "avg_logprob": -0.16506696821333053, "compression_ratio": 1.7074235807860263, "no_speech_prob": 6.233443855307996e-05}, {"id": 282, "seek": 110520, "start": 1129.2, "end": 1132.64, "text": " So it's quite useful, and it improves stability also.", "tokens": [407, 309, 311, 1596, 4420, 11, 293, 309, 24771, 11826, 611, 13], "temperature": 0.0, "avg_logprob": -0.16506696821333053, "compression_ratio": 1.7074235807860263, "no_speech_prob": 6.233443855307996e-05}, {"id": 283, "seek": 110520, "start": 1132.64, "end": 1134.32, "text": " So it's really cool.", "tokens": [407, 309, 311, 534, 1627, 13], "temperature": 0.0, "avg_logprob": -0.16506696821333053, "compression_ratio": 1.7074235807860263, "no_speech_prob": 6.233443855307996e-05}, {"id": 284, "seek": 113432, "start": 1134.32, "end": 1137.6, "text": " It's a good tool for your CI if you don't use it already.", "tokens": [467, 311, 257, 665, 2290, 337, 428, 37777, 498, 291, 500, 380, 764, 309, 1217, 13], "temperature": 0.0, "avg_logprob": -0.2190614456826068, "compression_ratio": 1.6213592233009708, "no_speech_prob": 5.23815760971047e-05}, {"id": 285, "seek": 113432, "start": 1137.6, "end": 1145.0, "text": " And last but not the least, we store all the job results in Squad.", "tokens": [400, 1036, 457, 406, 264, 1935, 11, 321, 3531, 439, 264, 1691, 3542, 294, 26596, 13], "temperature": 0.0, "avg_logprob": -0.2190614456826068, "compression_ratio": 1.6213592233009708, "no_speech_prob": 5.23815760971047e-05}, {"id": 286, "seek": 113432, "start": 1145.0, "end": 1147.8799999999999, "text": " So it's software quality dashboard.", "tokens": [407, 309, 311, 4722, 3125, 18342, 13], "temperature": 0.0, "avg_logprob": -0.2190614456826068, "compression_ratio": 1.6213592233009708, "no_speech_prob": 5.23815760971047e-05}, {"id": 287, "seek": 113432, "start": 1147.8799999999999, "end": 1149.48, "text": " It will store, it's a data lake.", "tokens": [467, 486, 3531, 11, 309, 311, 257, 1412, 11001, 13], "temperature": 0.0, "avg_logprob": -0.2190614456826068, "compression_ratio": 1.6213592233009708, "no_speech_prob": 5.23815760971047e-05}, {"id": 288, "seek": 113432, "start": 1149.48, "end": 1155.72, "text": " It will store all the results for you in different categories, and it will allow you to create", "tokens": [467, 486, 3531, 439, 264, 3542, 337, 291, 294, 819, 10479, 11, 293, 309, 486, 2089, 291, 281, 1884], "temperature": 0.0, "avg_logprob": -0.2190614456826068, "compression_ratio": 1.6213592233009708, "no_speech_prob": 5.23815760971047e-05}, {"id": 289, "seek": 113432, "start": 1155.72, "end": 1158.84, "text": " reports, so failures, regressions, et cetera.", "tokens": [7122, 11, 370, 20774, 11, 1121, 735, 626, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.2190614456826068, "compression_ratio": 1.6213592233009708, "no_speech_prob": 5.23815760971047e-05}, {"id": 290, "seek": 115884, "start": 1158.84, "end": 1165.9599999999998, "text": " Everything is stored in this one, and then we extract data and make report based on Squad.", "tokens": [5471, 307, 12187, 294, 341, 472, 11, 293, 550, 321, 8947, 1412, 293, 652, 2275, 2361, 322, 26596, 13], "temperature": 0.0, "avg_logprob": -0.2700325496613033, "compression_ratio": 1.3974358974358974, "no_speech_prob": 7.56628651288338e-05}, {"id": 291, "seek": 115884, "start": 1165.9599999999998, "end": 1168.1599999999999, "text": " And that's all.", "tokens": [400, 300, 311, 439, 13], "temperature": 0.0, "avg_logprob": -0.2700325496613033, "compression_ratio": 1.3974358974358974, "no_speech_prob": 7.56628651288338e-05}, {"id": 292, "seek": 115884, "start": 1168.1599999999999, "end": 1171.28, "text": " That's what I just explained.", "tokens": [663, 311, 437, 286, 445, 8825, 13], "temperature": 0.0, "avg_logprob": -0.2700325496613033, "compression_ratio": 1.3974358974358974, "no_speech_prob": 7.56628651288338e-05}, {"id": 293, "seek": 115884, "start": 1171.28, "end": 1174.6799999999998, "text": " If you have any questions, I have some time for questions.", "tokens": [759, 291, 362, 604, 1651, 11, 286, 362, 512, 565, 337, 1651, 13], "temperature": 0.0, "avg_logprob": -0.2700325496613033, "compression_ratio": 1.3974358974358974, "no_speech_prob": 7.56628651288338e-05}, {"id": 294, "seek": 115884, "start": 1174.6799999999998, "end": 1175.6799999999998, "text": " Five minutes.", "tokens": [9436, 2077, 13], "temperature": 0.0, "avg_logprob": -0.2700325496613033, "compression_ratio": 1.3974358974358974, "no_speech_prob": 7.56628651288338e-05}, {"id": 295, "seek": 115884, "start": 1175.6799999999998, "end": 1176.6799999999998, "text": " Perfect.", "tokens": [10246, 13], "temperature": 0.0, "avg_logprob": -0.2700325496613033, "compression_ratio": 1.3974358974358974, "no_speech_prob": 7.56628651288338e-05}, {"id": 296, "seek": 117668, "start": 1176.68, "end": 1191.92, "text": " Oh, yeah, that's good.", "tokens": [876, 11, 1338, 11, 300, 311, 665, 13], "temperature": 0.0, "avg_logprob": -0.3209099489099839, "compression_ratio": 1.1196581196581197, "no_speech_prob": 0.0008568700286559761}, {"id": 297, "seek": 117668, "start": 1191.92, "end": 1196.92, "text": " Testing methods?", "tokens": [45517, 7150, 30], "temperature": 0.0, "avg_logprob": -0.3209099489099839, "compression_ratio": 1.1196581196581197, "no_speech_prob": 0.0008568700286559761}, {"id": 298, "seek": 117668, "start": 1196.92, "end": 1206.16, "text": " We use LTP, KUNIT, KSELF-SES, all the kernel test suites that we don't, we are not creating", "tokens": [492, 764, 441, 16804, 11, 591, 3979, 3927, 11, 591, 50, 3158, 37, 12, 50, 2358, 11, 439, 264, 28256, 1500, 459, 3324, 300, 321, 500, 380, 11, 321, 366, 406, 4084], "temperature": 0.0, "avg_logprob": -0.3209099489099839, "compression_ratio": 1.1196581196581197, "no_speech_prob": 0.0008568700286559761}, {"id": 299, "seek": 120616, "start": 1206.16, "end": 1207.4, "text": " new test suites.", "tokens": [777, 1500, 459, 3324, 13], "temperature": 0.0, "avg_logprob": -0.1878635666587136, "compression_ratio": 1.8342857142857143, "no_speech_prob": 0.0001842523051891476}, {"id": 300, "seek": 120616, "start": 1207.4, "end": 1213.0400000000002, "text": " We are using test suites that does exist, and we build for the community, and we test", "tokens": [492, 366, 1228, 1500, 459, 3324, 300, 775, 2514, 11, 293, 321, 1322, 337, 264, 1768, 11, 293, 321, 1500], "temperature": 0.0, "avg_logprob": -0.1878635666587136, "compression_ratio": 1.8342857142857143, "no_speech_prob": 0.0001842523051891476}, {"id": 301, "seek": 120616, "start": 1213.0400000000002, "end": 1216.88, "text": " for the community, and then we provide reports.", "tokens": [337, 264, 1768, 11, 293, 550, 321, 2893, 7122, 13], "temperature": 0.0, "avg_logprob": -0.1878635666587136, "compression_ratio": 1.8342857142857143, "no_speech_prob": 0.0001842523051891476}, {"id": 302, "seek": 120616, "start": 1216.88, "end": 1221.76, "text": " We obviously interact a lot with the test suite maintainers, because we found bugs in", "tokens": [492, 2745, 4648, 257, 688, 365, 264, 1500, 14205, 6909, 433, 11, 570, 321, 1352, 15120, 294], "temperature": 0.0, "avg_logprob": -0.1878635666587136, "compression_ratio": 1.8342857142857143, "no_speech_prob": 0.0001842523051891476}, {"id": 303, "seek": 120616, "start": 1221.76, "end": 1223.0400000000002, "text": " the test suite, too.", "tokens": [264, 1500, 14205, 11, 886, 13], "temperature": 0.0, "avg_logprob": -0.1878635666587136, "compression_ratio": 1.8342857142857143, "no_speech_prob": 0.0001842523051891476}, {"id": 304, "seek": 120616, "start": 1223.0400000000002, "end": 1230.64, "text": " We have to report to them, and there's reporting a lot to them.", "tokens": [492, 362, 281, 2275, 281, 552, 11, 293, 456, 311, 10031, 257, 688, 281, 552, 13], "temperature": 0.0, "avg_logprob": -0.1878635666587136, "compression_ratio": 1.8342857142857143, "no_speech_prob": 0.0001842523051891476}, {"id": 305, "seek": 123064, "start": 1230.64, "end": 1236.96, "text": " And one of our projects is to test KSELF-SES in advance, test KSELF-SES master, to find", "tokens": [400, 472, 295, 527, 4455, 307, 281, 1500, 591, 50, 3158, 37, 12, 50, 2358, 294, 7295, 11, 1500, 591, 50, 3158, 37, 12, 50, 2358, 4505, 11, 281, 915], "temperature": 0.0, "avg_logprob": -0.2758391698201497, "compression_ratio": 1.3032786885245902, "no_speech_prob": 0.00027138771838508546}, {"id": 306, "seek": 123064, "start": 1236.96, "end": 1243.96, "text": " bugs in KSELF-SES before they are actually running in production after.", "tokens": [15120, 294, 591, 50, 3158, 37, 12, 50, 2358, 949, 436, 366, 767, 2614, 294, 4265, 934, 13], "temperature": 0.0, "avg_logprob": -0.2758391698201497, "compression_ratio": 1.3032786885245902, "no_speech_prob": 0.00027138771838508546}, {"id": 307, "seek": 124396, "start": 1243.96, "end": 1261.64, "text": " If you find any problems and report them, are current developers actually looking at", "tokens": [759, 291, 915, 604, 2740, 293, 2275, 552, 11, 366, 2190, 8849, 767, 1237, 412], "temperature": 0.0, "avg_logprob": -0.3020413800289756, "compression_ratio": 1.4756756756756757, "no_speech_prob": 0.0003389775229152292}, {"id": 308, "seek": 124396, "start": 1261.64, "end": 1262.64, "text": " them, or do you have to ping them and make sure they take care of the problem?", "tokens": [552, 11, 420, 360, 291, 362, 281, 26151, 552, 293, 652, 988, 436, 747, 1127, 295, 264, 1154, 30], "temperature": 0.0, "avg_logprob": -0.3020413800289756, "compression_ratio": 1.4756756756756757, "no_speech_prob": 0.0003389775229152292}, {"id": 309, "seek": 124396, "start": 1262.64, "end": 1266.0, "text": " Okay, so we have an SLA with Greg Croatman, so he's waiting for our results.", "tokens": [1033, 11, 370, 321, 362, 364, 318, 11435, 365, 11490, 18965, 267, 1601, 11, 370, 415, 311, 3806, 337, 527, 3542, 13], "temperature": 0.0, "avg_logprob": -0.3020413800289756, "compression_ratio": 1.4756756756756757, "no_speech_prob": 0.0003389775229152292}, {"id": 310, "seek": 124396, "start": 1266.0, "end": 1269.08, "text": " So they will look at it for LTS.", "tokens": [407, 436, 486, 574, 412, 309, 337, 441, 7327, 13], "temperature": 0.0, "avg_logprob": -0.3020413800289756, "compression_ratio": 1.4756756756756757, "no_speech_prob": 0.0003389775229152292}, {"id": 311, "seek": 126908, "start": 1269.08, "end": 1274.1599999999999, "text": " And for Mainline and Next, we are used to reports.", "tokens": [400, 337, 12383, 1889, 293, 3087, 11, 321, 366, 1143, 281, 7122, 13], "temperature": 0.0, "avg_logprob": -0.24582515750919376, "compression_ratio": 1.7081545064377683, "no_speech_prob": 0.00031099547049961984}, {"id": 312, "seek": 126908, "start": 1274.1599999999999, "end": 1277.76, "text": " We report a lot of issues, so they know us.", "tokens": [492, 2275, 257, 688, 295, 2663, 11, 370, 436, 458, 505, 13], "temperature": 0.0, "avg_logprob": -0.24582515750919376, "compression_ratio": 1.7081545064377683, "no_speech_prob": 0.00031099547049961984}, {"id": 313, "seek": 126908, "start": 1277.76, "end": 1285.08, "text": " If you look at LWN articles, about they classify the different contributions to the kernel,", "tokens": [759, 291, 574, 412, 441, 54, 45, 11290, 11, 466, 436, 33872, 264, 819, 15725, 281, 264, 28256, 11], "temperature": 0.0, "avg_logprob": -0.24582515750919376, "compression_ratio": 1.7081545064377683, "no_speech_prob": 0.00031099547049961984}, {"id": 314, "seek": 126908, "start": 1285.08, "end": 1290.3999999999999, "text": " and Linaro is in the tested-by top in the tested-by, so they know us a lot, so they", "tokens": [293, 441, 6470, 78, 307, 294, 264, 8246, 12, 2322, 1192, 294, 264, 8246, 12, 2322, 11, 370, 436, 458, 505, 257, 688, 11, 370, 436], "temperature": 0.0, "avg_logprob": -0.24582515750919376, "compression_ratio": 1.7081545064377683, "no_speech_prob": 0.00031099547049961984}, {"id": 315, "seek": 126908, "start": 1290.3999999999999, "end": 1292.6799999999998, "text": " know that we provide good results.", "tokens": [458, 300, 321, 2893, 665, 3542, 13], "temperature": 0.0, "avg_logprob": -0.24582515750919376, "compression_ratio": 1.7081545064377683, "no_speech_prob": 0.00031099547049961984}, {"id": 316, "seek": 126908, "start": 1292.6799999999998, "end": 1298.56, "text": " And when we provide a mail, there is everything that, every tool they need for reproducible.", "tokens": [400, 562, 321, 2893, 257, 10071, 11, 456, 307, 1203, 300, 11, 633, 2290, 436, 643, 337, 11408, 32128, 13], "temperature": 0.0, "avg_logprob": -0.24582515750919376, "compression_ratio": 1.7081545064377683, "no_speech_prob": 0.00031099547049961984}, {"id": 317, "seek": 129856, "start": 1298.56, "end": 1303.6799999999998, "text": " They are reproducing a build, so we provide all the binaries that they need for reproducing", "tokens": [814, 366, 11408, 2175, 257, 1322, 11, 370, 321, 2893, 439, 264, 5171, 4889, 300, 436, 643, 337, 11408, 2175], "temperature": 0.0, "avg_logprob": -0.25341256998353084, "compression_ratio": 1.9118942731277533, "no_speech_prob": 0.0007518433267250657}, {"id": 318, "seek": 129856, "start": 1303.6799999999998, "end": 1304.6799999999998, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.25341256998353084, "compression_ratio": 1.9118942731277533, "no_speech_prob": 0.0007518433267250657}, {"id": 319, "seek": 129856, "start": 1304.6799999999998, "end": 1308.04, "text": " If it's a big failure, we provide a tux-make command line that they can use, and they", "tokens": [759, 309, 311, 257, 955, 7763, 11, 321, 2893, 257, 256, 2449, 12, 38705, 5622, 1622, 300, 436, 393, 764, 11, 293, 436], "temperature": 0.0, "avg_logprob": -0.25341256998353084, "compression_ratio": 1.9118942731277533, "no_speech_prob": 0.0007518433267250657}, {"id": 320, "seek": 129856, "start": 1308.04, "end": 1311.04, "text": " are now used to use tux-make for rebuilding things.", "tokens": [366, 586, 1143, 281, 764, 256, 2449, 12, 38705, 337, 36717, 721, 13], "temperature": 0.0, "avg_logprob": -0.25341256998353084, "compression_ratio": 1.9118942731277533, "no_speech_prob": 0.0007518433267250657}, {"id": 321, "seek": 129856, "start": 1311.04, "end": 1318.6399999999999, "text": " And if it's a test failure, we provide the logs, obviously, the job definition, and all", "tokens": [400, 498, 309, 311, 257, 1500, 7763, 11, 321, 2893, 264, 20820, 11, 2745, 11, 264, 1691, 7123, 11, 293, 439], "temperature": 0.0, "avg_logprob": -0.25341256998353084, "compression_ratio": 1.9118942731277533, "no_speech_prob": 0.0007518433267250657}, {"id": 322, "seek": 129856, "start": 1318.6399999999999, "end": 1320.52, "text": " the binaries they need for reproducing it.", "tokens": [264, 5171, 4889, 436, 643, 337, 11408, 2175, 309, 13], "temperature": 0.0, "avg_logprob": -0.25341256998353084, "compression_ratio": 1.9118942731277533, "no_speech_prob": 0.0007518433267250657}, {"id": 323, "seek": 129856, "start": 1320.52, "end": 1325.44, "text": " Do you actually check that every problem you found is actually fixed?", "tokens": [1144, 291, 767, 1520, 300, 633, 1154, 291, 1352, 307, 767, 6806, 30], "temperature": 0.0, "avg_logprob": -0.25341256998353084, "compression_ratio": 1.9118942731277533, "no_speech_prob": 0.0007518433267250657}, {"id": 324, "seek": 132544, "start": 1325.44, "end": 1329.6000000000001, "text": " And those are all the bugs that we found fixed?", "tokens": [400, 729, 366, 439, 264, 15120, 300, 321, 1352, 6806, 30], "temperature": 0.0, "avg_logprob": -0.3842168408770894, "compression_ratio": 1.2, "no_speech_prob": 0.0011574279051274061}, {"id": 325, "seek": 132544, "start": 1329.6000000000001, "end": 1330.6000000000001, "text": " Not all of them?", "tokens": [1726, 439, 295, 552, 30], "temperature": 0.0, "avg_logprob": -0.3842168408770894, "compression_ratio": 1.2, "no_speech_prob": 0.0011574279051274061}, {"id": 326, "seek": 132544, "start": 1330.6000000000001, "end": 1347.1200000000001, "text": " Yeah, if you found some bugs on SH4, no one will care, for example.", "tokens": [865, 11, 498, 291, 1352, 512, 15120, 322, 7405, 19, 11, 572, 472, 486, 1127, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.3842168408770894, "compression_ratio": 1.2, "no_speech_prob": 0.0011574279051274061}, {"id": 327, "seek": 134712, "start": 1347.12, "end": 1363.2399999999998, "text": " The QMU 7.2 has been released recently, just not working on SH4.", "tokens": [440, 1249, 44, 52, 1614, 13, 17, 575, 668, 4736, 3938, 11, 445, 406, 1364, 322, 7405, 19, 13], "temperature": 0.0, "avg_logprob": -0.3801051159294284, "compression_ratio": 1.0956521739130434, "no_speech_prob": 0.00018132576951757073}, {"id": 328, "seek": 134712, "start": 1363.2399999999998, "end": 1365.6399999999999, "text": " I couldn't answer that.", "tokens": [286, 2809, 380, 1867, 300, 13], "temperature": 0.0, "avg_logprob": -0.3801051159294284, "compression_ratio": 1.0956521739130434, "no_speech_prob": 0.00018132576951757073}, {"id": 329, "seek": 134712, "start": 1365.6399999999999, "end": 1366.6399999999999, "text": " We use the WS.", "tokens": [492, 764, 264, 343, 50, 13], "temperature": 0.0, "avg_logprob": -0.3801051159294284, "compression_ratio": 1.0956521739130434, "no_speech_prob": 0.00018132576951757073}, {"id": 330, "seek": 134712, "start": 1366.6399999999999, "end": 1372.08, "text": " No, it's not that bad.", "tokens": [883, 11, 309, 311, 406, 300, 1578, 13], "temperature": 0.0, "avg_logprob": -0.3801051159294284, "compression_ratio": 1.0956521739130434, "no_speech_prob": 0.00018132576951757073}, {"id": 331, "seek": 137208, "start": 1372.08, "end": 1378.72, "text": " We build a dynamic system, which means that we do not rent 5,000 machines in parallel.", "tokens": [492, 1322, 257, 8546, 1185, 11, 597, 1355, 300, 321, 360, 406, 6214, 1025, 11, 1360, 8379, 294, 8952, 13], "temperature": 0.0, "avg_logprob": -0.2027277946472168, "compression_ratio": 1.556910569105691, "no_speech_prob": 0.00024189786927308887}, {"id": 332, "seek": 137208, "start": 1378.72, "end": 1379.72, "text": " Obviously not.", "tokens": [7580, 406, 13], "temperature": 0.0, "avg_logprob": -0.2027277946472168, "compression_ratio": 1.556910569105691, "no_speech_prob": 0.00024189786927308887}, {"id": 333, "seek": 137208, "start": 1379.72, "end": 1380.72, "text": " It's just impossible for us.", "tokens": [467, 311, 445, 6243, 337, 505, 13], "temperature": 0.0, "avg_logprob": -0.2027277946472168, "compression_ratio": 1.556910569105691, "no_speech_prob": 0.00024189786927308887}, {"id": 334, "seek": 137208, "start": 1380.72, "end": 1383.08, "text": " We are a small company.", "tokens": [492, 366, 257, 1359, 2237, 13], "temperature": 0.0, "avg_logprob": -0.2027277946472168, "compression_ratio": 1.556910569105691, "no_speech_prob": 0.00024189786927308887}, {"id": 335, "seek": 137208, "start": 1383.08, "end": 1388.9199999999998, "text": " Everything is dynamic, so from one second to another, if you look at the graph of usage,", "tokens": [5471, 307, 8546, 11, 370, 490, 472, 1150, 281, 1071, 11, 498, 291, 574, 412, 264, 4295, 295, 14924, 11], "temperature": 0.0, "avg_logprob": -0.2027277946472168, "compression_ratio": 1.556910569105691, "no_speech_prob": 0.00024189786927308887}, {"id": 336, "seek": 137208, "start": 1388.9199999999998, "end": 1395.12, "text": " when Anders submits a plan for testing, in one minute, we'll book 5,000 machines for", "tokens": [562, 33988, 8286, 1208, 257, 1393, 337, 4997, 11, 294, 472, 3456, 11, 321, 603, 1446, 1025, 11, 1360, 8379, 337], "temperature": 0.0, "avg_logprob": -0.2027277946472168, "compression_ratio": 1.556910569105691, "no_speech_prob": 0.00024189786927308887}, {"id": 337, "seek": 137208, "start": 1395.12, "end": 1399.6, "text": " building it, likely more 1.5,000 machines to build it.", "tokens": [2390, 309, 11, 3700, 544, 502, 13, 20, 11, 1360, 8379, 281, 1322, 309, 13], "temperature": 0.0, "avg_logprob": -0.2027277946472168, "compression_ratio": 1.556910569105691, "no_speech_prob": 0.00024189786927308887}, {"id": 338, "seek": 139960, "start": 1399.6, "end": 1406.7199999999998, "text": " They will build and they will just stop at the end.", "tokens": [814, 486, 1322, 293, 436, 486, 445, 1590, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.3387120420282537, "compression_ratio": 1.2033898305084745, "no_speech_prob": 0.000254680635407567}, {"id": 339, "seek": 139960, "start": 1406.7199999999998, "end": 1412.7199999999998, "text": " So no, we don't have 5,000 machines.", "tokens": [407, 572, 11, 321, 500, 380, 362, 1025, 11, 1360, 8379, 13], "temperature": 0.0, "avg_logprob": -0.3387120420282537, "compression_ratio": 1.2033898305084745, "no_speech_prob": 0.000254680635407567}, {"id": 340, "seek": 139960, "start": 1412.7199999999998, "end": 1423.0, "text": " How many devices do you have in your lava test brick?", "tokens": [1012, 867, 5759, 360, 291, 362, 294, 428, 22097, 1500, 16725, 30], "temperature": 0.0, "avg_logprob": -0.3387120420282537, "compression_ratio": 1.2033898305084745, "no_speech_prob": 0.000254680635407567}, {"id": 341, "seek": 142300, "start": 1423.0, "end": 1430.0, "text": " So for the LKFT, we have multiple lava instances in Linauro, in LKFT, how many devices?", "tokens": [407, 337, 264, 441, 42, 25469, 11, 321, 362, 3866, 22097, 14519, 294, 441, 1426, 7052, 11, 294, 441, 42, 25469, 11, 577, 867, 5759, 30], "temperature": 0.0, "avg_logprob": -0.46957953957950366, "compression_ratio": 1.3169398907103824, "no_speech_prob": 0.0001667365722823888}, {"id": 342, "seek": 142300, "start": 1430.0, "end": 1431.0, "text": " About 20.", "tokens": [7769, 945, 13], "temperature": 0.0, "avg_logprob": -0.46957953957950366, "compression_ratio": 1.3169398907103824, "no_speech_prob": 0.0001667365722823888}, {"id": 343, "seek": 142300, "start": 1431.0, "end": 1432.0, "text": " 20, yeah.", "tokens": [945, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.46957953957950366, "compression_ratio": 1.3169398907103824, "no_speech_prob": 0.0001667365722823888}, {"id": 344, "seek": 142300, "start": 1432.0, "end": 1445.16, "text": " And about 5 different device types, like Rolls-Royce, Dragon Balls, Junos, X8, X15.", "tokens": [400, 466, 1025, 819, 4302, 3467, 11, 411, 9926, 82, 12, 49, 939, 384, 11, 11517, 10744, 82, 11, 8492, 329, 11, 1783, 23, 11, 1783, 5211, 13], "temperature": 0.0, "avg_logprob": -0.46957953957950366, "compression_ratio": 1.3169398907103824, "no_speech_prob": 0.0001667365722823888}, {"id": 345, "seek": 142300, "start": 1445.16, "end": 1450.04, "text": " But yeah, you can have really large labs in lava.", "tokens": [583, 1338, 11, 291, 393, 362, 534, 2416, 20339, 294, 22097, 13], "temperature": 0.0, "avg_logprob": -0.46957953957950366, "compression_ratio": 1.3169398907103824, "no_speech_prob": 0.0001667365722823888}, {"id": 346, "seek": 145004, "start": 1450.04, "end": 1453.84, "text": " We have another one for just Linauro usage, where we have something like 100 balls, I", "tokens": [492, 362, 1071, 472, 337, 445, 441, 1426, 7052, 14924, 11, 689, 321, 362, 746, 411, 2319, 9803, 11, 286], "temperature": 0.0, "avg_logprob": -0.4532501514141376, "compression_ratio": 1.196078431372549, "no_speech_prob": 0.00019875186262652278}, {"id": 347, "seek": 145004, "start": 1453.84, "end": 1460.36, "text": " think, the main one.", "tokens": [519, 11, 264, 2135, 472, 13], "temperature": 0.0, "avg_logprob": -0.4532501514141376, "compression_ratio": 1.196078431372549, "no_speech_prob": 0.00019875186262652278}, {"id": 348, "seek": 145004, "start": 1460.36, "end": 1461.36, "text": " Thanks.", "tokens": [2561, 13], "temperature": 0.0, "avg_logprob": -0.4532501514141376, "compression_ratio": 1.196078431372549, "no_speech_prob": 0.00019875186262652278}, {"id": 349, "seek": 146136, "start": 1461.36, "end": 1480.6399999999999, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.4, "avg_logprob": -0.9613515308925084, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.0010386870708316565}], "language": "en"}