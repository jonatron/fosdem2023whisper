{"text": " So, I'm Jo\u00e3o. I work at SUSE, at the storage team. I used to work on SAF. Our storage team used to work on SAF, but due to restructurings, that's no longer the case. I'm going to talk to you about one of our latest projects, Yes Free Gateway, and I'm going to tell you why we're doing this mostly. What is S3 Gateway? How we're doing it? Hopefully there will be a demo, and then the next steps and what's ahead of us. So, why are we doing this? So, essentially, after our product was restructured, we needed to find something else to work on, and one of the ideas we had was to find a way to, on the one hand, to figure out a way to provide, I'm lacking the word, provide something that was lacking in the SUSE Rancher portfolio, which is basically an S3 service for local clusters, for local storage within a Kubernetes cluster, and what we aimed at was something as easy to deploy, as easily forgettable, and that just works for ideally local workloads, not necessarily something that is complex to manage. We didn't want anything that would be, we wanted something as light as possible, and that's what eventually became the S3 Gateway. It's an open source project as usual. It's driven by our team at SUSE, and ideally it will be an easy to use project that you just deploy on a Kubernetes cluster and it will just provide you an S3 service within your cluster. I say, ideally, because this has six months worth of development, and there are a lot of things still lacking, far more than I would actually like, but that's just how life is. It complements the Rancher portfolio, as I mentioned, but this was not necessarily the main driver when developing the S3 Gateway. It just happens to fit nicely within the stack. It helps doing backups of local Longhorn volumes, backups for other stuff within the stack, and one of our main criteria initially was that we would serve our storage from any PVC that a Kubernetes cluster could provide, and this just happens to be nice because given Longhorn allows us to put stuff on a Longhorn persistent volume, Longhorn will deal with all the nasty things like replication and whatnot so that we don't have to deal with that. Of course, we wanted a pretty UI for all of the operations and management, which is still ongoing, but we'll get there. How we're leveraging this, though, is basically using Rados Gateway from SEF. We didn't want to start something from scratch because we thought it would be a waste of time and resources, so we decided to leverage the Rados Gateway from SEF, which is quite amazing because it can be run standalone. Given the already built-in zipper layer of which there is a talk next, I think, we basically just had to create a new backend that is basically file-based on which our data is stored on the file-based backend instead of, say, the Rados store. Hence, we don't need a whole SEF cluster, we just need the binary running standalone. Essentially, as I was saying, we have the Rados Gateway consuming a file system, essentially whatever that file system is on. We keep a SQLite database for metadata for the objects and the directory hierarchy for the data. We decided to do this so that essentially all the things that can be indexed and to be essentially abstract could be kept as metadata in the SQLite, which will allow us to more easily search things, index things instead of having to go through a directory hierarchy to find, for instance, buckets and whatnot. So buckets are essentially a mapping of a name to a UUID and objects, as well, end up being entries in a database that associate the bucket name, the object name, and our map to a UUID. The data for the objects, though, will be based on the UUID and we will grab the UUID and create a directory hierarchy based on the first bytes of the UUID. The reasoning behind this was mostly because, given typically some buckets tend to grow larger than other buckets, if we were creating a directory hierarchy that was per bucket, per bucket name, we could end up with very large directories. This way, we kind of spread the objects around and even if we end up with larger directories, we don't have to list the directories themselves to find where the objects are or which objects are within those directories. We just have that stuff in the metadata devils within SQLite. Now, this is not pretty, I admit it, but it's the best I could do. This is roughly what translates to being the S3 gateway stuff being deployed on a Kubernetes cluster. We are essentially deploying two containers, one for the back end, which is rather a gateway, and another one for the UI. We deploy our store on whatever is the, whatever is supplying us with storage. In this case, this slide has Longhorn on it, which will deal with all the replication and availability and whatnot for us so that we don't have to care about this, but this runs as well in a local, I'm running this on my Synology NAS. I just have a volume that is exported through the back end. It really doesn't care about which whatever is providing the file system to it. The UI speaks directly with the back end and the user just consumes the S3 gateway if outside of the cluster through an ingress, if inside the cluster through magic, that I don't understand very well. I promise the demo, which might work or might not. It has been working 50% of the time in my computer, so let me see if I can get this to not this. So, you're not seeing anything, of course not. How do I, Jan, how do I... So, ideally what we'll see is already deployed K3S, which is running things, but it's not running the S3 gateway yet. What I'm going to do is to deploy with our chart, which usually works for people. My laptop is being difficult, so let me just see if I remember its home. So, basically we install this using the default values, and supposedly we will have the UI available at this URL here, not this one. Now, there is still a lot of kinks to figure out with this stuff. We have the UI being run here, but the UI right now is unable to talk with the back end, and this is because certificates. Because we are using a self-signed certificate that the browser does not understand, and the browser is directly talking to the back end through the UI, there is no actual demon in between the UI and the back end. What happens is that we have to go to the back end and accept the certificate, which is hilarious, and once this is done, we can log in to the UI. So, right now the UI is still under heavy development, and the UI cannot do a lot more than the back end does. We are still lacking a bunch of things implemented in the back end driver, but I just want to show you that if we do things on against the back end, it will actually happen in the front end. So, let me see. This is where this stuff is. It has three commands. If I put a one gigabyte object onto a bucket that I actually need to create first, if I create a bucket foo on the back end, it will actually show on the front end, which is expected, and it would suck if it didn't work. Putting an object there will also allow us to do some exploration. This is a big object, so you can see that multi-part upload actually works. I'm very proud of this part. It should be done to some extent. So, if we explore bucket foo, we have a one gigabyte object here that we could also download, and hopefully that works, or maybe it doesn't because of my but it should be downloadable. I think something is blocking my requests. Anyway, that was about it for the demo, but if we turn on the administration that we could also technically manage the users, I think. It's just been difficult. Oh yeah, and I got the object downloading now. Amazing. But yeah, we could create new users. We still don't support user quota, bucket quota, stuff like that. All of this is still very much work in progress. Creating buckets can also be done via the UI. We could enable versioning. Versioning is already supported. I'm just not doing that because I don't remember how to demo that part. We have tests for that stuff. Okay, so this is as far as the demo goes. Let me just list the buckets here so that you see that a bucket bar has been created, and we have a bucket bar over there. That's thrilling. Okay, let me just go back to the other thing and go back to the presentation. Slideshell, so from current slides. Okay, so next steps. So for now our roadmap is to actually increase the number of operations that we actually support because the operations, the RGW basically supports everything that exists, but then the driver behind it needs to comply with the expected semantics and the expected, you know, you request data to the backend and the backend should probably return the appropriate data so that the client is able to perform the operations. And we've been doing this gradually. There has been some challenges there of which, so we are in the process of implementing life cycle management for buckets, retention policies. The performance currently is far from ideal, but we're working towards that. And I really want statistics on the UI. I mean, having as much information that is useful to the user through the UI is something that is very much on my to the list, not necessarily on the to the list for the project, but that's another thing. In terms of challenges that we currently face is a semantic compliance with the S3 API. As I was saying, we have, we've been having some challenges ensuring that our driver replies or provides the right information when processing the operations that are requested by RGW. Fortunately, there is an amazing project called S3 tests within the self repository, which covers pretty much all of the API as far as we understand. And it's very, it's very good to actually ensure that we are in compliance with the API. Then we have performance. This has been a big learning curve, especially with SQLite. There have been decisions that were made during the implementation of the project, especially surrounding new taxes that bid us eventually. But fortunately, we have a Marcel actually looking into this stuff. So if, and that's one of the things that we have here, we have here a comparison of our performance, previous performance with the performance that comes with some of the work Marcel has been doing, it may not look like a lot, especially when we compare with FIO. But I mean, he filled with a few mutexes and we got a significant speed up on a very let's say that the machine is far from current. So we are also believing that we are CPU bound some extent and not taking full advantage of the IO path. But that's neither here nor there. What matters is that we are performing gradual performance improvements that will eventually pay off. This is just the latency distribution. So the YOLO branch is what Marcel called that branch mostly because it removed a bunch of mutexes. What we see here is that to some extent the latencies dropped a bit for put, even though they increased significantly forget. But we are also believing that we are, given that we are also having more operations in flight that we may actually be CPU bound and the operations may not be finishing because of concurrency issues and whatnot. After that, eventually world domination, I think, but probably not. But that's the hope. And that's it. If you want to find us, we are at S3 Gateway IO. And that's about it. Thank you for enduring my presentation. Thank you. Any questions? No? Awesome sauce. Great. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 14.200000000000001, "text": " So, I'm Jo\u00e3o. I work at SUSE, at the storage team. I used to work on SAF. Our storage team", "tokens": [50364, 407, 11, 286, 478, 21302, 13, 286, 589, 412, 40117, 36, 11, 412, 264, 6725, 1469, 13, 286, 1143, 281, 589, 322, 16482, 37, 13, 2621, 6725, 1469, 51074], "temperature": 0.0, "avg_logprob": -0.2152677717662993, "compression_ratio": 1.5166666666666666, "no_speech_prob": 0.32635697722435}, {"id": 1, "seek": 0, "start": 14.200000000000001, "end": 22.0, "text": " used to work on SAF, but due to restructurings, that's no longer the case. I'm going to talk", "tokens": [51074, 1143, 281, 589, 322, 16482, 37, 11, 457, 3462, 281, 1472, 1757, 374, 1109, 11, 300, 311, 572, 2854, 264, 1389, 13, 286, 478, 516, 281, 751, 51464], "temperature": 0.0, "avg_logprob": -0.2152677717662993, "compression_ratio": 1.5166666666666666, "no_speech_prob": 0.32635697722435}, {"id": 2, "seek": 0, "start": 22.0, "end": 29.2, "text": " to you about one of our latest projects, Yes Free Gateway, and I'm going to tell you why", "tokens": [51464, 281, 291, 466, 472, 295, 527, 6792, 4455, 11, 1079, 11551, 48394, 11, 293, 286, 478, 516, 281, 980, 291, 983, 51824], "temperature": 0.0, "avg_logprob": -0.2152677717662993, "compression_ratio": 1.5166666666666666, "no_speech_prob": 0.32635697722435}, {"id": 3, "seek": 2920, "start": 29.2, "end": 37.64, "text": " we're doing this mostly. What is S3 Gateway? How we're doing it? Hopefully there will be", "tokens": [50364, 321, 434, 884, 341, 5240, 13, 708, 307, 318, 18, 48394, 30, 1012, 321, 434, 884, 309, 30, 10429, 456, 486, 312, 50786], "temperature": 0.0, "avg_logprob": -0.20953975533539393, "compression_ratio": 1.3671875, "no_speech_prob": 0.10642555356025696}, {"id": 4, "seek": 2920, "start": 37.64, "end": 48.8, "text": " a demo, and then the next steps and what's ahead of us. So, why are we doing this? So,", "tokens": [50786, 257, 10723, 11, 293, 550, 264, 958, 4439, 293, 437, 311, 2286, 295, 505, 13, 407, 11, 983, 366, 321, 884, 341, 30, 407, 11, 51344], "temperature": 0.0, "avg_logprob": -0.20953975533539393, "compression_ratio": 1.3671875, "no_speech_prob": 0.10642555356025696}, {"id": 5, "seek": 4880, "start": 48.8, "end": 61.31999999999999, "text": " essentially, after our product was restructured, we needed to find something else to work on,", "tokens": [50364, 4476, 11, 934, 527, 1674, 390, 1472, 46847, 11, 321, 2978, 281, 915, 746, 1646, 281, 589, 322, 11, 50990], "temperature": 0.0, "avg_logprob": -0.23463234694107718, "compression_ratio": 1.1772151898734178, "no_speech_prob": 0.1486288160085678}, {"id": 6, "seek": 6132, "start": 61.32, "end": 80.52, "text": " and one of the ideas we had was to find a way to, on the one hand, to figure out a way to provide,", "tokens": [50364, 293, 472, 295, 264, 3487, 321, 632, 390, 281, 915, 257, 636, 281, 11, 322, 264, 472, 1011, 11, 281, 2573, 484, 257, 636, 281, 2893, 11, 51324], "temperature": 0.0, "avg_logprob": -0.20740510867192194, "compression_ratio": 1.441860465116279, "no_speech_prob": 0.6559233665466309}, {"id": 7, "seek": 6132, "start": 80.52, "end": 89.52, "text": " I'm lacking the word, provide something that was lacking in the SUSE Rancher portfolio,", "tokens": [51324, 286, 478, 20889, 264, 1349, 11, 2893, 746, 300, 390, 20889, 294, 264, 40117, 36, 37740, 260, 12583, 11, 51774], "temperature": 0.0, "avg_logprob": -0.20740510867192194, "compression_ratio": 1.441860465116279, "no_speech_prob": 0.6559233665466309}, {"id": 8, "seek": 8952, "start": 89.52, "end": 98.39999999999999, "text": " which is basically an S3 service for local clusters, for local storage within a Kubernetes", "tokens": [50364, 597, 307, 1936, 364, 318, 18, 2643, 337, 2654, 23313, 11, 337, 2654, 6725, 1951, 257, 23145, 50808], "temperature": 0.0, "avg_logprob": -0.16014048258463542, "compression_ratio": 1.569767441860465, "no_speech_prob": 0.10789482295513153}, {"id": 9, "seek": 8952, "start": 98.39999999999999, "end": 106.72, "text": " cluster, and what we aimed at was something as easy to deploy, as easily forgettable,", "tokens": [50808, 13630, 11, 293, 437, 321, 20540, 412, 390, 746, 382, 1858, 281, 7274, 11, 382, 3612, 2870, 23811, 11, 51224], "temperature": 0.0, "avg_logprob": -0.16014048258463542, "compression_ratio": 1.569767441860465, "no_speech_prob": 0.10789482295513153}, {"id": 10, "seek": 8952, "start": 106.72, "end": 116.0, "text": " and that just works for ideally local workloads, not necessarily something that is complex to", "tokens": [51224, 293, 300, 445, 1985, 337, 22915, 2654, 32452, 11, 406, 4725, 746, 300, 307, 3997, 281, 51688], "temperature": 0.0, "avg_logprob": -0.16014048258463542, "compression_ratio": 1.569767441860465, "no_speech_prob": 0.10789482295513153}, {"id": 11, "seek": 11600, "start": 116.0, "end": 123.72, "text": " manage. We didn't want anything that would be, we wanted something as light as possible,", "tokens": [50364, 3067, 13, 492, 994, 380, 528, 1340, 300, 576, 312, 11, 321, 1415, 746, 382, 1442, 382, 1944, 11, 50750], "temperature": 0.0, "avg_logprob": -0.21669650077819824, "compression_ratio": 1.3525179856115108, "no_speech_prob": 0.04257609322667122}, {"id": 12, "seek": 11600, "start": 123.72, "end": 135.36, "text": " and that's what eventually became the S3 Gateway. It's an open source project as usual. It's driven", "tokens": [50750, 293, 300, 311, 437, 4728, 3062, 264, 318, 18, 48394, 13, 467, 311, 364, 1269, 4009, 1716, 382, 7713, 13, 467, 311, 9555, 51332], "temperature": 0.0, "avg_logprob": -0.21669650077819824, "compression_ratio": 1.3525179856115108, "no_speech_prob": 0.04257609322667122}, {"id": 13, "seek": 13536, "start": 135.36, "end": 147.72000000000003, "text": " by our team at SUSE, and ideally it will be an easy to use project that you just deploy", "tokens": [50364, 538, 527, 1469, 412, 40117, 36, 11, 293, 22915, 309, 486, 312, 364, 1858, 281, 764, 1716, 300, 291, 445, 7274, 50982], "temperature": 0.0, "avg_logprob": -0.16434579655744028, "compression_ratio": 1.4972972972972973, "no_speech_prob": 0.11394407600164413}, {"id": 14, "seek": 13536, "start": 147.72000000000003, "end": 153.64000000000001, "text": " on a Kubernetes cluster and it will just provide you an S3 service within your cluster. I say,", "tokens": [50982, 322, 257, 23145, 13630, 293, 309, 486, 445, 2893, 291, 364, 318, 18, 2643, 1951, 428, 13630, 13, 286, 584, 11, 51278], "temperature": 0.0, "avg_logprob": -0.16434579655744028, "compression_ratio": 1.4972972972972973, "no_speech_prob": 0.11394407600164413}, {"id": 15, "seek": 13536, "start": 153.64000000000001, "end": 160.60000000000002, "text": " ideally, because this has six months worth of development, and there are a lot of things still", "tokens": [51278, 22915, 11, 570, 341, 575, 2309, 2493, 3163, 295, 3250, 11, 293, 456, 366, 257, 688, 295, 721, 920, 51626], "temperature": 0.0, "avg_logprob": -0.16434579655744028, "compression_ratio": 1.4972972972972973, "no_speech_prob": 0.11394407600164413}, {"id": 16, "seek": 16060, "start": 160.6, "end": 170.07999999999998, "text": " lacking, far more than I would actually like, but that's just how life is. It complements the", "tokens": [50364, 20889, 11, 1400, 544, 813, 286, 576, 767, 411, 11, 457, 300, 311, 445, 577, 993, 307, 13, 467, 715, 17988, 264, 50838], "temperature": 0.0, "avg_logprob": -0.1316665013631185, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.026270946487784386}, {"id": 17, "seek": 16060, "start": 170.07999999999998, "end": 176.32, "text": " Rancher portfolio, as I mentioned, but this was not necessarily the main driver when developing", "tokens": [50838, 37740, 260, 12583, 11, 382, 286, 2835, 11, 457, 341, 390, 406, 4725, 264, 2135, 6787, 562, 6416, 51150], "temperature": 0.0, "avg_logprob": -0.1316665013631185, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.026270946487784386}, {"id": 18, "seek": 16060, "start": 176.32, "end": 187.24, "text": " the S3 Gateway. It just happens to fit nicely within the stack. It helps doing backups of local", "tokens": [51150, 264, 318, 18, 48394, 13, 467, 445, 2314, 281, 3318, 9594, 1951, 264, 8630, 13, 467, 3665, 884, 50160, 295, 2654, 51696], "temperature": 0.0, "avg_logprob": -0.1316665013631185, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.026270946487784386}, {"id": 19, "seek": 18724, "start": 187.28, "end": 198.68, "text": " Longhorn volumes, backups for other stuff within the stack, and one of our main criteria initially", "tokens": [50366, 8282, 31990, 22219, 11, 50160, 337, 661, 1507, 1951, 264, 8630, 11, 293, 472, 295, 527, 2135, 11101, 9105, 50936], "temperature": 0.0, "avg_logprob": -0.15065139532089233, "compression_ratio": 1.518918918918919, "no_speech_prob": 0.009252455085515976}, {"id": 20, "seek": 18724, "start": 198.68, "end": 206.72, "text": " was that we would serve our storage from any PVC that a Kubernetes cluster could provide,", "tokens": [50936, 390, 300, 321, 576, 4596, 527, 6725, 490, 604, 46700, 300, 257, 23145, 13630, 727, 2893, 11, 51338], "temperature": 0.0, "avg_logprob": -0.15065139532089233, "compression_ratio": 1.518918918918919, "no_speech_prob": 0.009252455085515976}, {"id": 21, "seek": 18724, "start": 206.72, "end": 214.04000000000002, "text": " and this just happens to be nice because given Longhorn allows us to put stuff on a Longhorn", "tokens": [51338, 293, 341, 445, 2314, 281, 312, 1481, 570, 2212, 8282, 31990, 4045, 505, 281, 829, 1507, 322, 257, 8282, 31990, 51704], "temperature": 0.0, "avg_logprob": -0.15065139532089233, "compression_ratio": 1.518918918918919, "no_speech_prob": 0.009252455085515976}, {"id": 22, "seek": 21404, "start": 214.48, "end": 220.4, "text": " persistent volume, Longhorn will deal with all the nasty things like replication and whatnot", "tokens": [50386, 24315, 5523, 11, 8282, 31990, 486, 2028, 365, 439, 264, 17923, 721, 411, 39911, 293, 25882, 50682], "temperature": 0.0, "avg_logprob": -0.18587564019595876, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.008552800863981247}, {"id": 23, "seek": 21404, "start": 220.4, "end": 227.88, "text": " so that we don't have to deal with that. Of course, we wanted a pretty UI for all of the", "tokens": [50682, 370, 300, 321, 500, 380, 362, 281, 2028, 365, 300, 13, 2720, 1164, 11, 321, 1415, 257, 1238, 15682, 337, 439, 295, 264, 51056], "temperature": 0.0, "avg_logprob": -0.18587564019595876, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.008552800863981247}, {"id": 24, "seek": 21404, "start": 227.88, "end": 237.64, "text": " operations and management, which is still ongoing, but we'll get there. How we're leveraging this,", "tokens": [51056, 7705, 293, 4592, 11, 597, 307, 920, 10452, 11, 457, 321, 603, 483, 456, 13, 1012, 321, 434, 32666, 341, 11, 51544], "temperature": 0.0, "avg_logprob": -0.18587564019595876, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.008552800863981247}, {"id": 25, "seek": 23764, "start": 237.95999999999998, "end": 248.0, "text": " though, is basically using Rados Gateway from SEF. We didn't want to start something from scratch", "tokens": [50380, 1673, 11, 307, 1936, 1228, 497, 4181, 48394, 490, 10269, 37, 13, 492, 994, 380, 528, 281, 722, 746, 490, 8459, 50882], "temperature": 0.0, "avg_logprob": -0.27729737240335217, "compression_ratio": 1.3404255319148937, "no_speech_prob": 0.03102090023458004}, {"id": 26, "seek": 23764, "start": 248.0, "end": 256.15999999999997, "text": " because we thought it would be a waste of time and resources, so we decided to leverage the", "tokens": [50882, 570, 321, 1194, 309, 576, 312, 257, 5964, 295, 565, 293, 3593, 11, 370, 321, 3047, 281, 13982, 264, 51290], "temperature": 0.0, "avg_logprob": -0.27729737240335217, "compression_ratio": 1.3404255319148937, "no_speech_prob": 0.03102090023458004}, {"id": 27, "seek": 25616, "start": 256.24, "end": 269.20000000000005, "text": " Rados Gateway from SEF, which is quite amazing because it can be run standalone. Given the already", "tokens": [50368, 497, 4181, 48394, 490, 10269, 37, 11, 597, 307, 1596, 2243, 570, 309, 393, 312, 1190, 37454, 13, 18600, 264, 1217, 51016], "temperature": 0.0, "avg_logprob": -0.20652483023849189, "compression_ratio": 1.32, "no_speech_prob": 0.026227932423353195}, {"id": 28, "seek": 25616, "start": 269.20000000000005, "end": 278.68, "text": " built-in zipper layer of which there is a talk next, I think, we basically just had to create a new", "tokens": [51016, 3094, 12, 259, 29887, 4583, 295, 597, 456, 307, 257, 751, 958, 11, 286, 519, 11, 321, 1936, 445, 632, 281, 1884, 257, 777, 51490], "temperature": 0.0, "avg_logprob": -0.20652483023849189, "compression_ratio": 1.32, "no_speech_prob": 0.026227932423353195}, {"id": 29, "seek": 27868, "start": 278.72, "end": 291.48, "text": " backend that is basically file-based on which our data is stored on the file-based backend instead", "tokens": [50366, 38087, 300, 307, 1936, 3991, 12, 6032, 322, 597, 527, 1412, 307, 12187, 322, 264, 3991, 12, 6032, 38087, 2602, 51004], "temperature": 0.0, "avg_logprob": -0.18821833683894232, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.007092880085110664}, {"id": 30, "seek": 27868, "start": 291.48, "end": 300.2, "text": " of, say, the Rados store. Hence, we don't need a whole SEF cluster, we just need the binary", "tokens": [51004, 295, 11, 584, 11, 264, 497, 4181, 3531, 13, 22229, 11, 321, 500, 380, 643, 257, 1379, 10269, 37, 13630, 11, 321, 445, 643, 264, 17434, 51440], "temperature": 0.0, "avg_logprob": -0.18821833683894232, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.007092880085110664}, {"id": 31, "seek": 30020, "start": 300.24, "end": 312.76, "text": " running standalone. Essentially, as I was saying, we have the Rados Gateway consuming a file system,", "tokens": [50366, 2614, 37454, 13, 23596, 11, 382, 286, 390, 1566, 11, 321, 362, 264, 497, 4181, 48394, 19867, 257, 3991, 1185, 11, 50992], "temperature": 0.0, "avg_logprob": -0.16080953809950088, "compression_ratio": 1.3714285714285714, "no_speech_prob": 0.0039373221807181835}, {"id": 32, "seek": 30020, "start": 312.76, "end": 320.32, "text": " essentially whatever that file system is on. We keep a SQLite database for metadata for the", "tokens": [50992, 4476, 2035, 300, 3991, 1185, 307, 322, 13, 492, 1066, 257, 19200, 642, 8149, 337, 26603, 337, 264, 51370], "temperature": 0.0, "avg_logprob": -0.16080953809950088, "compression_ratio": 1.3714285714285714, "no_speech_prob": 0.0039373221807181835}, {"id": 33, "seek": 32032, "start": 320.36, "end": 334.4, "text": " objects and the directory hierarchy for the data. We decided to do this so that essentially all the", "tokens": [50366, 6565, 293, 264, 21120, 22333, 337, 264, 1412, 13, 492, 3047, 281, 360, 341, 370, 300, 4476, 439, 264, 51068], "temperature": 0.0, "avg_logprob": -0.186105085455853, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.010809147730469704}, {"id": 34, "seek": 32032, "start": 334.4, "end": 347.76, "text": " things that can be indexed and to be essentially abstract could be kept as metadata in the SQLite,", "tokens": [51068, 721, 300, 393, 312, 8186, 292, 293, 281, 312, 4476, 12649, 727, 312, 4305, 382, 26603, 294, 264, 19200, 642, 11, 51736], "temperature": 0.0, "avg_logprob": -0.186105085455853, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.010809147730469704}, {"id": 35, "seek": 34776, "start": 347.84, "end": 354.96, "text": " which will allow us to more easily search things, index things instead of having to go through", "tokens": [50368, 597, 486, 2089, 505, 281, 544, 3612, 3164, 721, 11, 8186, 721, 2602, 295, 1419, 281, 352, 807, 50724], "temperature": 0.0, "avg_logprob": -0.20777692514307358, "compression_ratio": 1.4974093264248705, "no_speech_prob": 0.008306189440190792}, {"id": 36, "seek": 34776, "start": 356.88, "end": 363.76, "text": " a directory hierarchy to find, for instance, buckets and whatnot. So buckets are essentially a mapping", "tokens": [50820, 257, 21120, 22333, 281, 915, 11, 337, 5197, 11, 32191, 293, 25882, 13, 407, 32191, 366, 4476, 257, 18350, 51164], "temperature": 0.0, "avg_logprob": -0.20777692514307358, "compression_ratio": 1.4974093264248705, "no_speech_prob": 0.008306189440190792}, {"id": 37, "seek": 34776, "start": 363.76, "end": 372.71999999999997, "text": " of a name to a UUID and objects, as well, end up being entries in a database that associate", "tokens": [51164, 295, 257, 1315, 281, 257, 624, 52, 2777, 293, 6565, 11, 382, 731, 11, 917, 493, 885, 23041, 294, 257, 8149, 300, 14644, 51612], "temperature": 0.0, "avg_logprob": -0.20777692514307358, "compression_ratio": 1.4974093264248705, "no_speech_prob": 0.008306189440190792}, {"id": 38, "seek": 37272, "start": 372.8, "end": 379.84000000000003, "text": " the bucket name, the object name, and our map to a UUID. The data for the objects, though,", "tokens": [50368, 264, 13058, 1315, 11, 264, 2657, 1315, 11, 293, 527, 4471, 281, 257, 624, 52, 2777, 13, 440, 1412, 337, 264, 6565, 11, 1673, 11, 50720], "temperature": 0.0, "avg_logprob": -0.10614825549878572, "compression_ratio": 1.6184971098265897, "no_speech_prob": 0.007931777276098728}, {"id": 39, "seek": 37272, "start": 380.64000000000004, "end": 387.68, "text": " will be based on the UUID and we will grab the UUID and create a directory hierarchy based on the", "tokens": [50760, 486, 312, 2361, 322, 264, 624, 52, 2777, 293, 321, 486, 4444, 264, 624, 52, 2777, 293, 1884, 257, 21120, 22333, 2361, 322, 264, 51112], "temperature": 0.0, "avg_logprob": -0.10614825549878572, "compression_ratio": 1.6184971098265897, "no_speech_prob": 0.007931777276098728}, {"id": 40, "seek": 37272, "start": 388.72, "end": 400.64000000000004, "text": " first bytes of the UUID. The reasoning behind this was mostly because, given typically some", "tokens": [51164, 700, 36088, 295, 264, 624, 52, 2777, 13, 440, 21577, 2261, 341, 390, 5240, 570, 11, 2212, 5850, 512, 51760], "temperature": 0.0, "avg_logprob": -0.10614825549878572, "compression_ratio": 1.6184971098265897, "no_speech_prob": 0.007931777276098728}, {"id": 41, "seek": 40064, "start": 400.64, "end": 406.96, "text": " buckets tend to grow larger than other buckets, if we were creating a directory hierarchy that", "tokens": [50364, 32191, 3928, 281, 1852, 4833, 813, 661, 32191, 11, 498, 321, 645, 4084, 257, 21120, 22333, 300, 50680], "temperature": 0.0, "avg_logprob": -0.09965217113494873, "compression_ratio": 1.8487804878048781, "no_speech_prob": 0.0029304337222129107}, {"id": 42, "seek": 40064, "start": 406.96, "end": 413.52, "text": " was per bucket, per bucket name, we could end up with very large directories. This way, we kind", "tokens": [50680, 390, 680, 13058, 11, 680, 13058, 1315, 11, 321, 727, 917, 493, 365, 588, 2416, 5391, 530, 13, 639, 636, 11, 321, 733, 51008], "temperature": 0.0, "avg_logprob": -0.09965217113494873, "compression_ratio": 1.8487804878048781, "no_speech_prob": 0.0029304337222129107}, {"id": 43, "seek": 40064, "start": 413.52, "end": 423.84, "text": " of spread the objects around and even if we end up with larger directories, we don't have to list", "tokens": [51008, 295, 3974, 264, 6565, 926, 293, 754, 498, 321, 917, 493, 365, 4833, 5391, 530, 11, 321, 500, 380, 362, 281, 1329, 51524], "temperature": 0.0, "avg_logprob": -0.09965217113494873, "compression_ratio": 1.8487804878048781, "no_speech_prob": 0.0029304337222129107}, {"id": 44, "seek": 40064, "start": 423.84, "end": 429.68, "text": " the directories themselves to find where the objects are or which objects are within those", "tokens": [51524, 264, 5391, 530, 2969, 281, 915, 689, 264, 6565, 366, 420, 597, 6565, 366, 1951, 729, 51816], "temperature": 0.0, "avg_logprob": -0.09965217113494873, "compression_ratio": 1.8487804878048781, "no_speech_prob": 0.0029304337222129107}, {"id": 45, "seek": 42968, "start": 429.68, "end": 436.8, "text": " directories. We just have that stuff in the metadata devils within SQLite.", "tokens": [50364, 5391, 530, 13, 492, 445, 362, 300, 1507, 294, 264, 26603, 1905, 4174, 1951, 19200, 642, 13, 50720], "temperature": 0.0, "avg_logprob": -0.10068404496605717, "compression_ratio": 1.4010695187165776, "no_speech_prob": 0.0016983953537419438}, {"id": 46, "seek": 42968, "start": 438.88, "end": 446.96000000000004, "text": " Now, this is not pretty, I admit it, but it's the best I could do. This is roughly what translates", "tokens": [50824, 823, 11, 341, 307, 406, 1238, 11, 286, 9796, 309, 11, 457, 309, 311, 264, 1151, 286, 727, 360, 13, 639, 307, 9810, 437, 28468, 51228], "temperature": 0.0, "avg_logprob": -0.10068404496605717, "compression_ratio": 1.4010695187165776, "no_speech_prob": 0.0016983953537419438}, {"id": 47, "seek": 42968, "start": 446.96000000000004, "end": 454.72, "text": " to being the S3 gateway stuff being deployed on a Kubernetes cluster. We are essentially", "tokens": [51228, 281, 885, 264, 318, 18, 28532, 1507, 885, 17826, 322, 257, 23145, 13630, 13, 492, 366, 4476, 51616], "temperature": 0.0, "avg_logprob": -0.10068404496605717, "compression_ratio": 1.4010695187165776, "no_speech_prob": 0.0016983953537419438}, {"id": 48, "seek": 45472, "start": 454.72, "end": 462.16, "text": " deploying two containers, one for the back end, which is rather a gateway, and another one for the", "tokens": [50364, 34198, 732, 17089, 11, 472, 337, 264, 646, 917, 11, 597, 307, 2831, 257, 28532, 11, 293, 1071, 472, 337, 264, 50736], "temperature": 0.0, "avg_logprob": -0.22352895931321748, "compression_ratio": 1.4661654135338347, "no_speech_prob": 0.02093728445470333}, {"id": 49, "seek": 45472, "start": 462.16, "end": 481.20000000000005, "text": " UI. We deploy our store on whatever is the, whatever is supplying us with storage. In this case,", "tokens": [50736, 15682, 13, 492, 7274, 527, 3531, 322, 2035, 307, 264, 11, 2035, 307, 46815, 505, 365, 6725, 13, 682, 341, 1389, 11, 51688], "temperature": 0.0, "avg_logprob": -0.22352895931321748, "compression_ratio": 1.4661654135338347, "no_speech_prob": 0.02093728445470333}, {"id": 50, "seek": 48120, "start": 481.28, "end": 489.76, "text": " this slide has Longhorn on it, which will deal with all the replication and availability and", "tokens": [50368, 341, 4137, 575, 8282, 31990, 322, 309, 11, 597, 486, 2028, 365, 439, 264, 39911, 293, 17945, 293, 50792], "temperature": 0.0, "avg_logprob": -0.12840194172329372, "compression_ratio": 1.450777202072539, "no_speech_prob": 0.036655865609645844}, {"id": 51, "seek": 48120, "start": 489.76, "end": 495.28, "text": " whatnot for us so that we don't have to care about this, but this runs as well in a local,", "tokens": [50792, 25882, 337, 505, 370, 300, 321, 500, 380, 362, 281, 1127, 466, 341, 11, 457, 341, 6676, 382, 731, 294, 257, 2654, 11, 51068], "temperature": 0.0, "avg_logprob": -0.12840194172329372, "compression_ratio": 1.450777202072539, "no_speech_prob": 0.036655865609645844}, {"id": 52, "seek": 48120, "start": 495.28, "end": 503.68, "text": " I'm running this on my Synology NAS. I just have a volume that is exported through the back end.", "tokens": [51068, 286, 478, 2614, 341, 322, 452, 26155, 1793, 10182, 13, 286, 445, 362, 257, 5523, 300, 307, 42055, 807, 264, 646, 917, 13, 51488], "temperature": 0.0, "avg_logprob": -0.12840194172329372, "compression_ratio": 1.450777202072539, "no_speech_prob": 0.036655865609645844}, {"id": 53, "seek": 50368, "start": 504.40000000000003, "end": 514.8, "text": " It really doesn't care about which whatever is providing the file system to it. The UI speaks", "tokens": [50400, 467, 534, 1177, 380, 1127, 466, 597, 2035, 307, 6530, 264, 3991, 1185, 281, 309, 13, 440, 15682, 10789, 50920], "temperature": 0.0, "avg_logprob": -0.15183685765121924, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.006565222516655922}, {"id": 54, "seek": 50368, "start": 514.8, "end": 524.8, "text": " directly with the back end and the user just consumes the S3 gateway if outside of the cluster", "tokens": [50920, 3838, 365, 264, 646, 917, 293, 264, 4195, 445, 48823, 264, 318, 18, 28532, 498, 2380, 295, 264, 13630, 51420], "temperature": 0.0, "avg_logprob": -0.15183685765121924, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.006565222516655922}, {"id": 55, "seek": 50368, "start": 524.8, "end": 530.24, "text": " through an ingress, if inside the cluster through magic, that I don't understand very well.", "tokens": [51420, 807, 364, 3957, 735, 11, 498, 1854, 264, 13630, 807, 5585, 11, 300, 286, 500, 380, 1223, 588, 731, 13, 51692], "temperature": 0.0, "avg_logprob": -0.15183685765121924, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.006565222516655922}, {"id": 56, "seek": 53024, "start": 530.4, "end": 544.88, "text": " I promise the demo, which might work or might not. It has been working 50% of the time in my computer,", "tokens": [50372, 286, 6228, 264, 10723, 11, 597, 1062, 589, 420, 1062, 406, 13, 467, 575, 668, 1364, 2625, 4, 295, 264, 565, 294, 452, 3820, 11, 51096], "temperature": 0.0, "avg_logprob": -0.240228457884355, "compression_ratio": 1.2894736842105263, "no_speech_prob": 0.00852880533784628}, {"id": 57, "seek": 53024, "start": 546.5600000000001, "end": 550.72, "text": " so let me see if I can get this to not this.", "tokens": [51180, 370, 718, 385, 536, 498, 286, 393, 483, 341, 281, 406, 341, 13, 51388], "temperature": 0.0, "avg_logprob": -0.240228457884355, "compression_ratio": 1.2894736842105263, "no_speech_prob": 0.00852880533784628}, {"id": 58, "seek": 55072, "start": 551.44, "end": 560.88, "text": " So, you're not seeing anything, of course not. How do I, Jan, how do I...", "tokens": [50400, 407, 11, 291, 434, 406, 2577, 1340, 11, 295, 1164, 406, 13, 1012, 360, 286, 11, 4956, 11, 577, 360, 286, 485, 50872], "temperature": 0.0, "avg_logprob": -0.5057655114393967, "compression_ratio": 1.028169014084507, "no_speech_prob": 0.06999711692333221}, {"id": 59, "seek": 56088, "start": 560.88, "end": 589.12, "text": " So, ideally what we'll see", "tokens": [50364, 407, 11, 22915, 437, 321, 603, 536, 51776], "temperature": 0.0, "avg_logprob": -0.619420051574707, "compression_ratio": 0.7647058823529411, "no_speech_prob": 0.07015778124332428}, {"id": 60, "seek": 59088, "start": 590.88, "end": 617.04, "text": " is already deployed K3S, which is running things, but it's not running", "tokens": [50364, 307, 1217, 17826, 591, 18, 50, 11, 597, 307, 2614, 721, 11, 457, 309, 311, 406, 2614, 51672], "temperature": 0.0, "avg_logprob": -0.2919858523777553, "compression_ratio": 1.0, "no_speech_prob": 0.008311053737998009}, {"id": 61, "seek": 61704, "start": 617.04, "end": 631.4399999999999, "text": " the S3 gateway yet. What I'm going to do is to deploy with our chart, which usually works", "tokens": [50364, 264, 318, 18, 28532, 1939, 13, 708, 286, 478, 516, 281, 360, 307, 281, 7274, 365, 527, 6927, 11, 597, 2673, 1985, 51084], "temperature": 0.0, "avg_logprob": -0.16155524055163065, "compression_ratio": 1.27007299270073, "no_speech_prob": 0.06752060353755951}, {"id": 62, "seek": 61704, "start": 631.4399999999999, "end": 643.1999999999999, "text": " for people. My laptop is being difficult, so let me just see if I remember its home.", "tokens": [51084, 337, 561, 13, 1222, 10732, 307, 885, 2252, 11, 370, 718, 385, 445, 536, 498, 286, 1604, 1080, 1280, 13, 51672], "temperature": 0.0, "avg_logprob": -0.16155524055163065, "compression_ratio": 1.27007299270073, "no_speech_prob": 0.06752060353755951}, {"id": 63, "seek": 64320, "start": 643.76, "end": 650.32, "text": " So, basically we install this using the default values,", "tokens": [50392, 407, 11, 1936, 321, 3625, 341, 1228, 264, 7576, 4190, 11, 50720], "temperature": 0.0, "avg_logprob": -0.20108754494610956, "compression_ratio": 1.233644859813084, "no_speech_prob": 0.0062646581791341305}, {"id": 64, "seek": 64320, "start": 653.0400000000001, "end": 662.6400000000001, "text": " and supposedly we will have the UI available at this URL here, not this one.", "tokens": [50856, 293, 20581, 321, 486, 362, 264, 15682, 2435, 412, 341, 12905, 510, 11, 406, 341, 472, 13, 51336], "temperature": 0.0, "avg_logprob": -0.20108754494610956, "compression_ratio": 1.233644859813084, "no_speech_prob": 0.0062646581791341305}, {"id": 65, "seek": 66264, "start": 663.52, "end": 674.96, "text": " Now, there is still a lot of kinks to figure out with this stuff. We have the UI being run", "tokens": [50408, 823, 11, 456, 307, 920, 257, 688, 295, 350, 16431, 281, 2573, 484, 365, 341, 1507, 13, 492, 362, 264, 15682, 885, 1190, 50980], "temperature": 0.0, "avg_logprob": -0.1659913592868381, "compression_ratio": 1.0714285714285714, "no_speech_prob": 0.008303086273372173}, {"id": 66, "seek": 67496, "start": 674.96, "end": 694.8000000000001, "text": " here, but the UI right now is unable to talk with the back end, and this is because", "tokens": [50364, 510, 11, 457, 264, 15682, 558, 586, 307, 11299, 281, 751, 365, 264, 646, 917, 11, 293, 341, 307, 570, 51356], "temperature": 0.0, "avg_logprob": -0.2554616729418437, "compression_ratio": 1.064102564102564, "no_speech_prob": 0.039267729967832565}, {"id": 67, "seek": 69480, "start": 695.3599999999999, "end": 706.0799999999999, "text": " certificates. Because we are using a self-signed certificate that the browser does not understand,", "tokens": [50392, 32941, 13, 1436, 321, 366, 1228, 257, 2698, 12, 82, 16690, 15953, 300, 264, 11185, 775, 406, 1223, 11, 50928], "temperature": 0.0, "avg_logprob": -0.19062112589351465, "compression_ratio": 1.579268292682927, "no_speech_prob": 0.07840713113546371}, {"id": 68, "seek": 69480, "start": 706.0799999999999, "end": 712.16, "text": " and the browser is directly talking to the back end through the UI, there is no", "tokens": [50928, 293, 264, 11185, 307, 3838, 1417, 281, 264, 646, 917, 807, 264, 15682, 11, 456, 307, 572, 51232], "temperature": 0.0, "avg_logprob": -0.19062112589351465, "compression_ratio": 1.579268292682927, "no_speech_prob": 0.07840713113546371}, {"id": 69, "seek": 69480, "start": 713.92, "end": 723.8399999999999, "text": " actual demon in between the UI and the back end. What happens is that we have to", "tokens": [51320, 3539, 14283, 294, 1296, 264, 15682, 293, 264, 646, 917, 13, 708, 2314, 307, 300, 321, 362, 281, 51816], "temperature": 0.0, "avg_logprob": -0.19062112589351465, "compression_ratio": 1.579268292682927, "no_speech_prob": 0.07840713113546371}, {"id": 70, "seek": 72480, "start": 725.3599999999999, "end": 739.68, "text": " go to the back end and accept the certificate, which is hilarious,", "tokens": [50392, 352, 281, 264, 646, 917, 293, 3241, 264, 15953, 11, 597, 307, 19796, 11, 51108], "temperature": 0.0, "avg_logprob": -0.15640941533175381, "compression_ratio": 1.3448275862068966, "no_speech_prob": 0.0006759997340850532}, {"id": 71, "seek": 72480, "start": 742.0799999999999, "end": 751.12, "text": " and once this is done, we can log in to the UI. So, right now the UI is still under heavy", "tokens": [51228, 293, 1564, 341, 307, 1096, 11, 321, 393, 3565, 294, 281, 264, 15682, 13, 407, 11, 558, 586, 264, 15682, 307, 920, 833, 4676, 51680], "temperature": 0.0, "avg_logprob": -0.15640941533175381, "compression_ratio": 1.3448275862068966, "no_speech_prob": 0.0006759997340850532}, {"id": 72, "seek": 75112, "start": 751.12, "end": 758.24, "text": " development, and the UI cannot do a lot more than the back end does. We are still lacking", "tokens": [50364, 3250, 11, 293, 264, 15682, 2644, 360, 257, 688, 544, 813, 264, 646, 917, 775, 13, 492, 366, 920, 20889, 50720], "temperature": 0.0, "avg_logprob": -0.11728501827158827, "compression_ratio": 1.3893129770992367, "no_speech_prob": 0.015337308868765831}, {"id": 73, "seek": 75112, "start": 758.88, "end": 766.24, "text": " a bunch of things implemented in the back end driver, but I just want to show you that if we", "tokens": [50752, 257, 3840, 295, 721, 12270, 294, 264, 646, 917, 6787, 11, 457, 286, 445, 528, 281, 855, 291, 300, 498, 321, 51120], "temperature": 0.0, "avg_logprob": -0.11728501827158827, "compression_ratio": 1.3893129770992367, "no_speech_prob": 0.015337308868765831}, {"id": 74, "seek": 76624, "start": 766.24, "end": 779.12, "text": " do things on against the back end, it will actually happen in the front end. So, let me see.", "tokens": [50364, 360, 721, 322, 1970, 264, 646, 917, 11, 309, 486, 767, 1051, 294, 264, 1868, 917, 13, 407, 11, 718, 385, 536, 13, 51008], "temperature": 0.0, "avg_logprob": -0.17525705999257613, "compression_ratio": 1.3157894736842106, "no_speech_prob": 0.0508539080619812}, {"id": 75, "seek": 76624, "start": 780.16, "end": 790.5600000000001, "text": " This is where this stuff is. It has three commands. If I put a one gigabyte object", "tokens": [51060, 639, 307, 689, 341, 1507, 307, 13, 467, 575, 1045, 16901, 13, 759, 286, 829, 257, 472, 8741, 34529, 2657, 51580], "temperature": 0.0, "avg_logprob": -0.17525705999257613, "compression_ratio": 1.3157894736842106, "no_speech_prob": 0.0508539080619812}, {"id": 76, "seek": 79056, "start": 791.52, "end": 804.0799999999999, "text": " onto a bucket that I actually need to create first, if I create a bucket foo on the back end,", "tokens": [50412, 3911, 257, 13058, 300, 286, 767, 643, 281, 1884, 700, 11, 498, 286, 1884, 257, 13058, 726, 78, 322, 264, 646, 917, 11, 51040], "temperature": 0.0, "avg_logprob": -0.20138337061955378, "compression_ratio": 1.5, "no_speech_prob": 0.012134993448853493}, {"id": 77, "seek": 79056, "start": 804.7199999999999, "end": 813.28, "text": " it will actually show on the front end, which is expected, and it would suck if it didn't work.", "tokens": [51072, 309, 486, 767, 855, 322, 264, 1868, 917, 11, 597, 307, 5176, 11, 293, 309, 576, 9967, 498, 309, 994, 380, 589, 13, 51500], "temperature": 0.0, "avg_logprob": -0.20138337061955378, "compression_ratio": 1.5, "no_speech_prob": 0.012134993448853493}, {"id": 78, "seek": 81328, "start": 813.4399999999999, "end": 822.24, "text": " Putting an object there will also allow us to do some exploration.", "tokens": [50372, 31367, 364, 2657, 456, 486, 611, 2089, 505, 281, 360, 512, 16197, 13, 50812], "temperature": 0.0, "avg_logprob": -0.21386977301703558, "compression_ratio": 1.2761194029850746, "no_speech_prob": 0.01745399460196495}, {"id": 79, "seek": 81328, "start": 823.92, "end": 830.48, "text": " This is a big object, so you can see that multi-part upload actually works. I'm very proud of this part.", "tokens": [50896, 639, 307, 257, 955, 2657, 11, 370, 291, 393, 536, 300, 4825, 12, 6971, 6580, 767, 1985, 13, 286, 478, 588, 4570, 295, 341, 644, 13, 51224], "temperature": 0.0, "avg_logprob": -0.21386977301703558, "compression_ratio": 1.2761194029850746, "no_speech_prob": 0.01745399460196495}, {"id": 80, "seek": 83048, "start": 831.2, "end": 846.48, "text": " It should be done to some extent. So, if we explore bucket foo, we have a one gigabyte", "tokens": [50400, 467, 820, 312, 1096, 281, 512, 8396, 13, 407, 11, 498, 321, 6839, 13058, 726, 78, 11, 321, 362, 257, 472, 8741, 34529, 51164], "temperature": 0.0, "avg_logprob": -0.2029353904724121, "compression_ratio": 1.3453237410071943, "no_speech_prob": 0.010500971227884293}, {"id": 81, "seek": 83048, "start": 846.48, "end": 855.6, "text": " object here that we could also download, and hopefully that works, or maybe it doesn't because of my", "tokens": [51164, 2657, 510, 300, 321, 727, 611, 5484, 11, 293, 4696, 300, 1985, 11, 420, 1310, 309, 1177, 380, 570, 295, 452, 51620], "temperature": 0.0, "avg_logprob": -0.2029353904724121, "compression_ratio": 1.3453237410071943, "no_speech_prob": 0.010500971227884293}, {"id": 82, "seek": 85560, "start": 856.5600000000001, "end": 862.72, "text": " but it should be downloadable. I think something is blocking my requests.", "tokens": [50412, 457, 309, 820, 312, 5484, 712, 13, 286, 519, 746, 307, 17776, 452, 12475, 13, 50720], "temperature": 0.0, "avg_logprob": -0.15013711306513572, "compression_ratio": 1.4113475177304964, "no_speech_prob": 0.017605669796466827}, {"id": 83, "seek": 85560, "start": 864.72, "end": 872.96, "text": " Anyway, that was about it for the demo, but if we turn on the administration", "tokens": [50820, 5684, 11, 300, 390, 466, 309, 337, 264, 10723, 11, 457, 498, 321, 1261, 322, 264, 7236, 51232], "temperature": 0.0, "avg_logprob": -0.15013711306513572, "compression_ratio": 1.4113475177304964, "no_speech_prob": 0.017605669796466827}, {"id": 84, "seek": 85560, "start": 874.4, "end": 877.12, "text": " that we could also technically manage the users,", "tokens": [51304, 300, 321, 727, 611, 12120, 3067, 264, 5022, 11, 51440], "temperature": 0.0, "avg_logprob": -0.15013711306513572, "compression_ratio": 1.4113475177304964, "no_speech_prob": 0.017605669796466827}, {"id": 85, "seek": 87712, "start": 878.0, "end": 892.48, "text": " I think. It's just been difficult. Oh yeah, and I got the object downloading now. Amazing.", "tokens": [50408, 286, 519, 13, 467, 311, 445, 668, 2252, 13, 876, 1338, 11, 293, 286, 658, 264, 2657, 32529, 586, 13, 14165, 13, 51132], "temperature": 0.0, "avg_logprob": -0.22597747582655686, "compression_ratio": 1.3129251700680271, "no_speech_prob": 0.015405088663101196}, {"id": 86, "seek": 87712, "start": 894.72, "end": 902.08, "text": " But yeah, we could create new users. We still don't support user quota, bucket quota, stuff like that.", "tokens": [51244, 583, 1338, 11, 321, 727, 1884, 777, 5022, 13, 492, 920, 500, 380, 1406, 4195, 45171, 11, 13058, 45171, 11, 1507, 411, 300, 13, 51612], "temperature": 0.0, "avg_logprob": -0.22597747582655686, "compression_ratio": 1.3129251700680271, "no_speech_prob": 0.015405088663101196}, {"id": 87, "seek": 90208, "start": 903.0400000000001, "end": 905.6800000000001, "text": " All of this is still very much work in progress.", "tokens": [50412, 1057, 295, 341, 307, 920, 588, 709, 589, 294, 4205, 13, 50544], "temperature": 0.0, "avg_logprob": -0.25989687884295426, "compression_ratio": 1.0444444444444445, "no_speech_prob": 0.029695700854063034}, {"id": 88, "seek": 90208, "start": 910.08, "end": 919.12, "text": " Creating buckets can also be done via the UI.", "tokens": [50764, 40002, 32191, 393, 611, 312, 1096, 5766, 264, 15682, 13, 51216], "temperature": 0.0, "avg_logprob": -0.25989687884295426, "compression_ratio": 1.0444444444444445, "no_speech_prob": 0.029695700854063034}, {"id": 89, "seek": 91912, "start": 920.08, "end": 929.12, "text": " We could enable versioning. Versioning is already supported.", "tokens": [50412, 492, 727, 9528, 3037, 278, 13, 35965, 278, 307, 1217, 8104, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10679705653871809, "compression_ratio": 1.3877551020408163, "no_speech_prob": 0.024465398862957954}, {"id": 90, "seek": 91912, "start": 931.52, "end": 936.48, "text": " I'm just not doing that because I don't remember how to demo that part.", "tokens": [50984, 286, 478, 445, 406, 884, 300, 570, 286, 500, 380, 1604, 577, 281, 10723, 300, 644, 13, 51232], "temperature": 0.0, "avg_logprob": -0.10679705653871809, "compression_ratio": 1.3877551020408163, "no_speech_prob": 0.024465398862957954}, {"id": 91, "seek": 91912, "start": 939.28, "end": 946.24, "text": " We have tests for that stuff. Okay, so this is as far as the demo goes.", "tokens": [51372, 492, 362, 6921, 337, 300, 1507, 13, 1033, 11, 370, 341, 307, 382, 1400, 382, 264, 10723, 1709, 13, 51720], "temperature": 0.0, "avg_logprob": -0.10679705653871809, "compression_ratio": 1.3877551020408163, "no_speech_prob": 0.024465398862957954}, {"id": 92, "seek": 94624, "start": 946.8, "end": 955.92, "text": " Let me just list the buckets here so that you see that a bucket bar has been created,", "tokens": [50392, 961, 385, 445, 1329, 264, 32191, 510, 370, 300, 291, 536, 300, 257, 13058, 2159, 575, 668, 2942, 11, 50848], "temperature": 0.0, "avg_logprob": -0.17257295484128204, "compression_ratio": 1.3852459016393444, "no_speech_prob": 0.003313894383609295}, {"id": 93, "seek": 94624, "start": 956.48, "end": 965.52, "text": " and we have a bucket bar over there. That's thrilling. Okay, let me just go back to", "tokens": [50876, 293, 321, 362, 257, 13058, 2159, 670, 456, 13, 663, 311, 39347, 13, 1033, 11, 718, 385, 445, 352, 646, 281, 51328], "temperature": 0.0, "avg_logprob": -0.17257295484128204, "compression_ratio": 1.3852459016393444, "no_speech_prob": 0.003313894383609295}, {"id": 94, "seek": 96552, "start": 965.68, "end": 970.8, "text": " the other thing", "tokens": [50372, 264, 661, 551, 50628], "temperature": 0.0, "avg_logprob": -0.277362683924233, "compression_ratio": 1.2654867256637168, "no_speech_prob": 0.011140753515064716}, {"id": 95, "seek": 96552, "start": 975.12, "end": 976.96, "text": " and go back to the presentation.", "tokens": [50844, 293, 352, 646, 281, 264, 5860, 13, 50936], "temperature": 0.0, "avg_logprob": -0.277362683924233, "compression_ratio": 1.2654867256637168, "no_speech_prob": 0.011140753515064716}, {"id": 96, "seek": 96552, "start": 983.1999999999999, "end": 994.56, "text": " Slideshell, so from current slides. Okay, so next steps. So for now our roadmap is to actually", "tokens": [51248, 6187, 1875, 21288, 11, 370, 490, 2190, 9788, 13, 1033, 11, 370, 958, 4439, 13, 407, 337, 586, 527, 35738, 307, 281, 767, 51816], "temperature": 0.0, "avg_logprob": -0.277362683924233, "compression_ratio": 1.2654867256637168, "no_speech_prob": 0.011140753515064716}, {"id": 97, "seek": 99456, "start": 994.64, "end": 998.2399999999999, "text": " increase the number of operations that we actually support because", "tokens": [50368, 3488, 264, 1230, 295, 7705, 300, 321, 767, 1406, 570, 50548], "temperature": 0.0, "avg_logprob": -0.1841227303088551, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.008688898757100105}, {"id": 98, "seek": 99456, "start": 1000.8, "end": 1007.8399999999999, "text": " the operations, the RGW basically supports everything that exists, but then the driver", "tokens": [50676, 264, 7705, 11, 264, 497, 38, 54, 1936, 9346, 1203, 300, 8198, 11, 457, 550, 264, 6787, 51028], "temperature": 0.0, "avg_logprob": -0.1841227303088551, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.008688898757100105}, {"id": 99, "seek": 99456, "start": 1007.8399999999999, "end": 1017.8399999999999, "text": " behind it needs to comply with the expected semantics and the expected, you know, you request", "tokens": [51028, 2261, 309, 2203, 281, 27956, 365, 264, 5176, 4361, 45298, 293, 264, 5176, 11, 291, 458, 11, 291, 5308, 51528], "temperature": 0.0, "avg_logprob": -0.1841227303088551, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.008688898757100105}, {"id": 100, "seek": 99456, "start": 1017.8399999999999, "end": 1023.92, "text": " data to the backend and the backend should probably return the appropriate data so that", "tokens": [51528, 1412, 281, 264, 38087, 293, 264, 38087, 820, 1391, 2736, 264, 6854, 1412, 370, 300, 51832], "temperature": 0.0, "avg_logprob": -0.1841227303088551, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.008688898757100105}, {"id": 101, "seek": 102392, "start": 1023.92, "end": 1033.36, "text": " the client is able to perform the operations. And we've been doing this gradually. There has been", "tokens": [50364, 264, 6423, 307, 1075, 281, 2042, 264, 7705, 13, 400, 321, 600, 668, 884, 341, 13145, 13, 821, 575, 668, 50836], "temperature": 0.0, "avg_logprob": -0.13192889649989242, "compression_ratio": 1.4754098360655739, "no_speech_prob": 0.00195446889847517}, {"id": 102, "seek": 102392, "start": 1033.36, "end": 1042.8799999999999, "text": " some challenges there of which, so we are in the process of implementing life cycle management", "tokens": [50836, 512, 4759, 456, 295, 597, 11, 370, 321, 366, 294, 264, 1399, 295, 18114, 993, 6586, 4592, 51312], "temperature": 0.0, "avg_logprob": -0.13192889649989242, "compression_ratio": 1.4754098360655739, "no_speech_prob": 0.00195446889847517}, {"id": 103, "seek": 102392, "start": 1043.44, "end": 1049.44, "text": " for buckets, retention policies. The performance currently is far from ideal,", "tokens": [51340, 337, 32191, 11, 22871, 7657, 13, 440, 3389, 4362, 307, 1400, 490, 7157, 11, 51640], "temperature": 0.0, "avg_logprob": -0.13192889649989242, "compression_ratio": 1.4754098360655739, "no_speech_prob": 0.00195446889847517}, {"id": 104, "seek": 104944, "start": 1049.44, "end": 1058.8, "text": " but we're working towards that. And I really want statistics on the UI. I mean, having as much", "tokens": [50364, 457, 321, 434, 1364, 3030, 300, 13, 400, 286, 534, 528, 12523, 322, 264, 15682, 13, 286, 914, 11, 1419, 382, 709, 50832], "temperature": 0.0, "avg_logprob": -0.13430746050848477, "compression_ratio": 1.5681818181818181, "no_speech_prob": 0.007453685160726309}, {"id": 105, "seek": 104944, "start": 1058.8, "end": 1067.6000000000001, "text": " information that is useful to the user through the UI is something that is very much on my", "tokens": [50832, 1589, 300, 307, 4420, 281, 264, 4195, 807, 264, 15682, 307, 746, 300, 307, 588, 709, 322, 452, 51272], "temperature": 0.0, "avg_logprob": -0.13430746050848477, "compression_ratio": 1.5681818181818181, "no_speech_prob": 0.007453685160726309}, {"id": 106, "seek": 104944, "start": 1069.76, "end": 1075.68, "text": " to the list, not necessarily on the to the list for the project, but that's another thing.", "tokens": [51380, 281, 264, 1329, 11, 406, 4725, 322, 264, 281, 264, 1329, 337, 264, 1716, 11, 457, 300, 311, 1071, 551, 13, 51676], "temperature": 0.0, "avg_logprob": -0.13430746050848477, "compression_ratio": 1.5681818181818181, "no_speech_prob": 0.007453685160726309}, {"id": 107, "seek": 107568, "start": 1076.16, "end": 1082.24, "text": " In terms of challenges that we currently face is a semantic compliance with the S3 API. As I was", "tokens": [50388, 682, 2115, 295, 4759, 300, 321, 4362, 1851, 307, 257, 47982, 15882, 365, 264, 318, 18, 9362, 13, 1018, 286, 390, 50692], "temperature": 0.0, "avg_logprob": -0.22258835253508194, "compression_ratio": 1.3517241379310345, "no_speech_prob": 0.005891968961805105}, {"id": 108, "seek": 107568, "start": 1082.24, "end": 1094.88, "text": " saying, we have, we've been having some challenges ensuring that our driver replies or provides the", "tokens": [50692, 1566, 11, 321, 362, 11, 321, 600, 668, 1419, 512, 4759, 16882, 300, 527, 6787, 42289, 420, 6417, 264, 51324], "temperature": 0.0, "avg_logprob": -0.22258835253508194, "compression_ratio": 1.3517241379310345, "no_speech_prob": 0.005891968961805105}, {"id": 109, "seek": 109488, "start": 1095.8400000000001, "end": 1111.1200000000001, "text": " right information when processing the operations that are requested by RGW. Fortunately, there is an", "tokens": [50412, 558, 1589, 562, 9007, 264, 7705, 300, 366, 16436, 538, 497, 38, 54, 13, 20652, 11, 456, 307, 364, 51176], "temperature": 0.0, "avg_logprob": -0.2130249821862509, "compression_ratio": 1.3424657534246576, "no_speech_prob": 0.02347370609641075}, {"id": 110, "seek": 109488, "start": 1111.1200000000001, "end": 1118.5600000000002, "text": " amazing project called S3 tests within the self repository, which covers pretty much all of the", "tokens": [51176, 2243, 1716, 1219, 318, 18, 6921, 1951, 264, 2698, 25841, 11, 597, 10538, 1238, 709, 439, 295, 264, 51548], "temperature": 0.0, "avg_logprob": -0.2130249821862509, "compression_ratio": 1.3424657534246576, "no_speech_prob": 0.02347370609641075}, {"id": 111, "seek": 111856, "start": 1118.8799999999999, "end": 1129.52, "text": " API as far as we understand. And it's very, it's very good to actually ensure that we are in", "tokens": [50380, 9362, 382, 1400, 382, 321, 1223, 13, 400, 309, 311, 588, 11, 309, 311, 588, 665, 281, 767, 5586, 300, 321, 366, 294, 50912], "temperature": 0.0, "avg_logprob": -0.11489861852982465, "compression_ratio": 1.4895833333333333, "no_speech_prob": 0.013780316337943077}, {"id": 112, "seek": 111856, "start": 1129.52, "end": 1136.96, "text": " compliance with the API. Then we have performance. This has been a big learning curve, especially", "tokens": [50912, 15882, 365, 264, 9362, 13, 1396, 321, 362, 3389, 13, 639, 575, 668, 257, 955, 2539, 7605, 11, 2318, 51284], "temperature": 0.0, "avg_logprob": -0.11489861852982465, "compression_ratio": 1.4895833333333333, "no_speech_prob": 0.013780316337943077}, {"id": 113, "seek": 111856, "start": 1136.96, "end": 1146.56, "text": " with SQLite. There have been decisions that were made during the implementation of the project,", "tokens": [51284, 365, 19200, 642, 13, 821, 362, 668, 5327, 300, 645, 1027, 1830, 264, 11420, 295, 264, 1716, 11, 51764], "temperature": 0.0, "avg_logprob": -0.11489861852982465, "compression_ratio": 1.4895833333333333, "no_speech_prob": 0.013780316337943077}, {"id": 114, "seek": 114656, "start": 1147.28, "end": 1158.1599999999999, "text": " especially surrounding new taxes that bid us eventually. But fortunately, we have a Marcel", "tokens": [50400, 2318, 11498, 777, 10041, 300, 12957, 505, 4728, 13, 583, 25511, 11, 321, 362, 257, 34738, 50944], "temperature": 0.0, "avg_logprob": -0.18592734866672092, "compression_ratio": 1.406015037593985, "no_speech_prob": 0.006989824585616589}, {"id": 115, "seek": 114656, "start": 1158.1599999999999, "end": 1169.44, "text": " actually looking into this stuff. So if, and that's one of the things that we have here, we have", "tokens": [50944, 767, 1237, 666, 341, 1507, 13, 407, 498, 11, 293, 300, 311, 472, 295, 264, 721, 300, 321, 362, 510, 11, 321, 362, 51508], "temperature": 0.0, "avg_logprob": -0.18592734866672092, "compression_ratio": 1.406015037593985, "no_speech_prob": 0.006989824585616589}, {"id": 116, "seek": 116944, "start": 1169.52, "end": 1177.04, "text": " here a comparison of our performance, previous performance with the performance that comes with", "tokens": [50368, 510, 257, 9660, 295, 527, 3389, 11, 3894, 3389, 365, 264, 3389, 300, 1487, 365, 50744], "temperature": 0.0, "avg_logprob": -0.14555213762366254, "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.07644899934530258}, {"id": 117, "seek": 116944, "start": 1177.04, "end": 1183.68, "text": " some of the work Marcel has been doing, it may not look like a lot, especially when we compare", "tokens": [50744, 512, 295, 264, 589, 34738, 575, 668, 884, 11, 309, 815, 406, 574, 411, 257, 688, 11, 2318, 562, 321, 6794, 51076], "temperature": 0.0, "avg_logprob": -0.14555213762366254, "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.07644899934530258}, {"id": 118, "seek": 116944, "start": 1183.68, "end": 1195.44, "text": " with FIO. But I mean, he filled with a few mutexes and we got a significant speed up on a very", "tokens": [51076, 365, 479, 15167, 13, 583, 286, 914, 11, 415, 6412, 365, 257, 1326, 24523, 47047, 293, 321, 658, 257, 4776, 3073, 493, 322, 257, 588, 51664], "temperature": 0.0, "avg_logprob": -0.14555213762366254, "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.07644899934530258}, {"id": 119, "seek": 119544, "start": 1195.92, "end": 1205.6000000000001, "text": " let's say that the machine is far from current. So we are also believing that we are CPU bound", "tokens": [50388, 718, 311, 584, 300, 264, 3479, 307, 1400, 490, 2190, 13, 407, 321, 366, 611, 16594, 300, 321, 366, 13199, 5472, 50872], "temperature": 0.0, "avg_logprob": -0.1395572539298765, "compression_ratio": 1.4973262032085561, "no_speech_prob": 0.008494270034134388}, {"id": 120, "seek": 119544, "start": 1206.24, "end": 1212.0, "text": " some extent and not taking full advantage of the IO path. But that's neither here nor there.", "tokens": [50904, 512, 8396, 293, 406, 1940, 1577, 5002, 295, 264, 39839, 3100, 13, 583, 300, 311, 9662, 510, 6051, 456, 13, 51192], "temperature": 0.0, "avg_logprob": -0.1395572539298765, "compression_ratio": 1.4973262032085561, "no_speech_prob": 0.008494270034134388}, {"id": 121, "seek": 119544, "start": 1212.64, "end": 1220.56, "text": " What matters is that we are performing gradual performance improvements that will eventually", "tokens": [51224, 708, 7001, 307, 300, 321, 366, 10205, 32890, 3389, 13797, 300, 486, 4728, 51620], "temperature": 0.0, "avg_logprob": -0.1395572539298765, "compression_ratio": 1.4973262032085561, "no_speech_prob": 0.008494270034134388}, {"id": 122, "seek": 122056, "start": 1221.52, "end": 1232.56, "text": " pay off. This is just the latency distribution. So the YOLO branch is what Marcel called that", "tokens": [50412, 1689, 766, 13, 639, 307, 445, 264, 27043, 7316, 13, 407, 264, 398, 5046, 46, 9819, 307, 437, 34738, 1219, 300, 50964], "temperature": 0.0, "avg_logprob": -0.1377103512103741, "compression_ratio": 1.4623655913978495, "no_speech_prob": 0.025735141709446907}, {"id": 123, "seek": 122056, "start": 1232.56, "end": 1241.28, "text": " branch mostly because it removed a bunch of mutexes. What we see here is that to some extent", "tokens": [50964, 9819, 5240, 570, 309, 7261, 257, 3840, 295, 24523, 47047, 13, 708, 321, 536, 510, 307, 300, 281, 512, 8396, 51400], "temperature": 0.0, "avg_logprob": -0.1377103512103741, "compression_ratio": 1.4623655913978495, "no_speech_prob": 0.025735141709446907}, {"id": 124, "seek": 122056, "start": 1242.1599999999999, "end": 1248.8799999999999, "text": " the latencies dropped a bit for put, even though they increased significantly forget.", "tokens": [51444, 264, 4465, 6464, 8119, 257, 857, 337, 829, 11, 754, 1673, 436, 6505, 10591, 2870, 13, 51780], "temperature": 0.0, "avg_logprob": -0.1377103512103741, "compression_ratio": 1.4623655913978495, "no_speech_prob": 0.025735141709446907}, {"id": 125, "seek": 124888, "start": 1248.88, "end": 1256.24, "text": " But we are also believing that we are, given that we are also having more operations in flight", "tokens": [50364, 583, 321, 366, 611, 16594, 300, 321, 366, 11, 2212, 300, 321, 366, 611, 1419, 544, 7705, 294, 7018, 50732], "temperature": 0.0, "avg_logprob": -0.15685536431484534, "compression_ratio": 1.5480225988700564, "no_speech_prob": 0.004654797725379467}, {"id": 126, "seek": 124888, "start": 1258.16, "end": 1265.0400000000002, "text": " that we may actually be CPU bound and the operations may not be finishing because of", "tokens": [50828, 300, 321, 815, 767, 312, 13199, 5472, 293, 264, 7705, 815, 406, 312, 12693, 570, 295, 51172], "temperature": 0.0, "avg_logprob": -0.15685536431484534, "compression_ratio": 1.5480225988700564, "no_speech_prob": 0.004654797725379467}, {"id": 127, "seek": 124888, "start": 1267.0400000000002, "end": 1276.0, "text": " concurrency issues and whatnot. After that, eventually world domination, I think, but probably", "tokens": [51272, 23702, 10457, 2663, 293, 25882, 13, 2381, 300, 11, 4728, 1002, 41502, 11, 286, 519, 11, 457, 1391, 51720], "temperature": 0.0, "avg_logprob": -0.15685536431484534, "compression_ratio": 1.5480225988700564, "no_speech_prob": 0.004654797725379467}, {"id": 128, "seek": 127600, "start": 1276.0, "end": 1284.72, "text": " not. But that's the hope. And that's it. If you want to find us, we are at S3 Gateway IO.", "tokens": [50364, 406, 13, 583, 300, 311, 264, 1454, 13, 400, 300, 311, 309, 13, 759, 291, 528, 281, 915, 505, 11, 321, 366, 412, 318, 18, 48394, 39839, 13, 50800], "temperature": 0.0, "avg_logprob": -0.14765308488090084, "compression_ratio": 1.3435114503816794, "no_speech_prob": 0.013027935288846493}, {"id": 129, "seek": 127600, "start": 1286.48, "end": 1297.6, "text": " And that's about it. Thank you for enduring my presentation. Thank you. Any questions?", "tokens": [50888, 400, 300, 311, 466, 309, 13, 1044, 291, 337, 36562, 452, 5860, 13, 1044, 291, 13, 2639, 1651, 30, 51444], "temperature": 0.0, "avg_logprob": -0.14765308488090084, "compression_ratio": 1.3435114503816794, "no_speech_prob": 0.013027935288846493}, {"id": 130, "seek": 129760, "start": 1297.6, "end": 1310.32, "text": " No? Awesome sauce. Great. Thank you.", "tokens": [50364, 883, 30, 10391, 4880, 13, 3769, 13, 1044, 291, 13, 51000], "temperature": 0.0, "avg_logprob": -0.423950562110314, "compression_ratio": 0.8181818181818182, "no_speech_prob": 0.05292791873216629}], "language": "en"}