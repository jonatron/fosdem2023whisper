{"text": " Okay, my name is Noel Brandon. I'm talking about the work that I've been doing making LibreOffice faster. We all know that LibreOffice is a bit of an elephant. It's a cute elephant, but it's still a bit of an elephant. We've got 10 million-odd lines of code, some of which is 20 years old. Practices that were perfectly acceptable 20 years ago are a little bit trickier these days. Optimizations that made perfect sense back in the day are not great anymore, and things have just changed around. For example, CPU memory bandwidth has dramatically changed. From around about the era of the 486DX2, we suddenly saw a dramatic shift in the speeds of main RAM versus the speeds of your main CPU. Up until that point, you were looking at CPUs that could touch location in memory at about the same speed as they could touch local cached memory. So you wrote your algorithms around those sorts of things. DX2 changed things, and it only got dramatically worse after that. To the point now where touching something in L1 cache versus touching main memory can be anywhere up to 50 times slower. Cache-finding algorithms are now the new game in town. We now have multiple cores. Everybody has multiple cores. It used to be that only people with vast sums of money had machines with multiple cores or the one CPU. Now, everybody has one. So locking algorithms that made perfect sense because they only got touched by a handful of people suddenly get exercised by everybody, and all sorts of interesting flaws come up. An interesting note, the locking algorithms that were written back then, nobody actually knew if they were truly solid or not because Intel's own engineers refused to commit to a cache coherency protocol until somewhere around the Pentium era. Up until then, they ruthlessly refused to say anything about it, and if queried, they would just say that it wasn't something they had locked down yet. So you were kind of in the dark. So we end up where we end up. So the good news. I've been at this for a little while, and the worst and ugliest of the stuff is largely gone. I mean, there's still lots of performance issues left in LibreOffice, but the stuff that used to hang LibreOffice up for minutes at a time, I've managed to nail most of that down. There's still lots of places where it's slow, but it's not like heinously slow like it used to be. The bad news is that the remaining stuff is really, really ugly. It's typically the stuff that I just couldn't get my head around the first time, and some of it I've looked at again, and I still can't get my head around it. Some of it I've been beating on for a while, and I'm not making a lot of progress. But anyhow, the point of this exercise is that you keep beating on things, and eventually either you break or it breaks. Hopefully, you get the right thing. So what worked out nicely? I'm going to talk today about what worked out well and what worked out worse, because I think it's important that we share both the successes and the failures because I think you learn equally for both. So what worked out nicely is reducing allocations primarily with things like using standard optional. So, for example, if we were allocating a temporary data structure inside a subroutine, I would switch that to use, to declare that thing using standard optional, which does have the downside that it takes up space on the stack even if you're not using it. But on the upside, using the stack is about 100 times faster than allocating for a main memory. The reasons for that are because any time you touch main, any time you have to allocate out of the heap, you have to take a lock. That leads to locking contention, et cetera, et cetera. You have to talk to main memory because you're doing all sorts of stuff there, whereas the stack is pretty much guaranteed to be an L1 cache, and so allocating out of there and allocating out of there is just ridiculously fast. So you can allocate quite large amounts of memory before you start falling out of L1 cache. We're talking up to a couple of kilobytes here. So throwing the sort of stuff at it worked well. The other thing I did was I've been converting usages of our internal OSL, colon, colon, mutex class to the standard mutex class. Now, despite the naming here, we're talking about two different kinds of mutexes. Our internal mutex is what's normally called standard recursive mutex, whereas standard mutex is a non-recursive mutex, and it is considerably faster. In an uncontended case, standard mutex is about 20 to 50 times faster than standard recursive mutex. In the contended case, they tend to fall back to roughly being about the same cost. So given that we run most of the time with very, very little concurrency, except for an occasional use case here and there, standard mutex is a major win. The downside with standard mutex is that if you use it and you try and lock that mutex the second time, you get a dead lock. So you have to write your code quite carefully. Most of our usages converted very easily. Did make a couple of mistakes. So there were a couple of regressions added to fix, and in a couple of cases, the regressions were simply unfixable, and so we backed it back out again. But in general, it's a win. And as a side effect, standard mutex is allocation-free. It's literally just a single word in memory. And the common unattended case of taking a standard mutex is what's called a CAS operation, compare and swap. So it's really fast, and it doesn't use even less memory than OSL mutex. So a win all around. What didn't work out? So we've got this SVL shared string pool. It's a really nice thing. We use it in spreadsheets primarily because spreadsheets often have many tens of thousands of strings, which are all identical across all the cells. So we still references to those strings in the shared pool. Now, this shared pool gets hit very hard at load time because other people implemented concurrent loading of spreadsheets. So we have this nice stuff where we fire up multiple threads and load a bunch of different chunks of the spreadsheet at the same time. This is great. Speeds things up. But it was bottlenecking very hard in SVL shared string pool. So I thought, oh, it's great. This is a hash map. Best case scenario, we can stick a concurrent lock-free hash map in there. I found this great cuckoo hash map written by some very bright people, much smarter than myself. I stuck it in there. Oh, it was great. It worked out brilliantly. It completely destroyed the bottleneck. Speeds sheet loading went up by a factor of two or three. And I thought I had a great win. And then the first bug came in where there was an edge case where it wasn't quite the locking because we were doing concurrent locking now. It was getting some weird edge cases. We had two different maps. They were both talking in the same thing. So I then had to fix those edge cases. I reworked it. We fixed those bugs. We reworked it again. I fixed the bugs. I eventually got it working. And then another bright guy came in. Lubosh came in and said, but wait. String pool is indeed a hotspot, but the hotspot is not actually the hash map inside string pool. The hotspot is the uppercase conversion. He improved the uppercase conversion. And by the time he was done improving the uppercase conversion, my concurrent hash map was making no difference whatsoever. So I had identified a hotspot, but I hadn't identified the hotspot. So we backed it out because there's no point in keeping a highly complicated piece of equipment like that around. So I was very sad to see it go. But I learned some stuff in the process. So no harm, no foul. And we backed it out again. And we're back to being faster than we were before. Red line processing in writer, which is our document editor section, is often very, very slow, especially if there's a ton of red lines in a big document. And it's slow both loading and slow at runtime because when we're doing red lines, we often are doing massive amounts of adding and deleting to data structures as we internally render and stuff. And so I thought I'd try and speed that up. However, this is writer. Writer is the most cash unfriendly program known to mankind. It just wants to chase pointers all over RAM. It's dataset once you are more than, once you have more than a sort of 10 page document, you are completely falling out of L1 cash. So you have no chance of getting cashed algorithms. The data structures are inherently very, very complicated. Human languages and documents just are. Consequences are we do pointer chasing. So we're constantly loading a pointer and then following it to some other location in memory, which is guaranteed to not to be an L1 cache. And then we're following that again to something else, which is also not an L1 cache. And in the process, we've now blown apart our cache. So if we need the first pointer again, that's also not an L1 cache. So we just chase our tails around in a very slow processing. I did my best to fix this. And that involves trying to speed up something called a node index and a content index. And I failed. I am now three levels deep, fixing different things related to that, no end in sight. And I'm currently bottlenecked on a piece of processing that I just don't understand. So that didn't work out. But in the process, I now know a lot more about writer. And so I consider that a win. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 20.64, "text": " Okay, my name is Noel Brandon. I'm talking about the work that I've been doing making", "tokens": [50364, 1033, 11, 452, 1315, 307, 38824, 22606, 13, 286, 478, 1417, 466, 264, 589, 300, 286, 600, 668, 884, 1455, 51396], "temperature": 0.0, "avg_logprob": -0.2119212814524204, "compression_ratio": 1.528735632183908, "no_speech_prob": 0.44634419679641724}, {"id": 1, "seek": 0, "start": 20.64, "end": 25.76, "text": " LibreOffice faster. We all know that LibreOffice is a bit of an elephant. It's a cute elephant,", "tokens": [51396, 15834, 265, 29745, 573, 4663, 13, 492, 439, 458, 300, 15834, 265, 29745, 573, 307, 257, 857, 295, 364, 19791, 13, 467, 311, 257, 4052, 19791, 11, 51652], "temperature": 0.0, "avg_logprob": -0.2119212814524204, "compression_ratio": 1.528735632183908, "no_speech_prob": 0.44634419679641724}, {"id": 2, "seek": 0, "start": 25.76, "end": 29.0, "text": " but it's still a bit of an elephant. We've got 10 million-odd lines of code, some of", "tokens": [51652, 457, 309, 311, 920, 257, 857, 295, 364, 19791, 13, 492, 600, 658, 1266, 2459, 12, 378, 67, 3876, 295, 3089, 11, 512, 295, 51814], "temperature": 0.0, "avg_logprob": -0.2119212814524204, "compression_ratio": 1.528735632183908, "no_speech_prob": 0.44634419679641724}, {"id": 3, "seek": 2900, "start": 29.0, "end": 34.72, "text": " which is 20 years old. Practices that were perfectly acceptable 20 years ago are a little", "tokens": [50364, 597, 307, 945, 924, 1331, 13, 19170, 1473, 300, 645, 6239, 15513, 945, 924, 2057, 366, 257, 707, 50650], "temperature": 0.0, "avg_logprob": -0.17043828064540648, "compression_ratio": 1.6007194244604317, "no_speech_prob": 0.19240747392177582}, {"id": 4, "seek": 2900, "start": 34.72, "end": 39.92, "text": " bit trickier these days. Optimizations that made perfect sense back in the day are not", "tokens": [50650, 857, 4282, 811, 613, 1708, 13, 35013, 14455, 300, 1027, 2176, 2020, 646, 294, 264, 786, 366, 406, 50910], "temperature": 0.0, "avg_logprob": -0.17043828064540648, "compression_ratio": 1.6007194244604317, "no_speech_prob": 0.19240747392177582}, {"id": 5, "seek": 2900, "start": 39.92, "end": 45.64, "text": " great anymore, and things have just changed around. For example, CPU memory bandwidth", "tokens": [50910, 869, 3602, 11, 293, 721, 362, 445, 3105, 926, 13, 1171, 1365, 11, 13199, 4675, 23647, 51196], "temperature": 0.0, "avg_logprob": -0.17043828064540648, "compression_ratio": 1.6007194244604317, "no_speech_prob": 0.19240747392177582}, {"id": 6, "seek": 2900, "start": 45.64, "end": 51.8, "text": " has dramatically changed. From around about the era of the 486DX2, we suddenly saw a dramatic", "tokens": [51196, 575, 17548, 3105, 13, 3358, 926, 466, 264, 4249, 295, 264, 11174, 21, 35, 55, 17, 11, 321, 5800, 1866, 257, 12023, 51504], "temperature": 0.0, "avg_logprob": -0.17043828064540648, "compression_ratio": 1.6007194244604317, "no_speech_prob": 0.19240747392177582}, {"id": 7, "seek": 2900, "start": 51.8, "end": 57.16, "text": " shift in the speeds of main RAM versus the speeds of your main CPU. Up until that point,", "tokens": [51504, 5513, 294, 264, 16411, 295, 2135, 14561, 5717, 264, 16411, 295, 428, 2135, 13199, 13, 5858, 1826, 300, 935, 11, 51772], "temperature": 0.0, "avg_logprob": -0.17043828064540648, "compression_ratio": 1.6007194244604317, "no_speech_prob": 0.19240747392177582}, {"id": 8, "seek": 5716, "start": 57.16, "end": 62.559999999999995, "text": " you were looking at CPUs that could touch location in memory at about the same speed", "tokens": [50364, 291, 645, 1237, 412, 13199, 82, 300, 727, 2557, 4914, 294, 4675, 412, 466, 264, 912, 3073, 50634], "temperature": 0.0, "avg_logprob": -0.2426765882051908, "compression_ratio": 1.6566037735849057, "no_speech_prob": 0.15804468095302582}, {"id": 9, "seek": 5716, "start": 62.559999999999995, "end": 70.12, "text": " as they could touch local cached memory. So you wrote your algorithms around those sorts", "tokens": [50634, 382, 436, 727, 2557, 2654, 269, 15095, 4675, 13, 407, 291, 4114, 428, 14642, 926, 729, 7527, 51012], "temperature": 0.0, "avg_logprob": -0.2426765882051908, "compression_ratio": 1.6566037735849057, "no_speech_prob": 0.15804468095302582}, {"id": 10, "seek": 5716, "start": 70.12, "end": 73.96, "text": " of things. DX2 changed things, and it only got dramatically worse after that. To the", "tokens": [51012, 295, 721, 13, 48817, 17, 3105, 721, 11, 293, 309, 787, 658, 17548, 5324, 934, 300, 13, 1407, 264, 51204], "temperature": 0.0, "avg_logprob": -0.2426765882051908, "compression_ratio": 1.6566037735849057, "no_speech_prob": 0.15804468095302582}, {"id": 11, "seek": 5716, "start": 73.96, "end": 79.96, "text": " point now where touching something in L1 cache versus touching main memory can be anywhere", "tokens": [51204, 935, 586, 689, 11175, 746, 294, 441, 16, 19459, 5717, 11175, 2135, 4675, 393, 312, 4992, 51504], "temperature": 0.0, "avg_logprob": -0.2426765882051908, "compression_ratio": 1.6566037735849057, "no_speech_prob": 0.15804468095302582}, {"id": 12, "seek": 5716, "start": 79.96, "end": 86.36, "text": " up to 50 times slower. Cache-finding algorithms are now the new game in town. We now have", "tokens": [51504, 493, 281, 2625, 1413, 14009, 13, 383, 6000, 12, 69, 9245, 14642, 366, 586, 264, 777, 1216, 294, 3954, 13, 492, 586, 362, 51824], "temperature": 0.0, "avg_logprob": -0.2426765882051908, "compression_ratio": 1.6566037735849057, "no_speech_prob": 0.15804468095302582}, {"id": 13, "seek": 8636, "start": 86.36, "end": 91.28, "text": " multiple cores. Everybody has multiple cores. It used to be that only people with vast sums", "tokens": [50364, 3866, 24826, 13, 7646, 575, 3866, 24826, 13, 467, 1143, 281, 312, 300, 787, 561, 365, 8369, 34499, 50610], "temperature": 0.0, "avg_logprob": -0.1782503922780355, "compression_ratio": 1.7751004016064258, "no_speech_prob": 0.318406343460083}, {"id": 14, "seek": 8636, "start": 91.28, "end": 97.12, "text": " of money had machines with multiple cores or the one CPU. Now, everybody has one. So", "tokens": [50610, 295, 1460, 632, 8379, 365, 3866, 24826, 420, 264, 472, 13199, 13, 823, 11, 2201, 575, 472, 13, 407, 50902], "temperature": 0.0, "avg_logprob": -0.1782503922780355, "compression_ratio": 1.7751004016064258, "no_speech_prob": 0.318406343460083}, {"id": 15, "seek": 8636, "start": 97.12, "end": 100.48, "text": " locking algorithms that made perfect sense because they only got touched by a handful", "tokens": [50902, 23954, 14642, 300, 1027, 2176, 2020, 570, 436, 787, 658, 9828, 538, 257, 16458, 51070], "temperature": 0.0, "avg_logprob": -0.1782503922780355, "compression_ratio": 1.7751004016064258, "no_speech_prob": 0.318406343460083}, {"id": 16, "seek": 8636, "start": 100.48, "end": 108.6, "text": " of people suddenly get exercised by everybody, and all sorts of interesting flaws come up.", "tokens": [51070, 295, 561, 5800, 483, 4057, 2640, 538, 2201, 11, 293, 439, 7527, 295, 1880, 27108, 808, 493, 13, 51476], "temperature": 0.0, "avg_logprob": -0.1782503922780355, "compression_ratio": 1.7751004016064258, "no_speech_prob": 0.318406343460083}, {"id": 17, "seek": 8636, "start": 108.6, "end": 112.64, "text": " An interesting note, the locking algorithms that were written back then, nobody actually", "tokens": [51476, 1107, 1880, 3637, 11, 264, 23954, 14642, 300, 645, 3720, 646, 550, 11, 5079, 767, 51678], "temperature": 0.0, "avg_logprob": -0.1782503922780355, "compression_ratio": 1.7751004016064258, "no_speech_prob": 0.318406343460083}, {"id": 18, "seek": 11264, "start": 112.64, "end": 118.88, "text": " knew if they were truly solid or not because Intel's own engineers refused to commit to", "tokens": [50364, 2586, 498, 436, 645, 4908, 5100, 420, 406, 570, 19762, 311, 1065, 11955, 14654, 281, 5599, 281, 50676], "temperature": 0.0, "avg_logprob": -0.13421635968344553, "compression_ratio": 1.6628787878787878, "no_speech_prob": 0.7166213393211365}, {"id": 19, "seek": 11264, "start": 118.88, "end": 126.44, "text": " a cache coherency protocol until somewhere around the Pentium era. Up until then, they", "tokens": [50676, 257, 19459, 26528, 3020, 10336, 1826, 4079, 926, 264, 20165, 2197, 4249, 13, 5858, 1826, 550, 11, 436, 51054], "temperature": 0.0, "avg_logprob": -0.13421635968344553, "compression_ratio": 1.6628787878787878, "no_speech_prob": 0.7166213393211365}, {"id": 20, "seek": 11264, "start": 126.44, "end": 131.56, "text": " ruthlessly refused to say anything about it, and if queried, they would just say that it", "tokens": [51054, 38225, 12048, 14654, 281, 584, 1340, 466, 309, 11, 293, 498, 7083, 1091, 11, 436, 576, 445, 584, 300, 309, 51310], "temperature": 0.0, "avg_logprob": -0.13421635968344553, "compression_ratio": 1.6628787878787878, "no_speech_prob": 0.7166213393211365}, {"id": 21, "seek": 11264, "start": 131.56, "end": 136.08, "text": " wasn't something they had locked down yet. So you were kind of in the dark. So we end", "tokens": [51310, 2067, 380, 746, 436, 632, 9376, 760, 1939, 13, 407, 291, 645, 733, 295, 294, 264, 2877, 13, 407, 321, 917, 51536], "temperature": 0.0, "avg_logprob": -0.13421635968344553, "compression_ratio": 1.6628787878787878, "no_speech_prob": 0.7166213393211365}, {"id": 22, "seek": 11264, "start": 136.08, "end": 142.28, "text": " up where we end up. So the good news. I've been at this for a little while, and the worst", "tokens": [51536, 493, 689, 321, 917, 493, 13, 407, 264, 665, 2583, 13, 286, 600, 668, 412, 341, 337, 257, 707, 1339, 11, 293, 264, 5855, 51846], "temperature": 0.0, "avg_logprob": -0.13421635968344553, "compression_ratio": 1.6628787878787878, "no_speech_prob": 0.7166213393211365}, {"id": 23, "seek": 14228, "start": 142.28, "end": 147.32, "text": " and ugliest of the stuff is largely gone. I mean, there's still lots of performance", "tokens": [50364, 293, 10743, 16850, 295, 264, 1507, 307, 11611, 2780, 13, 286, 914, 11, 456, 311, 920, 3195, 295, 3389, 50616], "temperature": 0.0, "avg_logprob": -0.14736877603733795, "compression_ratio": 1.8153310104529616, "no_speech_prob": 0.14008060097694397}, {"id": 24, "seek": 14228, "start": 147.32, "end": 152.6, "text": " issues left in LibreOffice, but the stuff that used to hang LibreOffice up for minutes", "tokens": [50616, 2663, 1411, 294, 15834, 265, 29745, 573, 11, 457, 264, 1507, 300, 1143, 281, 3967, 15834, 265, 29745, 573, 493, 337, 2077, 50880], "temperature": 0.0, "avg_logprob": -0.14736877603733795, "compression_ratio": 1.8153310104529616, "no_speech_prob": 0.14008060097694397}, {"id": 25, "seek": 14228, "start": 152.6, "end": 156.96, "text": " at a time, I've managed to nail most of that down. There's still lots of places where", "tokens": [50880, 412, 257, 565, 11, 286, 600, 6453, 281, 10173, 881, 295, 300, 760, 13, 821, 311, 920, 3195, 295, 3190, 689, 51098], "temperature": 0.0, "avg_logprob": -0.14736877603733795, "compression_ratio": 1.8153310104529616, "no_speech_prob": 0.14008060097694397}, {"id": 26, "seek": 14228, "start": 156.96, "end": 162.68, "text": " it's slow, but it's not like heinously slow like it used to be. The bad news is that the", "tokens": [51098, 309, 311, 2964, 11, 457, 309, 311, 406, 411, 16464, 5098, 2964, 411, 309, 1143, 281, 312, 13, 440, 1578, 2583, 307, 300, 264, 51384], "temperature": 0.0, "avg_logprob": -0.14736877603733795, "compression_ratio": 1.8153310104529616, "no_speech_prob": 0.14008060097694397}, {"id": 27, "seek": 14228, "start": 162.68, "end": 167.2, "text": " remaining stuff is really, really ugly. It's typically the stuff that I just couldn't get", "tokens": [51384, 8877, 1507, 307, 534, 11, 534, 12246, 13, 467, 311, 5850, 264, 1507, 300, 286, 445, 2809, 380, 483, 51610], "temperature": 0.0, "avg_logprob": -0.14736877603733795, "compression_ratio": 1.8153310104529616, "no_speech_prob": 0.14008060097694397}, {"id": 28, "seek": 14228, "start": 167.2, "end": 170.96, "text": " my head around the first time, and some of it I've looked at again, and I still can't", "tokens": [51610, 452, 1378, 926, 264, 700, 565, 11, 293, 512, 295, 309, 286, 600, 2956, 412, 797, 11, 293, 286, 920, 393, 380, 51798], "temperature": 0.0, "avg_logprob": -0.14736877603733795, "compression_ratio": 1.8153310104529616, "no_speech_prob": 0.14008060097694397}, {"id": 29, "seek": 17096, "start": 170.96, "end": 177.12, "text": " get my head around it. Some of it I've been beating on for a while, and I'm not making", "tokens": [50364, 483, 452, 1378, 926, 309, 13, 2188, 295, 309, 286, 600, 668, 13497, 322, 337, 257, 1339, 11, 293, 286, 478, 406, 1455, 50672], "temperature": 0.0, "avg_logprob": -0.13085739891808312, "compression_ratio": 1.696969696969697, "no_speech_prob": 0.12601745128631592}, {"id": 30, "seek": 17096, "start": 177.12, "end": 180.92000000000002, "text": " a lot of progress. But anyhow, the point of this exercise is that you keep beating on", "tokens": [50672, 257, 688, 295, 4205, 13, 583, 44995, 11, 264, 935, 295, 341, 5380, 307, 300, 291, 1066, 13497, 322, 50862], "temperature": 0.0, "avg_logprob": -0.13085739891808312, "compression_ratio": 1.696969696969697, "no_speech_prob": 0.12601745128631592}, {"id": 31, "seek": 17096, "start": 180.92000000000002, "end": 188.08, "text": " things, and eventually either you break or it breaks. Hopefully, you get the right thing.", "tokens": [50862, 721, 11, 293, 4728, 2139, 291, 1821, 420, 309, 9857, 13, 10429, 11, 291, 483, 264, 558, 551, 13, 51220], "temperature": 0.0, "avg_logprob": -0.13085739891808312, "compression_ratio": 1.696969696969697, "no_speech_prob": 0.12601745128631592}, {"id": 32, "seek": 17096, "start": 188.08, "end": 191.12, "text": " So what worked out nicely? I'm going to talk today about what worked out well and what", "tokens": [51220, 407, 437, 2732, 484, 9594, 30, 286, 478, 516, 281, 751, 965, 466, 437, 2732, 484, 731, 293, 437, 51372], "temperature": 0.0, "avg_logprob": -0.13085739891808312, "compression_ratio": 1.696969696969697, "no_speech_prob": 0.12601745128631592}, {"id": 33, "seek": 17096, "start": 191.12, "end": 196.68, "text": " worked out worse, because I think it's important that we share both the successes and the failures", "tokens": [51372, 2732, 484, 5324, 11, 570, 286, 519, 309, 311, 1021, 300, 321, 2073, 1293, 264, 26101, 293, 264, 20774, 51650], "temperature": 0.0, "avg_logprob": -0.13085739891808312, "compression_ratio": 1.696969696969697, "no_speech_prob": 0.12601745128631592}, {"id": 34, "seek": 19668, "start": 196.68, "end": 201.20000000000002, "text": " because I think you learn equally for both. So what worked out nicely is reducing allocations", "tokens": [50364, 570, 286, 519, 291, 1466, 12309, 337, 1293, 13, 407, 437, 2732, 484, 9594, 307, 12245, 12660, 763, 50590], "temperature": 0.0, "avg_logprob": -0.1749550441526017, "compression_ratio": 1.6977611940298507, "no_speech_prob": 0.22454985976219177}, {"id": 35, "seek": 19668, "start": 201.20000000000002, "end": 207.4, "text": " primarily with things like using standard optional. So, for example, if we were allocating", "tokens": [50590, 10029, 365, 721, 411, 1228, 3832, 17312, 13, 407, 11, 337, 1365, 11, 498, 321, 645, 12660, 990, 50900], "temperature": 0.0, "avg_logprob": -0.1749550441526017, "compression_ratio": 1.6977611940298507, "no_speech_prob": 0.22454985976219177}, {"id": 36, "seek": 19668, "start": 207.4, "end": 214.28, "text": " a temporary data structure inside a subroutine, I would switch that to use, to declare that", "tokens": [50900, 257, 13413, 1412, 3877, 1854, 257, 1422, 81, 45075, 11, 286, 576, 3679, 300, 281, 764, 11, 281, 19710, 300, 51244], "temperature": 0.0, "avg_logprob": -0.1749550441526017, "compression_ratio": 1.6977611940298507, "no_speech_prob": 0.22454985976219177}, {"id": 37, "seek": 19668, "start": 214.28, "end": 219.60000000000002, "text": " thing using standard optional, which does have the downside that it takes up space on the", "tokens": [51244, 551, 1228, 3832, 17312, 11, 597, 775, 362, 264, 25060, 300, 309, 2516, 493, 1901, 322, 264, 51510], "temperature": 0.0, "avg_logprob": -0.1749550441526017, "compression_ratio": 1.6977611940298507, "no_speech_prob": 0.22454985976219177}, {"id": 38, "seek": 19668, "start": 219.60000000000002, "end": 224.96, "text": " stack even if you're not using it. But on the upside, using the stack is about 100 times", "tokens": [51510, 8630, 754, 498, 291, 434, 406, 1228, 309, 13, 583, 322, 264, 14119, 11, 1228, 264, 8630, 307, 466, 2319, 1413, 51778], "temperature": 0.0, "avg_logprob": -0.1749550441526017, "compression_ratio": 1.6977611940298507, "no_speech_prob": 0.22454985976219177}, {"id": 39, "seek": 22496, "start": 224.96, "end": 229.88, "text": " faster than allocating for a main memory. The reasons for that are because any time", "tokens": [50364, 4663, 813, 12660, 990, 337, 257, 2135, 4675, 13, 440, 4112, 337, 300, 366, 570, 604, 565, 50610], "temperature": 0.0, "avg_logprob": -0.13851411640644073, "compression_ratio": 1.9272727272727272, "no_speech_prob": 0.6020233035087585}, {"id": 40, "seek": 22496, "start": 229.88, "end": 234.16, "text": " you touch main, any time you have to allocate out of the heap, you have to take a lock.", "tokens": [50610, 291, 2557, 2135, 11, 604, 565, 291, 362, 281, 35713, 484, 295, 264, 33591, 11, 291, 362, 281, 747, 257, 4017, 13, 50824], "temperature": 0.0, "avg_logprob": -0.13851411640644073, "compression_ratio": 1.9272727272727272, "no_speech_prob": 0.6020233035087585}, {"id": 41, "seek": 22496, "start": 234.16, "end": 237.4, "text": " That leads to locking contention, et cetera, et cetera. You have to talk to main memory", "tokens": [50824, 663, 6689, 281, 23954, 660, 1251, 11, 1030, 11458, 11, 1030, 11458, 13, 509, 362, 281, 751, 281, 2135, 4675, 50986], "temperature": 0.0, "avg_logprob": -0.13851411640644073, "compression_ratio": 1.9272727272727272, "no_speech_prob": 0.6020233035087585}, {"id": 42, "seek": 22496, "start": 237.4, "end": 241.44, "text": " because you're doing all sorts of stuff there, whereas the stack is pretty much guaranteed", "tokens": [50986, 570, 291, 434, 884, 439, 7527, 295, 1507, 456, 11, 9735, 264, 8630, 307, 1238, 709, 18031, 51188], "temperature": 0.0, "avg_logprob": -0.13851411640644073, "compression_ratio": 1.9272727272727272, "no_speech_prob": 0.6020233035087585}, {"id": 43, "seek": 22496, "start": 241.44, "end": 246.64000000000001, "text": " to be an L1 cache, and so allocating out of there and allocating out of there is just", "tokens": [51188, 281, 312, 364, 441, 16, 19459, 11, 293, 370, 12660, 990, 484, 295, 456, 293, 12660, 990, 484, 295, 456, 307, 445, 51448], "temperature": 0.0, "avg_logprob": -0.13851411640644073, "compression_ratio": 1.9272727272727272, "no_speech_prob": 0.6020233035087585}, {"id": 44, "seek": 22496, "start": 246.64000000000001, "end": 250.88, "text": " ridiculously fast. So you can allocate quite large amounts of memory before you start falling", "tokens": [51448, 41358, 2370, 13, 407, 291, 393, 35713, 1596, 2416, 11663, 295, 4675, 949, 291, 722, 7440, 51660], "temperature": 0.0, "avg_logprob": -0.13851411640644073, "compression_ratio": 1.9272727272727272, "no_speech_prob": 0.6020233035087585}, {"id": 45, "seek": 25088, "start": 250.88, "end": 255.2, "text": " out of L1 cache. We're talking up to a couple of kilobytes here. So throwing the sort of", "tokens": [50364, 484, 295, 441, 16, 19459, 13, 492, 434, 1417, 493, 281, 257, 1916, 295, 5128, 996, 43673, 510, 13, 407, 10238, 264, 1333, 295, 50580], "temperature": 0.0, "avg_logprob": -0.1698967899594988, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.11973406374454498}, {"id": 46, "seek": 25088, "start": 255.2, "end": 259.56, "text": " stuff at it worked well. The other thing I did was I've been converting", "tokens": [50580, 1507, 412, 309, 2732, 731, 13, 440, 661, 551, 286, 630, 390, 286, 600, 668, 29942, 50798], "temperature": 0.0, "avg_logprob": -0.1698967899594988, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.11973406374454498}, {"id": 47, "seek": 25088, "start": 259.56, "end": 266.4, "text": " usages of our internal OSL, colon, colon, mutex class to the standard mutex class. Now,", "tokens": [50798, 505, 1660, 295, 527, 6920, 12731, 43, 11, 8255, 11, 8255, 11, 24523, 87, 1508, 281, 264, 3832, 24523, 87, 1508, 13, 823, 11, 51140], "temperature": 0.0, "avg_logprob": -0.1698967899594988, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.11973406374454498}, {"id": 48, "seek": 25088, "start": 266.4, "end": 270.2, "text": " despite the naming here, we're talking about two different kinds of mutexes. Our internal", "tokens": [51140, 7228, 264, 25290, 510, 11, 321, 434, 1417, 466, 732, 819, 3685, 295, 24523, 47047, 13, 2621, 6920, 51330], "temperature": 0.0, "avg_logprob": -0.1698967899594988, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.11973406374454498}, {"id": 49, "seek": 25088, "start": 270.2, "end": 278.36, "text": " mutex is what's normally called standard recursive mutex, whereas standard mutex is", "tokens": [51330, 24523, 87, 307, 437, 311, 5646, 1219, 3832, 20560, 488, 24523, 87, 11, 9735, 3832, 24523, 87, 307, 51738], "temperature": 0.0, "avg_logprob": -0.1698967899594988, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.11973406374454498}, {"id": 50, "seek": 27836, "start": 278.36, "end": 284.76, "text": " a non-recursive mutex, and it is considerably faster. In an uncontended case, standard mutex", "tokens": [50364, 257, 2107, 12, 13867, 2156, 488, 24523, 87, 11, 293, 309, 307, 31308, 4663, 13, 682, 364, 36019, 3502, 1389, 11, 3832, 24523, 87, 50684], "temperature": 0.0, "avg_logprob": -0.13794014371674637, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.13713203370571136}, {"id": 51, "seek": 27836, "start": 284.76, "end": 290.32, "text": " is about 20 to 50 times faster than standard recursive mutex. In the contended case, they", "tokens": [50684, 307, 466, 945, 281, 2625, 1413, 4663, 813, 3832, 20560, 488, 24523, 87, 13, 682, 264, 660, 3502, 1389, 11, 436, 50962], "temperature": 0.0, "avg_logprob": -0.13794014371674637, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.13713203370571136}, {"id": 52, "seek": 27836, "start": 290.32, "end": 295.76, "text": " tend to fall back to roughly being about the same cost. So given that we run most of the", "tokens": [50962, 3928, 281, 2100, 646, 281, 9810, 885, 466, 264, 912, 2063, 13, 407, 2212, 300, 321, 1190, 881, 295, 264, 51234], "temperature": 0.0, "avg_logprob": -0.13794014371674637, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.13713203370571136}, {"id": 53, "seek": 27836, "start": 295.76, "end": 301.28000000000003, "text": " time with very, very little concurrency, except for an occasional use case here and there,", "tokens": [51234, 565, 365, 588, 11, 588, 707, 23702, 10457, 11, 3993, 337, 364, 31644, 764, 1389, 510, 293, 456, 11, 51510], "temperature": 0.0, "avg_logprob": -0.13794014371674637, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.13713203370571136}, {"id": 54, "seek": 27836, "start": 301.28000000000003, "end": 307.56, "text": " standard mutex is a major win. The downside with standard mutex is that if you use it", "tokens": [51510, 3832, 24523, 87, 307, 257, 2563, 1942, 13, 440, 25060, 365, 3832, 24523, 87, 307, 300, 498, 291, 764, 309, 51824], "temperature": 0.0, "avg_logprob": -0.13794014371674637, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.13713203370571136}, {"id": 55, "seek": 30756, "start": 307.56, "end": 313.16, "text": " and you try and lock that mutex the second time, you get a dead lock. So you have to", "tokens": [50364, 293, 291, 853, 293, 4017, 300, 24523, 87, 264, 1150, 565, 11, 291, 483, 257, 3116, 4017, 13, 407, 291, 362, 281, 50644], "temperature": 0.0, "avg_logprob": -0.16219520568847656, "compression_ratio": 1.6716981132075472, "no_speech_prob": 0.019271470606327057}, {"id": 56, "seek": 30756, "start": 313.16, "end": 319.36, "text": " write your code quite carefully. Most of our usages converted very easily. Did make a couple", "tokens": [50644, 2464, 428, 3089, 1596, 7500, 13, 4534, 295, 527, 505, 1660, 16424, 588, 3612, 13, 2589, 652, 257, 1916, 50954], "temperature": 0.0, "avg_logprob": -0.16219520568847656, "compression_ratio": 1.6716981132075472, "no_speech_prob": 0.019271470606327057}, {"id": 57, "seek": 30756, "start": 319.36, "end": 323.72, "text": " of mistakes. So there were a couple of regressions added to fix, and in a couple of cases, the", "tokens": [50954, 295, 8038, 13, 407, 456, 645, 257, 1916, 295, 1121, 735, 626, 3869, 281, 3191, 11, 293, 294, 257, 1916, 295, 3331, 11, 264, 51172], "temperature": 0.0, "avg_logprob": -0.16219520568847656, "compression_ratio": 1.6716981132075472, "no_speech_prob": 0.019271470606327057}, {"id": 58, "seek": 30756, "start": 323.72, "end": 327.64, "text": " regressions were simply unfixable, and so we backed it back out again. But in general,", "tokens": [51172, 1121, 735, 626, 645, 2935, 3971, 970, 712, 11, 293, 370, 321, 20391, 309, 646, 484, 797, 13, 583, 294, 2674, 11, 51368], "temperature": 0.0, "avg_logprob": -0.16219520568847656, "compression_ratio": 1.6716981132075472, "no_speech_prob": 0.019271470606327057}, {"id": 59, "seek": 30756, "start": 327.64, "end": 332.68, "text": " it's a win. And as a side effect, standard mutex is allocation-free. It's literally", "tokens": [51368, 309, 311, 257, 1942, 13, 400, 382, 257, 1252, 1802, 11, 3832, 24523, 87, 307, 27599, 12, 10792, 13, 467, 311, 3736, 51620], "temperature": 0.0, "avg_logprob": -0.16219520568847656, "compression_ratio": 1.6716981132075472, "no_speech_prob": 0.019271470606327057}, {"id": 60, "seek": 33268, "start": 332.68, "end": 339.2, "text": " just a single word in memory. And the common unattended case of taking a standard mutex", "tokens": [50364, 445, 257, 2167, 1349, 294, 4675, 13, 400, 264, 2689, 47316, 3502, 1389, 295, 1940, 257, 3832, 24523, 87, 50690], "temperature": 0.0, "avg_logprob": -0.1808802894923998, "compression_ratio": 1.5884476534296028, "no_speech_prob": 0.06229283660650253}, {"id": 61, "seek": 33268, "start": 339.2, "end": 345.04, "text": " is what's called a CAS operation, compare and swap. So it's really fast, and it doesn't", "tokens": [50690, 307, 437, 311, 1219, 257, 43268, 6916, 11, 6794, 293, 18135, 13, 407, 309, 311, 534, 2370, 11, 293, 309, 1177, 380, 50982], "temperature": 0.0, "avg_logprob": -0.1808802894923998, "compression_ratio": 1.5884476534296028, "no_speech_prob": 0.06229283660650253}, {"id": 62, "seek": 33268, "start": 345.04, "end": 351.8, "text": " use even less memory than OSL mutex. So a win all around. What didn't work out? So", "tokens": [50982, 764, 754, 1570, 4675, 813, 12731, 43, 24523, 87, 13, 407, 257, 1942, 439, 926, 13, 708, 994, 380, 589, 484, 30, 407, 51320], "temperature": 0.0, "avg_logprob": -0.1808802894923998, "compression_ratio": 1.5884476534296028, "no_speech_prob": 0.06229283660650253}, {"id": 63, "seek": 33268, "start": 351.8, "end": 356.48, "text": " we've got this SVL shared string pool. It's a really nice thing. We use it in spreadsheets", "tokens": [51320, 321, 600, 658, 341, 31910, 43, 5507, 6798, 7005, 13, 467, 311, 257, 534, 1481, 551, 13, 492, 764, 309, 294, 23651, 1385, 51554], "temperature": 0.0, "avg_logprob": -0.1808802894923998, "compression_ratio": 1.5884476534296028, "no_speech_prob": 0.06229283660650253}, {"id": 64, "seek": 33268, "start": 356.48, "end": 362.12, "text": " primarily because spreadsheets often have many tens of thousands of strings, which are all", "tokens": [51554, 10029, 570, 23651, 1385, 2049, 362, 867, 10688, 295, 5383, 295, 13985, 11, 597, 366, 439, 51836], "temperature": 0.0, "avg_logprob": -0.1808802894923998, "compression_ratio": 1.5884476534296028, "no_speech_prob": 0.06229283660650253}, {"id": 65, "seek": 36212, "start": 362.12, "end": 366.52, "text": " identical across all the cells. So we still references to those strings in the shared", "tokens": [50364, 14800, 2108, 439, 264, 5438, 13, 407, 321, 920, 15400, 281, 729, 13985, 294, 264, 5507, 50584], "temperature": 0.0, "avg_logprob": -0.15235267235682562, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.017881397157907486}, {"id": 66, "seek": 36212, "start": 366.52, "end": 374.76, "text": " pool. Now, this shared pool gets hit very hard at load time because other people implemented", "tokens": [50584, 7005, 13, 823, 11, 341, 5507, 7005, 2170, 2045, 588, 1152, 412, 3677, 565, 570, 661, 561, 12270, 50996], "temperature": 0.0, "avg_logprob": -0.15235267235682562, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.017881397157907486}, {"id": 67, "seek": 36212, "start": 374.76, "end": 379.56, "text": " concurrent loading of spreadsheets. So we have this nice stuff where we fire up multiple", "tokens": [50996, 37702, 15114, 295, 23651, 1385, 13, 407, 321, 362, 341, 1481, 1507, 689, 321, 2610, 493, 3866, 51236], "temperature": 0.0, "avg_logprob": -0.15235267235682562, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.017881397157907486}, {"id": 68, "seek": 36212, "start": 379.56, "end": 384.4, "text": " threads and load a bunch of different chunks of the spreadsheet at the same time. This is", "tokens": [51236, 19314, 293, 3677, 257, 3840, 295, 819, 24004, 295, 264, 27733, 412, 264, 912, 565, 13, 639, 307, 51478], "temperature": 0.0, "avg_logprob": -0.15235267235682562, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.017881397157907486}, {"id": 69, "seek": 36212, "start": 384.4, "end": 390.76, "text": " great. Speeds things up. But it was bottlenecking very hard in SVL shared string pool. So I", "tokens": [51478, 869, 13, 3550, 5147, 721, 493, 13, 583, 309, 390, 44641, 25723, 588, 1152, 294, 31910, 43, 5507, 6798, 7005, 13, 407, 286, 51796], "temperature": 0.0, "avg_logprob": -0.15235267235682562, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.017881397157907486}, {"id": 70, "seek": 39076, "start": 390.8, "end": 396.71999999999997, "text": " thought, oh, it's great. This is a hash map. Best case scenario, we can stick a concurrent", "tokens": [50366, 1194, 11, 1954, 11, 309, 311, 869, 13, 639, 307, 257, 22019, 4471, 13, 9752, 1389, 9005, 11, 321, 393, 2897, 257, 37702, 50662], "temperature": 0.0, "avg_logprob": -0.1766023567254595, "compression_ratio": 1.7034700315457414, "no_speech_prob": 0.042132314294576645}, {"id": 71, "seek": 39076, "start": 396.71999999999997, "end": 400.8, "text": " lock-free hash map in there. I found this great cuckoo hash map written by some very", "tokens": [50662, 4017, 12, 10792, 22019, 4471, 294, 456, 13, 286, 1352, 341, 869, 269, 1134, 1986, 22019, 4471, 3720, 538, 512, 588, 50866], "temperature": 0.0, "avg_logprob": -0.1766023567254595, "compression_ratio": 1.7034700315457414, "no_speech_prob": 0.042132314294576645}, {"id": 72, "seek": 39076, "start": 400.8, "end": 404.59999999999997, "text": " bright people, much smarter than myself. I stuck it in there. Oh, it was great. It worked", "tokens": [50866, 4730, 561, 11, 709, 20294, 813, 2059, 13, 286, 5541, 309, 294, 456, 13, 876, 11, 309, 390, 869, 13, 467, 2732, 51056], "temperature": 0.0, "avg_logprob": -0.1766023567254595, "compression_ratio": 1.7034700315457414, "no_speech_prob": 0.042132314294576645}, {"id": 73, "seek": 39076, "start": 404.59999999999997, "end": 409.48, "text": " out brilliantly. It completely destroyed the bottleneck. Speeds sheet loading went up by", "tokens": [51056, 484, 8695, 42580, 13, 467, 2584, 8937, 264, 44641, 547, 13, 3550, 5147, 8193, 15114, 1437, 493, 538, 51300], "temperature": 0.0, "avg_logprob": -0.1766023567254595, "compression_ratio": 1.7034700315457414, "no_speech_prob": 0.042132314294576645}, {"id": 74, "seek": 39076, "start": 409.48, "end": 413.88, "text": " a factor of two or three. And I thought I had a great win. And then the first bug came", "tokens": [51300, 257, 5952, 295, 732, 420, 1045, 13, 400, 286, 1194, 286, 632, 257, 869, 1942, 13, 400, 550, 264, 700, 7426, 1361, 51520], "temperature": 0.0, "avg_logprob": -0.1766023567254595, "compression_ratio": 1.7034700315457414, "no_speech_prob": 0.042132314294576645}, {"id": 75, "seek": 39076, "start": 413.88, "end": 418.56, "text": " in where there was an edge case where it wasn't quite the locking because we were doing concurrent", "tokens": [51520, 294, 689, 456, 390, 364, 4691, 1389, 689, 309, 2067, 380, 1596, 264, 23954, 570, 321, 645, 884, 37702, 51754], "temperature": 0.0, "avg_logprob": -0.1766023567254595, "compression_ratio": 1.7034700315457414, "no_speech_prob": 0.042132314294576645}, {"id": 76, "seek": 41856, "start": 418.6, "end": 423.32, "text": " locking now. It was getting some weird edge cases. We had two different maps. They were", "tokens": [50366, 23954, 586, 13, 467, 390, 1242, 512, 3657, 4691, 3331, 13, 492, 632, 732, 819, 11317, 13, 814, 645, 50602], "temperature": 0.0, "avg_logprob": -0.22033932334498355, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.014377561397850513}, {"id": 77, "seek": 41856, "start": 423.32, "end": 430.24, "text": " both talking in the same thing. So I then had to fix those edge cases. I reworked it.", "tokens": [50602, 1293, 1417, 294, 264, 912, 551, 13, 407, 286, 550, 632, 281, 3191, 729, 4691, 3331, 13, 286, 48376, 292, 309, 13, 50948], "temperature": 0.0, "avg_logprob": -0.22033932334498355, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.014377561397850513}, {"id": 78, "seek": 41856, "start": 430.24, "end": 435.56, "text": " We fixed those bugs. We reworked it again. I fixed the bugs. I eventually got it working.", "tokens": [50948, 492, 6806, 729, 15120, 13, 492, 48376, 292, 309, 797, 13, 286, 6806, 264, 15120, 13, 286, 4728, 658, 309, 1364, 13, 51214], "temperature": 0.0, "avg_logprob": -0.22033932334498355, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.014377561397850513}, {"id": 79, "seek": 41856, "start": 435.56, "end": 443.6, "text": " And then another bright guy came in. Lubosh came in and said, but wait. String pool is", "tokens": [51214, 400, 550, 1071, 4730, 2146, 1361, 294, 13, 43781, 3019, 1361, 294, 293, 848, 11, 457, 1699, 13, 745, 2937, 7005, 307, 51616], "temperature": 0.0, "avg_logprob": -0.22033932334498355, "compression_ratio": 1.650943396226415, "no_speech_prob": 0.014377561397850513}, {"id": 80, "seek": 44360, "start": 443.76000000000005, "end": 450.44, "text": " indeed a hotspot, but the hotspot is not actually the hash map inside string pool. The hotspot", "tokens": [50372, 6451, 257, 36121, 17698, 11, 457, 264, 36121, 17698, 307, 406, 767, 264, 22019, 4471, 1854, 6798, 7005, 13, 440, 36121, 17698, 50706], "temperature": 0.0, "avg_logprob": -0.15475370265819408, "compression_ratio": 1.8865546218487395, "no_speech_prob": 0.011570852249860764}, {"id": 81, "seek": 44360, "start": 450.44, "end": 455.64000000000004, "text": " is the uppercase conversion. He improved the uppercase conversion. And by the time he was", "tokens": [50706, 307, 264, 11775, 2869, 651, 14298, 13, 634, 9689, 264, 11775, 2869, 651, 14298, 13, 400, 538, 264, 565, 415, 390, 50966], "temperature": 0.0, "avg_logprob": -0.15475370265819408, "compression_ratio": 1.8865546218487395, "no_speech_prob": 0.011570852249860764}, {"id": 82, "seek": 44360, "start": 455.64000000000004, "end": 459.52000000000004, "text": " done improving the uppercase conversion, my concurrent hash map was making no difference", "tokens": [50966, 1096, 11470, 264, 11775, 2869, 651, 14298, 11, 452, 37702, 22019, 4471, 390, 1455, 572, 2649, 51160], "temperature": 0.0, "avg_logprob": -0.15475370265819408, "compression_ratio": 1.8865546218487395, "no_speech_prob": 0.011570852249860764}, {"id": 83, "seek": 44360, "start": 459.52000000000004, "end": 468.28000000000003, "text": " whatsoever. So I had identified a hotspot, but I hadn't identified the hotspot. So we", "tokens": [51160, 17076, 13, 407, 286, 632, 9234, 257, 36121, 17698, 11, 457, 286, 8782, 380, 9234, 264, 36121, 17698, 13, 407, 321, 51598], "temperature": 0.0, "avg_logprob": -0.15475370265819408, "compression_ratio": 1.8865546218487395, "no_speech_prob": 0.011570852249860764}, {"id": 84, "seek": 44360, "start": 468.28000000000003, "end": 472.64000000000004, "text": " backed it out because there's no point in keeping a highly complicated piece of equipment", "tokens": [51598, 20391, 309, 484, 570, 456, 311, 572, 935, 294, 5145, 257, 5405, 6179, 2522, 295, 5927, 51816], "temperature": 0.0, "avg_logprob": -0.15475370265819408, "compression_ratio": 1.8865546218487395, "no_speech_prob": 0.011570852249860764}, {"id": 85, "seek": 47264, "start": 472.68, "end": 476.52, "text": " like that around. So I was very sad to see it go. But I learned some stuff in the process.", "tokens": [50366, 411, 300, 926, 13, 407, 286, 390, 588, 4227, 281, 536, 309, 352, 13, 583, 286, 3264, 512, 1507, 294, 264, 1399, 13, 50558], "temperature": 0.0, "avg_logprob": -0.169961423955412, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.0036806294228881598}, {"id": 86, "seek": 47264, "start": 476.52, "end": 481.59999999999997, "text": " So no harm, no foul. And we backed it out again. And we're back to being faster than we were", "tokens": [50558, 407, 572, 6491, 11, 572, 23491, 13, 400, 321, 20391, 309, 484, 797, 13, 400, 321, 434, 646, 281, 885, 4663, 813, 321, 645, 50812], "temperature": 0.0, "avg_logprob": -0.169961423955412, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.0036806294228881598}, {"id": 87, "seek": 47264, "start": 481.59999999999997, "end": 490.03999999999996, "text": " before. Red line processing in writer, which is our document editor section, is often very,", "tokens": [50812, 949, 13, 4477, 1622, 9007, 294, 9936, 11, 597, 307, 527, 4166, 9839, 3541, 11, 307, 2049, 588, 11, 51234], "temperature": 0.0, "avg_logprob": -0.169961423955412, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.0036806294228881598}, {"id": 88, "seek": 47264, "start": 490.03999999999996, "end": 493.76, "text": " very slow, especially if there's a ton of red lines in a big document. And it's slow", "tokens": [51234, 588, 2964, 11, 2318, 498, 456, 311, 257, 2952, 295, 2182, 3876, 294, 257, 955, 4166, 13, 400, 309, 311, 2964, 51420], "temperature": 0.0, "avg_logprob": -0.169961423955412, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.0036806294228881598}, {"id": 89, "seek": 47264, "start": 493.76, "end": 499.12, "text": " both loading and slow at runtime because when we're doing red lines, we often are doing", "tokens": [51420, 1293, 15114, 293, 2964, 412, 34474, 570, 562, 321, 434, 884, 2182, 3876, 11, 321, 2049, 366, 884, 51688], "temperature": 0.0, "avg_logprob": -0.169961423955412, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.0036806294228881598}, {"id": 90, "seek": 49912, "start": 499.16, "end": 504.2, "text": " massive amounts of adding and deleting to data structures as we internally render and stuff.", "tokens": [50366, 5994, 11663, 295, 5127, 293, 48946, 281, 1412, 9227, 382, 321, 19501, 15529, 293, 1507, 13, 50618], "temperature": 0.0, "avg_logprob": -0.19368422031402588, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.025554699823260307}, {"id": 91, "seek": 49912, "start": 504.2, "end": 509.2, "text": " And so I thought I'd try and speed that up. However, this is writer. Writer is the most", "tokens": [50618, 400, 370, 286, 1194, 286, 1116, 853, 293, 3073, 300, 493, 13, 2908, 11, 341, 307, 9936, 13, 10159, 1681, 307, 264, 881, 50868], "temperature": 0.0, "avg_logprob": -0.19368422031402588, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.025554699823260307}, {"id": 92, "seek": 49912, "start": 509.2, "end": 514.5600000000001, "text": " cash unfriendly program known to mankind. It just wants to chase pointers all over RAM.", "tokens": [50868, 6388, 3971, 4896, 356, 1461, 2570, 281, 21220, 13, 467, 445, 2738, 281, 15359, 44548, 439, 670, 14561, 13, 51136], "temperature": 0.0, "avg_logprob": -0.19368422031402588, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.025554699823260307}, {"id": 93, "seek": 49912, "start": 514.5600000000001, "end": 521.16, "text": " It's dataset once you are more than, once you have more than a sort of 10 page document,", "tokens": [51136, 467, 311, 28872, 1564, 291, 366, 544, 813, 11, 1564, 291, 362, 544, 813, 257, 1333, 295, 1266, 3028, 4166, 11, 51466], "temperature": 0.0, "avg_logprob": -0.19368422031402588, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.025554699823260307}, {"id": 94, "seek": 49912, "start": 521.16, "end": 526.28, "text": " you are completely falling out of L1 cash. So you have no chance of getting cashed algorithms.", "tokens": [51466, 291, 366, 2584, 7440, 484, 295, 441, 16, 6388, 13, 407, 291, 362, 572, 2931, 295, 1242, 6388, 292, 14642, 13, 51722], "temperature": 0.0, "avg_logprob": -0.19368422031402588, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.025554699823260307}, {"id": 95, "seek": 52628, "start": 526.8, "end": 531.3199999999999, "text": " The data structures are inherently very, very complicated. Human languages and documents", "tokens": [50390, 440, 1412, 9227, 366, 27993, 588, 11, 588, 6179, 13, 10294, 8650, 293, 8512, 50616], "temperature": 0.0, "avg_logprob": -0.1392956461225237, "compression_ratio": 1.8409090909090908, "no_speech_prob": 0.01729501038789749}, {"id": 96, "seek": 52628, "start": 531.3199999999999, "end": 536.72, "text": " just are. Consequences are we do pointer chasing. So we're constantly loading a pointer and then", "tokens": [50616, 445, 366, 13, 2656, 11834, 2667, 366, 321, 360, 23918, 17876, 13, 407, 321, 434, 6460, 15114, 257, 23918, 293, 550, 50886], "temperature": 0.0, "avg_logprob": -0.1392956461225237, "compression_ratio": 1.8409090909090908, "no_speech_prob": 0.01729501038789749}, {"id": 97, "seek": 52628, "start": 536.72, "end": 540.8399999999999, "text": " following it to some other location in memory, which is guaranteed to not to be an L1 cache.", "tokens": [50886, 3480, 309, 281, 512, 661, 4914, 294, 4675, 11, 597, 307, 18031, 281, 406, 281, 312, 364, 441, 16, 19459, 13, 51092], "temperature": 0.0, "avg_logprob": -0.1392956461225237, "compression_ratio": 1.8409090909090908, "no_speech_prob": 0.01729501038789749}, {"id": 98, "seek": 52628, "start": 540.8399999999999, "end": 545.36, "text": " And then we're following that again to something else, which is also not an L1 cache. And in the", "tokens": [51092, 400, 550, 321, 434, 3480, 300, 797, 281, 746, 1646, 11, 597, 307, 611, 406, 364, 441, 16, 19459, 13, 400, 294, 264, 51318], "temperature": 0.0, "avg_logprob": -0.1392956461225237, "compression_ratio": 1.8409090909090908, "no_speech_prob": 0.01729501038789749}, {"id": 99, "seek": 52628, "start": 545.36, "end": 549.4399999999999, "text": " process, we've now blown apart our cache. So if we need the first pointer again, that's also not", "tokens": [51318, 1399, 11, 321, 600, 586, 16479, 4936, 527, 19459, 13, 407, 498, 321, 643, 264, 700, 23918, 797, 11, 300, 311, 611, 406, 51522], "temperature": 0.0, "avg_logprob": -0.1392956461225237, "compression_ratio": 1.8409090909090908, "no_speech_prob": 0.01729501038789749}, {"id": 100, "seek": 52628, "start": 549.4399999999999, "end": 554.4, "text": " an L1 cache. So we just chase our tails around in a very slow processing. I did my best to fix", "tokens": [51522, 364, 441, 16, 19459, 13, 407, 321, 445, 15359, 527, 28537, 926, 294, 257, 588, 2964, 9007, 13, 286, 630, 452, 1151, 281, 3191, 51770], "temperature": 0.0, "avg_logprob": -0.1392956461225237, "compression_ratio": 1.8409090909090908, "no_speech_prob": 0.01729501038789749}, {"id": 101, "seek": 55440, "start": 554.4399999999999, "end": 562.8, "text": " this. And that involves trying to speed up something called a node index and a content index. And I", "tokens": [50366, 341, 13, 400, 300, 11626, 1382, 281, 3073, 493, 746, 1219, 257, 9984, 8186, 293, 257, 2701, 8186, 13, 400, 286, 50784], "temperature": 0.0, "avg_logprob": -0.14750192642211915, "compression_ratio": 1.6032388663967612, "no_speech_prob": 0.005327364895492792}, {"id": 102, "seek": 55440, "start": 562.8, "end": 569.36, "text": " failed. I am now three levels deep, fixing different things related to that, no end in sight. And I'm", "tokens": [50784, 7612, 13, 286, 669, 586, 1045, 4358, 2452, 11, 19442, 819, 721, 4077, 281, 300, 11, 572, 917, 294, 7860, 13, 400, 286, 478, 51112], "temperature": 0.0, "avg_logprob": -0.14750192642211915, "compression_ratio": 1.6032388663967612, "no_speech_prob": 0.005327364895492792}, {"id": 103, "seek": 55440, "start": 569.36, "end": 573.68, "text": " currently bottlenecked on a piece of processing that I just don't understand. So that didn't", "tokens": [51112, 4362, 44641, 44118, 322, 257, 2522, 295, 9007, 300, 286, 445, 500, 380, 1223, 13, 407, 300, 994, 380, 51328], "temperature": 0.0, "avg_logprob": -0.14750192642211915, "compression_ratio": 1.6032388663967612, "no_speech_prob": 0.005327364895492792}, {"id": 104, "seek": 55440, "start": 573.68, "end": 579.64, "text": " work out. But in the process, I now know a lot more about writer. And so I consider that a win. Thank", "tokens": [51328, 589, 484, 13, 583, 294, 264, 1399, 11, 286, 586, 458, 257, 688, 544, 466, 9936, 13, 400, 370, 286, 1949, 300, 257, 1942, 13, 1044, 51626], "temperature": 0.0, "avg_logprob": -0.14750192642211915, "compression_ratio": 1.6032388663967612, "no_speech_prob": 0.005327364895492792}, {"id": 105, "seek": 57964, "start": 579.64, "end": 579.84, "text": " you.", "tokens": [50366, 291, 13, 50374], "temperature": 0.0, "avg_logprob": -0.9656258583068847, "compression_ratio": 0.3333333333333333, "no_speech_prob": 0.1908491849899292}], "language": "en"}