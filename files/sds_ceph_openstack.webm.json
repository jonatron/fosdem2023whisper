{"text": " Thank you, let's welcome Francesco on the present and future of the app. Okay, thanks everyone. This session is about Obestac and self-integration. With our quick glance to the Kubernetes world, it is a quick agenda. I'm going to talk about what would have been the integration with SAF in the Obestac system, in the Obestac community in general, what's the status of the app, what has been changed with the SAFADM in the Bermedal world and what means having Kubernetes in this picture. There is also a demo which is, I'm not going to show the demo, but it's linked in the session, so you can feel free to take it later. So, for reasons not familiar with the Obestac project, it's just pretty old at this point. It's infrastructure as a service. As you can see on the right side, there are a lot of projects being part of the Obestac ecosystem. Each of them provide interfaces to each other and they cooperate together to provide processing, storage, network resources. You basically can build your cloud infrastructure using all these projects together. It's open source, it's now 10 years, 13 years old, so there are a lot of projects that are stable now. And SAF is part of this picture in the sense that you can probably don't see this picture very well, but in both computes, in the storage part, we have SAF that basically can be used as a backend for NOVA, which is the compute processing component, so we can provide ephemeral disks using SAF as a backend. We have Manila providing object, there is a good providing file. Swift providing object, there is a long story with the integration with Radus Gateway. Glance for images, so basically all these components you see there in the storage area, they can use SAF as a backend and this is a justification to have this integration with these two technologies. So, why the integration and why it's relevant? There is the HCI, I will convert the infrastructures. Operators can save hardware resources, co-locating the compute part of the infrastructure and OSD nodes. This means that you can save hardware, you can have both technologies together serving as a full operational infrastructure. This is funny, I just asked chat GPT, why SAF is relevant and why the integration of these two projects is interesting. I just put there, you can not see probably this part on the right, but it's basically my marketing sentence on why this thing makes sense. And there is scalability, resiliency, scalability and all this kind of keywords that we like a lot. But this session is about orchestrating services, deploying object stack and SAF together. There have been a lot of deployment tools and strategies over the past and I want just to focus on Cefantibol and Cefadm because Cefantibol has been the official tool, the most useful tool in the past and now it's FADM. Things have changed a bit, especially in the object stack ecosystem. So, previously the approach was I need my infrastructure as a service, I need to deploy object stack and SAF. I want to describe my entire cluster with a lot, a ton of variables and I push that magic button that makes it so. So, Cefantibol was there in this picture during the deploying of object stack. There was this particular part where Cefantibol was triggered to deploy SAF behind the scene. So, the drawback of the solution is that if you need to change anything in your Cef cluster, you have to run again the playbook, the Cefantibol playbook, because there is this Ansible layer that manage everything for you and it needs to stay consistent with the status of the cluster. So, basically variables and the human operator is supposed to provide variables and maintain those variables, which is a bit different. Also, it affects scale down, scale up operation and day-to-operation, especially day-to-operation. I want to change something in my Cef cluster, I need to run the deployment again because I can rely on the Ansible hidden potency, basically. But with CefADM things are a bit different, the status of the cluster is not made by tons of Ansible variables. There is CefADM, which is able to keep the status of the cluster, watch the cluster and take an action. I want to deploy a new SD whenever I attach a new disk. I can do that and I don't have to run deployment again or do any fancy day-to-operation with my tool that is supposed to manage my infrastructure in general, which is made by both Opestuck and Cef. And this changes a bit because we had the Cefantibol word where we describe all our cloud infrastructure. We pushed that magic button and everything was deployed and sometimes broken. But now, operators are more aware about the steps, so things have changed because you have to provide networks. And networks means that you want to manage your hardware, you want to build your networks, you want to use... This is specifically for the triple project where we integrated Cefantibol in the past and now we're moving to CefADM. And now people should provide networks, should provide metal, the description of the nodes and they are just separated steps. The storage Cef is deployed first, so you can start deploying the Opestuck infrastructure with a minimal Cef cluster already deployed. And when you deploy Opestuck, you can say, I have Cinder, I need Volumes, I need the Volumes pool and I can finalize my cluster, creating and configuring the Cef cluster accordingly. I have Manila, I need CefFS, I can deploy MDS, doing other stuff behind the scene. But you're basically aware that according to the service you're deploying in your Opestuck infrastructure, you can enable pieces in your Cef cluster and you can just tune it accordingly. At that point, we still have the Ansible layer managing Opestuck and all the infrastructure in general, but at that point the Cef cluster is seen as a separated entity. So it's like having an external Cef cluster, even though you have the same tool deploying both technologies together. And CefADM and the manager, the orchestrator, is the piece in Cef that is supposed to provide interfaces where the operators can interact with. And it's basically the slide. We still have Ansible doing everything on top, but the orchestrator interface is what you can rely on to make changes in your cluster without having to redeploy everything again around the Ansible that can take a lot of time if your infrastructure is large. And we don't have any, the operator is not supposed to keep and maintain any variable in the Ansible system. You can just interact with the CefADM CLI, create a spec which is a little definition of the Cef service that will be translated by the orchestrator and it will be deployed as a new demon in the Cef cluster. So this is about why it's so easy. Because you have Ansible, at some point you can just bootstrap a minimal Cef cluster with this command, bootstrap, providing a monitor IP address because networks are already there at that stage. And you can create a spec file that is basically the description of the nodes that should be enrolled in Cef and you can just apply them. It's even easier rather than running Ansible with a lot of roles, execution time, which is long enough. And data operation are complicated. Are complicated because not just because of this slide and this is the interaction with the CefADM CLI, you can query the cluster and see the health status. You can see where demons are placed, you can list the hosts and manage their labels and assign roles to these hosts. You can do a lot of things. But the real point here is that with CefADM we don't have the need to run again all the deployment of the Opesta infrastructure. An example of this, of how projects are impacted by this change is Manila. It's not just because we need a new version of LibreVD, we need to be compatible and we are changing from CefAnsible to CefADM, but just because we are doing architectural changes to the use cases provided by Opesta. Manila is that service that curves out CefFS volumes and provides them to the virtual machine within tenants, which means that you have a dedicated network that you can use to mount your shares. And behind the scene we have CefFS or NFS with Ganesha. In the past, Manila was supposed to use the bus to interact with NFS Ganesha. And it was complicated because we had to run privileged containers. We had to use this interface to update and manage shares using Ganesha as a gateway. And from an architectural point of view, we had an active passive model made by Peacemaker and SystemD. So you basically had Peacemaker honing the virtual IP as an entry point and then one active Ganesha, even though you have more than one instance. And with some constraints with SystemD. Now with CefADM there is an interface with the manager, with the NFS cluster, and Manila can use a new driver to interact with this component. We don't have to do the bus anymore, we don't have to do the bus to the Ganesha container anymore. And that's the new model where we rely on the Ingress Demon provided by CefADM, and this Ingress Demon is made by HAProxy and KIPaLiveD. KIPaLiveD honing the V as an entry point, HAProxy for load balancing across the, and distributing the load across the NFS Ganesha server. It's a better approach, we still have some limitation on this area, because considering that Manila is an infrastructure service for Obestac, but providing shares within the tenant, virtual machines, with a dedicated network, we need client restrictions to avoid other tenants mounting the same share. And there is an effort doing the proxy protocol in Ganesha to make sure that we can use client restriction with this new model, which is the current limitation basically. Or at least there is some effort to provide floating stable IP addresses to the Ingress Demon and skip the HAProxy layer, which is always an additional hope. And in terms of performance, this can be something that has an impact, of course. Lastly, at this point we had Cefansible, we have CefADM, what Kubernetes means in this world. We have Rook as a way to deploy Cef within Kubernetes, regardless of how Obestac is deployed, we have several combinations of these two things together. You can have converged infrastructure where Obestac control plane is virtualized, or it can be containerized, so basically using the same model, the same approach to deploy both, it can be useful, because it's a unified approach to manage your infrastructure. It's easy, deployable and reproducible, because Kubernetes poses a standard approach to deploy things, so we don't have anymore that flexibility that today is triple O, but it's easier from that point of view. And the same Cef cluster can be shared between Obestac and Kubernetes. We have different workloads. Kubernetes is PVC interfaces provided by Rook. Obestac is mostly RBD and your workload runs virtual machines, and it's usually outside, so you have to reach from the compute node the Rook cluster, the Cef cluster deployed by Rook within Kubernetes, which poses some networking challenges that they can be managed using host networking through, so you're using Kubernetes as a platform to deploy your Cef cluster, but you're still relying on the host networking to reach it and provide RBD to the outside workload, and that's the point of this slide. There are some things that are not mentioned here, like some tuning in Rook, that is supposed to be done to make sure that there is Kubernetes in the middle, so it's not natively, the native Cef cluster we usually have. So at this point, the thing is that we should do some tuning, especially in the HCI world, because Iverconverged is still one of the most popular use cases, and HCI is provided out of the box by Kubernetes. You can tag your infra nodes, you can deploy Rook, you can assign those nodes for OSDs. That's it, that's it. But at that point, you have to make sure that both your cluster and the connection is valuable for the outside workload. So this can be done, it's done by this demo. I'm not going to show this, but it's all there, it's all described, it's all I was describing in this slide. So you can have your OB-STAC infrastructure deployed with DevStack or TripleO, and it's still bare metal, and it can consume a Cef cluster deployed by Rook using RBD. You can still use the CSI, actually, to provide PVC interface. It's not something that it's mutually exclusive, but it's just a good exercise to see how these two technologies can work together in the future, probably. And yeah, just some additional resources for those interested in looking at these slides offline and some contacts for people in the OB-STAC world if you want to dig more into these experiments. And that's it. Thank you very much.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 26.0, "text": " Thank you, let's welcome Francesco on the present and future of the app.", "tokens": [1044, 291, 11, 718, 311, 2928, 31441, 1291, 322, 264, 1974, 293, 2027, 295, 264, 724, 13], "temperature": 0.0, "avg_logprob": -0.8380470275878906, "compression_ratio": 0.972972972972973, "no_speech_prob": 0.4754057228565216}, {"id": 1, "seek": 2600, "start": 26.0, "end": 32.0, "text": " Okay, thanks everyone. This session is about Obestac and self-integration.", "tokens": [1033, 11, 3231, 1518, 13, 639, 5481, 307, 466, 4075, 377, 326, 293, 2698, 12, 31131, 399, 13], "temperature": 0.0, "avg_logprob": -0.28313315786966464, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.0027734648901969194}, {"id": 2, "seek": 2600, "start": 32.0, "end": 39.0, "text": " With our quick glance to the Kubernetes world, it is a quick agenda.", "tokens": [2022, 527, 1702, 21094, 281, 264, 23145, 1002, 11, 309, 307, 257, 1702, 9829, 13], "temperature": 0.0, "avg_logprob": -0.28313315786966464, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.0027734648901969194}, {"id": 3, "seek": 2600, "start": 39.0, "end": 46.0, "text": " I'm going to talk about what would have been the integration with SAF in the Obestac system,", "tokens": [286, 478, 516, 281, 751, 466, 437, 576, 362, 668, 264, 10980, 365, 16482, 37, 294, 264, 4075, 377, 326, 1185, 11], "temperature": 0.0, "avg_logprob": -0.28313315786966464, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.0027734648901969194}, {"id": 4, "seek": 2600, "start": 46.0, "end": 50.0, "text": " in the Obestac community in general, what's the status of the app,", "tokens": [294, 264, 4075, 377, 326, 1768, 294, 2674, 11, 437, 311, 264, 6558, 295, 264, 724, 11], "temperature": 0.0, "avg_logprob": -0.28313315786966464, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.0027734648901969194}, {"id": 5, "seek": 5000, "start": 50.0, "end": 57.0, "text": " what has been changed with the SAFADM in the Bermedal world and what means having Kubernetes in this picture.", "tokens": [437, 575, 668, 3105, 365, 264, 16482, 37, 6112, 44, 294, 264, 363, 966, 292, 304, 1002, 293, 437, 1355, 1419, 23145, 294, 341, 3036, 13], "temperature": 0.0, "avg_logprob": -0.15807170493929995, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.0005175709957256913}, {"id": 6, "seek": 5000, "start": 57.0, "end": 63.0, "text": " There is also a demo which is, I'm not going to show the demo, but it's linked in the session,", "tokens": [821, 307, 611, 257, 10723, 597, 307, 11, 286, 478, 406, 516, 281, 855, 264, 10723, 11, 457, 309, 311, 9408, 294, 264, 5481, 11], "temperature": 0.0, "avg_logprob": -0.15807170493929995, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.0005175709957256913}, {"id": 7, "seek": 5000, "start": 63.0, "end": 66.0, "text": " so you can feel free to take it later.", "tokens": [370, 291, 393, 841, 1737, 281, 747, 309, 1780, 13], "temperature": 0.0, "avg_logprob": -0.15807170493929995, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.0005175709957256913}, {"id": 8, "seek": 5000, "start": 66.0, "end": 72.0, "text": " So, for reasons not familiar with the Obestac project, it's just pretty old at this point.", "tokens": [407, 11, 337, 4112, 406, 4963, 365, 264, 4075, 377, 326, 1716, 11, 309, 311, 445, 1238, 1331, 412, 341, 935, 13], "temperature": 0.0, "avg_logprob": -0.15807170493929995, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.0005175709957256913}, {"id": 9, "seek": 5000, "start": 72.0, "end": 75.0, "text": " It's infrastructure as a service.", "tokens": [467, 311, 6896, 382, 257, 2643, 13], "temperature": 0.0, "avg_logprob": -0.15807170493929995, "compression_ratio": 1.5081967213114753, "no_speech_prob": 0.0005175709957256913}, {"id": 10, "seek": 7500, "start": 75.0, "end": 82.0, "text": " As you can see on the right side, there are a lot of projects being part of the Obestac ecosystem.", "tokens": [1018, 291, 393, 536, 322, 264, 558, 1252, 11, 456, 366, 257, 688, 295, 4455, 885, 644, 295, 264, 4075, 377, 326, 11311, 13], "temperature": 0.0, "avg_logprob": -0.08746215068932736, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.000268977542873472}, {"id": 11, "seek": 7500, "start": 82.0, "end": 90.0, "text": " Each of them provide interfaces to each other and they cooperate together to provide processing, storage, network resources.", "tokens": [6947, 295, 552, 2893, 28416, 281, 1184, 661, 293, 436, 26667, 1214, 281, 2893, 9007, 11, 6725, 11, 3209, 3593, 13], "temperature": 0.0, "avg_logprob": -0.08746215068932736, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.000268977542873472}, {"id": 12, "seek": 7500, "start": 90.0, "end": 96.0, "text": " You basically can build your cloud infrastructure using all these projects together.", "tokens": [509, 1936, 393, 1322, 428, 4588, 6896, 1228, 439, 613, 4455, 1214, 13], "temperature": 0.0, "avg_logprob": -0.08746215068932736, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.000268977542873472}, {"id": 13, "seek": 9600, "start": 96.0, "end": 106.0, "text": " It's open source, it's now 10 years, 13 years old, so there are a lot of projects that are stable now.", "tokens": [467, 311, 1269, 4009, 11, 309, 311, 586, 1266, 924, 11, 3705, 924, 1331, 11, 370, 456, 366, 257, 688, 295, 4455, 300, 366, 8351, 586, 13], "temperature": 0.0, "avg_logprob": -0.12189230506802783, "compression_ratio": 1.5204081632653061, "no_speech_prob": 0.00043710815953090787}, {"id": 14, "seek": 9600, "start": 106.0, "end": 113.0, "text": " And SAF is part of this picture in the sense that you can probably don't see this picture very well,", "tokens": [400, 16482, 37, 307, 644, 295, 341, 3036, 294, 264, 2020, 300, 291, 393, 1391, 500, 380, 536, 341, 3036, 588, 731, 11], "temperature": 0.0, "avg_logprob": -0.12189230506802783, "compression_ratio": 1.5204081632653061, "no_speech_prob": 0.00043710815953090787}, {"id": 15, "seek": 9600, "start": 113.0, "end": 123.0, "text": " but in both computes, in the storage part, we have SAF that basically can be used as a backend", "tokens": [457, 294, 1293, 715, 1819, 11, 294, 264, 6725, 644, 11, 321, 362, 16482, 37, 300, 1936, 393, 312, 1143, 382, 257, 38087], "temperature": 0.0, "avg_logprob": -0.12189230506802783, "compression_ratio": 1.5204081632653061, "no_speech_prob": 0.00043710815953090787}, {"id": 16, "seek": 12300, "start": 123.0, "end": 131.0, "text": " for NOVA, which is the compute processing component, so we can provide ephemeral disks using SAF as a backend.", "tokens": [337, 9146, 20914, 11, 597, 307, 264, 14722, 9007, 6542, 11, 370, 321, 393, 2893, 308, 41245, 2790, 41617, 1228, 16482, 37, 382, 257, 38087, 13], "temperature": 0.0, "avg_logprob": -0.1797891841215246, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.00010363176988903433}, {"id": 17, "seek": 12300, "start": 131.0, "end": 138.0, "text": " We have Manila providing object, there is a good providing file.", "tokens": [492, 362, 2458, 7371, 6530, 2657, 11, 456, 307, 257, 665, 6530, 3991, 13], "temperature": 0.0, "avg_logprob": -0.1797891841215246, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.00010363176988903433}, {"id": 18, "seek": 12300, "start": 138.0, "end": 145.0, "text": " Swift providing object, there is a long story with the integration with Radus Gateway.", "tokens": [25539, 6530, 2657, 11, 456, 307, 257, 938, 1657, 365, 264, 10980, 365, 9654, 301, 48394, 13], "temperature": 0.0, "avg_logprob": -0.1797891841215246, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.00010363176988903433}, {"id": 19, "seek": 12300, "start": 145.0, "end": 152.0, "text": " Glance for images, so basically all these components you see there in the storage area,", "tokens": [5209, 719, 337, 5267, 11, 370, 1936, 439, 613, 6677, 291, 536, 456, 294, 264, 6725, 1859, 11], "temperature": 0.0, "avg_logprob": -0.1797891841215246, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.00010363176988903433}, {"id": 20, "seek": 15200, "start": 152.0, "end": 161.0, "text": " they can use SAF as a backend and this is a justification to have this integration with these two technologies.", "tokens": [436, 393, 764, 16482, 37, 382, 257, 38087, 293, 341, 307, 257, 31591, 281, 362, 341, 10980, 365, 613, 732, 7943, 13], "temperature": 0.0, "avg_logprob": -0.20319285216154875, "compression_ratio": 1.4133333333333333, "no_speech_prob": 0.00028099457267671824}, {"id": 21, "seek": 15200, "start": 161.0, "end": 167.0, "text": " So, why the integration and why it's relevant?", "tokens": [407, 11, 983, 264, 10980, 293, 983, 309, 311, 7340, 30], "temperature": 0.0, "avg_logprob": -0.20319285216154875, "compression_ratio": 1.4133333333333333, "no_speech_prob": 0.00028099457267671824}, {"id": 22, "seek": 15200, "start": 167.0, "end": 171.0, "text": " There is the HCI, I will convert the infrastructures.", "tokens": [821, 307, 264, 389, 25240, 11, 286, 486, 7620, 264, 6534, 44513, 13], "temperature": 0.0, "avg_logprob": -0.20319285216154875, "compression_ratio": 1.4133333333333333, "no_speech_prob": 0.00028099457267671824}, {"id": 23, "seek": 17100, "start": 171.0, "end": 184.0, "text": " Operators can save hardware resources, co-locating the compute part of the infrastructure and OSD nodes.", "tokens": [12480, 3391, 393, 3155, 8837, 3593, 11, 598, 12, 5842, 990, 264, 14722, 644, 295, 264, 6896, 293, 12731, 35, 13891, 13], "temperature": 0.0, "avg_logprob": -0.1996646024742905, "compression_ratio": 1.5165562913907285, "no_speech_prob": 0.00020012095046695322}, {"id": 24, "seek": 17100, "start": 184.0, "end": 194.0, "text": " This means that you can save hardware, you can have both technologies together serving as a full operational infrastructure.", "tokens": [639, 1355, 300, 291, 393, 3155, 8837, 11, 291, 393, 362, 1293, 7943, 1214, 8148, 382, 257, 1577, 16607, 6896, 13], "temperature": 0.0, "avg_logprob": -0.1996646024742905, "compression_ratio": 1.5165562913907285, "no_speech_prob": 0.00020012095046695322}, {"id": 25, "seek": 19400, "start": 194.0, "end": 209.0, "text": " This is funny, I just asked chat GPT, why SAF is relevant and why the integration of these two projects is interesting.", "tokens": [639, 307, 4074, 11, 286, 445, 2351, 5081, 26039, 51, 11, 983, 16482, 37, 307, 7340, 293, 983, 264, 10980, 295, 613, 732, 4455, 307, 1880, 13], "temperature": 0.0, "avg_logprob": -0.21978582654680526, "compression_ratio": 1.449438202247191, "no_speech_prob": 0.0006035694968886673}, {"id": 26, "seek": 19400, "start": 209.0, "end": 222.0, "text": " I just put there, you can not see probably this part on the right, but it's basically my marketing sentence on why this thing makes sense.", "tokens": [286, 445, 829, 456, 11, 291, 393, 406, 536, 1391, 341, 644, 322, 264, 558, 11, 457, 309, 311, 1936, 452, 6370, 8174, 322, 983, 341, 551, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.21978582654680526, "compression_ratio": 1.449438202247191, "no_speech_prob": 0.0006035694968886673}, {"id": 27, "seek": 22200, "start": 222.0, "end": 231.0, "text": " And there is scalability, resiliency, scalability and all this kind of keywords that we like a lot.", "tokens": [400, 456, 307, 15664, 2310, 11, 48712, 11, 15664, 2310, 293, 439, 341, 733, 295, 21009, 300, 321, 411, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.18754611077246727, "compression_ratio": 1.555, "no_speech_prob": 0.0006242463714443147}, {"id": 28, "seek": 22200, "start": 231.0, "end": 238.0, "text": " But this session is about orchestrating services, deploying object stack and SAF together.", "tokens": [583, 341, 5481, 307, 466, 14161, 8754, 3328, 11, 34198, 2657, 8630, 293, 16482, 37, 1214, 13], "temperature": 0.0, "avg_logprob": -0.18754611077246727, "compression_ratio": 1.555, "no_speech_prob": 0.0006242463714443147}, {"id": 29, "seek": 22200, "start": 238.0, "end": 247.0, "text": " There have been a lot of deployment tools and strategies over the past and I want just to focus on Cefantibol and Cefadm", "tokens": [821, 362, 668, 257, 688, 295, 19317, 3873, 293, 9029, 670, 264, 1791, 293, 286, 528, 445, 281, 1879, 322, 383, 5666, 394, 897, 401, 293, 383, 5666, 345, 76], "temperature": 0.0, "avg_logprob": -0.18754611077246727, "compression_ratio": 1.555, "no_speech_prob": 0.0006242463714443147}, {"id": 30, "seek": 24700, "start": 247.0, "end": 254.0, "text": " because Cefantibol has been the official tool, the most useful tool in the past and now it's FADM.", "tokens": [570, 383, 5666, 394, 897, 401, 575, 668, 264, 4783, 2290, 11, 264, 881, 4420, 2290, 294, 264, 1791, 293, 586, 309, 311, 479, 6112, 44, 13], "temperature": 0.0, "avg_logprob": -0.10682861010233562, "compression_ratio": 1.4578947368421054, "no_speech_prob": 0.0003053339896723628}, {"id": 31, "seek": 24700, "start": 254.0, "end": 258.0, "text": " Things have changed a bit, especially in the object stack ecosystem.", "tokens": [9514, 362, 3105, 257, 857, 11, 2318, 294, 264, 2657, 8630, 11311, 13], "temperature": 0.0, "avg_logprob": -0.10682861010233562, "compression_ratio": 1.4578947368421054, "no_speech_prob": 0.0003053339896723628}, {"id": 32, "seek": 24700, "start": 258.0, "end": 267.0, "text": " So, previously the approach was I need my infrastructure as a service, I need to deploy object stack and SAF.", "tokens": [407, 11, 8046, 264, 3109, 390, 286, 643, 452, 6896, 382, 257, 2643, 11, 286, 643, 281, 7274, 2657, 8630, 293, 16482, 37, 13], "temperature": 0.0, "avg_logprob": -0.10682861010233562, "compression_ratio": 1.4578947368421054, "no_speech_prob": 0.0003053339896723628}, {"id": 33, "seek": 26700, "start": 267.0, "end": 277.0, "text": " I want to describe my entire cluster with a lot, a ton of variables and I push that magic button that makes it so.", "tokens": [286, 528, 281, 6786, 452, 2302, 13630, 365, 257, 688, 11, 257, 2952, 295, 9102, 293, 286, 2944, 300, 5585, 2960, 300, 1669, 309, 370, 13], "temperature": 0.0, "avg_logprob": -0.15634851197938662, "compression_ratio": 1.53475935828877, "no_speech_prob": 0.0003821853024419397}, {"id": 34, "seek": 26700, "start": 277.0, "end": 284.0, "text": " So, Cefantibol was there in this picture during the deploying of object stack.", "tokens": [407, 11, 383, 5666, 394, 897, 401, 390, 456, 294, 341, 3036, 1830, 264, 34198, 295, 2657, 8630, 13], "temperature": 0.0, "avg_logprob": -0.15634851197938662, "compression_ratio": 1.53475935828877, "no_speech_prob": 0.0003821853024419397}, {"id": 35, "seek": 26700, "start": 284.0, "end": 290.0, "text": " There was this particular part where Cefantibol was triggered to deploy SAF behind the scene.", "tokens": [821, 390, 341, 1729, 644, 689, 383, 5666, 394, 897, 401, 390, 21710, 281, 7274, 16482, 37, 2261, 264, 4145, 13], "temperature": 0.0, "avg_logprob": -0.15634851197938662, "compression_ratio": 1.53475935828877, "no_speech_prob": 0.0003821853024419397}, {"id": 36, "seek": 29000, "start": 290.0, "end": 301.0, "text": " So, the drawback of the solution is that if you need to change anything in your Cef cluster, you have to run again the playbook,", "tokens": [407, 11, 264, 2642, 3207, 295, 264, 3827, 307, 300, 498, 291, 643, 281, 1319, 1340, 294, 428, 383, 5666, 13630, 11, 291, 362, 281, 1190, 797, 264, 862, 2939, 11], "temperature": 0.0, "avg_logprob": -0.12510707697917506, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00013055656745564193}, {"id": 37, "seek": 29000, "start": 301.0, "end": 310.0, "text": " the Cefantibol playbook, because there is this Ansible layer that manage everything for you and it needs to stay consistent with the status of the cluster.", "tokens": [264, 383, 5666, 394, 897, 401, 862, 2939, 11, 570, 456, 307, 341, 14590, 964, 4583, 300, 3067, 1203, 337, 291, 293, 309, 2203, 281, 1754, 8398, 365, 264, 6558, 295, 264, 13630, 13], "temperature": 0.0, "avg_logprob": -0.12510707697917506, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00013055656745564193}, {"id": 38, "seek": 29000, "start": 310.0, "end": 319.0, "text": " So, basically variables and the human operator is supposed to provide variables and maintain those variables, which is a bit different.", "tokens": [407, 11, 1936, 9102, 293, 264, 1952, 12973, 307, 3442, 281, 2893, 9102, 293, 6909, 729, 9102, 11, 597, 307, 257, 857, 819, 13], "temperature": 0.0, "avg_logprob": -0.12510707697917506, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00013055656745564193}, {"id": 39, "seek": 31900, "start": 319.0, "end": 329.0, "text": " Also, it affects scale down, scale up operation and day-to-operation, especially day-to-operation.", "tokens": [2743, 11, 309, 11807, 4373, 760, 11, 4373, 493, 6916, 293, 786, 12, 1353, 12, 43493, 11, 2318, 786, 12, 1353, 12, 43493, 13], "temperature": 0.0, "avg_logprob": -0.19034244168189265, "compression_ratio": 1.4723926380368098, "no_speech_prob": 0.0001823091588448733}, {"id": 40, "seek": 31900, "start": 329.0, "end": 340.0, "text": " I want to change something in my Cef cluster, I need to run the deployment again because I can rely on the Ansible hidden potency, basically.", "tokens": [286, 528, 281, 1319, 746, 294, 452, 383, 5666, 13630, 11, 286, 643, 281, 1190, 264, 19317, 797, 570, 286, 393, 10687, 322, 264, 14590, 964, 7633, 1847, 3020, 11, 1936, 13], "temperature": 0.0, "avg_logprob": -0.19034244168189265, "compression_ratio": 1.4723926380368098, "no_speech_prob": 0.0001823091588448733}, {"id": 41, "seek": 34000, "start": 340.0, "end": 349.0, "text": " But with CefADM things are a bit different, the status of the cluster is not made by tons of Ansible variables.", "tokens": [583, 365, 383, 5666, 6112, 44, 721, 366, 257, 857, 819, 11, 264, 6558, 295, 264, 13630, 307, 406, 1027, 538, 9131, 295, 14590, 964, 9102, 13], "temperature": 0.0, "avg_logprob": -0.08138653437296549, "compression_ratio": 1.5664739884393064, "no_speech_prob": 7.85856245784089e-05}, {"id": 42, "seek": 34000, "start": 349.0, "end": 357.0, "text": " There is CefADM, which is able to keep the status of the cluster, watch the cluster and take an action.", "tokens": [821, 307, 383, 5666, 6112, 44, 11, 597, 307, 1075, 281, 1066, 264, 6558, 295, 264, 13630, 11, 1159, 264, 13630, 293, 747, 364, 3069, 13], "temperature": 0.0, "avg_logprob": -0.08138653437296549, "compression_ratio": 1.5664739884393064, "no_speech_prob": 7.85856245784089e-05}, {"id": 43, "seek": 34000, "start": 357.0, "end": 362.0, "text": " I want to deploy a new SD whenever I attach a new disk.", "tokens": [286, 528, 281, 7274, 257, 777, 14638, 5699, 286, 5085, 257, 777, 12355, 13], "temperature": 0.0, "avg_logprob": -0.08138653437296549, "compression_ratio": 1.5664739884393064, "no_speech_prob": 7.85856245784089e-05}, {"id": 44, "seek": 36200, "start": 362.0, "end": 374.0, "text": " I can do that and I don't have to run deployment again or do any fancy day-to-operation with my tool that is supposed to manage my infrastructure in general,", "tokens": [286, 393, 360, 300, 293, 286, 500, 380, 362, 281, 1190, 19317, 797, 420, 360, 604, 10247, 786, 12, 1353, 12, 43493, 365, 452, 2290, 300, 307, 3442, 281, 3067, 452, 6896, 294, 2674, 11], "temperature": 0.0, "avg_logprob": -0.1591811669178498, "compression_ratio": 1.5, "no_speech_prob": 0.00014891968748997897}, {"id": 45, "seek": 36200, "start": 374.0, "end": 380.0, "text": " which is made by both Opestuck and Cef.", "tokens": [597, 307, 1027, 538, 1293, 12011, 377, 1134, 293, 383, 5666, 13], "temperature": 0.0, "avg_logprob": -0.1591811669178498, "compression_ratio": 1.5, "no_speech_prob": 0.00014891968748997897}, {"id": 46, "seek": 36200, "start": 380.0, "end": 390.0, "text": " And this changes a bit because we had the Cefantibol word where we describe all our cloud infrastructure.", "tokens": [400, 341, 2962, 257, 857, 570, 321, 632, 264, 383, 5666, 394, 897, 401, 1349, 689, 321, 6786, 439, 527, 4588, 6896, 13], "temperature": 0.0, "avg_logprob": -0.1591811669178498, "compression_ratio": 1.5, "no_speech_prob": 0.00014891968748997897}, {"id": 47, "seek": 39000, "start": 390.0, "end": 397.0, "text": " We pushed that magic button and everything was deployed and sometimes broken.", "tokens": [492, 9152, 300, 5585, 2960, 293, 1203, 390, 17826, 293, 2171, 5463, 13], "temperature": 0.0, "avg_logprob": -0.12457588947180545, "compression_ratio": 1.625, "no_speech_prob": 0.0010711728828027844}, {"id": 48, "seek": 39000, "start": 397.0, "end": 406.0, "text": " But now, operators are more aware about the steps, so things have changed because you have to provide networks.", "tokens": [583, 586, 11, 19077, 366, 544, 3650, 466, 264, 4439, 11, 370, 721, 362, 3105, 570, 291, 362, 281, 2893, 9590, 13], "temperature": 0.0, "avg_logprob": -0.12457588947180545, "compression_ratio": 1.625, "no_speech_prob": 0.0010711728828027844}, {"id": 49, "seek": 39000, "start": 406.0, "end": 414.0, "text": " And networks means that you want to manage your hardware, you want to build your networks, you want to use...", "tokens": [400, 9590, 1355, 300, 291, 528, 281, 3067, 428, 8837, 11, 291, 528, 281, 1322, 428, 9590, 11, 291, 528, 281, 764, 485], "temperature": 0.0, "avg_logprob": -0.12457588947180545, "compression_ratio": 1.625, "no_speech_prob": 0.0010711728828027844}, {"id": 50, "seek": 41400, "start": 414.0, "end": 424.0, "text": " This is specifically for the triple project where we integrated Cefantibol in the past and now we're moving to CefADM.", "tokens": [639, 307, 4682, 337, 264, 15508, 1716, 689, 321, 10919, 383, 5666, 394, 897, 401, 294, 264, 1791, 293, 586, 321, 434, 2684, 281, 383, 5666, 6112, 44, 13], "temperature": 0.0, "avg_logprob": -0.14366962169778758, "compression_ratio": 1.5857740585774058, "no_speech_prob": 0.0001115803825086914}, {"id": 51, "seek": 41400, "start": 424.0, "end": 432.0, "text": " And now people should provide networks, should provide metal, the description of the nodes and they are just separated steps.", "tokens": [400, 586, 561, 820, 2893, 9590, 11, 820, 2893, 5760, 11, 264, 3855, 295, 264, 13891, 293, 436, 366, 445, 12005, 4439, 13], "temperature": 0.0, "avg_logprob": -0.14366962169778758, "compression_ratio": 1.5857740585774058, "no_speech_prob": 0.0001115803825086914}, {"id": 52, "seek": 41400, "start": 432.0, "end": 443.0, "text": " The storage Cef is deployed first, so you can start deploying the Opestuck infrastructure with a minimal Cef cluster already deployed.", "tokens": [440, 6725, 383, 5666, 307, 17826, 700, 11, 370, 291, 393, 722, 34198, 264, 12011, 377, 1134, 6896, 365, 257, 13206, 383, 5666, 13630, 1217, 17826, 13], "temperature": 0.0, "avg_logprob": -0.14366962169778758, "compression_ratio": 1.5857740585774058, "no_speech_prob": 0.0001115803825086914}, {"id": 53, "seek": 44300, "start": 443.0, "end": 452.0, "text": " And when you deploy Opestuck, you can say, I have Cinder, I need Volumes, I need the Volumes pool and I can finalize my cluster,", "tokens": [400, 562, 291, 7274, 12011, 377, 1134, 11, 291, 393, 584, 11, 286, 362, 383, 5669, 11, 286, 643, 8911, 10018, 11, 286, 643, 264, 8911, 10018, 7005, 293, 286, 393, 2572, 1125, 452, 13630, 11], "temperature": 0.0, "avg_logprob": -0.11666689163599259, "compression_ratio": 1.5229885057471264, "no_speech_prob": 0.0005151606164872646}, {"id": 54, "seek": 44300, "start": 452.0, "end": 455.0, "text": " creating and configuring the Cef cluster accordingly.", "tokens": [4084, 293, 6662, 1345, 264, 383, 5666, 13630, 19717, 13], "temperature": 0.0, "avg_logprob": -0.11666689163599259, "compression_ratio": 1.5229885057471264, "no_speech_prob": 0.0005151606164872646}, {"id": 55, "seek": 44300, "start": 455.0, "end": 462.0, "text": " I have Manila, I need CefFS, I can deploy MDS, doing other stuff behind the scene.", "tokens": [286, 362, 2458, 7371, 11, 286, 643, 383, 5666, 29318, 11, 286, 393, 7274, 376, 11844, 11, 884, 661, 1507, 2261, 264, 4145, 13], "temperature": 0.0, "avg_logprob": -0.11666689163599259, "compression_ratio": 1.5229885057471264, "no_speech_prob": 0.0005151606164872646}, {"id": 56, "seek": 46200, "start": 462.0, "end": 477.0, "text": " But you're basically aware that according to the service you're deploying in your Opestuck infrastructure, you can enable pieces in your Cef cluster and you can just tune it accordingly.", "tokens": [583, 291, 434, 1936, 3650, 300, 4650, 281, 264, 2643, 291, 434, 34198, 294, 428, 12011, 377, 1134, 6896, 11, 291, 393, 9528, 3755, 294, 428, 383, 5666, 13630, 293, 291, 393, 445, 10864, 309, 19717, 13], "temperature": 0.0, "avg_logprob": -0.10559446811676025, "compression_ratio": 1.7233009708737863, "no_speech_prob": 0.00010318777640350163}, {"id": 57, "seek": 46200, "start": 477.0, "end": 488.0, "text": " At that point, we still have the Ansible layer managing Opestuck and all the infrastructure in general, but at that point the Cef cluster is seen as a separated entity.", "tokens": [1711, 300, 935, 11, 321, 920, 362, 264, 14590, 964, 4583, 11642, 12011, 377, 1134, 293, 439, 264, 6896, 294, 2674, 11, 457, 412, 300, 935, 264, 383, 5666, 13630, 307, 1612, 382, 257, 12005, 13977, 13], "temperature": 0.0, "avg_logprob": -0.10559446811676025, "compression_ratio": 1.7233009708737863, "no_speech_prob": 0.00010318777640350163}, {"id": 58, "seek": 48800, "start": 488.0, "end": 496.0, "text": " So it's like having an external Cef cluster, even though you have the same tool deploying both technologies together.", "tokens": [407, 309, 311, 411, 1419, 364, 8320, 383, 5666, 13630, 11, 754, 1673, 291, 362, 264, 912, 2290, 34198, 1293, 7943, 1214, 13], "temperature": 0.0, "avg_logprob": -0.12057289530019291, "compression_ratio": 1.458100558659218, "no_speech_prob": 0.00014368635311257094}, {"id": 59, "seek": 48800, "start": 496.0, "end": 508.0, "text": " And CefADM and the manager, the orchestrator, is the piece in Cef that is supposed to provide interfaces where the operators can interact with.", "tokens": [400, 383, 5666, 6112, 44, 293, 264, 6598, 11, 264, 14161, 19802, 11, 307, 264, 2522, 294, 383, 5666, 300, 307, 3442, 281, 2893, 28416, 689, 264, 19077, 393, 4648, 365, 13], "temperature": 0.0, "avg_logprob": -0.12057289530019291, "compression_ratio": 1.458100558659218, "no_speech_prob": 0.00014368635311257094}, {"id": 60, "seek": 50800, "start": 508.0, "end": 523.0, "text": " And it's basically the slide. We still have Ansible doing everything on top, but the orchestrator interface is what you can rely on to make changes in your cluster", "tokens": [400, 309, 311, 1936, 264, 4137, 13, 492, 920, 362, 14590, 964, 884, 1203, 322, 1192, 11, 457, 264, 14161, 19802, 9226, 307, 437, 291, 393, 10687, 322, 281, 652, 2962, 294, 428, 13630], "temperature": 0.0, "avg_logprob": -0.08651217818260193, "compression_ratio": 1.543010752688172, "no_speech_prob": 0.0002817665517795831}, {"id": 61, "seek": 50800, "start": 523.0, "end": 533.0, "text": " without having to redeploy everything again around the Ansible that can take a lot of time if your infrastructure is large.", "tokens": [1553, 1419, 281, 14328, 2384, 1203, 797, 926, 264, 14590, 964, 300, 393, 747, 257, 688, 295, 565, 498, 428, 6896, 307, 2416, 13], "temperature": 0.0, "avg_logprob": -0.08651217818260193, "compression_ratio": 1.543010752688172, "no_speech_prob": 0.0002817665517795831}, {"id": 62, "seek": 53300, "start": 533.0, "end": 541.0, "text": " And we don't have any, the operator is not supposed to keep and maintain any variable in the Ansible system.", "tokens": [400, 321, 500, 380, 362, 604, 11, 264, 12973, 307, 406, 3442, 281, 1066, 293, 6909, 604, 7006, 294, 264, 14590, 964, 1185, 13], "temperature": 0.0, "avg_logprob": -0.0870934498460987, "compression_ratio": 1.5119617224880382, "no_speech_prob": 0.00014395330799743533}, {"id": 63, "seek": 53300, "start": 541.0, "end": 556.0, "text": " You can just interact with the CefADM CLI, create a spec which is a little definition of the Cef service that will be translated by the orchestrator and it will be deployed as a new demon in the Cef cluster.", "tokens": [509, 393, 445, 4648, 365, 264, 383, 5666, 6112, 44, 12855, 40, 11, 1884, 257, 1608, 597, 307, 257, 707, 7123, 295, 264, 383, 5666, 2643, 300, 486, 312, 16805, 538, 264, 14161, 19802, 293, 309, 486, 312, 17826, 382, 257, 777, 14283, 294, 264, 383, 5666, 13630, 13], "temperature": 0.0, "avg_logprob": -0.0870934498460987, "compression_ratio": 1.5119617224880382, "no_speech_prob": 0.00014395330799743533}, {"id": 64, "seek": 55600, "start": 556.0, "end": 568.0, "text": " So this is about why it's so easy. Because you have Ansible, at some point you can just bootstrap a minimal Cef cluster with this command,", "tokens": [407, 341, 307, 466, 983, 309, 311, 370, 1858, 13, 1436, 291, 362, 14590, 964, 11, 412, 512, 935, 291, 393, 445, 11450, 372, 4007, 257, 13206, 383, 5666, 13630, 365, 341, 5622, 11], "temperature": 0.0, "avg_logprob": -0.10358169641387596, "compression_ratio": 1.6, "no_speech_prob": 5.984726522001438e-05}, {"id": 65, "seek": 55600, "start": 568.0, "end": 574.0, "text": " bootstrap, providing a monitor IP address because networks are already there at that stage.", "tokens": [11450, 372, 4007, 11, 6530, 257, 6002, 8671, 2985, 570, 9590, 366, 1217, 456, 412, 300, 3233, 13], "temperature": 0.0, "avg_logprob": -0.10358169641387596, "compression_ratio": 1.6, "no_speech_prob": 5.984726522001438e-05}, {"id": 66, "seek": 55600, "start": 574.0, "end": 583.0, "text": " And you can create a spec file that is basically the description of the nodes that should be enrolled in Cef and you can just apply them.", "tokens": [400, 291, 393, 1884, 257, 1608, 3991, 300, 307, 1936, 264, 3855, 295, 264, 13891, 300, 820, 312, 25896, 294, 383, 5666, 293, 291, 393, 445, 3079, 552, 13], "temperature": 0.0, "avg_logprob": -0.10358169641387596, "compression_ratio": 1.6, "no_speech_prob": 5.984726522001438e-05}, {"id": 67, "seek": 58300, "start": 583.0, "end": 593.0, "text": " It's even easier rather than running Ansible with a lot of roles, execution time, which is long enough.", "tokens": [467, 311, 754, 3571, 2831, 813, 2614, 14590, 964, 365, 257, 688, 295, 9604, 11, 15058, 565, 11, 597, 307, 938, 1547, 13], "temperature": 0.0, "avg_logprob": -0.15323381020989216, "compression_ratio": 1.510204081632653, "no_speech_prob": 7.854818250052631e-05}, {"id": 68, "seek": 58300, "start": 593.0, "end": 596.0, "text": " And data operation are complicated.", "tokens": [400, 1412, 6916, 366, 6179, 13], "temperature": 0.0, "avg_logprob": -0.15323381020989216, "compression_ratio": 1.510204081632653, "no_speech_prob": 7.854818250052631e-05}, {"id": 69, "seek": 58300, "start": 596.0, "end": 606.0, "text": " Are complicated because not just because of this slide and this is the interaction with the CefADM CLI, you can query the cluster and see the health status.", "tokens": [2014, 6179, 570, 406, 445, 570, 295, 341, 4137, 293, 341, 307, 264, 9285, 365, 264, 383, 5666, 6112, 44, 12855, 40, 11, 291, 393, 14581, 264, 13630, 293, 536, 264, 1585, 6558, 13], "temperature": 0.0, "avg_logprob": -0.15323381020989216, "compression_ratio": 1.510204081632653, "no_speech_prob": 7.854818250052631e-05}, {"id": 70, "seek": 60600, "start": 606.0, "end": 615.0, "text": " You can see where demons are placed, you can list the hosts and manage their labels and assign roles to these hosts.", "tokens": [509, 393, 536, 689, 19733, 366, 7074, 11, 291, 393, 1329, 264, 21573, 293, 3067, 641, 16949, 293, 6269, 9604, 281, 613, 21573, 13], "temperature": 0.0, "avg_logprob": -0.13516154970441546, "compression_ratio": 1.5082872928176796, "no_speech_prob": 9.016088006319478e-05}, {"id": 71, "seek": 60600, "start": 615.0, "end": 617.0, "text": " You can do a lot of things.", "tokens": [509, 393, 360, 257, 688, 295, 721, 13], "temperature": 0.0, "avg_logprob": -0.13516154970441546, "compression_ratio": 1.5082872928176796, "no_speech_prob": 9.016088006319478e-05}, {"id": 72, "seek": 60600, "start": 617.0, "end": 630.0, "text": " But the real point here is that with CefADM we don't have the need to run again all the deployment of the Opesta infrastructure.", "tokens": [583, 264, 957, 935, 510, 307, 300, 365, 383, 5666, 6112, 44, 321, 500, 380, 362, 264, 643, 281, 1190, 797, 439, 264, 19317, 295, 264, 12011, 7841, 6896, 13], "temperature": 0.0, "avg_logprob": -0.13516154970441546, "compression_ratio": 1.5082872928176796, "no_speech_prob": 9.016088006319478e-05}, {"id": 73, "seek": 63000, "start": 630.0, "end": 636.0, "text": " An example of this, of how projects are impacted by this change is Manila.", "tokens": [1107, 1365, 295, 341, 11, 295, 577, 4455, 366, 15653, 538, 341, 1319, 307, 2458, 7371, 13], "temperature": 0.0, "avg_logprob": -0.15417006045957155, "compression_ratio": 1.515625, "no_speech_prob": 0.00010259784903610125}, {"id": 74, "seek": 63000, "start": 636.0, "end": 647.0, "text": " It's not just because we need a new version of LibreVD, we need to be compatible and we are changing from CefAnsible to CefADM,", "tokens": [467, 311, 406, 445, 570, 321, 643, 257, 777, 3037, 295, 15834, 265, 53, 35, 11, 321, 643, 281, 312, 18218, 293, 321, 366, 4473, 490, 383, 5666, 32, 3695, 964, 281, 383, 5666, 6112, 44, 11], "temperature": 0.0, "avg_logprob": -0.15417006045957155, "compression_ratio": 1.515625, "no_speech_prob": 0.00010259784903610125}, {"id": 75, "seek": 63000, "start": 647.0, "end": 653.0, "text": " but just because we are doing architectural changes to the use cases provided by Opesta.", "tokens": [457, 445, 570, 321, 366, 884, 26621, 2962, 281, 264, 764, 3331, 5649, 538, 12011, 7841, 13], "temperature": 0.0, "avg_logprob": -0.15417006045957155, "compression_ratio": 1.515625, "no_speech_prob": 0.00010259784903610125}, {"id": 76, "seek": 65300, "start": 653.0, "end": 661.0, "text": " Manila is that service that curves out CefFS volumes and provides them to the virtual machine within tenants,", "tokens": [2458, 7371, 307, 300, 2643, 300, 19490, 484, 383, 5666, 29318, 22219, 293, 6417, 552, 281, 264, 6374, 3479, 1951, 31216, 11], "temperature": 0.0, "avg_logprob": -0.13433610981908337, "compression_ratio": 1.5845410628019323, "no_speech_prob": 0.0005404739640653133}, {"id": 77, "seek": 65300, "start": 661.0, "end": 666.0, "text": " which means that you have a dedicated network that you can use to mount your shares.", "tokens": [597, 1355, 300, 291, 362, 257, 8374, 3209, 300, 291, 393, 764, 281, 3746, 428, 12182, 13], "temperature": 0.0, "avg_logprob": -0.13433610981908337, "compression_ratio": 1.5845410628019323, "no_speech_prob": 0.0005404739640653133}, {"id": 78, "seek": 65300, "start": 666.0, "end": 671.0, "text": " And behind the scene we have CefFS or NFS with Ganesha.", "tokens": [400, 2261, 264, 4145, 321, 362, 383, 5666, 29318, 420, 13576, 50, 365, 460, 12779, 1641, 13], "temperature": 0.0, "avg_logprob": -0.13433610981908337, "compression_ratio": 1.5845410628019323, "no_speech_prob": 0.0005404739640653133}, {"id": 79, "seek": 65300, "start": 671.0, "end": 678.0, "text": " In the past, Manila was supposed to use the bus to interact with NFS Ganesha.", "tokens": [682, 264, 1791, 11, 2458, 7371, 390, 3442, 281, 764, 264, 1255, 281, 4648, 365, 13576, 50, 460, 12779, 1641, 13], "temperature": 0.0, "avg_logprob": -0.13433610981908337, "compression_ratio": 1.5845410628019323, "no_speech_prob": 0.0005404739640653133}, {"id": 80, "seek": 67800, "start": 678.0, "end": 685.0, "text": " And it was complicated because we had to run privileged containers.", "tokens": [400, 309, 390, 6179, 570, 321, 632, 281, 1190, 25293, 17089, 13], "temperature": 0.0, "avg_logprob": -0.11862814810968214, "compression_ratio": 1.4301675977653632, "no_speech_prob": 0.00017901217506732792}, {"id": 81, "seek": 67800, "start": 685.0, "end": 694.0, "text": " We had to use this interface to update and manage shares using Ganesha as a gateway.", "tokens": [492, 632, 281, 764, 341, 9226, 281, 5623, 293, 3067, 12182, 1228, 460, 12779, 1641, 382, 257, 28532, 13], "temperature": 0.0, "avg_logprob": -0.11862814810968214, "compression_ratio": 1.4301675977653632, "no_speech_prob": 0.00017901217506732792}, {"id": 82, "seek": 67800, "start": 694.0, "end": 703.0, "text": " And from an architectural point of view, we had an active passive model made by Peacemaker and SystemD.", "tokens": [400, 490, 364, 26621, 935, 295, 1910, 11, 321, 632, 364, 4967, 14975, 2316, 1027, 538, 2396, 326, 49523, 293, 8910, 35, 13], "temperature": 0.0, "avg_logprob": -0.11862814810968214, "compression_ratio": 1.4301675977653632, "no_speech_prob": 0.00017901217506732792}, {"id": 83, "seek": 70300, "start": 703.0, "end": 709.0, "text": " So you basically had Peacemaker honing the virtual IP as an entry point and then one active Ganesha,", "tokens": [407, 291, 1936, 632, 2396, 326, 49523, 2157, 278, 264, 6374, 8671, 382, 364, 8729, 935, 293, 550, 472, 4967, 460, 12779, 1641, 11], "temperature": 0.0, "avg_logprob": -0.11873678229321008, "compression_ratio": 1.5185185185185186, "no_speech_prob": 4.288789568818174e-05}, {"id": 84, "seek": 70300, "start": 709.0, "end": 712.0, "text": " even though you have more than one instance.", "tokens": [754, 1673, 291, 362, 544, 813, 472, 5197, 13], "temperature": 0.0, "avg_logprob": -0.11873678229321008, "compression_ratio": 1.5185185185185186, "no_speech_prob": 4.288789568818174e-05}, {"id": 85, "seek": 70300, "start": 712.0, "end": 717.0, "text": " And with some constraints with SystemD.", "tokens": [400, 365, 512, 18491, 365, 8910, 35, 13], "temperature": 0.0, "avg_logprob": -0.11873678229321008, "compression_ratio": 1.5185185185185186, "no_speech_prob": 4.288789568818174e-05}, {"id": 86, "seek": 70300, "start": 717.0, "end": 723.0, "text": " Now with CefADM there is an interface with the manager, with the NFS cluster,", "tokens": [823, 365, 383, 5666, 6112, 44, 456, 307, 364, 9226, 365, 264, 6598, 11, 365, 264, 13576, 50, 13630, 11], "temperature": 0.0, "avg_logprob": -0.11873678229321008, "compression_ratio": 1.5185185185185186, "no_speech_prob": 4.288789568818174e-05}, {"id": 87, "seek": 70300, "start": 723.0, "end": 729.0, "text": " and Manila can use a new driver to interact with this component.", "tokens": [293, 2458, 7371, 393, 764, 257, 777, 6787, 281, 4648, 365, 341, 6542, 13], "temperature": 0.0, "avg_logprob": -0.11873678229321008, "compression_ratio": 1.5185185185185186, "no_speech_prob": 4.288789568818174e-05}, {"id": 88, "seek": 72900, "start": 729.0, "end": 735.0, "text": " We don't have to do the bus anymore, we don't have to do the bus to the Ganesha container anymore.", "tokens": [492, 500, 380, 362, 281, 360, 264, 1255, 3602, 11, 321, 500, 380, 362, 281, 360, 264, 1255, 281, 264, 460, 12779, 1641, 10129, 3602, 13], "temperature": 0.0, "avg_logprob": -0.14970303396893364, "compression_ratio": 1.7031963470319635, "no_speech_prob": 0.00026242778403684497}, {"id": 89, "seek": 72900, "start": 735.0, "end": 743.0, "text": " And that's the new model where we rely on the Ingress Demon provided by CefADM,", "tokens": [400, 300, 311, 264, 777, 2316, 689, 321, 10687, 322, 264, 682, 3091, 29683, 5649, 538, 383, 5666, 6112, 44, 11], "temperature": 0.0, "avg_logprob": -0.14970303396893364, "compression_ratio": 1.7031963470319635, "no_speech_prob": 0.00026242778403684497}, {"id": 90, "seek": 72900, "start": 743.0, "end": 747.0, "text": " and this Ingress Demon is made by HAProxy and KIPaLiveD.", "tokens": [293, 341, 682, 3091, 29683, 307, 1027, 538, 389, 4715, 340, 12876, 293, 591, 9139, 64, 43, 488, 35, 13], "temperature": 0.0, "avg_logprob": -0.14970303396893364, "compression_ratio": 1.7031963470319635, "no_speech_prob": 0.00026242778403684497}, {"id": 91, "seek": 72900, "start": 747.0, "end": 753.0, "text": " KIPaLiveD honing the V as an entry point, HAProxy for load balancing across the,", "tokens": [591, 9139, 64, 43, 488, 35, 2157, 278, 264, 691, 382, 364, 8729, 935, 11, 389, 4715, 340, 12876, 337, 3677, 22495, 2108, 264, 11], "temperature": 0.0, "avg_logprob": -0.14970303396893364, "compression_ratio": 1.7031963470319635, "no_speech_prob": 0.00026242778403684497}, {"id": 92, "seek": 72900, "start": 753.0, "end": 758.0, "text": " and distributing the load across the NFS Ganesha server.", "tokens": [293, 41406, 264, 3677, 2108, 264, 13576, 50, 460, 12779, 1641, 7154, 13], "temperature": 0.0, "avg_logprob": -0.14970303396893364, "compression_ratio": 1.7031963470319635, "no_speech_prob": 0.00026242778403684497}, {"id": 93, "seek": 75800, "start": 758.0, "end": 762.0, "text": " It's a better approach, we still have some limitation on this area,", "tokens": [467, 311, 257, 1101, 3109, 11, 321, 920, 362, 512, 27432, 322, 341, 1859, 11], "temperature": 0.0, "avg_logprob": -0.1526239025059031, "compression_ratio": 1.52020202020202, "no_speech_prob": 0.00017441948875784874}, {"id": 94, "seek": 75800, "start": 762.0, "end": 767.0, "text": " because considering that Manila is an infrastructure service for Obestac,", "tokens": [570, 8079, 300, 2458, 7371, 307, 364, 6896, 2643, 337, 4075, 377, 326, 11], "temperature": 0.0, "avg_logprob": -0.1526239025059031, "compression_ratio": 1.52020202020202, "no_speech_prob": 0.00017441948875784874}, {"id": 95, "seek": 75800, "start": 767.0, "end": 774.0, "text": " but providing shares within the tenant, virtual machines, with a dedicated network,", "tokens": [457, 6530, 12182, 1951, 264, 31000, 11, 6374, 8379, 11, 365, 257, 8374, 3209, 11], "temperature": 0.0, "avg_logprob": -0.1526239025059031, "compression_ratio": 1.52020202020202, "no_speech_prob": 0.00017441948875784874}, {"id": 96, "seek": 75800, "start": 774.0, "end": 781.0, "text": " we need client restrictions to avoid other tenants mounting the same share.", "tokens": [321, 643, 6423, 14191, 281, 5042, 661, 31216, 22986, 264, 912, 2073, 13], "temperature": 0.0, "avg_logprob": -0.1526239025059031, "compression_ratio": 1.52020202020202, "no_speech_prob": 0.00017441948875784874}, {"id": 97, "seek": 78100, "start": 781.0, "end": 789.0, "text": " And there is an effort doing the proxy protocol in Ganesha to make sure that we can", "tokens": [400, 456, 307, 364, 4630, 884, 264, 29690, 10336, 294, 460, 12779, 1641, 281, 652, 988, 300, 321, 393], "temperature": 0.0, "avg_logprob": -0.09939275032434708, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.00014463432307820767}, {"id": 98, "seek": 78100, "start": 789.0, "end": 796.0, "text": " use client restriction with this new model, which is the current limitation basically.", "tokens": [764, 6423, 29529, 365, 341, 777, 2316, 11, 597, 307, 264, 2190, 27432, 1936, 13], "temperature": 0.0, "avg_logprob": -0.09939275032434708, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.00014463432307820767}, {"id": 99, "seek": 78100, "start": 796.0, "end": 803.0, "text": " Or at least there is some effort to provide floating stable IP addresses to the Ingress Demon", "tokens": [1610, 412, 1935, 456, 307, 512, 4630, 281, 2893, 12607, 8351, 8671, 16862, 281, 264, 682, 3091, 29683], "temperature": 0.0, "avg_logprob": -0.09939275032434708, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.00014463432307820767}, {"id": 100, "seek": 78100, "start": 803.0, "end": 808.0, "text": " and skip the HAProxy layer, which is always an additional hope.", "tokens": [293, 10023, 264, 389, 4715, 340, 12876, 4583, 11, 597, 307, 1009, 364, 4497, 1454, 13], "temperature": 0.0, "avg_logprob": -0.09939275032434708, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.00014463432307820767}, {"id": 101, "seek": 80800, "start": 808.0, "end": 816.0, "text": " And in terms of performance, this can be something that has an impact, of course.", "tokens": [400, 294, 2115, 295, 3389, 11, 341, 393, 312, 746, 300, 575, 364, 2712, 11, 295, 1164, 13], "temperature": 0.0, "avg_logprob": -0.15502185821533204, "compression_ratio": 1.4619565217391304, "no_speech_prob": 0.0006276190979406238}, {"id": 102, "seek": 80800, "start": 816.0, "end": 825.0, "text": " Lastly, at this point we had Cefansible, we have CefADM, what Kubernetes means in this world.", "tokens": [18072, 11, 412, 341, 935, 321, 632, 383, 5666, 599, 964, 11, 321, 362, 383, 5666, 6112, 44, 11, 437, 23145, 1355, 294, 341, 1002, 13], "temperature": 0.0, "avg_logprob": -0.15502185821533204, "compression_ratio": 1.4619565217391304, "no_speech_prob": 0.0006276190979406238}, {"id": 103, "seek": 80800, "start": 825.0, "end": 834.0, "text": " We have Rook as a way to deploy Cef within Kubernetes, regardless of how Obestac is deployed,", "tokens": [492, 362, 497, 1212, 382, 257, 636, 281, 7274, 383, 5666, 1951, 23145, 11, 10060, 295, 577, 4075, 377, 326, 307, 17826, 11], "temperature": 0.0, "avg_logprob": -0.15502185821533204, "compression_ratio": 1.4619565217391304, "no_speech_prob": 0.0006276190979406238}, {"id": 104, "seek": 83400, "start": 834.0, "end": 838.0, "text": " we have several combinations of these two things together.", "tokens": [321, 362, 2940, 21267, 295, 613, 732, 721, 1214, 13], "temperature": 0.0, "avg_logprob": -0.10848929087320963, "compression_ratio": 1.6224489795918366, "no_speech_prob": 0.00020567586761899292}, {"id": 105, "seek": 83400, "start": 838.0, "end": 844.0, "text": " You can have converged infrastructure where Obestac control plane is virtualized,", "tokens": [509, 393, 362, 9652, 3004, 6896, 689, 4075, 377, 326, 1969, 5720, 307, 6374, 1602, 11], "temperature": 0.0, "avg_logprob": -0.10848929087320963, "compression_ratio": 1.6224489795918366, "no_speech_prob": 0.00020567586761899292}, {"id": 106, "seek": 83400, "start": 844.0, "end": 852.0, "text": " or it can be containerized, so basically using the same model,", "tokens": [420, 309, 393, 312, 10129, 1602, 11, 370, 1936, 1228, 264, 912, 2316, 11], "temperature": 0.0, "avg_logprob": -0.10848929087320963, "compression_ratio": 1.6224489795918366, "no_speech_prob": 0.00020567586761899292}, {"id": 107, "seek": 83400, "start": 852.0, "end": 855.0, "text": " the same approach to deploy both, it can be useful,", "tokens": [264, 912, 3109, 281, 7274, 1293, 11, 309, 393, 312, 4420, 11], "temperature": 0.0, "avg_logprob": -0.10848929087320963, "compression_ratio": 1.6224489795918366, "no_speech_prob": 0.00020567586761899292}, {"id": 108, "seek": 83400, "start": 855.0, "end": 859.0, "text": " because it's a unified approach to manage your infrastructure.", "tokens": [570, 309, 311, 257, 26787, 3109, 281, 3067, 428, 6896, 13], "temperature": 0.0, "avg_logprob": -0.10848929087320963, "compression_ratio": 1.6224489795918366, "no_speech_prob": 0.00020567586761899292}, {"id": 109, "seek": 85900, "start": 859.0, "end": 869.0, "text": " It's easy, deployable and reproducible, because Kubernetes poses a standard approach to deploy things,", "tokens": [467, 311, 1858, 11, 7274, 712, 293, 11408, 32128, 11, 570, 23145, 26059, 257, 3832, 3109, 281, 7274, 721, 11], "temperature": 0.0, "avg_logprob": -0.12813955942789715, "compression_ratio": 1.4784688995215312, "no_speech_prob": 0.0004711826622951776}, {"id": 110, "seek": 85900, "start": 869.0, "end": 877.0, "text": " so we don't have anymore that flexibility that today is triple O, but it's easier from that point of view.", "tokens": [370, 321, 500, 380, 362, 3602, 300, 12635, 300, 965, 307, 15508, 422, 11, 457, 309, 311, 3571, 490, 300, 935, 295, 1910, 13], "temperature": 0.0, "avg_logprob": -0.12813955942789715, "compression_ratio": 1.4784688995215312, "no_speech_prob": 0.0004711826622951776}, {"id": 111, "seek": 85900, "start": 877.0, "end": 882.0, "text": " And the same Cef cluster can be shared between Obestac and Kubernetes.", "tokens": [400, 264, 912, 383, 5666, 13630, 393, 312, 5507, 1296, 4075, 377, 326, 293, 23145, 13], "temperature": 0.0, "avg_logprob": -0.12813955942789715, "compression_ratio": 1.4784688995215312, "no_speech_prob": 0.0004711826622951776}, {"id": 112, "seek": 85900, "start": 882.0, "end": 884.0, "text": " We have different workloads.", "tokens": [492, 362, 819, 32452, 13], "temperature": 0.0, "avg_logprob": -0.12813955942789715, "compression_ratio": 1.4784688995215312, "no_speech_prob": 0.0004711826622951776}, {"id": 113, "seek": 88400, "start": 884.0, "end": 894.0, "text": " Kubernetes is PVC interfaces provided by Rook. Obestac is mostly RBD and your workload runs virtual machines,", "tokens": [23145, 307, 46700, 28416, 5649, 538, 497, 1212, 13, 4075, 377, 326, 307, 5240, 40302, 35, 293, 428, 20139, 6676, 6374, 8379, 11], "temperature": 0.0, "avg_logprob": -0.1465002182991274, "compression_ratio": 1.4502923976608186, "no_speech_prob": 0.00036226073279976845}, {"id": 114, "seek": 88400, "start": 894.0, "end": 901.0, "text": " and it's usually outside, so you have to reach from the compute node the Rook cluster,", "tokens": [293, 309, 311, 2673, 2380, 11, 370, 291, 362, 281, 2524, 490, 264, 14722, 9984, 264, 497, 1212, 13630, 11], "temperature": 0.0, "avg_logprob": -0.1465002182991274, "compression_ratio": 1.4502923976608186, "no_speech_prob": 0.00036226073279976845}, {"id": 115, "seek": 88400, "start": 901.0, "end": 905.0, "text": " the Cef cluster deployed by Rook within Kubernetes,", "tokens": [264, 383, 5666, 13630, 17826, 538, 497, 1212, 1951, 23145, 11], "temperature": 0.0, "avg_logprob": -0.1465002182991274, "compression_ratio": 1.4502923976608186, "no_speech_prob": 0.00036226073279976845}, {"id": 116, "seek": 90500, "start": 905.0, "end": 914.0, "text": " which poses some networking challenges that they can be managed using host networking through,", "tokens": [597, 26059, 512, 17985, 4759, 300, 436, 393, 312, 6453, 1228, 3975, 17985, 807, 11], "temperature": 0.0, "avg_logprob": -0.11587633405412946, "compression_ratio": 1.5544041450777202, "no_speech_prob": 0.00028593724709935486}, {"id": 117, "seek": 90500, "start": 914.0, "end": 921.0, "text": " so you're using Kubernetes as a platform to deploy your Cef cluster,", "tokens": [370, 291, 434, 1228, 23145, 382, 257, 3663, 281, 7274, 428, 383, 5666, 13630, 11], "temperature": 0.0, "avg_logprob": -0.11587633405412946, "compression_ratio": 1.5544041450777202, "no_speech_prob": 0.00028593724709935486}, {"id": 118, "seek": 90500, "start": 921.0, "end": 928.0, "text": " but you're still relying on the host networking to reach it and provide RBD to the outside workload,", "tokens": [457, 291, 434, 920, 24140, 322, 264, 3975, 17985, 281, 2524, 309, 293, 2893, 40302, 35, 281, 264, 2380, 20139, 11], "temperature": 0.0, "avg_logprob": -0.11587633405412946, "compression_ratio": 1.5544041450777202, "no_speech_prob": 0.00028593724709935486}, {"id": 119, "seek": 90500, "start": 928.0, "end": 932.0, "text": " and that's the point of this slide.", "tokens": [293, 300, 311, 264, 935, 295, 341, 4137, 13], "temperature": 0.0, "avg_logprob": -0.11587633405412946, "compression_ratio": 1.5544041450777202, "no_speech_prob": 0.00028593724709935486}, {"id": 120, "seek": 93200, "start": 932.0, "end": 938.0, "text": " There are some things that are not mentioned here, like some tuning in Rook,", "tokens": [821, 366, 512, 721, 300, 366, 406, 2835, 510, 11, 411, 512, 15164, 294, 497, 1212, 11], "temperature": 0.0, "avg_logprob": -0.09221123486030393, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.0002975668467115611}, {"id": 121, "seek": 93200, "start": 938.0, "end": 944.0, "text": " that is supposed to be done to make sure that there is Kubernetes in the middle,", "tokens": [300, 307, 3442, 281, 312, 1096, 281, 652, 988, 300, 456, 307, 23145, 294, 264, 2808, 11], "temperature": 0.0, "avg_logprob": -0.09221123486030393, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.0002975668467115611}, {"id": 122, "seek": 93200, "start": 944.0, "end": 951.0, "text": " so it's not natively, the native Cef cluster we usually have.", "tokens": [370, 309, 311, 406, 8470, 356, 11, 264, 8470, 383, 5666, 13630, 321, 2673, 362, 13], "temperature": 0.0, "avg_logprob": -0.09221123486030393, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.0002975668467115611}, {"id": 123, "seek": 93200, "start": 951.0, "end": 958.0, "text": " So at this point, the thing is that we should do some tuning, especially in the HCI world,", "tokens": [407, 412, 341, 935, 11, 264, 551, 307, 300, 321, 820, 360, 512, 15164, 11, 2318, 294, 264, 389, 25240, 1002, 11], "temperature": 0.0, "avg_logprob": -0.09221123486030393, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.0002975668467115611}, {"id": 124, "seek": 95800, "start": 958.0, "end": 963.0, "text": " because Iverconverged is still one of the most popular use cases,", "tokens": [570, 286, 331, 1671, 331, 3004, 307, 920, 472, 295, 264, 881, 3743, 764, 3331, 11], "temperature": 0.0, "avg_logprob": -0.14992760106136924, "compression_ratio": 1.524229074889868, "no_speech_prob": 0.0001011032290989533}, {"id": 125, "seek": 95800, "start": 963.0, "end": 967.0, "text": " and HCI is provided out of the box by Kubernetes.", "tokens": [293, 389, 25240, 307, 5649, 484, 295, 264, 2424, 538, 23145, 13], "temperature": 0.0, "avg_logprob": -0.14992760106136924, "compression_ratio": 1.524229074889868, "no_speech_prob": 0.0001011032290989533}, {"id": 126, "seek": 95800, "start": 967.0, "end": 973.0, "text": " You can tag your infra nodes, you can deploy Rook, you can assign those nodes for OSDs.", "tokens": [509, 393, 6162, 428, 23654, 13891, 11, 291, 393, 7274, 497, 1212, 11, 291, 393, 6269, 729, 13891, 337, 12731, 35, 82, 13], "temperature": 0.0, "avg_logprob": -0.14992760106136924, "compression_ratio": 1.524229074889868, "no_speech_prob": 0.0001011032290989533}, {"id": 127, "seek": 95800, "start": 973.0, "end": 975.0, "text": " That's it, that's it.", "tokens": [663, 311, 309, 11, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.14992760106136924, "compression_ratio": 1.524229074889868, "no_speech_prob": 0.0001011032290989533}, {"id": 128, "seek": 95800, "start": 975.0, "end": 984.0, "text": " But at that point, you have to make sure that both your cluster and the connection is valuable for the outside workload.", "tokens": [583, 412, 300, 935, 11, 291, 362, 281, 652, 988, 300, 1293, 428, 13630, 293, 264, 4984, 307, 8263, 337, 264, 2380, 20139, 13], "temperature": 0.0, "avg_logprob": -0.14992760106136924, "compression_ratio": 1.524229074889868, "no_speech_prob": 0.0001011032290989533}, {"id": 129, "seek": 98400, "start": 984.0, "end": 988.0, "text": " So this can be done, it's done by this demo.", "tokens": [407, 341, 393, 312, 1096, 11, 309, 311, 1096, 538, 341, 10723, 13], "temperature": 0.0, "avg_logprob": -0.18924583991368613, "compression_ratio": 1.5533980582524272, "no_speech_prob": 0.00047748524229973555}, {"id": 130, "seek": 98400, "start": 988.0, "end": 994.0, "text": " I'm not going to show this, but it's all there, it's all described,", "tokens": [286, 478, 406, 516, 281, 855, 341, 11, 457, 309, 311, 439, 456, 11, 309, 311, 439, 7619, 11], "temperature": 0.0, "avg_logprob": -0.18924583991368613, "compression_ratio": 1.5533980582524272, "no_speech_prob": 0.00047748524229973555}, {"id": 131, "seek": 98400, "start": 994.0, "end": 997.0, "text": " it's all I was describing in this slide.", "tokens": [309, 311, 439, 286, 390, 16141, 294, 341, 4137, 13], "temperature": 0.0, "avg_logprob": -0.18924583991368613, "compression_ratio": 1.5533980582524272, "no_speech_prob": 0.00047748524229973555}, {"id": 132, "seek": 98400, "start": 997.0, "end": 1004.0, "text": " So you can have your OB-STAC infrastructure deployed with DevStack or TripleO,", "tokens": [407, 291, 393, 362, 428, 35538, 12, 6840, 4378, 6896, 17826, 365, 9096, 4520, 501, 420, 32159, 46, 11], "temperature": 0.0, "avg_logprob": -0.18924583991368613, "compression_ratio": 1.5533980582524272, "no_speech_prob": 0.00047748524229973555}, {"id": 133, "seek": 98400, "start": 1004.0, "end": 1012.0, "text": " and it's still bare metal, and it can consume a Cef cluster deployed by Rook using RBD.", "tokens": [293, 309, 311, 920, 6949, 5760, 11, 293, 309, 393, 14732, 257, 383, 5666, 13630, 17826, 538, 497, 1212, 1228, 40302, 35, 13], "temperature": 0.0, "avg_logprob": -0.18924583991368613, "compression_ratio": 1.5533980582524272, "no_speech_prob": 0.00047748524229973555}, {"id": 134, "seek": 101200, "start": 1012.0, "end": 1018.0, "text": " You can still use the CSI, actually, to provide PVC interface.", "tokens": [509, 393, 920, 764, 264, 9460, 40, 11, 767, 11, 281, 2893, 46700, 9226, 13], "temperature": 0.0, "avg_logprob": -0.16945020207818948, "compression_ratio": 1.3540372670807452, "no_speech_prob": 0.00018109346274286509}, {"id": 135, "seek": 101200, "start": 1018.0, "end": 1031.0, "text": " It's not something that it's mutually exclusive, but it's just a good exercise to see how these two technologies can work together in the future, probably.", "tokens": [467, 311, 406, 746, 300, 309, 311, 39144, 13005, 11, 457, 309, 311, 445, 257, 665, 5380, 281, 536, 577, 613, 732, 7943, 393, 589, 1214, 294, 264, 2027, 11, 1391, 13], "temperature": 0.0, "avg_logprob": -0.16945020207818948, "compression_ratio": 1.3540372670807452, "no_speech_prob": 0.00018109346274286509}, {"id": 136, "seek": 103100, "start": 1031.0, "end": 1044.0, "text": " And yeah, just some additional resources for those interested in looking at these slides offline and some contacts for people in the OB-STAC world", "tokens": [400, 1338, 11, 445, 512, 4497, 3593, 337, 729, 3102, 294, 1237, 412, 613, 9788, 21857, 293, 512, 15836, 337, 561, 294, 264, 35538, 12, 6840, 4378, 1002], "temperature": 0.0, "avg_logprob": -0.1273370916193182, "compression_ratio": 1.3855421686746987, "no_speech_prob": 0.0001652712089708075}, {"id": 137, "seek": 103100, "start": 1044.0, "end": 1049.0, "text": " if you want to dig more into these experiments.", "tokens": [498, 291, 528, 281, 2528, 544, 666, 613, 12050, 13], "temperature": 0.0, "avg_logprob": -0.1273370916193182, "compression_ratio": 1.3855421686746987, "no_speech_prob": 0.0001652712089708075}, {"id": 138, "seek": 104900, "start": 1049.0, "end": 1062.0, "text": " And that's it. Thank you very much.", "tokens": [50364, 400, 300, 311, 309, 13, 1044, 291, 588, 709, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1683668540074275, "compression_ratio": 0.813953488372093, "no_speech_prob": 0.0010467814281582832}], "language": "en"}