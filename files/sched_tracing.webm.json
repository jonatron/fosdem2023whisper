{"text": " Julia is going to be talking about graphing tools for scheduler tracing. Okay. Do you hear me? Okay, so thank you. So, I'm going to be talking about graphing tools for scheduler tracing. So, I'll start out, like actually someone started out yesterday with what is a task scheduler. So, for me, a task scheduler is like an important part of the Linux kernel. It does two things. It places tasks on cores when they either are created with fork or when they wake up or when there's load balancing. And it also, when a core becomes empty, it decides what core should run next. Basically, I'm interested in the Linux kernel files core.c and fare.c, so CFS. I'm not at all interested in the second point. I'm interested in where cores get placed when they wake up. So, that's going to be the entire focus of this talk. So, the next question is, how does the scheduler impact application performance? Basically, a scheduler is confronted with a bunch of tasks and a bunch of cores, and it needs to make decisions where is it going to put these tasks on these cores? And so, sometimes, if you make a bad decision, sorry, it can have a short-term impact. Maybe some task has to wait a little bit extra time, but this impact can kind of, there's kind of a domino effect. You do one bad decision and then other bad decisions follow from that. So, one issue that one might be concerned with is what's called work conservation. So, we have a machine that has four cores. We have a task that wakes up. T1, where should we put it? So, based on the information we have right now, we've just got an empty machine and a random task. Maybe we have no idea, we'll just put it on core zero. Now, another task wakes up. What should we do with T2? So, kind of intuitively, it might be good to put T2 on either core one or core two or core three, because they're not doing anything at the moment. Putting on a core zero would perhaps be a poor choice because it will have to wait for task one to finish. So, that seems completely obvious as a human looking at boxes on the screen, but the scheduler is going to have to hunt around to find those empty cores. And so, actually, CFS is not actually work conserving. The basic principle is no core should be overloaded if any core is idle. So, if you have an overload, you should have put it on the idle core instead. Another issue is locality. So, instead of having just four random cores, we may have a multi-socket machine. We've got cores zero and one, which are together on one socket. Core zero, two and three are together on a socket. We have T1 is on core zero. Where should we put T2? So, we have those three idle cores, but maybe core one would be a better choice if either T2 has already all allocated all of its data on the first socket or if T2 wants to discuss things with T1. If we put it on two or three, things might get slower. So, basically, you can see that there's a lot of potential for things to go wrong. So, we need to understand maybe what the scheduler is actually doing, but this problem is the scheduler is like buried down there in the OS. When you're on the application, you don't really know where your tasks are running. So, we want to consider how we can see what the scheduler is doing. So, fortunately, there's some tools that are available. So, the most helpful one, I would say, is trace command. So, trace command allows you to trace all different kinds of kernel events. Basically, it's a front-end on F trace, but in particular, it lets you trace scheduling events, so that's the part we're interested in. So, you can see a trace here, and if you get this trace, it will have basically all the information you need to solve all of your scheduling problems. On the other hand, it unfortunately has all the information you need to solve all of your scheduling problems. That is, it's ordered, it's like sequential thing. It's ordered according to time. If your application runs for a certain amount of time, you'll end up with a huge file. And you can see even in this little tiny trace that I'm showing, we've got different, the activities on different cores are mixed up. We have core 26 and core 62 here. And so, in practice, it can get very hard to actually sort out what's going on, who's doing what, and so on. And so, the next tool, which is very helpful, is one that's called kernel shark. So, this gives you a graphical interface that lets you see what's going on on your different cores. And it also gives you that same textual output at the bottom. You can kind of correlate them to each other. You can zoom in quite easily, and so on. On the other hand, in my personal application, where I'm interested in actually very large machines, kernel shark has some kind of a bit difficult to use in some cases. It's great for if you want to really zoom in on a specific problem. It's not so great if you actually don't really know where your problem is, and you want to see somehow an overview with everything at once. Here, I'm only showing two cores. You can see that the display is kind of a bit spread out. It's going to be hard to get 128 cores to fit on your screen and be reasonably understandable. So, what we would like is some way of understanding what's going on on big machines. So, the thing I put the emphasis on previously is that we want to see what's going on on all the cores at once. Something that I've also found extremely useful in practice is to be able to collect traces, collect these images, share them with my colleagues, put them in papers, put them in slides, and so on. So, I found it useful to make, collect lots of traces, compare them, store them, look at them later, and so on. On the other hand, I have, at least for the moment, completely abandoned this nice feature of kernel shark, which is that you can zoom in or zoom out and find out exactly what you want to see at what time. My proposed approach that I'm going to present in this talk is completely uninteractive. So, you run a command, you get a picture, you look at your picture, and you run another command, you get another picture, and you look at that picture. So, actually, in the last few years, I've made lots and lots of tools that start out with trace command input and visualize it in different ways. Sort of the ones that have stood the test of time are the ones I'm going to present, which are datagraph and running weighting. The names are not super imaginative, perhaps. Datagraph takes a dat file, so that's what the trace command produces, and it makes a graph for you. So, basically, it's going to show you, we have the x-axis and the y-axis, the x-axis is the time, and then on the y-axis, we have our cores, and we see what's running on each core at each time. So, kind of like what Colonel Sharpe showed you, but in much more compressed format. And running weighting is just a line graph. It shows you how many tasks are running at a particular time and how many tasks are waiting on a run queue and are not able to run. So, we'll see how that's used. So, the rest of this talk, I'm going to present these two tools, and I'm going to be motivated by this patch that I submitted a few years ago. I'm not going to discuss the patch in detail now. We'll see it later after we've seen all the examples. The application I'm going to be interested in is part of the NAS parallel benchmarks. These are a bunch of, it says what, you can read what it says. It's small kernels having to do with HPC kind of things. We're going to focus on the UA benchmark. It does something. What's important for our purposes is that it has n tasks, and they're running on n cores. And so, they kind of run, they seem, at least superficially, they seem to run all the time. You would expect that they would just choose their cores, stay on their cores, and run on those cores forever. So, you would expect this benchmark to be completely uninteresting from a scheduling point of view. So, if we take this benchmark and we run it a few times, so I run it 10 times, you can, and I've taken these runs and I've started it by increasing run time. You can see that something is going on, because there's kind of these runs on the left-hand side here, which start out around 20 seconds. And there's a definite gap here. I mean, it gets a bit longer, a small amount, but there's a definite gap here, and then it jumps up to closer to 30 seconds. So, maybe we have 40% overhead between the fastest one and the slowest one. It's only 10 runs. It's quite a lot of variation for a benchmark that we expect will just run like this and not doing anything interesting at all. So, we can ask why so much variation. So, now we can actually look and see what's going on at the scheduling level. So, this is the graphs. We have, as I said, we have the time on the x-axis, and we have the, what's going on on the different cores on the y-axis. What I have, it says socket order on the different cores. What I've done is, actually on my machine, the numbers go kind of round robin between the different sockets, but I have organized it so that we have the first socket at the bottom, second socket kind of in the middle, and so on. It's not very important at this point, though. So, I don't know. We have a graph and we see what it's doing. So, this is the fastest run. It looks kind of like what we expected. The thing's not moving around. Not much is happening. This is a much slower run. So, this previous one was 22 seconds. This next one is 28 seconds. So, that's kind of a big overhead. And here we can see that things are not going as well at all because, in particular, over here in this region, we have these white spaces. And white spaces means that nothing is happening on that core. So, there could be two reasons why nothing is happening. One of them is that there's nothing to do. So, maybe one of these tasks has gotten way ahead of the other one, and so it needs to sleep and to wait for the others to finish what they want to do. The more unpleasant reason that nothing is happening is because several of these tasks can be stuck on the same core and they're going to have to bounce back and forth between each other. And actually, nothing. We have a work conservation problem. Some of the cores are idle. So, we can see which case we're in by looking at the running weighting graph. So, here we have, again, we have our, this time we have the number of tasks on the y-axis, but we have n tasks on n cores, so it's the same. At the top, we have a dotted line, which is the number of cores on the machine. And then the green lines are things, the number of tasks that are running. So, it's kind of like all the tasks are running all the time, but not exactly. There's sometimes when only a very few tasks are running down here. And then we have over here in this situation, this is the place where we had the gaps on the other graph. And here we have often, we have like almost all the cores, all the tasks that are running, but not quite. And we have this red lines here, and so red lines means tasks that are waiting. So, we're in an overload situation. So, some tasks have been placed on the same cores as each other, and so they have to wait for each other to finish. So, this is kind of more of a problem for this kind of application. So, basically the two problems we have, we have problem tasks that are moving around, and we have some cores that are overloaded, and so the tasks don't get to run as much as they ought to be. So, now what we're going to do is we're going to zoom in to some of these situations and see what the problem could be. So, here's the first one of these situations. If you look over here, basically around three seconds, at this point that I've circled, you can see we have an orange core, sorry, orange task and then a blue task. And so, something is happening to cause one cores to change to another one. And if you look up a bit, a bit more, there's some other situations where that happens, kind of all in the same area. So, we can look into that in more detail. If we zoom in a bit, so here I have the command line that you have to write. This socket order is to get the cores ordered in a certain way. Min and max are the, we want to go from three seconds to 3.2 seconds. Target UA is it's going to give our application special colors and other things that happen are going to be black. So, then we can see other, if there's some other strange things that are happening on the machine. So, now that we have zoomed in at this level, we can see that things actually are not as nice as they looked when we were in the zoomed out situation. Here we have like everybody, almost everybody has stopped for a little gap here. And then here, this is basically the fourth socket. There's a lot of unfortunate things happening up here. So, we can zoom in a bit more. So, now I've zoomed in just on this big vertical line here. And when we zoom in a bit more, then we start to see that there are some other things going on on our machine. So, they're the colored lines and then we have some little black lines. So, we can try to find out what the little black lines are. So, this data graph, it has another option. What are the black lines? It has another option where we can have colors to see, it's colored by command. The colors are chosen not by the PID, but by what the command is. So, mostly we have our command, which is blue, UA. But we have some demons here. So, these are kind of inevitable. The kernel needs to do some things. And so, basically, if we jump back here, we can see that if we look, for example, in this place, our task is running along, a demon comes, and then it interrupts our task. So, our task is not going to be working, but at least our task is staying in the same place. And so, nothing extremely horrible happens, but these things get a bit unsynchronized. Some of them get slowed down and so on. So, that's one kind of slowdown that we can have. But, in principle, it shouldn't have a huge long-term impact. So, now we can move a bit further off to the right. We can see there are some more of these little black things here. Here, what we have, here we have an orange task. Here, we have a black line. And here, we have another orange task up here that happens sort of at the same thing. The same position. It's a little bit off to the right. So, what's happening here is we're doing load balancing. And so, the kernel thinks, okay, so there are two things going on here. We should find one of these many idle cores up here and use one of them to put this task. But that's actually quite a poor choice in this case because, basically, in this application, we have all the sockets being filled up with all of their tasks. And so, by doing this load balancing, we have put an extra task up there on the fourth socket. And that's something we will come to regret later, one might say. Even though it seems okay for the moment. So, what this leads to, though, is, so as I just said, it's going to lead to a cascade of migrations. We put something on that task. Someone else is going to wake up for that core. It will have to go somewhere else. And that other place is someone's going to wake up for that and so on. So, then the third situation, this is actually in the same position. We see another situation over here. Here's another case where we are changing from one task to another one. But this time, there's no little black dot which is motivating this change somehow. Nothing strange seems to be happening. It just seems to be happening spontaneously by itself. So, we can look again at the running weighting graph to see what's happening. It's not super easy to see. But basically what's happening is we have a green line which is below the number of cores. And we have a red line that's just above it. And again, we have an overload situation. So, there's one thread which is actually this orange one here. This blue and orange core here, orange tasks are sharing the same core. And so, they're going to have to bounce back and forth between them. So, we can try to look and see how did we end up with this situation. So, this here, this is a graph that I made by hand more or less. This is just focusing on the two specific cores that we're interested in. Here we have this orange task. It's running along. It prefers to be on this core number 111. It then goes to sleep. And then after some time, we move along over here. At this point, it wakes up. And we want to figure out it's waking up. It's actually the task on core number 68 that is going to wake it up. And so, we need to find a place to put it. So, the obvious place to put it would be on core 111. That's where it was before. And that core, the important thing is that core is actually not doing anything. But that's not what happens. What happens is it gets placed on core number 68. It gets placed on the core of the parent as opposed to the core where it was running before. So, this seems very surprising. We expect that we prefer to put cores where it can run immediately. Why does it, for no particular reason, get moved off? So, the key to the situation is this mysterious little dot here. So, it's a key worker that woke up and took advantage of this empty space so it could run immediately. And at the time, this is like Linux 5.10 when all of these graphs come from. At this time, there, basically, there's a decision whenever a core wakes up, should it go on the socket where it was before? Should it go on the socket of the waker? And there are different sockets in this case. And the issue is that when you make that decision, you take into account the load average. And the load average is something, is this collected over time, and then the old information gets decreased a bit over time. And so, because we have this K worker here, the load average is not zero. And so, this core is seen as being not completely idle, even though it is completely idle. And so, once, when that situation arises, then there's some kind of competition between the parent, the waker, and the place where you were before. And for some reason, this core number 111 is going to lose this competition in this situation. And so, the kernel thinks that this core down here would be a better place for it, which in this case, it definitely is not. So, that's where this comes in, there's a little patch, all it does is it checks is if the core where the task was previously, if that is completely idle, then just use that instead of considering other possibilities. So, if we apply that patch, then here we have the pink lines here. So, now we still have a slight increase, we still have our task moving around, it's not going to solve all the problems, but we don't have this big jump, which happens when the overload situation is introduced. And we can see how they impact on another completely different application. So, this is a Java program, it's part of the DeCapo benchmark suite. And this patch causes tasks to kind of have a better chance of remaining where they were before. And on this benchmark, what happens after we have the patch is that all the tasks manage to stay on the same socket, because there's actually not that many of them that run at a time and they fit there nicely. Previously, they were tending to kind of move over the entire machine. And we have here much like more than 20 seconds between the fastest and slowest here, we have a much more uniform running time, and obviously the running time is also much faster. So, it had multiple benefits. So, in conclusion, if you want to understand what your scheduler is really doing, you have to actually look and see what your scheduler is really doing. Just seeing that the number, now it's faster, now it's slower, something like that, it doesn't really give you a good understanding of what's going on. Different perspectives, we found that it provides different kinds of information. The running rating graph is actually very coarse-grained, but it actually sometimes can show you like the problem is exactly in this region because there's overload in this region. So, we have our two tools, data graph, what's going on at what time, and running weighting, how much is happening at each point in time. In future work, these graphs are a little bit slow to generate because we, at the moment for technical reasons, we go through PostScript and then go to PDF. So, it'd be nice to be able to generate them more quickly to be a bit more interactive looking. And also, as I mentioned in the beginning, I've made lots of tools. If these tools could become a bit more configurable, then maybe I wouldn't have to restart the implementation each time and it'd be more useful for other people. So, everything is publicly available. So, thank you. Thank you. We have time for one or two questions. Thanks for the talk. I have two questions, basically. Do you have a solution to visualize the latencies due to cache misses, for example, after a migration? The second one is, do you have a way to visualize when tasks are synchronizing on the mutex, for example, that also can bring some latencies? So, no, we haven't done anything for cache misses. It could be interesting. I mean, I have another tool which deals with events, and I think there's some way to add events to datagraphs and maybe you could see when different locking operations are happening. I mean, I definitely think that's useful. I don't think the support is as great as one might like at the moment, but it's definitely an important thing. I have one more question. Hello, Julien. I was wondering, is there a way to show the CPU state at the time you are printing the time? Because your graph is making the assumption that, typically, the CPU frequency or whatever is stable over time. It would be very interesting to know the physical state of the processor at the time we are printing, because maybe the task is running on a faster... The CPU frequency is higher on one cause than the other. So to visualize that this application is running on a fast or slow CPU could be very interesting to know the... Actually, the tool does that, but the unfortunate thing, I didn't talk about it because you have to actually go and add a line, a trace print K line to your kernel to actually print out that information, because it doesn't exist anywhere in the kernel. So that's the only issue, but actually the tool, once you print it out in the proper format, it actually does everything and it can show you just the frequencies, so you can see the different colors for how fast it's going. You can also see the merged thing where you have the frequency in one line and you have the activity in the other line. Sorry, we're out of time. Thank you for the talk, thank you for the questions. We can't take all questions, but I'm sure you can find Julia later.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.0, "text": " Julia is going to be talking about graphing tools for scheduler tracing.", "tokens": [18551, 307, 516, 281, 312, 1417, 466, 1295, 79, 571, 3873, 337, 12000, 260, 25262, 13], "temperature": 0.0, "avg_logprob": -0.2700633722193101, "compression_ratio": 1.7605633802816902, "no_speech_prob": 0.21369823813438416}, {"id": 1, "seek": 0, "start": 10.0, "end": 19.0, "text": " Okay. Do you hear me? Okay, so thank you. So, I'm going to be talking about graphing", "tokens": [1033, 13, 1144, 291, 1568, 385, 30, 1033, 11, 370, 1309, 291, 13, 407, 11, 286, 478, 516, 281, 312, 1417, 466, 1295, 79, 571], "temperature": 0.0, "avg_logprob": -0.2700633722193101, "compression_ratio": 1.7605633802816902, "no_speech_prob": 0.21369823813438416}, {"id": 2, "seek": 0, "start": 19.0, "end": 25.0, "text": " tools for scheduler tracing. So, I'll start out, like actually someone started out yesterday", "tokens": [3873, 337, 12000, 260, 25262, 13, 407, 11, 286, 603, 722, 484, 11, 411, 767, 1580, 1409, 484, 5186], "temperature": 0.0, "avg_logprob": -0.2700633722193101, "compression_ratio": 1.7605633802816902, "no_speech_prob": 0.21369823813438416}, {"id": 3, "seek": 2500, "start": 25.0, "end": 30.0, "text": " with what is a task scheduler. So, for me, a task scheduler is like an important part", "tokens": [365, 437, 307, 257, 5633, 12000, 260, 13, 407, 11, 337, 385, 11, 257, 5633, 12000, 260, 307, 411, 364, 1021, 644], "temperature": 0.0, "avg_logprob": -0.12174721101743985, "compression_ratio": 1.66, "no_speech_prob": 1.919515671033878e-05}, {"id": 4, "seek": 2500, "start": 30.0, "end": 37.0, "text": " of the Linux kernel. It does two things. It places tasks on cores when they either are", "tokens": [295, 264, 18734, 28256, 13, 467, 775, 732, 721, 13, 467, 3190, 9608, 322, 24826, 562, 436, 2139, 366], "temperature": 0.0, "avg_logprob": -0.12174721101743985, "compression_ratio": 1.66, "no_speech_prob": 1.919515671033878e-05}, {"id": 5, "seek": 2500, "start": 37.0, "end": 42.0, "text": " created with fork or when they wake up or when there's load balancing. And it also,", "tokens": [2942, 365, 17716, 420, 562, 436, 6634, 493, 420, 562, 456, 311, 3677, 22495, 13, 400, 309, 611, 11], "temperature": 0.0, "avg_logprob": -0.12174721101743985, "compression_ratio": 1.66, "no_speech_prob": 1.919515671033878e-05}, {"id": 6, "seek": 2500, "start": 42.0, "end": 47.0, "text": " when a core becomes empty, it decides what core should run next. Basically, I'm", "tokens": [562, 257, 4965, 3643, 6707, 11, 309, 14898, 437, 4965, 820, 1190, 958, 13, 8537, 11, 286, 478], "temperature": 0.0, "avg_logprob": -0.12174721101743985, "compression_ratio": 1.66, "no_speech_prob": 1.919515671033878e-05}, {"id": 7, "seek": 2500, "start": 47.0, "end": 54.0, "text": " interested in the Linux kernel files core.c and fare.c, so CFS. I'm not at all", "tokens": [3102, 294, 264, 18734, 28256, 7098, 4965, 13, 66, 293, 11994, 13, 66, 11, 370, 21792, 50, 13, 286, 478, 406, 412, 439], "temperature": 0.0, "avg_logprob": -0.12174721101743985, "compression_ratio": 1.66, "no_speech_prob": 1.919515671033878e-05}, {"id": 8, "seek": 5400, "start": 54.0, "end": 58.0, "text": " interested in the second point. I'm interested in where cores get placed when they wake", "tokens": [3102, 294, 264, 1150, 935, 13, 286, 478, 3102, 294, 689, 24826, 483, 7074, 562, 436, 6634], "temperature": 0.0, "avg_logprob": -0.11160158429827009, "compression_ratio": 1.7224489795918367, "no_speech_prob": 3.137317889922997e-06}, {"id": 9, "seek": 5400, "start": 58.0, "end": 63.0, "text": " up. So, that's going to be the entire focus of this talk. So, the next question is,", "tokens": [493, 13, 407, 11, 300, 311, 516, 281, 312, 264, 2302, 1879, 295, 341, 751, 13, 407, 11, 264, 958, 1168, 307, 11], "temperature": 0.0, "avg_logprob": -0.11160158429827009, "compression_ratio": 1.7224489795918367, "no_speech_prob": 3.137317889922997e-06}, {"id": 10, "seek": 5400, "start": 63.0, "end": 69.0, "text": " how does the scheduler impact application performance? Basically, a scheduler is", "tokens": [577, 775, 264, 12000, 260, 2712, 3861, 3389, 30, 8537, 11, 257, 12000, 260, 307], "temperature": 0.0, "avg_logprob": -0.11160158429827009, "compression_ratio": 1.7224489795918367, "no_speech_prob": 3.137317889922997e-06}, {"id": 11, "seek": 5400, "start": 69.0, "end": 73.0, "text": " confronted with a bunch of tasks and a bunch of cores, and it needs to make decisions", "tokens": [31257, 365, 257, 3840, 295, 9608, 293, 257, 3840, 295, 24826, 11, 293, 309, 2203, 281, 652, 5327], "temperature": 0.0, "avg_logprob": -0.11160158429827009, "compression_ratio": 1.7224489795918367, "no_speech_prob": 3.137317889922997e-06}, {"id": 12, "seek": 5400, "start": 73.0, "end": 78.0, "text": " where is it going to put these tasks on these cores? And so, sometimes, if you make", "tokens": [689, 307, 309, 516, 281, 829, 613, 9608, 322, 613, 24826, 30, 400, 370, 11, 2171, 11, 498, 291, 652], "temperature": 0.0, "avg_logprob": -0.11160158429827009, "compression_ratio": 1.7224489795918367, "no_speech_prob": 3.137317889922997e-06}, {"id": 13, "seek": 7800, "start": 78.0, "end": 84.0, "text": " a bad decision, sorry, it can have a short-term impact. Maybe some task has to", "tokens": [257, 1578, 3537, 11, 2597, 11, 309, 393, 362, 257, 2099, 12, 7039, 2712, 13, 2704, 512, 5633, 575, 281], "temperature": 0.0, "avg_logprob": -0.11746016752372668, "compression_ratio": 1.7012987012987013, "no_speech_prob": 6.642315383942332e-06}, {"id": 14, "seek": 7800, "start": 84.0, "end": 89.0, "text": " wait a little bit extra time, but this impact can kind of, there's kind of a", "tokens": [1699, 257, 707, 857, 2857, 565, 11, 457, 341, 2712, 393, 733, 295, 11, 456, 311, 733, 295, 257], "temperature": 0.0, "avg_logprob": -0.11746016752372668, "compression_ratio": 1.7012987012987013, "no_speech_prob": 6.642315383942332e-06}, {"id": 15, "seek": 7800, "start": 89.0, "end": 94.0, "text": " domino effect. You do one bad decision and then other bad decisions follow from", "tokens": [3285, 2982, 1802, 13, 509, 360, 472, 1578, 3537, 293, 550, 661, 1578, 5327, 1524, 490], "temperature": 0.0, "avg_logprob": -0.11746016752372668, "compression_ratio": 1.7012987012987013, "no_speech_prob": 6.642315383942332e-06}, {"id": 16, "seek": 7800, "start": 94.0, "end": 100.0, "text": " that. So, one issue that one might be concerned with is what's called work", "tokens": [300, 13, 407, 11, 472, 2734, 300, 472, 1062, 312, 5922, 365, 307, 437, 311, 1219, 589], "temperature": 0.0, "avg_logprob": -0.11746016752372668, "compression_ratio": 1.7012987012987013, "no_speech_prob": 6.642315383942332e-06}, {"id": 17, "seek": 7800, "start": 100.0, "end": 104.0, "text": " conservation. So, we have a machine that has four cores. We have a task that wakes", "tokens": [16185, 13, 407, 11, 321, 362, 257, 3479, 300, 575, 1451, 24826, 13, 492, 362, 257, 5633, 300, 29610], "temperature": 0.0, "avg_logprob": -0.11746016752372668, "compression_ratio": 1.7012987012987013, "no_speech_prob": 6.642315383942332e-06}, {"id": 18, "seek": 10400, "start": 104.0, "end": 109.0, "text": " up. T1, where should we put it? So, based on the information we have right now,", "tokens": [493, 13, 314, 16, 11, 689, 820, 321, 829, 309, 30, 407, 11, 2361, 322, 264, 1589, 321, 362, 558, 586, 11], "temperature": 0.0, "avg_logprob": -0.08138852506070524, "compression_ratio": 1.5967078189300412, "no_speech_prob": 6.240141829039203e-06}, {"id": 19, "seek": 10400, "start": 109.0, "end": 113.0, "text": " we've just got an empty machine and a random task. Maybe we have no idea,", "tokens": [321, 600, 445, 658, 364, 6707, 3479, 293, 257, 4974, 5633, 13, 2704, 321, 362, 572, 1558, 11], "temperature": 0.0, "avg_logprob": -0.08138852506070524, "compression_ratio": 1.5967078189300412, "no_speech_prob": 6.240141829039203e-06}, {"id": 20, "seek": 10400, "start": 113.0, "end": 119.0, "text": " we'll just put it on core zero. Now, another task wakes up. What should we do", "tokens": [321, 603, 445, 829, 309, 322, 4965, 4018, 13, 823, 11, 1071, 5633, 29610, 493, 13, 708, 820, 321, 360], "temperature": 0.0, "avg_logprob": -0.08138852506070524, "compression_ratio": 1.5967078189300412, "no_speech_prob": 6.240141829039203e-06}, {"id": 21, "seek": 10400, "start": 119.0, "end": 125.0, "text": " with T2? So, kind of intuitively, it might be good to put T2 on either core one", "tokens": [365, 314, 17, 30, 407, 11, 733, 295, 46506, 11, 309, 1062, 312, 665, 281, 829, 314, 17, 322, 2139, 4965, 472], "temperature": 0.0, "avg_logprob": -0.08138852506070524, "compression_ratio": 1.5967078189300412, "no_speech_prob": 6.240141829039203e-06}, {"id": 22, "seek": 10400, "start": 125.0, "end": 129.0, "text": " or core two or core three, because they're not doing anything at the moment.", "tokens": [420, 4965, 732, 420, 4965, 1045, 11, 570, 436, 434, 406, 884, 1340, 412, 264, 1623, 13], "temperature": 0.0, "avg_logprob": -0.08138852506070524, "compression_ratio": 1.5967078189300412, "no_speech_prob": 6.240141829039203e-06}, {"id": 23, "seek": 12900, "start": 129.0, "end": 134.0, "text": " Putting on a core zero would perhaps be a poor choice because it will have to", "tokens": [31367, 322, 257, 4965, 4018, 576, 4317, 312, 257, 4716, 3922, 570, 309, 486, 362, 281], "temperature": 0.0, "avg_logprob": -0.08677978189582498, "compression_ratio": 1.6801470588235294, "no_speech_prob": 3.9661240407440346e-06}, {"id": 24, "seek": 12900, "start": 134.0, "end": 139.0, "text": " wait for task one to finish. So, that seems completely obvious as a human", "tokens": [1699, 337, 5633, 472, 281, 2413, 13, 407, 11, 300, 2544, 2584, 6322, 382, 257, 1952], "temperature": 0.0, "avg_logprob": -0.08677978189582498, "compression_ratio": 1.6801470588235294, "no_speech_prob": 3.9661240407440346e-06}, {"id": 25, "seek": 12900, "start": 139.0, "end": 144.0, "text": " looking at boxes on the screen, but the scheduler is going to have to hunt around", "tokens": [1237, 412, 9002, 322, 264, 2568, 11, 457, 264, 12000, 260, 307, 516, 281, 362, 281, 12454, 926], "temperature": 0.0, "avg_logprob": -0.08677978189582498, "compression_ratio": 1.6801470588235294, "no_speech_prob": 3.9661240407440346e-06}, {"id": 26, "seek": 12900, "start": 144.0, "end": 148.0, "text": " to find those empty cores. And so, actually, CFS is not actually work", "tokens": [281, 915, 729, 6707, 24826, 13, 400, 370, 11, 767, 11, 21792, 50, 307, 406, 767, 589], "temperature": 0.0, "avg_logprob": -0.08677978189582498, "compression_ratio": 1.6801470588235294, "no_speech_prob": 3.9661240407440346e-06}, {"id": 27, "seek": 12900, "start": 148.0, "end": 154.0, "text": " conserving. The basic principle is no core should be overloaded if any core is", "tokens": [1014, 20186, 13, 440, 3875, 8665, 307, 572, 4965, 820, 312, 28777, 292, 498, 604, 4965, 307], "temperature": 0.0, "avg_logprob": -0.08677978189582498, "compression_ratio": 1.6801470588235294, "no_speech_prob": 3.9661240407440346e-06}, {"id": 28, "seek": 12900, "start": 154.0, "end": 157.0, "text": " idle. So, if you have an overload, you should have put it on the idle core", "tokens": [30650, 13, 407, 11, 498, 291, 362, 364, 28777, 11, 291, 820, 362, 829, 309, 322, 264, 30650, 4965], "temperature": 0.0, "avg_logprob": -0.08677978189582498, "compression_ratio": 1.6801470588235294, "no_speech_prob": 3.9661240407440346e-06}, {"id": 29, "seek": 15700, "start": 157.0, "end": 163.0, "text": " instead. Another issue is locality. So, instead of having just four random", "tokens": [2602, 13, 3996, 2734, 307, 1628, 1860, 13, 407, 11, 2602, 295, 1419, 445, 1451, 4974], "temperature": 0.0, "avg_logprob": -0.08458293159053011, "compression_ratio": 1.6710526315789473, "no_speech_prob": 8.01278838480357e-06}, {"id": 30, "seek": 15700, "start": 163.0, "end": 168.0, "text": " cores, we may have a multi-socket machine. We've got cores zero and one,", "tokens": [24826, 11, 321, 815, 362, 257, 4825, 12, 539, 4737, 3479, 13, 492, 600, 658, 24826, 4018, 293, 472, 11], "temperature": 0.0, "avg_logprob": -0.08458293159053011, "compression_ratio": 1.6710526315789473, "no_speech_prob": 8.01278838480357e-06}, {"id": 31, "seek": 15700, "start": 168.0, "end": 171.0, "text": " which are together on one socket. Core zero, two and three are together on a", "tokens": [597, 366, 1214, 322, 472, 19741, 13, 14798, 4018, 11, 732, 293, 1045, 366, 1214, 322, 257], "temperature": 0.0, "avg_logprob": -0.08458293159053011, "compression_ratio": 1.6710526315789473, "no_speech_prob": 8.01278838480357e-06}, {"id": 32, "seek": 15700, "start": 171.0, "end": 178.0, "text": " socket. We have T1 is on core zero. Where should we put T2? So, we have those", "tokens": [19741, 13, 492, 362, 314, 16, 307, 322, 4965, 4018, 13, 2305, 820, 321, 829, 314, 17, 30, 407, 11, 321, 362, 729], "temperature": 0.0, "avg_logprob": -0.08458293159053011, "compression_ratio": 1.6710526315789473, "no_speech_prob": 8.01278838480357e-06}, {"id": 33, "seek": 15700, "start": 178.0, "end": 184.0, "text": " three idle cores, but maybe core one would be a better choice if either T2 has", "tokens": [1045, 30650, 24826, 11, 457, 1310, 4965, 472, 576, 312, 257, 1101, 3922, 498, 2139, 314, 17, 575], "temperature": 0.0, "avg_logprob": -0.08458293159053011, "compression_ratio": 1.6710526315789473, "no_speech_prob": 8.01278838480357e-06}, {"id": 34, "seek": 18400, "start": 184.0, "end": 189.0, "text": " already all allocated all of its data on the first socket or if T2 wants to", "tokens": [1217, 439, 29772, 439, 295, 1080, 1412, 322, 264, 700, 19741, 420, 498, 314, 17, 2738, 281], "temperature": 0.0, "avg_logprob": -0.07100308736165364, "compression_ratio": 1.6382978723404256, "no_speech_prob": 3.392797225387767e-06}, {"id": 35, "seek": 18400, "start": 189.0, "end": 195.0, "text": " discuss things with T1. If we put it on two or three, things might get slower.", "tokens": [2248, 721, 365, 314, 16, 13, 759, 321, 829, 309, 322, 732, 420, 1045, 11, 721, 1062, 483, 14009, 13], "temperature": 0.0, "avg_logprob": -0.07100308736165364, "compression_ratio": 1.6382978723404256, "no_speech_prob": 3.392797225387767e-06}, {"id": 36, "seek": 18400, "start": 195.0, "end": 199.0, "text": " So, basically, you can see that there's a lot of potential for things to go", "tokens": [407, 11, 1936, 11, 291, 393, 536, 300, 456, 311, 257, 688, 295, 3995, 337, 721, 281, 352], "temperature": 0.0, "avg_logprob": -0.07100308736165364, "compression_ratio": 1.6382978723404256, "no_speech_prob": 3.392797225387767e-06}, {"id": 37, "seek": 18400, "start": 199.0, "end": 205.0, "text": " wrong. So, we need to understand maybe what the scheduler is actually doing,", "tokens": [2085, 13, 407, 11, 321, 643, 281, 1223, 1310, 437, 264, 12000, 260, 307, 767, 884, 11], "temperature": 0.0, "avg_logprob": -0.07100308736165364, "compression_ratio": 1.6382978723404256, "no_speech_prob": 3.392797225387767e-06}, {"id": 38, "seek": 18400, "start": 205.0, "end": 209.0, "text": " but this problem is the scheduler is like buried down there in the OS. When", "tokens": [457, 341, 1154, 307, 264, 12000, 260, 307, 411, 14101, 760, 456, 294, 264, 12731, 13, 1133], "temperature": 0.0, "avg_logprob": -0.07100308736165364, "compression_ratio": 1.6382978723404256, "no_speech_prob": 3.392797225387767e-06}, {"id": 39, "seek": 18400, "start": 209.0, "end": 213.0, "text": " you're on the application, you don't really know where your tasks are running.", "tokens": [291, 434, 322, 264, 3861, 11, 291, 500, 380, 534, 458, 689, 428, 9608, 366, 2614, 13], "temperature": 0.0, "avg_logprob": -0.07100308736165364, "compression_ratio": 1.6382978723404256, "no_speech_prob": 3.392797225387767e-06}, {"id": 40, "seek": 21300, "start": 213.0, "end": 217.0, "text": " So, we want to consider how we can see what the scheduler is doing. So,", "tokens": [407, 11, 321, 528, 281, 1949, 577, 321, 393, 536, 437, 264, 12000, 260, 307, 884, 13, 407, 11], "temperature": 0.0, "avg_logprob": -0.0758188703785772, "compression_ratio": 1.7516778523489933, "no_speech_prob": 4.6364866648218594e-06}, {"id": 41, "seek": 21300, "start": 217.0, "end": 221.0, "text": " fortunately, there's some tools that are available. So, the most helpful one,", "tokens": [25511, 11, 456, 311, 512, 3873, 300, 366, 2435, 13, 407, 11, 264, 881, 4961, 472, 11], "temperature": 0.0, "avg_logprob": -0.0758188703785772, "compression_ratio": 1.7516778523489933, "no_speech_prob": 4.6364866648218594e-06}, {"id": 42, "seek": 21300, "start": 221.0, "end": 225.0, "text": " I would say, is trace command. So, trace command allows you to trace all", "tokens": [286, 576, 584, 11, 307, 13508, 5622, 13, 407, 11, 13508, 5622, 4045, 291, 281, 13508, 439], "temperature": 0.0, "avg_logprob": -0.0758188703785772, "compression_ratio": 1.7516778523489933, "no_speech_prob": 4.6364866648218594e-06}, {"id": 43, "seek": 21300, "start": 225.0, "end": 230.0, "text": " different kinds of kernel events. Basically, it's a front-end on F trace,", "tokens": [819, 3685, 295, 28256, 3931, 13, 8537, 11, 309, 311, 257, 1868, 12, 521, 322, 479, 13508, 11], "temperature": 0.0, "avg_logprob": -0.0758188703785772, "compression_ratio": 1.7516778523489933, "no_speech_prob": 4.6364866648218594e-06}, {"id": 44, "seek": 21300, "start": 230.0, "end": 233.0, "text": " but in particular, it lets you trace scheduling events, so that's the part", "tokens": [457, 294, 1729, 11, 309, 6653, 291, 13508, 29055, 3931, 11, 370, 300, 311, 264, 644], "temperature": 0.0, "avg_logprob": -0.0758188703785772, "compression_ratio": 1.7516778523489933, "no_speech_prob": 4.6364866648218594e-06}, {"id": 45, "seek": 21300, "start": 233.0, "end": 237.0, "text": " we're interested in. So, you can see a trace here, and if you get this trace,", "tokens": [321, 434, 3102, 294, 13, 407, 11, 291, 393, 536, 257, 13508, 510, 11, 293, 498, 291, 483, 341, 13508, 11], "temperature": 0.0, "avg_logprob": -0.0758188703785772, "compression_ratio": 1.7516778523489933, "no_speech_prob": 4.6364866648218594e-06}, {"id": 46, "seek": 21300, "start": 237.0, "end": 241.0, "text": " it will have basically all the information you need to solve all of your", "tokens": [309, 486, 362, 1936, 439, 264, 1589, 291, 643, 281, 5039, 439, 295, 428], "temperature": 0.0, "avg_logprob": -0.0758188703785772, "compression_ratio": 1.7516778523489933, "no_speech_prob": 4.6364866648218594e-06}, {"id": 47, "seek": 24100, "start": 241.0, "end": 245.0, "text": " scheduling problems. On the other hand, it unfortunately has all the information", "tokens": [29055, 2740, 13, 1282, 264, 661, 1011, 11, 309, 7015, 575, 439, 264, 1589], "temperature": 0.0, "avg_logprob": -0.09572943121986052, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.2026311196677852e-05}, {"id": 48, "seek": 24100, "start": 245.0, "end": 249.0, "text": " you need to solve all of your scheduling problems. That is, it's ordered,", "tokens": [291, 643, 281, 5039, 439, 295, 428, 29055, 2740, 13, 663, 307, 11, 309, 311, 8866, 11], "temperature": 0.0, "avg_logprob": -0.09572943121986052, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.2026311196677852e-05}, {"id": 49, "seek": 24100, "start": 249.0, "end": 254.0, "text": " it's like sequential thing. It's ordered according to time. If your application", "tokens": [309, 311, 411, 42881, 551, 13, 467, 311, 8866, 4650, 281, 565, 13, 759, 428, 3861], "temperature": 0.0, "avg_logprob": -0.09572943121986052, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.2026311196677852e-05}, {"id": 50, "seek": 24100, "start": 254.0, "end": 258.0, "text": " runs for a certain amount of time, you'll end up with a huge file. And you can", "tokens": [6676, 337, 257, 1629, 2372, 295, 565, 11, 291, 603, 917, 493, 365, 257, 2603, 3991, 13, 400, 291, 393], "temperature": 0.0, "avg_logprob": -0.09572943121986052, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.2026311196677852e-05}, {"id": 51, "seek": 24100, "start": 258.0, "end": 261.0, "text": " see even in this little tiny trace that I'm showing, we've got different,", "tokens": [536, 754, 294, 341, 707, 5870, 13508, 300, 286, 478, 4099, 11, 321, 600, 658, 819, 11], "temperature": 0.0, "avg_logprob": -0.09572943121986052, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.2026311196677852e-05}, {"id": 52, "seek": 24100, "start": 261.0, "end": 266.0, "text": " the activities on different cores are mixed up. We have core 26 and core", "tokens": [264, 5354, 322, 819, 24826, 366, 7467, 493, 13, 492, 362, 4965, 7551, 293, 4965], "temperature": 0.0, "avg_logprob": -0.09572943121986052, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.2026311196677852e-05}, {"id": 53, "seek": 26600, "start": 266.0, "end": 272.0, "text": " 62 here. And so, in practice, it can get very hard to actually sort out what's", "tokens": [24536, 510, 13, 400, 370, 11, 294, 3124, 11, 309, 393, 483, 588, 1152, 281, 767, 1333, 484, 437, 311], "temperature": 0.0, "avg_logprob": -0.09502394104003906, "compression_ratio": 1.7358490566037736, "no_speech_prob": 5.59338286620914e-06}, {"id": 54, "seek": 26600, "start": 272.0, "end": 276.0, "text": " going on, who's doing what, and so on. And so, the next tool, which is very", "tokens": [516, 322, 11, 567, 311, 884, 437, 11, 293, 370, 322, 13, 400, 370, 11, 264, 958, 2290, 11, 597, 307, 588], "temperature": 0.0, "avg_logprob": -0.09502394104003906, "compression_ratio": 1.7358490566037736, "no_speech_prob": 5.59338286620914e-06}, {"id": 55, "seek": 26600, "start": 276.0, "end": 280.0, "text": " helpful, is one that's called kernel shark. So, this gives you a graphical", "tokens": [4961, 11, 307, 472, 300, 311, 1219, 28256, 13327, 13, 407, 11, 341, 2709, 291, 257, 35942], "temperature": 0.0, "avg_logprob": -0.09502394104003906, "compression_ratio": 1.7358490566037736, "no_speech_prob": 5.59338286620914e-06}, {"id": 56, "seek": 26600, "start": 280.0, "end": 285.0, "text": " interface that lets you see what's going on on your different cores. And it", "tokens": [9226, 300, 6653, 291, 536, 437, 311, 516, 322, 322, 428, 819, 24826, 13, 400, 309], "temperature": 0.0, "avg_logprob": -0.09502394104003906, "compression_ratio": 1.7358490566037736, "no_speech_prob": 5.59338286620914e-06}, {"id": 57, "seek": 26600, "start": 285.0, "end": 289.0, "text": " also gives you that same textual output at the bottom. You can kind of correlate", "tokens": [611, 2709, 291, 300, 912, 2487, 901, 5598, 412, 264, 2767, 13, 509, 393, 733, 295, 48742], "temperature": 0.0, "avg_logprob": -0.09502394104003906, "compression_ratio": 1.7358490566037736, "no_speech_prob": 5.59338286620914e-06}, {"id": 58, "seek": 26600, "start": 289.0, "end": 295.0, "text": " them to each other. You can zoom in quite easily, and so on. On the other", "tokens": [552, 281, 1184, 661, 13, 509, 393, 8863, 294, 1596, 3612, 11, 293, 370, 322, 13, 1282, 264, 661], "temperature": 0.0, "avg_logprob": -0.09502394104003906, "compression_ratio": 1.7358490566037736, "no_speech_prob": 5.59338286620914e-06}, {"id": 59, "seek": 29500, "start": 295.0, "end": 300.0, "text": " hand, in my personal application, where I'm interested in actually very large", "tokens": [1011, 11, 294, 452, 2973, 3861, 11, 689, 286, 478, 3102, 294, 767, 588, 2416], "temperature": 0.0, "avg_logprob": -0.07040990300539161, "compression_ratio": 1.737037037037037, "no_speech_prob": 1.458596671000123e-06}, {"id": 60, "seek": 29500, "start": 300.0, "end": 306.0, "text": " machines, kernel shark has some kind of a bit difficult to use in some cases.", "tokens": [8379, 11, 28256, 13327, 575, 512, 733, 295, 257, 857, 2252, 281, 764, 294, 512, 3331, 13], "temperature": 0.0, "avg_logprob": -0.07040990300539161, "compression_ratio": 1.737037037037037, "no_speech_prob": 1.458596671000123e-06}, {"id": 61, "seek": 29500, "start": 306.0, "end": 310.0, "text": " It's great for if you want to really zoom in on a specific problem. It's not so", "tokens": [467, 311, 869, 337, 498, 291, 528, 281, 534, 8863, 294, 322, 257, 2685, 1154, 13, 467, 311, 406, 370], "temperature": 0.0, "avg_logprob": -0.07040990300539161, "compression_ratio": 1.737037037037037, "no_speech_prob": 1.458596671000123e-06}, {"id": 62, "seek": 29500, "start": 310.0, "end": 313.0, "text": " great if you actually don't really know where your problem is, and you want to", "tokens": [869, 498, 291, 767, 500, 380, 534, 458, 689, 428, 1154, 307, 11, 293, 291, 528, 281], "temperature": 0.0, "avg_logprob": -0.07040990300539161, "compression_ratio": 1.737037037037037, "no_speech_prob": 1.458596671000123e-06}, {"id": 63, "seek": 29500, "start": 313.0, "end": 318.0, "text": " see somehow an overview with everything at once. Here, I'm only showing two", "tokens": [536, 6063, 364, 12492, 365, 1203, 412, 1564, 13, 1692, 11, 286, 478, 787, 4099, 732], "temperature": 0.0, "avg_logprob": -0.07040990300539161, "compression_ratio": 1.737037037037037, "no_speech_prob": 1.458596671000123e-06}, {"id": 64, "seek": 29500, "start": 318.0, "end": 323.0, "text": " cores. You can see that the display is kind of a bit spread out. It's going to", "tokens": [24826, 13, 509, 393, 536, 300, 264, 4674, 307, 733, 295, 257, 857, 3974, 484, 13, 467, 311, 516, 281], "temperature": 0.0, "avg_logprob": -0.07040990300539161, "compression_ratio": 1.737037037037037, "no_speech_prob": 1.458596671000123e-06}, {"id": 65, "seek": 32300, "start": 323.0, "end": 332.0, "text": " be hard to get 128 cores to fit on your screen and be reasonably understandable.", "tokens": [312, 1152, 281, 483, 29810, 24826, 281, 3318, 322, 428, 2568, 293, 312, 23551, 25648, 13], "temperature": 0.0, "avg_logprob": -0.07493916535988832, "compression_ratio": 1.6041666666666667, "no_speech_prob": 4.22134462496615e-06}, {"id": 66, "seek": 32300, "start": 332.0, "end": 336.0, "text": " So, what we would like is some way of understanding what's going on on big", "tokens": [407, 11, 437, 321, 576, 411, 307, 512, 636, 295, 3701, 437, 311, 516, 322, 322, 955], "temperature": 0.0, "avg_logprob": -0.07493916535988832, "compression_ratio": 1.6041666666666667, "no_speech_prob": 4.22134462496615e-06}, {"id": 67, "seek": 32300, "start": 336.0, "end": 342.0, "text": " machines. So, the thing I put the emphasis on previously is that we want to", "tokens": [8379, 13, 407, 11, 264, 551, 286, 829, 264, 16271, 322, 8046, 307, 300, 321, 528, 281], "temperature": 0.0, "avg_logprob": -0.07493916535988832, "compression_ratio": 1.6041666666666667, "no_speech_prob": 4.22134462496615e-06}, {"id": 68, "seek": 32300, "start": 342.0, "end": 348.0, "text": " see what's going on on all the cores at once. Something that I've also found", "tokens": [536, 437, 311, 516, 322, 322, 439, 264, 24826, 412, 1564, 13, 6595, 300, 286, 600, 611, 1352], "temperature": 0.0, "avg_logprob": -0.07493916535988832, "compression_ratio": 1.6041666666666667, "no_speech_prob": 4.22134462496615e-06}, {"id": 69, "seek": 34800, "start": 348.0, "end": 354.0, "text": " extremely useful in practice is to be able to collect traces, collect these", "tokens": [4664, 4420, 294, 3124, 307, 281, 312, 1075, 281, 2500, 26076, 11, 2500, 613], "temperature": 0.0, "avg_logprob": -0.06480167893802419, "compression_ratio": 1.7828054298642535, "no_speech_prob": 2.94685651169857e-06}, {"id": 70, "seek": 34800, "start": 354.0, "end": 359.0, "text": " images, share them with my colleagues, put them in papers, put them in slides,", "tokens": [5267, 11, 2073, 552, 365, 452, 7734, 11, 829, 552, 294, 10577, 11, 829, 552, 294, 9788, 11], "temperature": 0.0, "avg_logprob": -0.06480167893802419, "compression_ratio": 1.7828054298642535, "no_speech_prob": 2.94685651169857e-06}, {"id": 71, "seek": 34800, "start": 359.0, "end": 365.0, "text": " and so on. So, I found it useful to make, collect lots of traces, compare them,", "tokens": [293, 370, 322, 13, 407, 11, 286, 1352, 309, 4420, 281, 652, 11, 2500, 3195, 295, 26076, 11, 6794, 552, 11], "temperature": 0.0, "avg_logprob": -0.06480167893802419, "compression_ratio": 1.7828054298642535, "no_speech_prob": 2.94685651169857e-06}, {"id": 72, "seek": 34800, "start": 365.0, "end": 372.0, "text": " store them, look at them later, and so on. On the other hand, I have, at least for", "tokens": [3531, 552, 11, 574, 412, 552, 1780, 11, 293, 370, 322, 13, 1282, 264, 661, 1011, 11, 286, 362, 11, 412, 1935, 337], "temperature": 0.0, "avg_logprob": -0.06480167893802419, "compression_ratio": 1.7828054298642535, "no_speech_prob": 2.94685651169857e-06}, {"id": 73, "seek": 34800, "start": 372.0, "end": 377.0, "text": " the moment, completely abandoned this nice feature of kernel shark, which is", "tokens": [264, 1623, 11, 2584, 13732, 341, 1481, 4111, 295, 28256, 13327, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.06480167893802419, "compression_ratio": 1.7828054298642535, "no_speech_prob": 2.94685651169857e-06}, {"id": 74, "seek": 37700, "start": 377.0, "end": 380.0, "text": " that you can zoom in or zoom out and find out exactly what you want to see at", "tokens": [300, 291, 393, 8863, 294, 420, 8863, 484, 293, 915, 484, 2293, 437, 291, 528, 281, 536, 412], "temperature": 0.0, "avg_logprob": -0.08223087504758674, "compression_ratio": 1.8537549407114624, "no_speech_prob": 1.0284656127623748e-05}, {"id": 75, "seek": 37700, "start": 380.0, "end": 386.0, "text": " what time. My proposed approach that I'm going to present in this talk is", "tokens": [437, 565, 13, 1222, 10348, 3109, 300, 286, 478, 516, 281, 1974, 294, 341, 751, 307], "temperature": 0.0, "avg_logprob": -0.08223087504758674, "compression_ratio": 1.8537549407114624, "no_speech_prob": 1.0284656127623748e-05}, {"id": 76, "seek": 37700, "start": 386.0, "end": 390.0, "text": " completely uninteractive. So, you run a command, you get a picture, you look at", "tokens": [2584, 49234, 12596, 13, 407, 11, 291, 1190, 257, 5622, 11, 291, 483, 257, 3036, 11, 291, 574, 412], "temperature": 0.0, "avg_logprob": -0.08223087504758674, "compression_ratio": 1.8537549407114624, "no_speech_prob": 1.0284656127623748e-05}, {"id": 77, "seek": 37700, "start": 390.0, "end": 393.0, "text": " your picture, and you run another command, you get another picture, and you look", "tokens": [428, 3036, 11, 293, 291, 1190, 1071, 5622, 11, 291, 483, 1071, 3036, 11, 293, 291, 574], "temperature": 0.0, "avg_logprob": -0.08223087504758674, "compression_ratio": 1.8537549407114624, "no_speech_prob": 1.0284656127623748e-05}, {"id": 78, "seek": 37700, "start": 393.0, "end": 399.0, "text": " at that picture. So, actually, in the last few years, I've made lots and lots", "tokens": [412, 300, 3036, 13, 407, 11, 767, 11, 294, 264, 1036, 1326, 924, 11, 286, 600, 1027, 3195, 293, 3195], "temperature": 0.0, "avg_logprob": -0.08223087504758674, "compression_ratio": 1.8537549407114624, "no_speech_prob": 1.0284656127623748e-05}, {"id": 79, "seek": 37700, "start": 399.0, "end": 403.0, "text": " of tools that start out with trace command input and visualize it in different", "tokens": [295, 3873, 300, 722, 484, 365, 13508, 5622, 4846, 293, 23273, 309, 294, 819], "temperature": 0.0, "avg_logprob": -0.08223087504758674, "compression_ratio": 1.8537549407114624, "no_speech_prob": 1.0284656127623748e-05}, {"id": 80, "seek": 40300, "start": 403.0, "end": 408.0, "text": " ways. Sort of the ones that have stood the test of time are the ones I'm going", "tokens": [2098, 13, 26149, 295, 264, 2306, 300, 362, 9371, 264, 1500, 295, 565, 366, 264, 2306, 286, 478, 516], "temperature": 0.0, "avg_logprob": -0.09618869357638889, "compression_ratio": 1.8228346456692914, "no_speech_prob": 8.797202099231072e-06}, {"id": 81, "seek": 40300, "start": 408.0, "end": 413.0, "text": " to present, which are datagraph and running weighting. The names are not", "tokens": [281, 1974, 11, 597, 366, 1137, 559, 2662, 293, 2614, 3364, 278, 13, 440, 5288, 366, 406], "temperature": 0.0, "avg_logprob": -0.09618869357638889, "compression_ratio": 1.8228346456692914, "no_speech_prob": 8.797202099231072e-06}, {"id": 82, "seek": 40300, "start": 413.0, "end": 417.0, "text": " super imaginative, perhaps. Datagraph takes a dat file, so that's what the", "tokens": [1687, 23427, 1166, 11, 4317, 13, 9315, 559, 2662, 2516, 257, 1137, 3991, 11, 370, 300, 311, 437, 264], "temperature": 0.0, "avg_logprob": -0.09618869357638889, "compression_ratio": 1.8228346456692914, "no_speech_prob": 8.797202099231072e-06}, {"id": 83, "seek": 40300, "start": 417.0, "end": 421.0, "text": " trace command produces, and it makes a graph for you. So, basically, it's going", "tokens": [13508, 5622, 14725, 11, 293, 309, 1669, 257, 4295, 337, 291, 13, 407, 11, 1936, 11, 309, 311, 516], "temperature": 0.0, "avg_logprob": -0.09618869357638889, "compression_ratio": 1.8228346456692914, "no_speech_prob": 8.797202099231072e-06}, {"id": 84, "seek": 40300, "start": 421.0, "end": 426.0, "text": " to show you, we have the x-axis and the y-axis, the x-axis is the time, and then", "tokens": [281, 855, 291, 11, 321, 362, 264, 2031, 12, 24633, 293, 264, 288, 12, 24633, 11, 264, 2031, 12, 24633, 307, 264, 565, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.09618869357638889, "compression_ratio": 1.8228346456692914, "no_speech_prob": 8.797202099231072e-06}, {"id": 85, "seek": 40300, "start": 426.0, "end": 430.0, "text": " on the y-axis, we have our cores, and we see what's running on each core at", "tokens": [322, 264, 288, 12, 24633, 11, 321, 362, 527, 24826, 11, 293, 321, 536, 437, 311, 2614, 322, 1184, 4965, 412], "temperature": 0.0, "avg_logprob": -0.09618869357638889, "compression_ratio": 1.8228346456692914, "no_speech_prob": 8.797202099231072e-06}, {"id": 86, "seek": 43000, "start": 430.0, "end": 434.0, "text": " each time. So, kind of like what Colonel Sharpe showed you, but in much more", "tokens": [1184, 565, 13, 407, 11, 733, 295, 411, 437, 28478, 22030, 494, 4712, 291, 11, 457, 294, 709, 544], "temperature": 0.0, "avg_logprob": -0.08275075185866583, "compression_ratio": 1.6967509025270757, "no_speech_prob": 5.9540320762607735e-06}, {"id": 87, "seek": 43000, "start": 434.0, "end": 439.0, "text": " compressed format. And running weighting is just a line graph. It shows you how", "tokens": [30353, 7877, 13, 400, 2614, 3364, 278, 307, 445, 257, 1622, 4295, 13, 467, 3110, 291, 577], "temperature": 0.0, "avg_logprob": -0.08275075185866583, "compression_ratio": 1.6967509025270757, "no_speech_prob": 5.9540320762607735e-06}, {"id": 88, "seek": 43000, "start": 439.0, "end": 445.0, "text": " many tasks are running at a particular time and how many tasks are waiting on", "tokens": [867, 9608, 366, 2614, 412, 257, 1729, 565, 293, 577, 867, 9608, 366, 3806, 322], "temperature": 0.0, "avg_logprob": -0.08275075185866583, "compression_ratio": 1.6967509025270757, "no_speech_prob": 5.9540320762607735e-06}, {"id": 89, "seek": 43000, "start": 445.0, "end": 451.0, "text": " a run queue and are not able to run. So, we'll see how that's used. So, the rest", "tokens": [257, 1190, 18639, 293, 366, 406, 1075, 281, 1190, 13, 407, 11, 321, 603, 536, 577, 300, 311, 1143, 13, 407, 11, 264, 1472], "temperature": 0.0, "avg_logprob": -0.08275075185866583, "compression_ratio": 1.6967509025270757, "no_speech_prob": 5.9540320762607735e-06}, {"id": 90, "seek": 43000, "start": 451.0, "end": 455.0, "text": " of this talk, I'm going to present these two tools, and I'm going to be", "tokens": [295, 341, 751, 11, 286, 478, 516, 281, 1974, 613, 732, 3873, 11, 293, 286, 478, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.08275075185866583, "compression_ratio": 1.6967509025270757, "no_speech_prob": 5.9540320762607735e-06}, {"id": 91, "seek": 43000, "start": 455.0, "end": 459.0, "text": " motivated by this patch that I submitted a few years ago. I'm not going to discuss", "tokens": [14515, 538, 341, 9972, 300, 286, 14405, 257, 1326, 924, 2057, 13, 286, 478, 406, 516, 281, 2248], "temperature": 0.0, "avg_logprob": -0.08275075185866583, "compression_ratio": 1.6967509025270757, "no_speech_prob": 5.9540320762607735e-06}, {"id": 92, "seek": 45900, "start": 459.0, "end": 464.0, "text": " the patch in detail now. We'll see it later after we've seen all the examples.", "tokens": [264, 9972, 294, 2607, 586, 13, 492, 603, 536, 309, 1780, 934, 321, 600, 1612, 439, 264, 5110, 13], "temperature": 0.0, "avg_logprob": -0.08962094205097088, "compression_ratio": 1.5725806451612903, "no_speech_prob": 1.2602597053046338e-05}, {"id": 93, "seek": 45900, "start": 464.0, "end": 469.0, "text": " The application I'm going to be interested in is part of the NAS parallel", "tokens": [440, 3861, 286, 478, 516, 281, 312, 3102, 294, 307, 644, 295, 264, 10182, 8952], "temperature": 0.0, "avg_logprob": -0.08962094205097088, "compression_ratio": 1.5725806451612903, "no_speech_prob": 1.2602597053046338e-05}, {"id": 94, "seek": 45900, "start": 469.0, "end": 474.0, "text": " benchmarks. These are a bunch of, it says what, you can read what it says. It's", "tokens": [43751, 13, 1981, 366, 257, 3840, 295, 11, 309, 1619, 437, 11, 291, 393, 1401, 437, 309, 1619, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.08962094205097088, "compression_ratio": 1.5725806451612903, "no_speech_prob": 1.2602597053046338e-05}, {"id": 95, "seek": 45900, "start": 474.0, "end": 480.0, "text": " small kernels having to do with HPC kind of things. We're going to focus on the", "tokens": [1359, 23434, 1625, 1419, 281, 360, 365, 12557, 34, 733, 295, 721, 13, 492, 434, 516, 281, 1879, 322, 264], "temperature": 0.0, "avg_logprob": -0.08962094205097088, "compression_ratio": 1.5725806451612903, "no_speech_prob": 1.2602597053046338e-05}, {"id": 96, "seek": 45900, "start": 480.0, "end": 486.0, "text": " UA benchmark. It does something. What's important for our purposes is that it", "tokens": [32765, 18927, 13, 467, 775, 746, 13, 708, 311, 1021, 337, 527, 9932, 307, 300, 309], "temperature": 0.0, "avg_logprob": -0.08962094205097088, "compression_ratio": 1.5725806451612903, "no_speech_prob": 1.2602597053046338e-05}, {"id": 97, "seek": 48600, "start": 486.0, "end": 491.0, "text": " has n tasks, and they're running on n cores. And so, they kind of run, they", "tokens": [575, 297, 9608, 11, 293, 436, 434, 2614, 322, 297, 24826, 13, 400, 370, 11, 436, 733, 295, 1190, 11, 436], "temperature": 0.0, "avg_logprob": -0.10861365757291279, "compression_ratio": 1.8431372549019607, "no_speech_prob": 2.929215770564042e-05}, {"id": 98, "seek": 48600, "start": 491.0, "end": 495.0, "text": " seem, at least superficially, they seem to run all the time. You would expect", "tokens": [1643, 11, 412, 1935, 23881, 2270, 11, 436, 1643, 281, 1190, 439, 264, 565, 13, 509, 576, 2066], "temperature": 0.0, "avg_logprob": -0.10861365757291279, "compression_ratio": 1.8431372549019607, "no_speech_prob": 2.929215770564042e-05}, {"id": 99, "seek": 48600, "start": 495.0, "end": 500.0, "text": " that they would just choose their cores, stay on their cores, and run on those", "tokens": [300, 436, 576, 445, 2826, 641, 24826, 11, 1754, 322, 641, 24826, 11, 293, 1190, 322, 729], "temperature": 0.0, "avg_logprob": -0.10861365757291279, "compression_ratio": 1.8431372549019607, "no_speech_prob": 2.929215770564042e-05}, {"id": 100, "seek": 48600, "start": 500.0, "end": 504.0, "text": " cores forever. So, you would expect this benchmark to be completely uninteresting", "tokens": [24826, 5680, 13, 407, 11, 291, 576, 2066, 341, 18927, 281, 312, 2584, 49234, 8714], "temperature": 0.0, "avg_logprob": -0.10861365757291279, "compression_ratio": 1.8431372549019607, "no_speech_prob": 2.929215770564042e-05}, {"id": 101, "seek": 48600, "start": 504.0, "end": 509.0, "text": " from a scheduling point of view. So, if we take this benchmark and we run it a", "tokens": [490, 257, 29055, 935, 295, 1910, 13, 407, 11, 498, 321, 747, 341, 18927, 293, 321, 1190, 309, 257], "temperature": 0.0, "avg_logprob": -0.10861365757291279, "compression_ratio": 1.8431372549019607, "no_speech_prob": 2.929215770564042e-05}, {"id": 102, "seek": 48600, "start": 509.0, "end": 514.0, "text": " few times, so I run it 10 times, you can, and I've taken these runs and I've", "tokens": [1326, 1413, 11, 370, 286, 1190, 309, 1266, 1413, 11, 291, 393, 11, 293, 286, 600, 2726, 613, 6676, 293, 286, 600], "temperature": 0.0, "avg_logprob": -0.10861365757291279, "compression_ratio": 1.8431372549019607, "no_speech_prob": 2.929215770564042e-05}, {"id": 103, "seek": 51400, "start": 514.0, "end": 519.0, "text": " started it by increasing run time. You can see that something is going on,", "tokens": [1409, 309, 538, 5662, 1190, 565, 13, 509, 393, 536, 300, 746, 307, 516, 322, 11], "temperature": 0.0, "avg_logprob": -0.11560061344733605, "compression_ratio": 1.6443514644351465, "no_speech_prob": 5.2548593885148875e-06}, {"id": 104, "seek": 51400, "start": 519.0, "end": 524.0, "text": " because there's kind of these runs on the left-hand side here, which start out", "tokens": [570, 456, 311, 733, 295, 613, 6676, 322, 264, 1411, 12, 5543, 1252, 510, 11, 597, 722, 484], "temperature": 0.0, "avg_logprob": -0.11560061344733605, "compression_ratio": 1.6443514644351465, "no_speech_prob": 5.2548593885148875e-06}, {"id": 105, "seek": 51400, "start": 524.0, "end": 529.0, "text": " around 20 seconds. And there's a definite gap here. I mean, it gets a bit longer,", "tokens": [926, 945, 3949, 13, 400, 456, 311, 257, 25131, 7417, 510, 13, 286, 914, 11, 309, 2170, 257, 857, 2854, 11], "temperature": 0.0, "avg_logprob": -0.11560061344733605, "compression_ratio": 1.6443514644351465, "no_speech_prob": 5.2548593885148875e-06}, {"id": 106, "seek": 51400, "start": 529.0, "end": 534.0, "text": " a small amount, but there's a definite gap here, and then it jumps up to closer", "tokens": [257, 1359, 2372, 11, 457, 456, 311, 257, 25131, 7417, 510, 11, 293, 550, 309, 16704, 493, 281, 4966], "temperature": 0.0, "avg_logprob": -0.11560061344733605, "compression_ratio": 1.6443514644351465, "no_speech_prob": 5.2548593885148875e-06}, {"id": 107, "seek": 51400, "start": 534.0, "end": 539.0, "text": " to 30 seconds. So, maybe we have 40% overhead between the fastest one and the", "tokens": [281, 2217, 3949, 13, 407, 11, 1310, 321, 362, 3356, 4, 19922, 1296, 264, 14573, 472, 293, 264], "temperature": 0.0, "avg_logprob": -0.11560061344733605, "compression_ratio": 1.6443514644351465, "no_speech_prob": 5.2548593885148875e-06}, {"id": 108, "seek": 53900, "start": 539.0, "end": 545.0, "text": " slowest one. It's only 10 runs. It's quite a lot of variation for a benchmark", "tokens": [2964, 377, 472, 13, 467, 311, 787, 1266, 6676, 13, 467, 311, 1596, 257, 688, 295, 12990, 337, 257, 18927], "temperature": 0.0, "avg_logprob": -0.08507075610461536, "compression_ratio": 1.6853448275862069, "no_speech_prob": 3.339926479384303e-06}, {"id": 109, "seek": 53900, "start": 545.0, "end": 549.0, "text": " that we expect will just run like this and not doing anything interesting at", "tokens": [300, 321, 2066, 486, 445, 1190, 411, 341, 293, 406, 884, 1340, 1880, 412], "temperature": 0.0, "avg_logprob": -0.08507075610461536, "compression_ratio": 1.6853448275862069, "no_speech_prob": 3.339926479384303e-06}, {"id": 110, "seek": 53900, "start": 549.0, "end": 554.0, "text": " all. So, we can ask why so much variation. So, now we can actually look and see", "tokens": [439, 13, 407, 11, 321, 393, 1029, 983, 370, 709, 12990, 13, 407, 11, 586, 321, 393, 767, 574, 293, 536], "temperature": 0.0, "avg_logprob": -0.08507075610461536, "compression_ratio": 1.6853448275862069, "no_speech_prob": 3.339926479384303e-06}, {"id": 111, "seek": 53900, "start": 554.0, "end": 560.0, "text": " what's going on at the scheduling level. So, this is the graphs. We have, as I", "tokens": [437, 311, 516, 322, 412, 264, 29055, 1496, 13, 407, 11, 341, 307, 264, 24877, 13, 492, 362, 11, 382, 286], "temperature": 0.0, "avg_logprob": -0.08507075610461536, "compression_ratio": 1.6853448275862069, "no_speech_prob": 3.339926479384303e-06}, {"id": 112, "seek": 53900, "start": 560.0, "end": 565.0, "text": " said, we have the time on the x-axis, and we have the, what's going on on the", "tokens": [848, 11, 321, 362, 264, 565, 322, 264, 2031, 12, 24633, 11, 293, 321, 362, 264, 11, 437, 311, 516, 322, 322, 264], "temperature": 0.0, "avg_logprob": -0.08507075610461536, "compression_ratio": 1.6853448275862069, "no_speech_prob": 3.339926479384303e-06}, {"id": 113, "seek": 56500, "start": 565.0, "end": 569.0, "text": " different cores on the y-axis. What I have, it says socket order on the", "tokens": [819, 24826, 322, 264, 288, 12, 24633, 13, 708, 286, 362, 11, 309, 1619, 19741, 1668, 322, 264], "temperature": 0.0, "avg_logprob": -0.08639565247755784, "compression_ratio": 1.7946768060836502, "no_speech_prob": 7.0693126872356515e-06}, {"id": 114, "seek": 56500, "start": 569.0, "end": 573.0, "text": " different cores. What I've done is, actually on my machine, the numbers go", "tokens": [819, 24826, 13, 708, 286, 600, 1096, 307, 11, 767, 322, 452, 3479, 11, 264, 3547, 352], "temperature": 0.0, "avg_logprob": -0.08639565247755784, "compression_ratio": 1.7946768060836502, "no_speech_prob": 7.0693126872356515e-06}, {"id": 115, "seek": 56500, "start": 573.0, "end": 577.0, "text": " kind of round robin between the different sockets, but I have organized it so that", "tokens": [733, 295, 3098, 3870, 259, 1296, 264, 819, 370, 11984, 11, 457, 286, 362, 9983, 309, 370, 300], "temperature": 0.0, "avg_logprob": -0.08639565247755784, "compression_ratio": 1.7946768060836502, "no_speech_prob": 7.0693126872356515e-06}, {"id": 116, "seek": 56500, "start": 577.0, "end": 581.0, "text": " we have the first socket at the bottom, second socket kind of in the middle, and", "tokens": [321, 362, 264, 700, 19741, 412, 264, 2767, 11, 1150, 19741, 733, 295, 294, 264, 2808, 11, 293], "temperature": 0.0, "avg_logprob": -0.08639565247755784, "compression_ratio": 1.7946768060836502, "no_speech_prob": 7.0693126872356515e-06}, {"id": 117, "seek": 56500, "start": 581.0, "end": 587.0, "text": " so on. It's not very important at this point, though. So, I don't know. We have", "tokens": [370, 322, 13, 467, 311, 406, 588, 1021, 412, 341, 935, 11, 1673, 13, 407, 11, 286, 500, 380, 458, 13, 492, 362], "temperature": 0.0, "avg_logprob": -0.08639565247755784, "compression_ratio": 1.7946768060836502, "no_speech_prob": 7.0693126872356515e-06}, {"id": 118, "seek": 56500, "start": 587.0, "end": 593.0, "text": " a graph and we see what it's doing. So, this is the fastest run. It looks kind of", "tokens": [257, 4295, 293, 321, 536, 437, 309, 311, 884, 13, 407, 11, 341, 307, 264, 14573, 1190, 13, 467, 1542, 733, 295], "temperature": 0.0, "avg_logprob": -0.08639565247755784, "compression_ratio": 1.7946768060836502, "no_speech_prob": 7.0693126872356515e-06}, {"id": 119, "seek": 59300, "start": 593.0, "end": 597.0, "text": " like what we expected. The thing's not moving around. Not much is happening.", "tokens": [411, 437, 321, 5176, 13, 440, 551, 311, 406, 2684, 926, 13, 1726, 709, 307, 2737, 13], "temperature": 0.0, "avg_logprob": -0.09039408617680615, "compression_ratio": 1.6866952789699572, "no_speech_prob": 6.538834441016661e-06}, {"id": 120, "seek": 59300, "start": 597.0, "end": 602.0, "text": " This is a much slower run. So, this previous one was 22 seconds. This next one", "tokens": [639, 307, 257, 709, 14009, 1190, 13, 407, 11, 341, 3894, 472, 390, 5853, 3949, 13, 639, 958, 472], "temperature": 0.0, "avg_logprob": -0.09039408617680615, "compression_ratio": 1.6866952789699572, "no_speech_prob": 6.538834441016661e-06}, {"id": 121, "seek": 59300, "start": 602.0, "end": 607.0, "text": " is 28 seconds. So, that's kind of a big overhead. And here we can see that things", "tokens": [307, 7562, 3949, 13, 407, 11, 300, 311, 733, 295, 257, 955, 19922, 13, 400, 510, 321, 393, 536, 300, 721], "temperature": 0.0, "avg_logprob": -0.09039408617680615, "compression_ratio": 1.6866952789699572, "no_speech_prob": 6.538834441016661e-06}, {"id": 122, "seek": 59300, "start": 607.0, "end": 612.0, "text": " are not going as well at all because, in particular, over here in this region,", "tokens": [366, 406, 516, 382, 731, 412, 439, 570, 11, 294, 1729, 11, 670, 510, 294, 341, 4458, 11], "temperature": 0.0, "avg_logprob": -0.09039408617680615, "compression_ratio": 1.6866952789699572, "no_speech_prob": 6.538834441016661e-06}, {"id": 123, "seek": 59300, "start": 612.0, "end": 621.0, "text": " we have these white spaces. And white spaces means that nothing is happening", "tokens": [321, 362, 613, 2418, 7673, 13, 400, 2418, 7673, 1355, 300, 1825, 307, 2737], "temperature": 0.0, "avg_logprob": -0.09039408617680615, "compression_ratio": 1.6866952789699572, "no_speech_prob": 6.538834441016661e-06}, {"id": 124, "seek": 62100, "start": 621.0, "end": 625.0, "text": " on that core. So, there could be two reasons why nothing is happening. One of", "tokens": [322, 300, 4965, 13, 407, 11, 456, 727, 312, 732, 4112, 983, 1825, 307, 2737, 13, 1485, 295], "temperature": 0.0, "avg_logprob": -0.05855764141519561, "compression_ratio": 1.8566433566433567, "no_speech_prob": 1.5203409930109046e-05}, {"id": 125, "seek": 62100, "start": 625.0, "end": 629.0, "text": " them is that there's nothing to do. So, maybe one of these tasks has gotten way", "tokens": [552, 307, 300, 456, 311, 1825, 281, 360, 13, 407, 11, 1310, 472, 295, 613, 9608, 575, 5768, 636], "temperature": 0.0, "avg_logprob": -0.05855764141519561, "compression_ratio": 1.8566433566433567, "no_speech_prob": 1.5203409930109046e-05}, {"id": 126, "seek": 62100, "start": 629.0, "end": 633.0, "text": " ahead of the other one, and so it needs to sleep and to wait for the others to", "tokens": [2286, 295, 264, 661, 472, 11, 293, 370, 309, 2203, 281, 2817, 293, 281, 1699, 337, 264, 2357, 281], "temperature": 0.0, "avg_logprob": -0.05855764141519561, "compression_ratio": 1.8566433566433567, "no_speech_prob": 1.5203409930109046e-05}, {"id": 127, "seek": 62100, "start": 633.0, "end": 637.0, "text": " finish what they want to do. The more unpleasant reason that nothing is", "tokens": [2413, 437, 436, 528, 281, 360, 13, 440, 544, 29128, 1778, 300, 1825, 307], "temperature": 0.0, "avg_logprob": -0.05855764141519561, "compression_ratio": 1.8566433566433567, "no_speech_prob": 1.5203409930109046e-05}, {"id": 128, "seek": 62100, "start": 637.0, "end": 642.0, "text": " happening is because several of these tasks can be stuck on the same core", "tokens": [2737, 307, 570, 2940, 295, 613, 9608, 393, 312, 5541, 322, 264, 912, 4965], "temperature": 0.0, "avg_logprob": -0.05855764141519561, "compression_ratio": 1.8566433566433567, "no_speech_prob": 1.5203409930109046e-05}, {"id": 129, "seek": 62100, "start": 642.0, "end": 645.0, "text": " and they're going to have to bounce back and forth between each other. And", "tokens": [293, 436, 434, 516, 281, 362, 281, 15894, 646, 293, 5220, 1296, 1184, 661, 13, 400], "temperature": 0.0, "avg_logprob": -0.05855764141519561, "compression_ratio": 1.8566433566433567, "no_speech_prob": 1.5203409930109046e-05}, {"id": 130, "seek": 62100, "start": 645.0, "end": 649.0, "text": " actually, nothing. We have a work conservation problem. Some of the cores", "tokens": [767, 11, 1825, 13, 492, 362, 257, 589, 16185, 1154, 13, 2188, 295, 264, 24826], "temperature": 0.0, "avg_logprob": -0.05855764141519561, "compression_ratio": 1.8566433566433567, "no_speech_prob": 1.5203409930109046e-05}, {"id": 131, "seek": 64900, "start": 649.0, "end": 654.0, "text": " are idle. So, we can see which case we're in by looking at the running", "tokens": [366, 30650, 13, 407, 11, 321, 393, 536, 597, 1389, 321, 434, 294, 538, 1237, 412, 264, 2614], "temperature": 0.0, "avg_logprob": -0.09970644722997615, "compression_ratio": 1.8428571428571427, "no_speech_prob": 4.859936325374292e-06}, {"id": 132, "seek": 64900, "start": 654.0, "end": 660.0, "text": " weighting graph. So, here we have, again, we have our, this time we have the", "tokens": [3364, 278, 4295, 13, 407, 11, 510, 321, 362, 11, 797, 11, 321, 362, 527, 11, 341, 565, 321, 362, 264], "temperature": 0.0, "avg_logprob": -0.09970644722997615, "compression_ratio": 1.8428571428571427, "no_speech_prob": 4.859936325374292e-06}, {"id": 133, "seek": 64900, "start": 660.0, "end": 665.0, "text": " number of tasks on the y-axis, but we have n tasks on n cores, so it's the same.", "tokens": [1230, 295, 9608, 322, 264, 288, 12, 24633, 11, 457, 321, 362, 297, 9608, 322, 297, 24826, 11, 370, 309, 311, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.09970644722997615, "compression_ratio": 1.8428571428571427, "no_speech_prob": 4.859936325374292e-06}, {"id": 134, "seek": 64900, "start": 665.0, "end": 670.0, "text": " At the top, we have a dotted line, which is the number of cores on the machine.", "tokens": [1711, 264, 1192, 11, 321, 362, 257, 37459, 1622, 11, 597, 307, 264, 1230, 295, 24826, 322, 264, 3479, 13], "temperature": 0.0, "avg_logprob": -0.09970644722997615, "compression_ratio": 1.8428571428571427, "no_speech_prob": 4.859936325374292e-06}, {"id": 135, "seek": 64900, "start": 670.0, "end": 675.0, "text": " And then the green lines are things, the number of tasks that are running. So,", "tokens": [400, 550, 264, 3092, 3876, 366, 721, 11, 264, 1230, 295, 9608, 300, 366, 2614, 13, 407, 11], "temperature": 0.0, "avg_logprob": -0.09970644722997615, "compression_ratio": 1.8428571428571427, "no_speech_prob": 4.859936325374292e-06}, {"id": 136, "seek": 67500, "start": 675.0, "end": 679.0, "text": " it's kind of like all the tasks are running all the time, but not exactly.", "tokens": [309, 311, 733, 295, 411, 439, 264, 9608, 366, 2614, 439, 264, 565, 11, 457, 406, 2293, 13], "temperature": 0.0, "avg_logprob": -0.08955268859863282, "compression_ratio": 1.9285714285714286, "no_speech_prob": 1.9328874714119593e-06}, {"id": 137, "seek": 67500, "start": 679.0, "end": 684.0, "text": " There's sometimes when only a very few tasks are running down here. And then", "tokens": [821, 311, 2171, 562, 787, 257, 588, 1326, 9608, 366, 2614, 760, 510, 13, 400, 550], "temperature": 0.0, "avg_logprob": -0.08955268859863282, "compression_ratio": 1.9285714285714286, "no_speech_prob": 1.9328874714119593e-06}, {"id": 138, "seek": 67500, "start": 684.0, "end": 688.0, "text": " we have over here in this situation, this is the place where we had the gaps on", "tokens": [321, 362, 670, 510, 294, 341, 2590, 11, 341, 307, 264, 1081, 689, 321, 632, 264, 15031, 322], "temperature": 0.0, "avg_logprob": -0.08955268859863282, "compression_ratio": 1.9285714285714286, "no_speech_prob": 1.9328874714119593e-06}, {"id": 139, "seek": 67500, "start": 688.0, "end": 694.0, "text": " the other graph. And here we have often, we have like almost all the cores,", "tokens": [264, 661, 4295, 13, 400, 510, 321, 362, 2049, 11, 321, 362, 411, 1920, 439, 264, 24826, 11], "temperature": 0.0, "avg_logprob": -0.08955268859863282, "compression_ratio": 1.9285714285714286, "no_speech_prob": 1.9328874714119593e-06}, {"id": 140, "seek": 67500, "start": 694.0, "end": 699.0, "text": " all the tasks that are running, but not quite. And we have this red lines here,", "tokens": [439, 264, 9608, 300, 366, 2614, 11, 457, 406, 1596, 13, 400, 321, 362, 341, 2182, 3876, 510, 11], "temperature": 0.0, "avg_logprob": -0.08955268859863282, "compression_ratio": 1.9285714285714286, "no_speech_prob": 1.9328874714119593e-06}, {"id": 141, "seek": 67500, "start": 699.0, "end": 702.0, "text": " and so red lines means tasks that are waiting. So, we're in an overload", "tokens": [293, 370, 2182, 3876, 1355, 9608, 300, 366, 3806, 13, 407, 11, 321, 434, 294, 364, 28777], "temperature": 0.0, "avg_logprob": -0.08955268859863282, "compression_ratio": 1.9285714285714286, "no_speech_prob": 1.9328874714119593e-06}, {"id": 142, "seek": 70200, "start": 702.0, "end": 706.0, "text": " situation. So, some tasks have been placed on the same cores as each other,", "tokens": [2590, 13, 407, 11, 512, 9608, 362, 668, 7074, 322, 264, 912, 24826, 382, 1184, 661, 11], "temperature": 0.0, "avg_logprob": -0.09337120819091797, "compression_ratio": 1.91869918699187, "no_speech_prob": 6.852943442936521e-06}, {"id": 143, "seek": 70200, "start": 706.0, "end": 710.0, "text": " and so they have to wait for each other to finish. So, this is kind of more of a", "tokens": [293, 370, 436, 362, 281, 1699, 337, 1184, 661, 281, 2413, 13, 407, 11, 341, 307, 733, 295, 544, 295, 257], "temperature": 0.0, "avg_logprob": -0.09337120819091797, "compression_ratio": 1.91869918699187, "no_speech_prob": 6.852943442936521e-06}, {"id": 144, "seek": 70200, "start": 710.0, "end": 715.0, "text": " problem for this kind of application. So, basically the two problems we have,", "tokens": [1154, 337, 341, 733, 295, 3861, 13, 407, 11, 1936, 264, 732, 2740, 321, 362, 11], "temperature": 0.0, "avg_logprob": -0.09337120819091797, "compression_ratio": 1.91869918699187, "no_speech_prob": 6.852943442936521e-06}, {"id": 145, "seek": 70200, "start": 715.0, "end": 719.0, "text": " we have problem tasks that are moving around, and we have some cores that are", "tokens": [321, 362, 1154, 9608, 300, 366, 2684, 926, 11, 293, 321, 362, 512, 24826, 300, 366], "temperature": 0.0, "avg_logprob": -0.09337120819091797, "compression_ratio": 1.91869918699187, "no_speech_prob": 6.852943442936521e-06}, {"id": 146, "seek": 70200, "start": 719.0, "end": 725.0, "text": " overloaded, and so the tasks don't get to run as much as they ought to be. So,", "tokens": [28777, 292, 11, 293, 370, 264, 9608, 500, 380, 483, 281, 1190, 382, 709, 382, 436, 13416, 281, 312, 13, 407, 11], "temperature": 0.0, "avg_logprob": -0.09337120819091797, "compression_ratio": 1.91869918699187, "no_speech_prob": 6.852943442936521e-06}, {"id": 147, "seek": 70200, "start": 725.0, "end": 728.0, "text": " now what we're going to do is we're going to zoom in to some of these situations", "tokens": [586, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 8863, 294, 281, 512, 295, 613, 6851], "temperature": 0.0, "avg_logprob": -0.09337120819091797, "compression_ratio": 1.91869918699187, "no_speech_prob": 6.852943442936521e-06}, {"id": 148, "seek": 72800, "start": 728.0, "end": 733.0, "text": " and see what the problem could be. So, here's the first one of these situations.", "tokens": [293, 536, 437, 264, 1154, 727, 312, 13, 407, 11, 510, 311, 264, 700, 472, 295, 613, 6851, 13], "temperature": 0.0, "avg_logprob": -0.0974375951857794, "compression_ratio": 1.7167381974248928, "no_speech_prob": 4.288495802029502e-06}, {"id": 149, "seek": 72800, "start": 733.0, "end": 738.0, "text": " If you look over here, basically around three seconds, at this point that I've", "tokens": [759, 291, 574, 670, 510, 11, 1936, 926, 1045, 3949, 11, 412, 341, 935, 300, 286, 600], "temperature": 0.0, "avg_logprob": -0.0974375951857794, "compression_ratio": 1.7167381974248928, "no_speech_prob": 4.288495802029502e-06}, {"id": 150, "seek": 72800, "start": 738.0, "end": 743.0, "text": " circled, you can see we have an orange core, sorry, orange task and then a blue", "tokens": [3510, 1493, 11, 291, 393, 536, 321, 362, 364, 7671, 4965, 11, 2597, 11, 7671, 5633, 293, 550, 257, 3344], "temperature": 0.0, "avg_logprob": -0.0974375951857794, "compression_ratio": 1.7167381974248928, "no_speech_prob": 4.288495802029502e-06}, {"id": 151, "seek": 72800, "start": 743.0, "end": 751.0, "text": " task. And so, something is happening to cause one cores to change to another one.", "tokens": [5633, 13, 400, 370, 11, 746, 307, 2737, 281, 3082, 472, 24826, 281, 1319, 281, 1071, 472, 13], "temperature": 0.0, "avg_logprob": -0.0974375951857794, "compression_ratio": 1.7167381974248928, "no_speech_prob": 4.288495802029502e-06}, {"id": 152, "seek": 72800, "start": 751.0, "end": 754.0, "text": " And if you look up a bit, a bit more, there's some other situations where that", "tokens": [400, 498, 291, 574, 493, 257, 857, 11, 257, 857, 544, 11, 456, 311, 512, 661, 6851, 689, 300], "temperature": 0.0, "avg_logprob": -0.0974375951857794, "compression_ratio": 1.7167381974248928, "no_speech_prob": 4.288495802029502e-06}, {"id": 153, "seek": 75400, "start": 754.0, "end": 762.0, "text": " happens, kind of all in the same area. So, we can look into that in more detail.", "tokens": [2314, 11, 733, 295, 439, 294, 264, 912, 1859, 13, 407, 11, 321, 393, 574, 666, 300, 294, 544, 2607, 13], "temperature": 0.0, "avg_logprob": -0.10201973915100097, "compression_ratio": 1.5757575757575757, "no_speech_prob": 6.240100901777623e-06}, {"id": 154, "seek": 75400, "start": 762.0, "end": 768.0, "text": " If we zoom in a bit, so here I have the command line that you have to write.", "tokens": [759, 321, 8863, 294, 257, 857, 11, 370, 510, 286, 362, 264, 5622, 1622, 300, 291, 362, 281, 2464, 13], "temperature": 0.0, "avg_logprob": -0.10201973915100097, "compression_ratio": 1.5757575757575757, "no_speech_prob": 6.240100901777623e-06}, {"id": 155, "seek": 75400, "start": 768.0, "end": 772.0, "text": " This socket order is to get the cores ordered in a certain way.", "tokens": [639, 19741, 1668, 307, 281, 483, 264, 24826, 8866, 294, 257, 1629, 636, 13], "temperature": 0.0, "avg_logprob": -0.10201973915100097, "compression_ratio": 1.5757575757575757, "no_speech_prob": 6.240100901777623e-06}, {"id": 156, "seek": 75400, "start": 772.0, "end": 777.0, "text": " Min and max are the, we want to go from three seconds to 3.2 seconds.", "tokens": [2829, 293, 11469, 366, 264, 11, 321, 528, 281, 352, 490, 1045, 3949, 281, 805, 13, 17, 3949, 13], "temperature": 0.0, "avg_logprob": -0.10201973915100097, "compression_ratio": 1.5757575757575757, "no_speech_prob": 6.240100901777623e-06}, {"id": 157, "seek": 75400, "start": 777.0, "end": 783.0, "text": " Target UA is it's going to give our application special colors and other", "tokens": [24586, 32765, 307, 309, 311, 516, 281, 976, 527, 3861, 2121, 4577, 293, 661], "temperature": 0.0, "avg_logprob": -0.10201973915100097, "compression_ratio": 1.5757575757575757, "no_speech_prob": 6.240100901777623e-06}, {"id": 158, "seek": 78300, "start": 783.0, "end": 787.0, "text": " things that happen are going to be black. So, then we can see other, if there's", "tokens": [721, 300, 1051, 366, 516, 281, 312, 2211, 13, 407, 11, 550, 321, 393, 536, 661, 11, 498, 456, 311], "temperature": 0.0, "avg_logprob": -0.08488014315770677, "compression_ratio": 1.8212927756653992, "no_speech_prob": 1.1298685421934351e-05}, {"id": 159, "seek": 78300, "start": 787.0, "end": 789.0, "text": " some other strange things that are happening on the machine.", "tokens": [512, 661, 5861, 721, 300, 366, 2737, 322, 264, 3479, 13], "temperature": 0.0, "avg_logprob": -0.08488014315770677, "compression_ratio": 1.8212927756653992, "no_speech_prob": 1.1298685421934351e-05}, {"id": 160, "seek": 78300, "start": 789.0, "end": 794.0, "text": " So, now that we have zoomed in at this level, we can see that things actually are", "tokens": [407, 11, 586, 300, 321, 362, 8863, 292, 294, 412, 341, 1496, 11, 321, 393, 536, 300, 721, 767, 366], "temperature": 0.0, "avg_logprob": -0.08488014315770677, "compression_ratio": 1.8212927756653992, "no_speech_prob": 1.1298685421934351e-05}, {"id": 161, "seek": 78300, "start": 794.0, "end": 798.0, "text": " not as nice as they looked when we were in the zoomed out situation.", "tokens": [406, 382, 1481, 382, 436, 2956, 562, 321, 645, 294, 264, 8863, 292, 484, 2590, 13], "temperature": 0.0, "avg_logprob": -0.08488014315770677, "compression_ratio": 1.8212927756653992, "no_speech_prob": 1.1298685421934351e-05}, {"id": 162, "seek": 78300, "start": 798.0, "end": 803.0, "text": " Here we have like everybody, almost everybody has stopped for a little gap here.", "tokens": [1692, 321, 362, 411, 2201, 11, 1920, 2201, 575, 5936, 337, 257, 707, 7417, 510, 13], "temperature": 0.0, "avg_logprob": -0.08488014315770677, "compression_ratio": 1.8212927756653992, "no_speech_prob": 1.1298685421934351e-05}, {"id": 163, "seek": 78300, "start": 803.0, "end": 806.0, "text": " And then here, this is basically the fourth socket.", "tokens": [400, 550, 510, 11, 341, 307, 1936, 264, 6409, 19741, 13], "temperature": 0.0, "avg_logprob": -0.08488014315770677, "compression_ratio": 1.8212927756653992, "no_speech_prob": 1.1298685421934351e-05}, {"id": 164, "seek": 78300, "start": 806.0, "end": 810.0, "text": " There's a lot of unfortunate things happening up here.", "tokens": [821, 311, 257, 688, 295, 17843, 721, 2737, 493, 510, 13], "temperature": 0.0, "avg_logprob": -0.08488014315770677, "compression_ratio": 1.8212927756653992, "no_speech_prob": 1.1298685421934351e-05}, {"id": 165, "seek": 81000, "start": 810.0, "end": 814.0, "text": " So, we can zoom in a bit more.", "tokens": [407, 11, 321, 393, 8863, 294, 257, 857, 544, 13], "temperature": 0.0, "avg_logprob": -0.10225278954756888, "compression_ratio": 1.7712765957446808, "no_speech_prob": 4.710534085461404e-06}, {"id": 166, "seek": 81000, "start": 814.0, "end": 819.0, "text": " So, now I've zoomed in just on this big vertical line here.", "tokens": [407, 11, 586, 286, 600, 8863, 292, 294, 445, 322, 341, 955, 9429, 1622, 510, 13], "temperature": 0.0, "avg_logprob": -0.10225278954756888, "compression_ratio": 1.7712765957446808, "no_speech_prob": 4.710534085461404e-06}, {"id": 167, "seek": 81000, "start": 819.0, "end": 824.0, "text": " And when we zoom in a bit more, then we start to see that there are some other", "tokens": [400, 562, 321, 8863, 294, 257, 857, 544, 11, 550, 321, 722, 281, 536, 300, 456, 366, 512, 661], "temperature": 0.0, "avg_logprob": -0.10225278954756888, "compression_ratio": 1.7712765957446808, "no_speech_prob": 4.710534085461404e-06}, {"id": 168, "seek": 81000, "start": 824.0, "end": 829.0, "text": " things going on on our machine. So, they're the colored lines and then we have", "tokens": [721, 516, 322, 322, 527, 3479, 13, 407, 11, 436, 434, 264, 14332, 3876, 293, 550, 321, 362], "temperature": 0.0, "avg_logprob": -0.10225278954756888, "compression_ratio": 1.7712765957446808, "no_speech_prob": 4.710534085461404e-06}, {"id": 169, "seek": 81000, "start": 829.0, "end": 833.0, "text": " some little black lines. So, we can try to find out what the little black lines are.", "tokens": [512, 707, 2211, 3876, 13, 407, 11, 321, 393, 853, 281, 915, 484, 437, 264, 707, 2211, 3876, 366, 13], "temperature": 0.0, "avg_logprob": -0.10225278954756888, "compression_ratio": 1.7712765957446808, "no_speech_prob": 4.710534085461404e-06}, {"id": 170, "seek": 83300, "start": 833.0, "end": 840.0, "text": " So, this data graph, it has another option. What are the black lines?", "tokens": [407, 11, 341, 1412, 4295, 11, 309, 575, 1071, 3614, 13, 708, 366, 264, 2211, 3876, 30], "temperature": 0.0, "avg_logprob": -0.10669681400928682, "compression_ratio": 1.6486486486486487, "no_speech_prob": 5.2547634368238505e-06}, {"id": 171, "seek": 83300, "start": 840.0, "end": 845.0, "text": " It has another option where we can have colors to see, it's colored by command.", "tokens": [467, 575, 1071, 3614, 689, 321, 393, 362, 4577, 281, 536, 11, 309, 311, 14332, 538, 5622, 13], "temperature": 0.0, "avg_logprob": -0.10669681400928682, "compression_ratio": 1.6486486486486487, "no_speech_prob": 5.2547634368238505e-06}, {"id": 172, "seek": 83300, "start": 845.0, "end": 849.0, "text": " The colors are chosen not by the PID, but by what the command is.", "tokens": [440, 4577, 366, 8614, 406, 538, 264, 430, 2777, 11, 457, 538, 437, 264, 5622, 307, 13], "temperature": 0.0, "avg_logprob": -0.10669681400928682, "compression_ratio": 1.6486486486486487, "no_speech_prob": 5.2547634368238505e-06}, {"id": 173, "seek": 83300, "start": 849.0, "end": 852.0, "text": " So, mostly we have our command, which is blue, UA.", "tokens": [407, 11, 5240, 321, 362, 527, 5622, 11, 597, 307, 3344, 11, 32765, 13], "temperature": 0.0, "avg_logprob": -0.10669681400928682, "compression_ratio": 1.6486486486486487, "no_speech_prob": 5.2547634368238505e-06}, {"id": 174, "seek": 83300, "start": 852.0, "end": 856.0, "text": " But we have some demons here. So, these are kind of inevitable.", "tokens": [583, 321, 362, 512, 19733, 510, 13, 407, 11, 613, 366, 733, 295, 21451, 13], "temperature": 0.0, "avg_logprob": -0.10669681400928682, "compression_ratio": 1.6486486486486487, "no_speech_prob": 5.2547634368238505e-06}, {"id": 175, "seek": 83300, "start": 856.0, "end": 858.0, "text": " The kernel needs to do some things.", "tokens": [440, 28256, 2203, 281, 360, 512, 721, 13], "temperature": 0.0, "avg_logprob": -0.10669681400928682, "compression_ratio": 1.6486486486486487, "no_speech_prob": 5.2547634368238505e-06}, {"id": 176, "seek": 85800, "start": 858.0, "end": 863.0, "text": " And so, basically, if we jump back here, we can see that if we look, for example,", "tokens": [400, 370, 11, 1936, 11, 498, 321, 3012, 646, 510, 11, 321, 393, 536, 300, 498, 321, 574, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.09567446824980945, "compression_ratio": 1.7027027027027026, "no_speech_prob": 2.5454592105234042e-05}, {"id": 177, "seek": 85800, "start": 863.0, "end": 869.0, "text": " in this place, our task is running along, a demon comes, and then it interrupts our task.", "tokens": [294, 341, 1081, 11, 527, 5633, 307, 2614, 2051, 11, 257, 14283, 1487, 11, 293, 550, 309, 12729, 82, 527, 5633, 13], "temperature": 0.0, "avg_logprob": -0.09567446824980945, "compression_ratio": 1.7027027027027026, "no_speech_prob": 2.5454592105234042e-05}, {"id": 178, "seek": 85800, "start": 869.0, "end": 873.0, "text": " So, our task is not going to be working, but at least our task is staying in the same place.", "tokens": [407, 11, 527, 5633, 307, 406, 516, 281, 312, 1364, 11, 457, 412, 1935, 527, 5633, 307, 7939, 294, 264, 912, 1081, 13], "temperature": 0.0, "avg_logprob": -0.09567446824980945, "compression_ratio": 1.7027027027027026, "no_speech_prob": 2.5454592105234042e-05}, {"id": 179, "seek": 85800, "start": 873.0, "end": 879.0, "text": " And so, nothing extremely horrible happens, but these things get a bit unsynchronized.", "tokens": [400, 370, 11, 1825, 4664, 9263, 2314, 11, 457, 613, 721, 483, 257, 857, 2693, 36420, 1602, 13], "temperature": 0.0, "avg_logprob": -0.09567446824980945, "compression_ratio": 1.7027027027027026, "no_speech_prob": 2.5454592105234042e-05}, {"id": 180, "seek": 85800, "start": 879.0, "end": 882.0, "text": " Some of them get slowed down and so on.", "tokens": [2188, 295, 552, 483, 32057, 760, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.09567446824980945, "compression_ratio": 1.7027027027027026, "no_speech_prob": 2.5454592105234042e-05}, {"id": 181, "seek": 85800, "start": 882.0, "end": 887.0, "text": " So, that's one kind of slowdown that we can have.", "tokens": [407, 11, 300, 311, 472, 733, 295, 2964, 5093, 300, 321, 393, 362, 13], "temperature": 0.0, "avg_logprob": -0.09567446824980945, "compression_ratio": 1.7027027027027026, "no_speech_prob": 2.5454592105234042e-05}, {"id": 182, "seek": 88700, "start": 887.0, "end": 891.0, "text": " But, in principle, it shouldn't have a huge long-term impact.", "tokens": [583, 11, 294, 8665, 11, 309, 4659, 380, 362, 257, 2603, 938, 12, 7039, 2712, 13], "temperature": 0.0, "avg_logprob": -0.10838184858623304, "compression_ratio": 1.7566371681415929, "no_speech_prob": 7.645505320397206e-06}, {"id": 183, "seek": 88700, "start": 891.0, "end": 895.0, "text": " So, now we can move a bit further off to the right.", "tokens": [407, 11, 586, 321, 393, 1286, 257, 857, 3052, 766, 281, 264, 558, 13], "temperature": 0.0, "avg_logprob": -0.10838184858623304, "compression_ratio": 1.7566371681415929, "no_speech_prob": 7.645505320397206e-06}, {"id": 184, "seek": 88700, "start": 895.0, "end": 901.0, "text": " We can see there are some more of these little black things here.", "tokens": [492, 393, 536, 456, 366, 512, 544, 295, 613, 707, 2211, 721, 510, 13], "temperature": 0.0, "avg_logprob": -0.10838184858623304, "compression_ratio": 1.7566371681415929, "no_speech_prob": 7.645505320397206e-06}, {"id": 185, "seek": 88700, "start": 901.0, "end": 904.0, "text": " Here, what we have, here we have an orange task.", "tokens": [1692, 11, 437, 321, 362, 11, 510, 321, 362, 364, 7671, 5633, 13], "temperature": 0.0, "avg_logprob": -0.10838184858623304, "compression_ratio": 1.7566371681415929, "no_speech_prob": 7.645505320397206e-06}, {"id": 186, "seek": 88700, "start": 904.0, "end": 907.0, "text": " Here, we have a black line.", "tokens": [1692, 11, 321, 362, 257, 2211, 1622, 13], "temperature": 0.0, "avg_logprob": -0.10838184858623304, "compression_ratio": 1.7566371681415929, "no_speech_prob": 7.645505320397206e-06}, {"id": 187, "seek": 88700, "start": 907.0, "end": 912.0, "text": " And here, we have another orange task up here that happens sort of at the same thing.", "tokens": [400, 510, 11, 321, 362, 1071, 7671, 5633, 493, 510, 300, 2314, 1333, 295, 412, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.10838184858623304, "compression_ratio": 1.7566371681415929, "no_speech_prob": 7.645505320397206e-06}, {"id": 188, "seek": 88700, "start": 912.0, "end": 915.0, "text": " The same position. It's a little bit off to the right.", "tokens": [440, 912, 2535, 13, 467, 311, 257, 707, 857, 766, 281, 264, 558, 13], "temperature": 0.0, "avg_logprob": -0.10838184858623304, "compression_ratio": 1.7566371681415929, "no_speech_prob": 7.645505320397206e-06}, {"id": 189, "seek": 91500, "start": 915.0, "end": 918.0, "text": " So, what's happening here is we're doing load balancing.", "tokens": [407, 11, 437, 311, 2737, 510, 307, 321, 434, 884, 3677, 22495, 13], "temperature": 0.0, "avg_logprob": -0.09670987676401607, "compression_ratio": 1.7677902621722847, "no_speech_prob": 1.3005636901652906e-05}, {"id": 190, "seek": 91500, "start": 918.0, "end": 922.0, "text": " And so, the kernel thinks, okay, so there are two things going on here.", "tokens": [400, 370, 11, 264, 28256, 7309, 11, 1392, 11, 370, 456, 366, 732, 721, 516, 322, 510, 13], "temperature": 0.0, "avg_logprob": -0.09670987676401607, "compression_ratio": 1.7677902621722847, "no_speech_prob": 1.3005636901652906e-05}, {"id": 191, "seek": 91500, "start": 922.0, "end": 929.0, "text": " We should find one of these many idle cores up here and use one of them to put this task.", "tokens": [492, 820, 915, 472, 295, 613, 867, 30650, 24826, 493, 510, 293, 764, 472, 295, 552, 281, 829, 341, 5633, 13], "temperature": 0.0, "avg_logprob": -0.09670987676401607, "compression_ratio": 1.7677902621722847, "no_speech_prob": 1.3005636901652906e-05}, {"id": 192, "seek": 91500, "start": 929.0, "end": 934.0, "text": " But that's actually quite a poor choice in this case because, basically, in this application,", "tokens": [583, 300, 311, 767, 1596, 257, 4716, 3922, 294, 341, 1389, 570, 11, 1936, 11, 294, 341, 3861, 11], "temperature": 0.0, "avg_logprob": -0.09670987676401607, "compression_ratio": 1.7677902621722847, "no_speech_prob": 1.3005636901652906e-05}, {"id": 193, "seek": 91500, "start": 934.0, "end": 937.0, "text": " we have all the sockets being filled up with all of their tasks.", "tokens": [321, 362, 439, 264, 370, 11984, 885, 6412, 493, 365, 439, 295, 641, 9608, 13], "temperature": 0.0, "avg_logprob": -0.09670987676401607, "compression_ratio": 1.7677902621722847, "no_speech_prob": 1.3005636901652906e-05}, {"id": 194, "seek": 91500, "start": 937.0, "end": 942.0, "text": " And so, by doing this load balancing, we have put an extra task up there on the fourth socket.", "tokens": [400, 370, 11, 538, 884, 341, 3677, 22495, 11, 321, 362, 829, 364, 2857, 5633, 493, 456, 322, 264, 6409, 19741, 13], "temperature": 0.0, "avg_logprob": -0.09670987676401607, "compression_ratio": 1.7677902621722847, "no_speech_prob": 1.3005636901652906e-05}, {"id": 195, "seek": 94200, "start": 942.0, "end": 946.0, "text": " And that's something we will come to regret later, one might say.", "tokens": [400, 300, 311, 746, 321, 486, 808, 281, 10879, 1780, 11, 472, 1062, 584, 13], "temperature": 0.0, "avg_logprob": -0.0927935566818505, "compression_ratio": 1.7130434782608697, "no_speech_prob": 2.045443034148775e-05}, {"id": 196, "seek": 94200, "start": 946.0, "end": 950.0, "text": " Even though it seems okay for the moment.", "tokens": [2754, 1673, 309, 2544, 1392, 337, 264, 1623, 13], "temperature": 0.0, "avg_logprob": -0.0927935566818505, "compression_ratio": 1.7130434782608697, "no_speech_prob": 2.045443034148775e-05}, {"id": 197, "seek": 94200, "start": 950.0, "end": 955.0, "text": " So, what this leads to, though, is, so as I just said,", "tokens": [407, 11, 437, 341, 6689, 281, 11, 1673, 11, 307, 11, 370, 382, 286, 445, 848, 11], "temperature": 0.0, "avg_logprob": -0.0927935566818505, "compression_ratio": 1.7130434782608697, "no_speech_prob": 2.045443034148775e-05}, {"id": 198, "seek": 94200, "start": 955.0, "end": 958.0, "text": " it's going to lead to a cascade of migrations.", "tokens": [309, 311, 516, 281, 1477, 281, 257, 50080, 295, 6186, 12154, 13], "temperature": 0.0, "avg_logprob": -0.0927935566818505, "compression_ratio": 1.7130434782608697, "no_speech_prob": 2.045443034148775e-05}, {"id": 199, "seek": 94200, "start": 958.0, "end": 960.0, "text": " We put something on that task.", "tokens": [492, 829, 746, 322, 300, 5633, 13], "temperature": 0.0, "avg_logprob": -0.0927935566818505, "compression_ratio": 1.7130434782608697, "no_speech_prob": 2.045443034148775e-05}, {"id": 200, "seek": 94200, "start": 960.0, "end": 962.0, "text": " Someone else is going to wake up for that core.", "tokens": [8734, 1646, 307, 516, 281, 6634, 493, 337, 300, 4965, 13], "temperature": 0.0, "avg_logprob": -0.0927935566818505, "compression_ratio": 1.7130434782608697, "no_speech_prob": 2.045443034148775e-05}, {"id": 201, "seek": 94200, "start": 962.0, "end": 964.0, "text": " It will have to go somewhere else.", "tokens": [467, 486, 362, 281, 352, 4079, 1646, 13], "temperature": 0.0, "avg_logprob": -0.0927935566818505, "compression_ratio": 1.7130434782608697, "no_speech_prob": 2.045443034148775e-05}, {"id": 202, "seek": 94200, "start": 964.0, "end": 968.0, "text": " And that other place is someone's going to wake up for that and so on.", "tokens": [400, 300, 661, 1081, 307, 1580, 311, 516, 281, 6634, 493, 337, 300, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.0927935566818505, "compression_ratio": 1.7130434782608697, "no_speech_prob": 2.045443034148775e-05}, {"id": 203, "seek": 96800, "start": 968.0, "end": 975.0, "text": " So, then the third situation, this is actually in the same position.", "tokens": [407, 11, 550, 264, 2636, 2590, 11, 341, 307, 767, 294, 264, 912, 2535, 13], "temperature": 0.0, "avg_logprob": -0.06571644828433082, "compression_ratio": 1.728, "no_speech_prob": 8.396084922424052e-06}, {"id": 204, "seek": 96800, "start": 975.0, "end": 977.0, "text": " We see another situation over here.", "tokens": [492, 536, 1071, 2590, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.06571644828433082, "compression_ratio": 1.728, "no_speech_prob": 8.396084922424052e-06}, {"id": 205, "seek": 96800, "start": 977.0, "end": 982.0, "text": " Here's another case where we are changing from one task to another one.", "tokens": [1692, 311, 1071, 1389, 689, 321, 366, 4473, 490, 472, 5633, 281, 1071, 472, 13], "temperature": 0.0, "avg_logprob": -0.06571644828433082, "compression_ratio": 1.728, "no_speech_prob": 8.396084922424052e-06}, {"id": 206, "seek": 96800, "start": 982.0, "end": 987.0, "text": " But this time, there's no little black dot which is motivating this change somehow.", "tokens": [583, 341, 565, 11, 456, 311, 572, 707, 2211, 5893, 597, 307, 41066, 341, 1319, 6063, 13], "temperature": 0.0, "avg_logprob": -0.06571644828433082, "compression_ratio": 1.728, "no_speech_prob": 8.396084922424052e-06}, {"id": 207, "seek": 96800, "start": 987.0, "end": 989.0, "text": " Nothing strange seems to be happening.", "tokens": [6693, 5861, 2544, 281, 312, 2737, 13], "temperature": 0.0, "avg_logprob": -0.06571644828433082, "compression_ratio": 1.728, "no_speech_prob": 8.396084922424052e-06}, {"id": 208, "seek": 96800, "start": 989.0, "end": 993.0, "text": " It just seems to be happening spontaneously by itself.", "tokens": [467, 445, 2544, 281, 312, 2737, 47632, 538, 2564, 13], "temperature": 0.0, "avg_logprob": -0.06571644828433082, "compression_ratio": 1.728, "no_speech_prob": 8.396084922424052e-06}, {"id": 209, "seek": 96800, "start": 993.0, "end": 996.0, "text": " So, we can look again at the running weighting graph to see what's happening.", "tokens": [407, 11, 321, 393, 574, 797, 412, 264, 2614, 3364, 278, 4295, 281, 536, 437, 311, 2737, 13], "temperature": 0.0, "avg_logprob": -0.06571644828433082, "compression_ratio": 1.728, "no_speech_prob": 8.396084922424052e-06}, {"id": 210, "seek": 99600, "start": 996.0, "end": 998.0, "text": " It's not super easy to see.", "tokens": [467, 311, 406, 1687, 1858, 281, 536, 13], "temperature": 0.0, "avg_logprob": -0.07918665390606075, "compression_ratio": 1.7806691449814127, "no_speech_prob": 5.681704806193011e-06}, {"id": 211, "seek": 99600, "start": 998.0, "end": 1002.0, "text": " But basically what's happening is we have a green line which is below the number of cores.", "tokens": [583, 1936, 437, 311, 2737, 307, 321, 362, 257, 3092, 1622, 597, 307, 2507, 264, 1230, 295, 24826, 13], "temperature": 0.0, "avg_logprob": -0.07918665390606075, "compression_ratio": 1.7806691449814127, "no_speech_prob": 5.681704806193011e-06}, {"id": 212, "seek": 99600, "start": 1002.0, "end": 1004.0, "text": " And we have a red line that's just above it.", "tokens": [400, 321, 362, 257, 2182, 1622, 300, 311, 445, 3673, 309, 13], "temperature": 0.0, "avg_logprob": -0.07918665390606075, "compression_ratio": 1.7806691449814127, "no_speech_prob": 5.681704806193011e-06}, {"id": 213, "seek": 99600, "start": 1004.0, "end": 1006.0, "text": " And again, we have an overload situation.", "tokens": [400, 797, 11, 321, 362, 364, 28777, 2590, 13], "temperature": 0.0, "avg_logprob": -0.07918665390606075, "compression_ratio": 1.7806691449814127, "no_speech_prob": 5.681704806193011e-06}, {"id": 214, "seek": 99600, "start": 1006.0, "end": 1011.0, "text": " So, there's one thread which is actually this orange one here.", "tokens": [407, 11, 456, 311, 472, 7207, 597, 307, 767, 341, 7671, 472, 510, 13], "temperature": 0.0, "avg_logprob": -0.07918665390606075, "compression_ratio": 1.7806691449814127, "no_speech_prob": 5.681704806193011e-06}, {"id": 215, "seek": 99600, "start": 1011.0, "end": 1018.0, "text": " This blue and orange core here, orange tasks are sharing the same core.", "tokens": [639, 3344, 293, 7671, 4965, 510, 11, 7671, 9608, 366, 5414, 264, 912, 4965, 13], "temperature": 0.0, "avg_logprob": -0.07918665390606075, "compression_ratio": 1.7806691449814127, "no_speech_prob": 5.681704806193011e-06}, {"id": 216, "seek": 99600, "start": 1018.0, "end": 1021.0, "text": " And so, they're going to have to bounce back and forth between them.", "tokens": [400, 370, 11, 436, 434, 516, 281, 362, 281, 15894, 646, 293, 5220, 1296, 552, 13], "temperature": 0.0, "avg_logprob": -0.07918665390606075, "compression_ratio": 1.7806691449814127, "no_speech_prob": 5.681704806193011e-06}, {"id": 217, "seek": 99600, "start": 1021.0, "end": 1025.0, "text": " So, we can try to look and see how did we end up with this situation.", "tokens": [407, 11, 321, 393, 853, 281, 574, 293, 536, 577, 630, 321, 917, 493, 365, 341, 2590, 13], "temperature": 0.0, "avg_logprob": -0.07918665390606075, "compression_ratio": 1.7806691449814127, "no_speech_prob": 5.681704806193011e-06}, {"id": 218, "seek": 102500, "start": 1025.0, "end": 1031.0, "text": " So, this here, this is a graph that I made by hand more or less.", "tokens": [407, 11, 341, 510, 11, 341, 307, 257, 4295, 300, 286, 1027, 538, 1011, 544, 420, 1570, 13], "temperature": 0.0, "avg_logprob": -0.07273475646972656, "compression_ratio": 1.5560747663551402, "no_speech_prob": 1.0781534911075141e-05}, {"id": 219, "seek": 102500, "start": 1031.0, "end": 1035.0, "text": " This is just focusing on the two specific cores that we're interested in.", "tokens": [639, 307, 445, 8416, 322, 264, 732, 2685, 24826, 300, 321, 434, 3102, 294, 13], "temperature": 0.0, "avg_logprob": -0.07273475646972656, "compression_ratio": 1.5560747663551402, "no_speech_prob": 1.0781534911075141e-05}, {"id": 220, "seek": 102500, "start": 1035.0, "end": 1037.0, "text": " Here we have this orange task.", "tokens": [1692, 321, 362, 341, 7671, 5633, 13], "temperature": 0.0, "avg_logprob": -0.07273475646972656, "compression_ratio": 1.5560747663551402, "no_speech_prob": 1.0781534911075141e-05}, {"id": 221, "seek": 102500, "start": 1037.0, "end": 1038.0, "text": " It's running along.", "tokens": [467, 311, 2614, 2051, 13], "temperature": 0.0, "avg_logprob": -0.07273475646972656, "compression_ratio": 1.5560747663551402, "no_speech_prob": 1.0781534911075141e-05}, {"id": 222, "seek": 102500, "start": 1038.0, "end": 1041.0, "text": " It prefers to be on this core number 111.", "tokens": [467, 44334, 281, 312, 322, 341, 4965, 1230, 2975, 16, 13], "temperature": 0.0, "avg_logprob": -0.07273475646972656, "compression_ratio": 1.5560747663551402, "no_speech_prob": 1.0781534911075141e-05}, {"id": 223, "seek": 102500, "start": 1041.0, "end": 1043.0, "text": " It then goes to sleep.", "tokens": [467, 550, 1709, 281, 2817, 13], "temperature": 0.0, "avg_logprob": -0.07273475646972656, "compression_ratio": 1.5560747663551402, "no_speech_prob": 1.0781534911075141e-05}, {"id": 224, "seek": 102500, "start": 1043.0, "end": 1047.0, "text": " And then after some time, we move along over here.", "tokens": [400, 550, 934, 512, 565, 11, 321, 1286, 2051, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.07273475646972656, "compression_ratio": 1.5560747663551402, "no_speech_prob": 1.0781534911075141e-05}, {"id": 225, "seek": 102500, "start": 1047.0, "end": 1049.0, "text": " At this point, it wakes up.", "tokens": [1711, 341, 935, 11, 309, 29610, 493, 13], "temperature": 0.0, "avg_logprob": -0.07273475646972656, "compression_ratio": 1.5560747663551402, "no_speech_prob": 1.0781534911075141e-05}, {"id": 226, "seek": 104900, "start": 1049.0, "end": 1055.0, "text": " And we want to figure out it's waking up.", "tokens": [400, 321, 528, 281, 2573, 484, 309, 311, 20447, 493, 13], "temperature": 0.0, "avg_logprob": -0.07297338402789572, "compression_ratio": 1.7713004484304933, "no_speech_prob": 6.437379397539189e-06}, {"id": 227, "seek": 104900, "start": 1055.0, "end": 1059.0, "text": " It's actually the task on core number 68 that is going to wake it up.", "tokens": [467, 311, 767, 264, 5633, 322, 4965, 1230, 23317, 300, 307, 516, 281, 6634, 309, 493, 13], "temperature": 0.0, "avg_logprob": -0.07297338402789572, "compression_ratio": 1.7713004484304933, "no_speech_prob": 6.437379397539189e-06}, {"id": 228, "seek": 104900, "start": 1059.0, "end": 1062.0, "text": " And so, we need to find a place to put it.", "tokens": [400, 370, 11, 321, 643, 281, 915, 257, 1081, 281, 829, 309, 13], "temperature": 0.0, "avg_logprob": -0.07297338402789572, "compression_ratio": 1.7713004484304933, "no_speech_prob": 6.437379397539189e-06}, {"id": 229, "seek": 104900, "start": 1062.0, "end": 1065.0, "text": " So, the obvious place to put it would be on core 111.", "tokens": [407, 11, 264, 6322, 1081, 281, 829, 309, 576, 312, 322, 4965, 2975, 16, 13], "temperature": 0.0, "avg_logprob": -0.07297338402789572, "compression_ratio": 1.7713004484304933, "no_speech_prob": 6.437379397539189e-06}, {"id": 230, "seek": 104900, "start": 1065.0, "end": 1066.0, "text": " That's where it was before.", "tokens": [663, 311, 689, 309, 390, 949, 13], "temperature": 0.0, "avg_logprob": -0.07297338402789572, "compression_ratio": 1.7713004484304933, "no_speech_prob": 6.437379397539189e-06}, {"id": 231, "seek": 104900, "start": 1066.0, "end": 1072.0, "text": " And that core, the important thing is that core is actually not doing anything.", "tokens": [400, 300, 4965, 11, 264, 1021, 551, 307, 300, 4965, 307, 767, 406, 884, 1340, 13], "temperature": 0.0, "avg_logprob": -0.07297338402789572, "compression_ratio": 1.7713004484304933, "no_speech_prob": 6.437379397539189e-06}, {"id": 232, "seek": 104900, "start": 1072.0, "end": 1074.0, "text": " But that's not what happens.", "tokens": [583, 300, 311, 406, 437, 2314, 13], "temperature": 0.0, "avg_logprob": -0.07297338402789572, "compression_ratio": 1.7713004484304933, "no_speech_prob": 6.437379397539189e-06}, {"id": 233, "seek": 104900, "start": 1074.0, "end": 1078.0, "text": " What happens is it gets placed on core number 68.", "tokens": [708, 2314, 307, 309, 2170, 7074, 322, 4965, 1230, 23317, 13], "temperature": 0.0, "avg_logprob": -0.07297338402789572, "compression_ratio": 1.7713004484304933, "no_speech_prob": 6.437379397539189e-06}, {"id": 234, "seek": 107800, "start": 1078.0, "end": 1084.0, "text": " It gets placed on the core of the parent as opposed to the core where it was running before.", "tokens": [467, 2170, 7074, 322, 264, 4965, 295, 264, 2596, 382, 8851, 281, 264, 4965, 689, 309, 390, 2614, 949, 13], "temperature": 0.0, "avg_logprob": -0.06770069485618954, "compression_ratio": 1.6829268292682926, "no_speech_prob": 3.7849019918212434e-06}, {"id": 235, "seek": 107800, "start": 1084.0, "end": 1086.0, "text": " So, this seems very surprising.", "tokens": [407, 11, 341, 2544, 588, 8830, 13], "temperature": 0.0, "avg_logprob": -0.06770069485618954, "compression_ratio": 1.6829268292682926, "no_speech_prob": 3.7849019918212434e-06}, {"id": 236, "seek": 107800, "start": 1086.0, "end": 1091.0, "text": " We expect that we prefer to put cores where it can run immediately.", "tokens": [492, 2066, 300, 321, 4382, 281, 829, 24826, 689, 309, 393, 1190, 4258, 13], "temperature": 0.0, "avg_logprob": -0.06770069485618954, "compression_ratio": 1.6829268292682926, "no_speech_prob": 3.7849019918212434e-06}, {"id": 237, "seek": 107800, "start": 1091.0, "end": 1097.0, "text": " Why does it, for no particular reason, get moved off?", "tokens": [1545, 775, 309, 11, 337, 572, 1729, 1778, 11, 483, 4259, 766, 30], "temperature": 0.0, "avg_logprob": -0.06770069485618954, "compression_ratio": 1.6829268292682926, "no_speech_prob": 3.7849019918212434e-06}, {"id": 238, "seek": 107800, "start": 1097.0, "end": 1102.0, "text": " So, the key to the situation is this mysterious little dot here.", "tokens": [407, 11, 264, 2141, 281, 264, 2590, 307, 341, 13831, 707, 5893, 510, 13], "temperature": 0.0, "avg_logprob": -0.06770069485618954, "compression_ratio": 1.6829268292682926, "no_speech_prob": 3.7849019918212434e-06}, {"id": 239, "seek": 107800, "start": 1102.0, "end": 1107.0, "text": " So, it's a key worker that woke up and took advantage of this empty space so it could run immediately.", "tokens": [407, 11, 309, 311, 257, 2141, 11346, 300, 12852, 493, 293, 1890, 5002, 295, 341, 6707, 1901, 370, 309, 727, 1190, 4258, 13], "temperature": 0.0, "avg_logprob": -0.06770069485618954, "compression_ratio": 1.6829268292682926, "no_speech_prob": 3.7849019918212434e-06}, {"id": 240, "seek": 110700, "start": 1107.0, "end": 1115.0, "text": " And at the time, this is like Linux 5.10 when all of these graphs come from.", "tokens": [400, 412, 264, 565, 11, 341, 307, 411, 18734, 1025, 13, 3279, 562, 439, 295, 613, 24877, 808, 490, 13], "temperature": 0.0, "avg_logprob": -0.1135266001631574, "compression_ratio": 1.591160220994475, "no_speech_prob": 6.5391568568884395e-06}, {"id": 241, "seek": 110700, "start": 1115.0, "end": 1121.0, "text": " At this time, there, basically, there's a decision whenever a core wakes up,", "tokens": [1711, 341, 565, 11, 456, 11, 1936, 11, 456, 311, 257, 3537, 5699, 257, 4965, 29610, 493, 11], "temperature": 0.0, "avg_logprob": -0.1135266001631574, "compression_ratio": 1.591160220994475, "no_speech_prob": 6.5391568568884395e-06}, {"id": 242, "seek": 110700, "start": 1121.0, "end": 1124.0, "text": " should it go on the socket where it was before?", "tokens": [820, 309, 352, 322, 264, 19741, 689, 309, 390, 949, 30], "temperature": 0.0, "avg_logprob": -0.1135266001631574, "compression_ratio": 1.591160220994475, "no_speech_prob": 6.5391568568884395e-06}, {"id": 243, "seek": 110700, "start": 1124.0, "end": 1127.0, "text": " Should it go on the socket of the waker?", "tokens": [6454, 309, 352, 322, 264, 19741, 295, 264, 261, 4003, 30], "temperature": 0.0, "avg_logprob": -0.1135266001631574, "compression_ratio": 1.591160220994475, "no_speech_prob": 6.5391568568884395e-06}, {"id": 244, "seek": 110700, "start": 1127.0, "end": 1129.0, "text": " And there are different sockets in this case.", "tokens": [400, 456, 366, 819, 370, 11984, 294, 341, 1389, 13], "temperature": 0.0, "avg_logprob": -0.1135266001631574, "compression_ratio": 1.591160220994475, "no_speech_prob": 6.5391568568884395e-06}, {"id": 245, "seek": 112900, "start": 1129.0, "end": 1139.0, "text": " And the issue is that when you make that decision, you take into account the load average.", "tokens": [400, 264, 2734, 307, 300, 562, 291, 652, 300, 3537, 11, 291, 747, 666, 2696, 264, 3677, 4274, 13], "temperature": 0.0, "avg_logprob": -0.07022918031570759, "compression_ratio": 1.8269230769230769, "no_speech_prob": 8.93724791239947e-06}, {"id": 246, "seek": 112900, "start": 1139.0, "end": 1144.0, "text": " And the load average is something, is this collected over time,", "tokens": [400, 264, 3677, 4274, 307, 746, 11, 307, 341, 11087, 670, 565, 11], "temperature": 0.0, "avg_logprob": -0.07022918031570759, "compression_ratio": 1.8269230769230769, "no_speech_prob": 8.93724791239947e-06}, {"id": 247, "seek": 112900, "start": 1144.0, "end": 1148.0, "text": " and then the old information gets decreased a bit over time.", "tokens": [293, 550, 264, 1331, 1589, 2170, 24436, 257, 857, 670, 565, 13], "temperature": 0.0, "avg_logprob": -0.07022918031570759, "compression_ratio": 1.8269230769230769, "no_speech_prob": 8.93724791239947e-06}, {"id": 248, "seek": 112900, "start": 1148.0, "end": 1152.0, "text": " And so, because we have this K worker here, the load average is not zero.", "tokens": [400, 370, 11, 570, 321, 362, 341, 591, 11346, 510, 11, 264, 3677, 4274, 307, 406, 4018, 13], "temperature": 0.0, "avg_logprob": -0.07022918031570759, "compression_ratio": 1.8269230769230769, "no_speech_prob": 8.93724791239947e-06}, {"id": 249, "seek": 112900, "start": 1152.0, "end": 1158.0, "text": " And so, this core is seen as being not completely idle, even though it is completely idle.", "tokens": [400, 370, 11, 341, 4965, 307, 1612, 382, 885, 406, 2584, 30650, 11, 754, 1673, 309, 307, 2584, 30650, 13], "temperature": 0.0, "avg_logprob": -0.07022918031570759, "compression_ratio": 1.8269230769230769, "no_speech_prob": 8.93724791239947e-06}, {"id": 250, "seek": 115800, "start": 1158.0, "end": 1165.0, "text": " And so, once, when that situation arises, then there's some kind of competition between the parent,", "tokens": [400, 370, 11, 1564, 11, 562, 300, 2590, 27388, 11, 550, 456, 311, 512, 733, 295, 6211, 1296, 264, 2596, 11], "temperature": 0.0, "avg_logprob": -0.09921927147723258, "compression_ratio": 1.7476190476190476, "no_speech_prob": 8.937602615333162e-06}, {"id": 251, "seek": 115800, "start": 1165.0, "end": 1168.0, "text": " the waker, and the place where you were before.", "tokens": [264, 261, 4003, 11, 293, 264, 1081, 689, 291, 645, 949, 13], "temperature": 0.0, "avg_logprob": -0.09921927147723258, "compression_ratio": 1.7476190476190476, "no_speech_prob": 8.937602615333162e-06}, {"id": 252, "seek": 115800, "start": 1168.0, "end": 1175.0, "text": " And for some reason, this core number 111 is going to lose this competition in this situation.", "tokens": [400, 337, 512, 1778, 11, 341, 4965, 1230, 2975, 16, 307, 516, 281, 3624, 341, 6211, 294, 341, 2590, 13], "temperature": 0.0, "avg_logprob": -0.09921927147723258, "compression_ratio": 1.7476190476190476, "no_speech_prob": 8.937602615333162e-06}, {"id": 253, "seek": 115800, "start": 1175.0, "end": 1179.0, "text": " And so, the kernel thinks that this core down here would be a better place for it,", "tokens": [400, 370, 11, 264, 28256, 7309, 300, 341, 4965, 760, 510, 576, 312, 257, 1101, 1081, 337, 309, 11], "temperature": 0.0, "avg_logprob": -0.09921927147723258, "compression_ratio": 1.7476190476190476, "no_speech_prob": 8.937602615333162e-06}, {"id": 254, "seek": 115800, "start": 1179.0, "end": 1183.0, "text": " which in this case, it definitely is not.", "tokens": [597, 294, 341, 1389, 11, 309, 2138, 307, 406, 13], "temperature": 0.0, "avg_logprob": -0.09921927147723258, "compression_ratio": 1.7476190476190476, "no_speech_prob": 8.937602615333162e-06}, {"id": 255, "seek": 118300, "start": 1183.0, "end": 1188.0, "text": " So, that's where this comes in, there's a little patch, all it does is it checks is", "tokens": [407, 11, 300, 311, 689, 341, 1487, 294, 11, 456, 311, 257, 707, 9972, 11, 439, 309, 775, 307, 309, 13834, 307], "temperature": 0.0, "avg_logprob": -0.09777254428503648, "compression_ratio": 1.738197424892704, "no_speech_prob": 6.5394569901400246e-06}, {"id": 256, "seek": 118300, "start": 1188.0, "end": 1194.0, "text": " if the core where the task was previously, if that is completely idle,", "tokens": [498, 264, 4965, 689, 264, 5633, 390, 8046, 11, 498, 300, 307, 2584, 30650, 11], "temperature": 0.0, "avg_logprob": -0.09777254428503648, "compression_ratio": 1.738197424892704, "no_speech_prob": 6.5394569901400246e-06}, {"id": 257, "seek": 118300, "start": 1194.0, "end": 1200.0, "text": " then just use that instead of considering other possibilities.", "tokens": [550, 445, 764, 300, 2602, 295, 8079, 661, 12178, 13], "temperature": 0.0, "avg_logprob": -0.09777254428503648, "compression_ratio": 1.738197424892704, "no_speech_prob": 6.5394569901400246e-06}, {"id": 258, "seek": 118300, "start": 1200.0, "end": 1204.0, "text": " So, if we apply that patch, then here we have the pink lines here.", "tokens": [407, 11, 498, 321, 3079, 300, 9972, 11, 550, 510, 321, 362, 264, 7022, 3876, 510, 13], "temperature": 0.0, "avg_logprob": -0.09777254428503648, "compression_ratio": 1.738197424892704, "no_speech_prob": 6.5394569901400246e-06}, {"id": 259, "seek": 118300, "start": 1204.0, "end": 1208.0, "text": " So, now we still have a slight increase, we still have our task moving around,", "tokens": [407, 11, 586, 321, 920, 362, 257, 4036, 3488, 11, 321, 920, 362, 527, 5633, 2684, 926, 11], "temperature": 0.0, "avg_logprob": -0.09777254428503648, "compression_ratio": 1.738197424892704, "no_speech_prob": 6.5394569901400246e-06}, {"id": 260, "seek": 118300, "start": 1208.0, "end": 1210.0, "text": " it's not going to solve all the problems,", "tokens": [309, 311, 406, 516, 281, 5039, 439, 264, 2740, 11], "temperature": 0.0, "avg_logprob": -0.09777254428503648, "compression_ratio": 1.738197424892704, "no_speech_prob": 6.5394569901400246e-06}, {"id": 261, "seek": 121000, "start": 1210.0, "end": 1218.0, "text": " but we don't have this big jump, which happens when the overload situation is introduced.", "tokens": [457, 321, 500, 380, 362, 341, 955, 3012, 11, 597, 2314, 562, 264, 28777, 2590, 307, 7268, 13], "temperature": 0.0, "avg_logprob": -0.12026442757135705, "compression_ratio": 1.497737556561086, "no_speech_prob": 8.396286830247846e-06}, {"id": 262, "seek": 121000, "start": 1218.0, "end": 1222.0, "text": " And we can see how they impact on another completely different application.", "tokens": [400, 321, 393, 536, 577, 436, 2712, 322, 1071, 2584, 819, 3861, 13], "temperature": 0.0, "avg_logprob": -0.12026442757135705, "compression_ratio": 1.497737556561086, "no_speech_prob": 8.396286830247846e-06}, {"id": 263, "seek": 121000, "start": 1222.0, "end": 1226.0, "text": " So, this is a Java program, it's part of the DeCapo benchmark suite.", "tokens": [407, 11, 341, 307, 257, 10745, 1461, 11, 309, 311, 644, 295, 264, 1346, 34, 37615, 18927, 14205, 13], "temperature": 0.0, "avg_logprob": -0.12026442757135705, "compression_ratio": 1.497737556561086, "no_speech_prob": 8.396286830247846e-06}, {"id": 264, "seek": 121000, "start": 1226.0, "end": 1234.0, "text": " And this patch causes tasks to kind of have a better chance of remaining where they were before.", "tokens": [400, 341, 9972, 7700, 9608, 281, 733, 295, 362, 257, 1101, 2931, 295, 8877, 689, 436, 645, 949, 13], "temperature": 0.0, "avg_logprob": -0.12026442757135705, "compression_ratio": 1.497737556561086, "no_speech_prob": 8.396286830247846e-06}, {"id": 265, "seek": 123400, "start": 1234.0, "end": 1240.0, "text": " And on this benchmark, what happens after we have the patch is that all the tasks", "tokens": [400, 322, 341, 18927, 11, 437, 2314, 934, 321, 362, 264, 9972, 307, 300, 439, 264, 9608], "temperature": 0.0, "avg_logprob": -0.11318906148274739, "compression_ratio": 1.7490494296577948, "no_speech_prob": 8.526762030669488e-06}, {"id": 266, "seek": 123400, "start": 1240.0, "end": 1244.0, "text": " manage to stay on the same socket, because there's actually not that many of them", "tokens": [3067, 281, 1754, 322, 264, 912, 19741, 11, 570, 456, 311, 767, 406, 300, 867, 295, 552], "temperature": 0.0, "avg_logprob": -0.11318906148274739, "compression_ratio": 1.7490494296577948, "no_speech_prob": 8.526762030669488e-06}, {"id": 267, "seek": 123400, "start": 1244.0, "end": 1246.0, "text": " that run at a time and they fit there nicely.", "tokens": [300, 1190, 412, 257, 565, 293, 436, 3318, 456, 9594, 13], "temperature": 0.0, "avg_logprob": -0.11318906148274739, "compression_ratio": 1.7490494296577948, "no_speech_prob": 8.526762030669488e-06}, {"id": 268, "seek": 123400, "start": 1246.0, "end": 1251.0, "text": " Previously, they were tending to kind of move over the entire machine.", "tokens": [33606, 11, 436, 645, 256, 2029, 281, 733, 295, 1286, 670, 264, 2302, 3479, 13], "temperature": 0.0, "avg_logprob": -0.11318906148274739, "compression_ratio": 1.7490494296577948, "no_speech_prob": 8.526762030669488e-06}, {"id": 269, "seek": 123400, "start": 1251.0, "end": 1257.0, "text": " And we have here much like more than 20 seconds between the fastest and slowest here,", "tokens": [400, 321, 362, 510, 709, 411, 544, 813, 945, 3949, 1296, 264, 14573, 293, 2964, 377, 510, 11], "temperature": 0.0, "avg_logprob": -0.11318906148274739, "compression_ratio": 1.7490494296577948, "no_speech_prob": 8.526762030669488e-06}, {"id": 270, "seek": 123400, "start": 1257.0, "end": 1262.0, "text": " we have a much more uniform running time, and obviously the running time is also much faster.", "tokens": [321, 362, 257, 709, 544, 9452, 2614, 565, 11, 293, 2745, 264, 2614, 565, 307, 611, 709, 4663, 13], "temperature": 0.0, "avg_logprob": -0.11318906148274739, "compression_ratio": 1.7490494296577948, "no_speech_prob": 8.526762030669488e-06}, {"id": 271, "seek": 126200, "start": 1262.0, "end": 1265.0, "text": " So, it had multiple benefits.", "tokens": [407, 11, 309, 632, 3866, 5311, 13], "temperature": 0.0, "avg_logprob": -0.06623397202327333, "compression_ratio": 1.788679245283019, "no_speech_prob": 1.0775338523671962e-05}, {"id": 272, "seek": 126200, "start": 1265.0, "end": 1270.0, "text": " So, in conclusion, if you want to understand what your scheduler is really doing,", "tokens": [407, 11, 294, 10063, 11, 498, 291, 528, 281, 1223, 437, 428, 12000, 260, 307, 534, 884, 11], "temperature": 0.0, "avg_logprob": -0.06623397202327333, "compression_ratio": 1.788679245283019, "no_speech_prob": 1.0775338523671962e-05}, {"id": 273, "seek": 126200, "start": 1270.0, "end": 1273.0, "text": " you have to actually look and see what your scheduler is really doing.", "tokens": [291, 362, 281, 767, 574, 293, 536, 437, 428, 12000, 260, 307, 534, 884, 13], "temperature": 0.0, "avg_logprob": -0.06623397202327333, "compression_ratio": 1.788679245283019, "no_speech_prob": 1.0775338523671962e-05}, {"id": 274, "seek": 126200, "start": 1273.0, "end": 1278.0, "text": " Just seeing that the number, now it's faster, now it's slower, something like that,", "tokens": [1449, 2577, 300, 264, 1230, 11, 586, 309, 311, 4663, 11, 586, 309, 311, 14009, 11, 746, 411, 300, 11], "temperature": 0.0, "avg_logprob": -0.06623397202327333, "compression_ratio": 1.788679245283019, "no_speech_prob": 1.0775338523671962e-05}, {"id": 275, "seek": 126200, "start": 1278.0, "end": 1281.0, "text": " it doesn't really give you a good understanding of what's going on.", "tokens": [309, 1177, 380, 534, 976, 291, 257, 665, 3701, 295, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.06623397202327333, "compression_ratio": 1.788679245283019, "no_speech_prob": 1.0775338523671962e-05}, {"id": 276, "seek": 126200, "start": 1281.0, "end": 1285.0, "text": " Different perspectives, we found that it provides different kinds of information.", "tokens": [20825, 16766, 11, 321, 1352, 300, 309, 6417, 819, 3685, 295, 1589, 13], "temperature": 0.0, "avg_logprob": -0.06623397202327333, "compression_ratio": 1.788679245283019, "no_speech_prob": 1.0775338523671962e-05}, {"id": 277, "seek": 126200, "start": 1285.0, "end": 1288.0, "text": " The running rating graph is actually very coarse-grained,", "tokens": [440, 2614, 10990, 4295, 307, 767, 588, 39312, 12, 20735, 2001, 11], "temperature": 0.0, "avg_logprob": -0.06623397202327333, "compression_ratio": 1.788679245283019, "no_speech_prob": 1.0775338523671962e-05}, {"id": 278, "seek": 128800, "start": 1288.0, "end": 1293.0, "text": " but it actually sometimes can show you like the problem is exactly in this region", "tokens": [457, 309, 767, 2171, 393, 855, 291, 411, 264, 1154, 307, 2293, 294, 341, 4458], "temperature": 0.0, "avg_logprob": -0.10727337428501674, "compression_ratio": 1.6124031007751938, "no_speech_prob": 4.564501523418585e-06}, {"id": 279, "seek": 128800, "start": 1293.0, "end": 1296.0, "text": " because there's overload in this region.", "tokens": [570, 456, 311, 28777, 294, 341, 4458, 13], "temperature": 0.0, "avg_logprob": -0.10727337428501674, "compression_ratio": 1.6124031007751938, "no_speech_prob": 4.564501523418585e-06}, {"id": 280, "seek": 128800, "start": 1296.0, "end": 1300.0, "text": " So, we have our two tools, data graph, what's going on at what time,", "tokens": [407, 11, 321, 362, 527, 732, 3873, 11, 1412, 4295, 11, 437, 311, 516, 322, 412, 437, 565, 11], "temperature": 0.0, "avg_logprob": -0.10727337428501674, "compression_ratio": 1.6124031007751938, "no_speech_prob": 4.564501523418585e-06}, {"id": 281, "seek": 128800, "start": 1300.0, "end": 1304.0, "text": " and running weighting, how much is happening at each point in time.", "tokens": [293, 2614, 3364, 278, 11, 577, 709, 307, 2737, 412, 1184, 935, 294, 565, 13], "temperature": 0.0, "avg_logprob": -0.10727337428501674, "compression_ratio": 1.6124031007751938, "no_speech_prob": 4.564501523418585e-06}, {"id": 282, "seek": 128800, "start": 1304.0, "end": 1308.0, "text": " In future work, these graphs are a little bit slow to generate", "tokens": [682, 2027, 589, 11, 613, 24877, 366, 257, 707, 857, 2964, 281, 8460], "temperature": 0.0, "avg_logprob": -0.10727337428501674, "compression_ratio": 1.6124031007751938, "no_speech_prob": 4.564501523418585e-06}, {"id": 283, "seek": 128800, "start": 1308.0, "end": 1314.0, "text": " because we, at the moment for technical reasons, we go through PostScript and then go to PDF.", "tokens": [570, 321, 11, 412, 264, 1623, 337, 6191, 4112, 11, 321, 352, 807, 10223, 14237, 293, 550, 352, 281, 17752, 13], "temperature": 0.0, "avg_logprob": -0.10727337428501674, "compression_ratio": 1.6124031007751938, "no_speech_prob": 4.564501523418585e-06}, {"id": 284, "seek": 131400, "start": 1314.0, "end": 1319.0, "text": " So, it'd be nice to be able to generate them more quickly to be a bit more interactive looking.", "tokens": [407, 11, 309, 1116, 312, 1481, 281, 312, 1075, 281, 8460, 552, 544, 2661, 281, 312, 257, 857, 544, 15141, 1237, 13], "temperature": 0.0, "avg_logprob": -0.06668857097625733, "compression_ratio": 1.6223175965665235, "no_speech_prob": 6.044766450941097e-06}, {"id": 285, "seek": 131400, "start": 1319.0, "end": 1323.0, "text": " And also, as I mentioned in the beginning, I've made lots of tools.", "tokens": [400, 611, 11, 382, 286, 2835, 294, 264, 2863, 11, 286, 600, 1027, 3195, 295, 3873, 13], "temperature": 0.0, "avg_logprob": -0.06668857097625733, "compression_ratio": 1.6223175965665235, "no_speech_prob": 6.044766450941097e-06}, {"id": 286, "seek": 131400, "start": 1323.0, "end": 1326.0, "text": " If these tools could become a bit more configurable,", "tokens": [759, 613, 3873, 727, 1813, 257, 857, 544, 22192, 712, 11], "temperature": 0.0, "avg_logprob": -0.06668857097625733, "compression_ratio": 1.6223175965665235, "no_speech_prob": 6.044766450941097e-06}, {"id": 287, "seek": 131400, "start": 1326.0, "end": 1330.0, "text": " then maybe I wouldn't have to restart the implementation each time", "tokens": [550, 1310, 286, 2759, 380, 362, 281, 21022, 264, 11420, 1184, 565], "temperature": 0.0, "avg_logprob": -0.06668857097625733, "compression_ratio": 1.6223175965665235, "no_speech_prob": 6.044766450941097e-06}, {"id": 288, "seek": 131400, "start": 1330.0, "end": 1332.0, "text": " and it'd be more useful for other people.", "tokens": [293, 309, 1116, 312, 544, 4420, 337, 661, 561, 13], "temperature": 0.0, "avg_logprob": -0.06668857097625733, "compression_ratio": 1.6223175965665235, "no_speech_prob": 6.044766450941097e-06}, {"id": 289, "seek": 131400, "start": 1332.0, "end": 1337.0, "text": " So, everything is publicly available.", "tokens": [407, 11, 1203, 307, 14843, 2435, 13], "temperature": 0.0, "avg_logprob": -0.06668857097625733, "compression_ratio": 1.6223175965665235, "no_speech_prob": 6.044766450941097e-06}, {"id": 290, "seek": 131400, "start": 1337.0, "end": 1339.0, "text": " So, thank you.", "tokens": [407, 11, 1309, 291, 13], "temperature": 0.0, "avg_logprob": -0.06668857097625733, "compression_ratio": 1.6223175965665235, "no_speech_prob": 6.044766450941097e-06}, {"id": 291, "seek": 133900, "start": 1339.0, "end": 1346.0, "text": " Thank you.", "tokens": [1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.15115289030403928, "compression_ratio": 1.4109589041095891, "no_speech_prob": 0.0021852252539247274}, {"id": 292, "seek": 133900, "start": 1346.0, "end": 1354.0, "text": " We have time for one or two questions.", "tokens": [492, 362, 565, 337, 472, 420, 732, 1651, 13], "temperature": 0.0, "avg_logprob": -0.15115289030403928, "compression_ratio": 1.4109589041095891, "no_speech_prob": 0.0021852252539247274}, {"id": 293, "seek": 133900, "start": 1354.0, "end": 1356.0, "text": " Thanks for the talk.", "tokens": [2561, 337, 264, 751, 13], "temperature": 0.0, "avg_logprob": -0.15115289030403928, "compression_ratio": 1.4109589041095891, "no_speech_prob": 0.0021852252539247274}, {"id": 294, "seek": 133900, "start": 1356.0, "end": 1358.0, "text": " I have two questions, basically.", "tokens": [286, 362, 732, 1651, 11, 1936, 13], "temperature": 0.0, "avg_logprob": -0.15115289030403928, "compression_ratio": 1.4109589041095891, "no_speech_prob": 0.0021852252539247274}, {"id": 295, "seek": 133900, "start": 1358.0, "end": 1366.0, "text": " Do you have a solution to visualize the latencies due to cache misses, for example, after a migration?", "tokens": [1144, 291, 362, 257, 3827, 281, 23273, 264, 4465, 6464, 3462, 281, 19459, 29394, 11, 337, 1365, 11, 934, 257, 17011, 30], "temperature": 0.0, "avg_logprob": -0.15115289030403928, "compression_ratio": 1.4109589041095891, "no_speech_prob": 0.0021852252539247274}, {"id": 296, "seek": 136600, "start": 1366.0, "end": 1371.0, "text": " The second one is, do you have a way to visualize when tasks are synchronizing", "tokens": [440, 1150, 472, 307, 11, 360, 291, 362, 257, 636, 281, 23273, 562, 9608, 366, 19331, 3319], "temperature": 0.0, "avg_logprob": -0.11885217998338782, "compression_ratio": 1.5069124423963134, "no_speech_prob": 0.0003454211400821805}, {"id": 297, "seek": 136600, "start": 1371.0, "end": 1375.0, "text": " on the mutex, for example, that also can bring some latencies?", "tokens": [322, 264, 24523, 87, 11, 337, 1365, 11, 300, 611, 393, 1565, 512, 4465, 6464, 30], "temperature": 0.0, "avg_logprob": -0.11885217998338782, "compression_ratio": 1.5069124423963134, "no_speech_prob": 0.0003454211400821805}, {"id": 298, "seek": 136600, "start": 1375.0, "end": 1379.0, "text": " So, no, we haven't done anything for cache misses.", "tokens": [407, 11, 572, 11, 321, 2378, 380, 1096, 1340, 337, 19459, 29394, 13], "temperature": 0.0, "avg_logprob": -0.11885217998338782, "compression_ratio": 1.5069124423963134, "no_speech_prob": 0.0003454211400821805}, {"id": 299, "seek": 136600, "start": 1379.0, "end": 1384.0, "text": " It could be interesting.", "tokens": [467, 727, 312, 1880, 13], "temperature": 0.0, "avg_logprob": -0.11885217998338782, "compression_ratio": 1.5069124423963134, "no_speech_prob": 0.0003454211400821805}, {"id": 300, "seek": 136600, "start": 1384.0, "end": 1389.0, "text": " I mean, I have another tool which deals with events,", "tokens": [286, 914, 11, 286, 362, 1071, 2290, 597, 11215, 365, 3931, 11], "temperature": 0.0, "avg_logprob": -0.11885217998338782, "compression_ratio": 1.5069124423963134, "no_speech_prob": 0.0003454211400821805}, {"id": 301, "seek": 136600, "start": 1389.0, "end": 1394.0, "text": " and I think there's some way to add events to datagraphs", "tokens": [293, 286, 519, 456, 311, 512, 636, 281, 909, 3931, 281, 1137, 559, 2662, 82], "temperature": 0.0, "avg_logprob": -0.11885217998338782, "compression_ratio": 1.5069124423963134, "no_speech_prob": 0.0003454211400821805}, {"id": 302, "seek": 139400, "start": 1394.0, "end": 1398.0, "text": " and maybe you could see when different locking operations are happening.", "tokens": [293, 1310, 291, 727, 536, 562, 819, 23954, 7705, 366, 2737, 13], "temperature": 0.0, "avg_logprob": -0.08592499809703608, "compression_ratio": 1.5311004784688995, "no_speech_prob": 0.00043082598131150007}, {"id": 303, "seek": 139400, "start": 1398.0, "end": 1400.0, "text": " I mean, I definitely think that's useful.", "tokens": [286, 914, 11, 286, 2138, 519, 300, 311, 4420, 13], "temperature": 0.0, "avg_logprob": -0.08592499809703608, "compression_ratio": 1.5311004784688995, "no_speech_prob": 0.00043082598131150007}, {"id": 304, "seek": 139400, "start": 1400.0, "end": 1403.0, "text": " I don't think the support is as great as one might like at the moment,", "tokens": [286, 500, 380, 519, 264, 1406, 307, 382, 869, 382, 472, 1062, 411, 412, 264, 1623, 11], "temperature": 0.0, "avg_logprob": -0.08592499809703608, "compression_ratio": 1.5311004784688995, "no_speech_prob": 0.00043082598131150007}, {"id": 305, "seek": 139400, "start": 1403.0, "end": 1407.0, "text": " but it's definitely an important thing.", "tokens": [457, 309, 311, 2138, 364, 1021, 551, 13], "temperature": 0.0, "avg_logprob": -0.08592499809703608, "compression_ratio": 1.5311004784688995, "no_speech_prob": 0.00043082598131150007}, {"id": 306, "seek": 139400, "start": 1407.0, "end": 1414.0, "text": " I have one more question.", "tokens": [286, 362, 472, 544, 1168, 13], "temperature": 0.0, "avg_logprob": -0.08592499809703608, "compression_ratio": 1.5311004784688995, "no_speech_prob": 0.00043082598131150007}, {"id": 307, "seek": 139400, "start": 1414.0, "end": 1416.0, "text": " Hello, Julien.", "tokens": [2425, 11, 7174, 1053, 13], "temperature": 0.0, "avg_logprob": -0.08592499809703608, "compression_ratio": 1.5311004784688995, "no_speech_prob": 0.00043082598131150007}, {"id": 308, "seek": 139400, "start": 1416.0, "end": 1423.0, "text": " I was wondering, is there a way to show the CPU state", "tokens": [286, 390, 6359, 11, 307, 456, 257, 636, 281, 855, 264, 13199, 1785], "temperature": 0.0, "avg_logprob": -0.08592499809703608, "compression_ratio": 1.5311004784688995, "no_speech_prob": 0.00043082598131150007}, {"id": 309, "seek": 142300, "start": 1423.0, "end": 1425.0, "text": " at the time you are printing the time?", "tokens": [412, 264, 565, 291, 366, 14699, 264, 565, 30], "temperature": 0.0, "avg_logprob": -0.1885528564453125, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.0005362494266591966}, {"id": 310, "seek": 142300, "start": 1425.0, "end": 1429.0, "text": " Because your graph is making the assumption that,", "tokens": [1436, 428, 4295, 307, 1455, 264, 15302, 300, 11], "temperature": 0.0, "avg_logprob": -0.1885528564453125, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.0005362494266591966}, {"id": 311, "seek": 142300, "start": 1429.0, "end": 1433.0, "text": " typically, the CPU frequency or whatever is stable over time.", "tokens": [5850, 11, 264, 13199, 7893, 420, 2035, 307, 8351, 670, 565, 13], "temperature": 0.0, "avg_logprob": -0.1885528564453125, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.0005362494266591966}, {"id": 312, "seek": 142300, "start": 1433.0, "end": 1438.0, "text": " It would be very interesting to know the physical state of the processor", "tokens": [467, 576, 312, 588, 1880, 281, 458, 264, 4001, 1785, 295, 264, 15321], "temperature": 0.0, "avg_logprob": -0.1885528564453125, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.0005362494266591966}, {"id": 313, "seek": 142300, "start": 1438.0, "end": 1444.0, "text": " at the time we are printing, because maybe the task is running on a faster...", "tokens": [412, 264, 565, 321, 366, 14699, 11, 570, 1310, 264, 5633, 307, 2614, 322, 257, 4663, 485], "temperature": 0.0, "avg_logprob": -0.1885528564453125, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.0005362494266591966}, {"id": 314, "seek": 142300, "start": 1444.0, "end": 1448.0, "text": " The CPU frequency is higher on one cause than the other.", "tokens": [440, 13199, 7893, 307, 2946, 322, 472, 3082, 813, 264, 661, 13], "temperature": 0.0, "avg_logprob": -0.1885528564453125, "compression_ratio": 1.6651162790697673, "no_speech_prob": 0.0005362494266591966}, {"id": 315, "seek": 144800, "start": 1448.0, "end": 1453.0, "text": " So to visualize that this application is running on a fast or slow CPU", "tokens": [407, 281, 23273, 300, 341, 3861, 307, 2614, 322, 257, 2370, 420, 2964, 13199], "temperature": 0.0, "avg_logprob": -0.165061616897583, "compression_ratio": 1.7446043165467626, "no_speech_prob": 5.373152453103103e-05}, {"id": 316, "seek": 144800, "start": 1453.0, "end": 1455.0, "text": " could be very interesting to know the...", "tokens": [727, 312, 588, 1880, 281, 458, 264, 485], "temperature": 0.0, "avg_logprob": -0.165061616897583, "compression_ratio": 1.7446043165467626, "no_speech_prob": 5.373152453103103e-05}, {"id": 317, "seek": 144800, "start": 1455.0, "end": 1458.0, "text": " Actually, the tool does that, but the unfortunate thing,", "tokens": [5135, 11, 264, 2290, 775, 300, 11, 457, 264, 17843, 551, 11], "temperature": 0.0, "avg_logprob": -0.165061616897583, "compression_ratio": 1.7446043165467626, "no_speech_prob": 5.373152453103103e-05}, {"id": 318, "seek": 144800, "start": 1458.0, "end": 1462.0, "text": " I didn't talk about it because you have to actually go and add a line,", "tokens": [286, 994, 380, 751, 466, 309, 570, 291, 362, 281, 767, 352, 293, 909, 257, 1622, 11], "temperature": 0.0, "avg_logprob": -0.165061616897583, "compression_ratio": 1.7446043165467626, "no_speech_prob": 5.373152453103103e-05}, {"id": 319, "seek": 144800, "start": 1462.0, "end": 1465.0, "text": " a trace print K line to your kernel to actually print out that information,", "tokens": [257, 13508, 4482, 591, 1622, 281, 428, 28256, 281, 767, 4482, 484, 300, 1589, 11], "temperature": 0.0, "avg_logprob": -0.165061616897583, "compression_ratio": 1.7446043165467626, "no_speech_prob": 5.373152453103103e-05}, {"id": 320, "seek": 144800, "start": 1465.0, "end": 1469.0, "text": " because it doesn't exist anywhere in the kernel.", "tokens": [570, 309, 1177, 380, 2514, 4992, 294, 264, 28256, 13], "temperature": 0.0, "avg_logprob": -0.165061616897583, "compression_ratio": 1.7446043165467626, "no_speech_prob": 5.373152453103103e-05}, {"id": 321, "seek": 144800, "start": 1469.0, "end": 1472.0, "text": " So that's the only issue, but actually the tool,", "tokens": [407, 300, 311, 264, 787, 2734, 11, 457, 767, 264, 2290, 11], "temperature": 0.0, "avg_logprob": -0.165061616897583, "compression_ratio": 1.7446043165467626, "no_speech_prob": 5.373152453103103e-05}, {"id": 322, "seek": 144800, "start": 1472.0, "end": 1476.0, "text": " once you print it out in the proper format, it actually does everything", "tokens": [1564, 291, 4482, 309, 484, 294, 264, 2296, 7877, 11, 309, 767, 775, 1203], "temperature": 0.0, "avg_logprob": -0.165061616897583, "compression_ratio": 1.7446043165467626, "no_speech_prob": 5.373152453103103e-05}, {"id": 323, "seek": 147600, "start": 1476.0, "end": 1479.0, "text": " and it can show you just the frequencies,", "tokens": [293, 309, 393, 855, 291, 445, 264, 20250, 11], "temperature": 0.0, "avg_logprob": -0.10395004235061944, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.00013900833437219262}, {"id": 324, "seek": 147600, "start": 1479.0, "end": 1482.0, "text": " so you can see the different colors for how fast it's going.", "tokens": [370, 291, 393, 536, 264, 819, 4577, 337, 577, 2370, 309, 311, 516, 13], "temperature": 0.0, "avg_logprob": -0.10395004235061944, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.00013900833437219262}, {"id": 325, "seek": 147600, "start": 1482.0, "end": 1487.0, "text": " You can also see the merged thing where you have the frequency in one line", "tokens": [509, 393, 611, 536, 264, 36427, 551, 689, 291, 362, 264, 7893, 294, 472, 1622], "temperature": 0.0, "avg_logprob": -0.10395004235061944, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.00013900833437219262}, {"id": 326, "seek": 147600, "start": 1487.0, "end": 1492.0, "text": " and you have the activity in the other line.", "tokens": [293, 291, 362, 264, 5191, 294, 264, 661, 1622, 13], "temperature": 0.0, "avg_logprob": -0.10395004235061944, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.00013900833437219262}, {"id": 327, "seek": 147600, "start": 1492.0, "end": 1494.0, "text": " Sorry, we're out of time.", "tokens": [4919, 11, 321, 434, 484, 295, 565, 13], "temperature": 0.0, "avg_logprob": -0.10395004235061944, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.00013900833437219262}, {"id": 328, "seek": 147600, "start": 1494.0, "end": 1496.0, "text": " Thank you for the talk, thank you for the questions.", "tokens": [1044, 291, 337, 264, 751, 11, 1309, 291, 337, 264, 1651, 13], "temperature": 0.0, "avg_logprob": -0.10395004235061944, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.00013900833437219262}, {"id": 329, "seek": 149600, "start": 1496.0, "end": 1507.0, "text": " We can't take all questions, but I'm sure you can find Julia later.", "tokens": [50364, 492, 393, 380, 747, 439, 1651, 11, 457, 286, 478, 988, 291, 393, 915, 18551, 1780, 13, 50914], "temperature": 0.0, "avg_logprob": -0.18692309856414796, "compression_ratio": 0.9305555555555556, "no_speech_prob": 0.0002631452225614339}], "language": "en"}