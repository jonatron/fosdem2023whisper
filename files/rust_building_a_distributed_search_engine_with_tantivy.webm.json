{"text": " I'll just yell. But yeah, so this is effectively my talk. It started out as a really big thing and then I realised 40 minutes wasn't actually that much time and so we sort of had to compress it down into a bit of a slightly smaller talk but hopefully covering the most interesting points in my opinion. So a bit about me. I'm Harrison. I come from London. I live in London and I work for QuickWit where as Paul has said we build basically a distributed search engine for logs. I am the creator of LNX which is a slightly different design of search engine probably more akin to something like Elasticsearch or Algolia for all your lovely e-commerce websites. And you can contact me at Harrison at QuickWit.io. A little bit about LNX since this is basically the origin story of this talk really. It's a search engine built on top of Tantrave. It's akin to Elasticsearch or Algolia as I've said. It's aimed at user facing search. That's things like your e-commerce websites, your Netflix streaming platforms, things like that. It's not aimed to be your cost effective log search engine. It doesn't really handle those hundreds of terabytes a day type workloads but it will handle thousands of queries a second per core. It's very easily configurable. It's designed to be really fast out of the box because it uses Tantrave and it has an indexing throughput of about 30 to 60 megabytes a second on reasonable hardware. With high availability coming soon which is the presence of this talk. So what is user facing search? I've stolen Crunchy Well's website and I've typed some bad spelling in there and you see that a lot of the top results actually account for the fact that I can't spell. That's basically the biggest principle with these user facing search engines is you have this concept of typo tolerance. This is a really good thing for users because users can't spell. The downside of this is that it has a lot of CPU time when we're checking those additional words and it makes things a lot more complicated and often documents are mutable and a lot of other things but also when you have these nice search experiences and you want no latency something called search as you type has become more popular now and that means your amount of search as you're doing for a single user is increasing several times over because now every key stroke you press is a search versus typing it all in one go hitting enter user gets a bunch of results back goes oh no I've spelt something wrong or I can't see what I want on here so I'm going to type it again. And so that is effectively the principle of these search engines. You see we have Algolia at the bottom which is a very common one which I think most people know very popular for document searching. But you know we decided hey we don't want to use one of these pre-built systems we don't want to use Elasticsearch that's big that's scary I don't like it. We don't want to use Algolia because I don't have that much money I'm just a lonely paid software developer I can't be spending thousands of pounds on that. And we look at some of the others but we're going there we're just going to write it ourselves and that's where we have a little look because we hear something about Tantavi we hear something about Rust that being blazingly fast as all things must be and so we go okay I like this I like what it says it says yeah Apache Lucene I think I've heard that before somewhere written in Rust I think I've definitely heard that before. And so we take a little look at what it is and it is effectively akin to Lucene which if you don't know what that is it's a full text search engine as it's called. Tantavi in particular supports things like BM25 scoring which is just a fancy way of saying what words are relevant to this query it supports something called incremental indexing which basically just means you don't have to re-index all of your documents every time you change one thing. You have fasted search you have range queries and we have things like JSON fields which allow for a schemeless indexing as such. You can do aggregations which have some limitations in particular around JSON fields being a little bit limited but in the biggest thing is it has a cheesy logo with a horse which I believe Paul drew himself so I think that needs a clap on its own. But there are other features which I just haven't yes. But there are more features which I couldn't fit on this slide and timers of the essence. So you might be wondering what the basic implementation of Tantavi looks like and because it's a library it's actually really quite simple to do. So we have a couple of core things starting at the top is we define what's called a schema. Since Tantavi was originally a schema based system still is we need some way of telling Tantavi what the structure of our documents are and defining what properties they have. We can use something like a JSON field to give the impression of a schemeless index but you know schemas are good we should use them. They come with lots of nice bells and whistles so in this case we've created a schema with the title field and you can see there we've added the text and stored flag which all that really says is I'm going to tokenize this field and then I'm going to store it so we can retrieve it later on once we've done the search. The second thing we do once we've done that is we create our index writer and in this case we're just letting Tantavi select the number of threads so by default sorry when you create this index writer and we give it a memory buffer in this case about 50 megabytes. Tantavi will allocate n number of threads I think up to eight threads depending on what your system is using so you don't really have to put much thought into the multi-threaded indexing and then we're just adding a document really so we've created our document we've added the text field we've given it in this case the old man of the sea and we're going to put it to our indexer which is essentially just adding it to a queue for the threads to pull off process spit out onto disk and then if we want to actually have that be visible to our users for searching and things like that we need to commit the index so in Tantavi you can either commit or you can roll back and if you have a power failure midway through indexing when you reload from disk it will be at the point of that last commit which is very very useful so you don't leave with partial state and all that all that nasty things and then once we've done that we can actually search and in this case you can either build queries using traits which are very nice and you can mash them all together with lots of boxing and things or you can use the query parser which basically parses a nice little query language in this case we've got a very simple phrase query as it's called trouble that up and it spits out a query for us we then pass that into our search executor which in this case we're executing the query and then we're passing what called collectors and they are effectively just a simple thing to process the documents which are matched so in this case I believe we've got the count collector and the top docs collector and the count collector does well it counts a big surprise there and we have the top docs which collects the top k documents up to a given limit so in this case we've selected 10 we only have one document to match so this doesn't matter that much but if you have more you can limit your results you can adjust how things are scored etc. Now that's all well and good in this example but this doesn't actually really account for spelling and as we discussed earlier users aren't very good at spelling or at least I'm not so we maybe we want a bit of typo tolerance and in this case Tanturi does provide us with some additional way of doing this in the form of the fuzzy term query it uses something called lever science distance it's a very common form of effectively working out how much modification you need to do to a word in order to actually get it to match and we call that the edit distance as such typically you're between one and two edits so you're swapping a word around you're removing it you're adding a new word a bit of magic there really and as you can see at the bottom this is effectively if we use just the regular full text search well if we enter the term hello we'll only match with the word hello if we go with the term hell we'll only match with the word hell if we use some fuzzy term query here we can actually match hell and hello which is very useful especially for the prefix search this is built upon Tanturi's inverted index which uses something called a FST which is effectively a fancy word for saying we threw state machines at it and then made them return results that's as much as I can describe how they work the person who originally wrote the FST library in Rust burnt sushi he has a blog on this goes into a lot of depth really really useful for that sort of thing but I can't elaborate any more on that but all of this additional walking through our index and matching these additional words does come at the cost of some additional CPU and once we've sort of got that what we're left with is this nice block of data on our disks really so we have some metadata files here in particular meta meta.json that contains your schema along with a couple other things and we have our sort of core files which look very similar if they look very similar to these scenes that's because they are in particular we have our field norms our terms our store which is effectively a row level store log file our positions our IDs and our fast fields and fast fields are effectively fast because we cut somewhat simple and equally vague name but now that we've got all this stuff on disk if we wrap it up in an API we sort of we've got that we've mostly we've got everything in this case we've got a demo of LNX working here and we've got about I think 27 million documents and we're searching it with about millisecond latency I think in total it's about 20 gigabytes on disk compressed which is pretty nice but there's sort of a bit of an issue here which is if we deploy this production and our site is very nice we get lots of traffic things increase we go hmm well search traffic is increased our server is not coping let's just scale up the server and we can repeat this for quite a lot and in fact things like AWS allow you a stupid amount of cores and things like that which you can scale up very easily but you keep going along with this and eventually something happens and in this case your data centers burnt down if anyone remembers this this happened in 2021 OVH basically caught fire and that was an end of I think a lot of sleeping people and so yeah your data centers on fire search isn't able to do anything you're losing losing money no one's buying anything management's breathing down your neck for a fix you're having to load from a backup what are you gonna do and well you think ah I should have made some replicas I should have done something called high availability and in this case what this means is we have instead of having one node on one server ready to burn down we have three nodes available to burn down at any point in time and in this case we hope that we put them in different what are called availability zones which mean hey if one data center burns down there's a very small likelihood or at least as it possible for another data center to burn down in the meantime and this allows us to effectively operate even though one server is currently on fire or lost to the ether or I don't know network has torn itself to pieces and this does also mean we can upgrade if we want to tear a server down and we want to restart it with some newer hardware we can do that without interrupting our existing system but this is sort of a hard thing to do because now we've got to work out a way of getting the same documents across all of our nodes in this case it's sort of a share nothing architecture this is done by elastic search and basically most most systems so we're just replicating the documents we're not replicating all of that process data we've just done we need to apply them to each node and doing this approach makes it a bit simpler in reality LNX and QuickWit do something a little bit different but this is this is easier I say this is easier because the initial solution would be you know just just spin up more nodes you know what can add some RPC in there what can go wrong and then deep down you work out it's like oh do you mean networks are reliable what's a raft and things like that and so at that point you go okay this is this is harder than I thought and you realize the world is in fact a scary place outside your happy little data center and you need some way of organizing states independent on things catching on fire and this is this is a hard problem to solve and so you have a little look around and you go well Rust is quite a new system it's quite a young ecosystem we're quite limited so we can't necessarily pick a Paxos implementation off the shelf we maybe have something called raft so that's a leader-based approach and that means we elect a leader and we say okay leader tell us what to do and it will say okay you you handle these documents go go do things with them it's a very well-known algorithm very easy to understand it's probably the only algorithm which is really implemented widely in Rust so there's two implementations one of them by the pink cap group called raft RS and the other by data fuse labs called open raft varying levels of completion or pre-made so in this case you think okay I don't really know what I'm doing here so maybe I shouldn't be managing my own raft cluster and you hear something about eventual consistency and you hear oh it's it's leaderless any any node can handle the rights and then ship off to the other nodes as long as the operations are idempotent and that's a very key point which means you can basically ship the same document over and over and over again and they're not going to duplicate themselves or at least they don't act like they duplicate and this gives us realistically a bit more freedom if we want to change we can change and so we decide let's go with eventual consistency because yeah I like an easy life and it seemed easier yes people laughing will agree that yes things that seem easier probably aren't and so our diagram sort of looks something like this and I'm scared to cross the white line so I'll try and point but we have step one a client sends the documents to a any node it doesn't really care which one that client then goes okay I'm going to send it to some of my peers and then wait for them to tell me that they've got the document it's safe and then once we've got the majority which is a very common approach in these systems we can tell the client okay your document is safe even if OHV burns down again we're probably going to be okay it doesn't need to wait for all of the nodes to respond because otherwise you're not really highly available because if one node goes down you can't progress and so this system is this system is pretty good there's just one small problem which is how in God's name do you do this many questions need to be answered many things how do you test this or who's going to have the time to do this and well luckily someone aka me spent the better part of six months of their free time dealing with this and so I made a library and in this case it's called data cake whoo yes in this case this is called data cake I originally was going to call it data lake but unfortunately that already exists so we added cake at the end and called it a day it is effectively a tooling to create your own distributed systems it doesn't have to be eventually consistent but it just is designed to make your life a lot easier and it only took about six rewrites to get it to the stage that it is because yeah things are hard and trying to work out what you want to do with something like that is awkward but some of the features it includes is it includes the zero copy RPC framework and this is built upon the popular archive framework which is really really useful if you're shipping a lot of data because you don't actually have to deserialize and allocate everything all over again you can just treat an initial buffer as if it's the data which if that sounds wildly and safe it is but there's a lot of tests and I didn't write it so you're safe. We also add the membership and failure detection and this is done using chit chat which is a library we made at quick quit it uses the same algorithm as something like Cassandra or DynamoDB and this allows the system to essentially work out what nodes are actually its friends and what it can do and in this case we've also implemented an eventually consistent store in the form of a key value system which only requires one trait to implement and the reason why I went with this is because if you implement anything more than one trait people seem to turn off and frankly I did when I looked at the raft implementations. So we went with one storage trait that's all you need to get this to work. We also have some pre-built implementations I particularly like abusing SQLite so there is an SQLite implementation and a memory version and it also gives you some CRDTs which are conflict-free replicated data types I should say and also something called a hybrid logical clock which means it's a clock which you can have across your cluster where the nodes will stabilize themselves and prevent you from effectively having to deal with this concept of causality and causality is definitely the biggest issue you will ever run into with distributed systems because time is suddenly not reliable. And so we go back to our original thing of well first we actually need a cluster and this case it's really simple to do all we need to do is we just create our node builder we tell data cake okay we've got your address is this your peers are this or you can start with one peer and they'll discover themselves who their neighbors are and you give them a node ID. They're integers they're not strings and the reason for that is because there's a lot of bit packing of certain data types going on and strings do not do well. And here we can also effectively wait for nodes to come onto the system so our cluster is stable and ready to go before we actually do anything else. And by the time we get to this point our RPC systems are working nodes are communicating your clocks have synchronized themselves mostly and you can actually start adding something called extensions. Now extensions essentially allow you to extend your existing cluster you don't you can do this at runtime they can be added and they can be unloaded all at runtime without any with state cleanup and everything else which makes life a lot easier especially for testing. They have access to the running node on this local system which allows you to access things like the cluster clock the RPC network as it's called which is the pre-established RPC connections and you can essentially make this as simple or as complex as possible which is essentially what I've done here so I've created this nice little extension which is absolutely nothing other than print what the current time is which realistically I could do without but nonetheless I went with it. And this is what the eventual consistency store actually does under the hood is it's just an extension and here we can see that we're passing in a I can't point that far but we pass in a mem store which is our storage trait we pass in our create our eventual consistency extension using this and we pass it to the data cake node and say okay go add this extension give me the result back when you're ready and in this case our eventual consistency cluster actually returns us a storage handle which allows us to do basically all of our lovely key value operations should we wish including delete, put, get that's about all there is on the key value store but there are also some bulk operations which allow for much more efficient replication of data. The only problem with this approach is it's not suitable for billion scale databases so if you're trying to make the next Cassandra or Silla don't use this particular extension because it keeps the key value or the keys sorry in memory which it uses to work out what keys have and have not been processed and the reason for this is effectively because I didn't really trust users implementing this on the storage site correctly which turned out to be a good choice because the amount of unit tests that this failed initially was a lot and so now we've sort of got this ability to replicate our key values our life is a lot easier in particular we can actually go as far as essentially saying okay we've established our data connection our key values let's just use Tantive as our persistence store and this is effectively the simplest way to do it and I've made a little demo here which you can go to that link I basically abused and slightly ignored certain things in particular correctness but this will replicate your data you may end up with duplicate documents because I didn't handle de-duping but in this case we can fetch we can delete and we can index documents with Tantive and that's our persistence store and here you can see we're doing about 20,000 documents in 400 milliseconds in the local cluster yes and that is effectively the end so are there any questions how long do we have left how long do we have left 15 minutes so actually kind of so in there do you have like a way to provide from outside to the Tantive transaction or links transaction an external ID that I can use to integrate with the standard storage so change the question would be an easier way do you have a way to say which which level of data has been indexed yes in this case I've sort of glossed over it a little bit because in reality it's a little bit more complicated when you implement it so in reality when you actually implement this you would probably have a essentially use the replication to replicate the initial documents and then you would have a check mark to essentially work out what documents have and have not been indexed yet or you would add some additional step like a right ahead log so that way you know that as long as the documents are there you can make sure that your check your commit point is always updated to the latest thing in the next it's actually a little bit different again because the way it creates indexes is they are per check point so in a new index is created every commit effectively but you don't have to do that and in this method I didn't so you could you can it doesn't do it here but you can add a right ahead log and do you can do basically do anything as long as the trait is implemented hello hello hi yeah all right so congratulations for the presentation sorry I think I can see you yes hello so let me see if I can got that question right so you was is that about it sending time to me so if you want to go beyond something like bm25 or leave a size distance and things like that things like I think things like vector search or word embedding search is still something which is quite far away and we need quite a big push to do with time to be specifically but if you want to add additional queries or additional functionality it's quite easy to add with time to be so it's actually just a query trait so one of the things that and the next does it actually has another a query mode called fast fuzzy which actually uses another algorithm for pre-computing dictionaries in order to do the edit distance lookup and that basically is just involves creating another query and you can customize effectively all of your query logic all of your collecting logic and things like that so providing your within the scope of the API time to be will allow you to implement it yourself otherwise things like the word embeddings which are a little bit more complicated and require a bit more on the storage side would need to an issue and a very motivated individual to probably implement that which currently we we don't really have so it's pretty little question on all your sketches the network the subject network was fully connected is that important let me see if I can find which one that was was it was it this one or was it this one well on this one it's it does not look fully connected but I'm not sure if these diagram depicts kind of connectivity connect home or just which messages has actually been dispatched so I'm going to cross the forbidden white line here because we're doing questions and effectively these are just indicating sending responses and getting things back so these notes don't actually in a real system that you could have a network petition here and your node one can no longer talk to no three it's effectively lost to the ether and maybe no two can also not do it and in this case it doesn't actually really care all that you need to do is you need to achieve what's called a consistency level so which means that if you want to progress you have to reach that level otherwise things are counted as not happening and so in this case if no three is down or can't be contacted as long as node one can contact node two and no two acknowledges the messages things can still progress this is the same with raft as well so raft operates on what's called a quorum which yeah but effectively any node any one node can go down in a three node group and the other two nodes can still progress providing they have what's called what's the majority so I understand full connection of the network is not an important factor here well it's nice to know thank you thank you for our talk I see here that there is basically a consistency mechanism for indexing do you check as well for that on over nodes when there is a search request as well say that again sorry I didn't quite pick that up do you check the data on over nodes when there is a search request not an indexing request in this case we have relaxed reads essentially so we don't do we're not it's searching across several nodes and getting the most updated version from that which is part of the trade-off you make with the eventual consistency you will have that with raft as well effectively unless you contact the leader you won't have the most update data when searching but one of the things you do have to do if you go with the eventual consistency eventual consistency approach like we do here is you would need to effectively handle the idea that maybe you will have duplicate documents because something's been recent in the meantime and so you'll need to be able to deduplicate that when you're searching or have some other method of handling it and deleting it from the index so that means that effectively every node must have a copy of the data like I cannot have five nodes unlike a free with the car system or something about yeah so as long as if you've got like a five node cluster and three nodes respond you can immediately search from if those three nodes have got the data they can immediately be searched from effectively if you want but the other nodes may take a little bit of time to catch up which is the principle with eventual consistency they'll eventually align themselves but they're not all immediately immediately able to reflect changes hello just simple one in hindsight would you take the raft part in hindsight probably not still and the reason for that is because the current state of the rust ecosystem with it means that there's a lot of black holes effectively around it and so you either going with an implementation which is very very stripped down just the state machine part or going with an implementation which is very very trait heavy and is a little bit opaque around what you need to test what you don't need to test and how it behaves under failure so in this case it's I like this approach more because it may allow me to implement things like network simulation which the RPC program supports so we can actually simulate network fit networks failing locally in tests and things like that which makes me feel a little bit more confident than trying to just have the state machine and implement everything and all the handling correctly but I think in future yeah you could you could use it but it's just not not quite at that state so I'm not sure I quite got how how if the engine actually does any data sharding or there's a hatchery yeah in this so in this approach it's simplicity of time really we're not actually doing any data sharding servers are really quite big nowadays so you can even for your e-commerce website you can get a pretty huge server and the biggest issue tends to be replication and the high availability the data sharding is something that some quick wits is something that would be concerned about because you've got so much data you need to spread it across machines and things like that when you're searching but in e-commerce at the point in which you're searching across multiple machines you're probably going to be looking at the higher latencies so you would you'd be better off dedicating one machine per search rather than several machines per per search really.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 12.8, "text": " I'll just yell. But yeah, so this is effectively my talk. It started out as a really big thing", "tokens": [286, 603, 445, 20525, 13, 583, 1338, 11, 370, 341, 307, 8659, 452, 751, 13, 467, 1409, 484, 382, 257, 534, 955, 551], "temperature": 0.0, "avg_logprob": -0.18023399808513585, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.12464479357004166}, {"id": 1, "seek": 0, "start": 12.8, "end": 18.04, "text": " and then I realised 40 minutes wasn't actually that much time and so we sort of had to compress", "tokens": [293, 550, 286, 21337, 3356, 2077, 2067, 380, 767, 300, 709, 565, 293, 370, 321, 1333, 295, 632, 281, 14778], "temperature": 0.0, "avg_logprob": -0.18023399808513585, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.12464479357004166}, {"id": 2, "seek": 0, "start": 18.04, "end": 24.080000000000002, "text": " it down into a bit of a slightly smaller talk but hopefully covering the most interesting", "tokens": [309, 760, 666, 257, 857, 295, 257, 4748, 4356, 751, 457, 4696, 10322, 264, 881, 1880], "temperature": 0.0, "avg_logprob": -0.18023399808513585, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.12464479357004166}, {"id": 3, "seek": 2408, "start": 24.08, "end": 30.119999999999997, "text": " points in my opinion. So a bit about me. I'm Harrison. I come from London. I live in London", "tokens": [2793, 294, 452, 4800, 13, 407, 257, 857, 466, 385, 13, 286, 478, 34272, 13, 286, 808, 490, 7042, 13, 286, 1621, 294, 7042], "temperature": 0.0, "avg_logprob": -0.19110843699465516, "compression_ratio": 1.5, "no_speech_prob": 5.060412877355702e-05}, {"id": 4, "seek": 2408, "start": 30.119999999999997, "end": 35.72, "text": " and I work for QuickWit where as Paul has said we build basically a distributed search", "tokens": [293, 286, 589, 337, 12101, 54, 270, 689, 382, 4552, 575, 848, 321, 1322, 1936, 257, 12631, 3164], "temperature": 0.0, "avg_logprob": -0.19110843699465516, "compression_ratio": 1.5, "no_speech_prob": 5.060412877355702e-05}, {"id": 5, "seek": 2408, "start": 35.72, "end": 45.2, "text": " engine for logs. I am the creator of LNX which is a slightly different design of search engine", "tokens": [2848, 337, 20820, 13, 286, 669, 264, 14181, 295, 441, 45, 55, 597, 307, 257, 4748, 819, 1715, 295, 3164, 2848], "temperature": 0.0, "avg_logprob": -0.19110843699465516, "compression_ratio": 1.5, "no_speech_prob": 5.060412877355702e-05}, {"id": 6, "seek": 2408, "start": 45.2, "end": 51.480000000000004, "text": " probably more akin to something like Elasticsearch or Algolia for all your lovely e-commerce", "tokens": [1391, 544, 47540, 281, 746, 411, 2699, 2750, 405, 1178, 420, 35014, 29760, 337, 439, 428, 7496, 308, 12, 26926], "temperature": 0.0, "avg_logprob": -0.19110843699465516, "compression_ratio": 1.5, "no_speech_prob": 5.060412877355702e-05}, {"id": 7, "seek": 5148, "start": 51.48, "end": 60.16, "text": " websites. And you can contact me at Harrison at QuickWit.io. A little bit about LNX since", "tokens": [12891, 13, 400, 291, 393, 3385, 385, 412, 34272, 412, 12101, 54, 270, 13, 1004, 13, 316, 707, 857, 466, 441, 45, 55, 1670], "temperature": 0.0, "avg_logprob": -0.1556477838633012, "compression_ratio": 1.5063291139240507, "no_speech_prob": 3.6540848213917343e-06}, {"id": 8, "seek": 5148, "start": 60.16, "end": 65.39999999999999, "text": " this is basically the origin story of this talk really. It's a search engine built on", "tokens": [341, 307, 1936, 264, 4957, 1657, 295, 341, 751, 534, 13, 467, 311, 257, 3164, 2848, 3094, 322], "temperature": 0.0, "avg_logprob": -0.1556477838633012, "compression_ratio": 1.5063291139240507, "no_speech_prob": 3.6540848213917343e-06}, {"id": 9, "seek": 5148, "start": 65.39999999999999, "end": 70.4, "text": " top of Tantrave. It's akin to Elasticsearch or Algolia as I've said. It's aimed at user", "tokens": [1192, 295, 314, 394, 424, 303, 13, 467, 311, 47540, 281, 2699, 2750, 405, 1178, 420, 35014, 29760, 382, 286, 600, 848, 13, 467, 311, 20540, 412, 4195], "temperature": 0.0, "avg_logprob": -0.1556477838633012, "compression_ratio": 1.5063291139240507, "no_speech_prob": 3.6540848213917343e-06}, {"id": 10, "seek": 5148, "start": 70.4, "end": 76.12, "text": " facing search. That's things like your e-commerce websites, your Netflix streaming platforms,", "tokens": [7170, 3164, 13, 663, 311, 721, 411, 428, 308, 12, 26926, 12891, 11, 428, 12778, 11791, 9473, 11], "temperature": 0.0, "avg_logprob": -0.1556477838633012, "compression_ratio": 1.5063291139240507, "no_speech_prob": 3.6540848213917343e-06}, {"id": 11, "seek": 7612, "start": 76.12, "end": 82.28, "text": " things like that. It's not aimed to be your cost effective log search engine. It doesn't", "tokens": [721, 411, 300, 13, 467, 311, 406, 20540, 281, 312, 428, 2063, 4942, 3565, 3164, 2848, 13, 467, 1177, 380], "temperature": 0.0, "avg_logprob": -0.10377280825660343, "compression_ratio": 1.6468401486988848, "no_speech_prob": 3.7931263250357006e-06}, {"id": 12, "seek": 7612, "start": 82.28, "end": 87.04, "text": " really handle those hundreds of terabytes a day type workloads but it will handle thousands", "tokens": [534, 4813, 729, 6779, 295, 1796, 24538, 257, 786, 2010, 32452, 457, 309, 486, 4813, 5383], "temperature": 0.0, "avg_logprob": -0.10377280825660343, "compression_ratio": 1.6468401486988848, "no_speech_prob": 3.7931263250357006e-06}, {"id": 13, "seek": 7612, "start": 87.04, "end": 92.08000000000001, "text": " of queries a second per core. It's very easily configurable. It's designed to be really", "tokens": [295, 24109, 257, 1150, 680, 4965, 13, 467, 311, 588, 3612, 22192, 712, 13, 467, 311, 4761, 281, 312, 534], "temperature": 0.0, "avg_logprob": -0.10377280825660343, "compression_ratio": 1.6468401486988848, "no_speech_prob": 3.7931263250357006e-06}, {"id": 14, "seek": 7612, "start": 92.08000000000001, "end": 97.2, "text": " fast out of the box because it uses Tantrave and it has an indexing throughput of about", "tokens": [2370, 484, 295, 264, 2424, 570, 309, 4960, 314, 394, 424, 303, 293, 309, 575, 364, 8186, 278, 44629, 295, 466], "temperature": 0.0, "avg_logprob": -0.10377280825660343, "compression_ratio": 1.6468401486988848, "no_speech_prob": 3.7931263250357006e-06}, {"id": 15, "seek": 7612, "start": 97.2, "end": 103.48, "text": " 30 to 60 megabytes a second on reasonable hardware. With high availability coming soon", "tokens": [2217, 281, 4060, 10816, 24538, 257, 1150, 322, 10585, 8837, 13, 2022, 1090, 17945, 1348, 2321], "temperature": 0.0, "avg_logprob": -0.10377280825660343, "compression_ratio": 1.6468401486988848, "no_speech_prob": 3.7931263250357006e-06}, {"id": 16, "seek": 10348, "start": 103.48, "end": 110.72, "text": " which is the presence of this talk. So what is user facing search? I've stolen Crunchy", "tokens": [597, 307, 264, 6814, 295, 341, 751, 13, 407, 437, 307, 4195, 7170, 3164, 30, 286, 600, 15900, 44233, 88], "temperature": 0.0, "avg_logprob": -0.144901860435054, "compression_ratio": 1.73828125, "no_speech_prob": 2.494859836588148e-05}, {"id": 17, "seek": 10348, "start": 110.72, "end": 115.80000000000001, "text": " Well's website and I've typed some bad spelling in there and you see that a lot of the top", "tokens": [1042, 311, 3144, 293, 286, 600, 33941, 512, 1578, 22254, 294, 456, 293, 291, 536, 300, 257, 688, 295, 264, 1192], "temperature": 0.0, "avg_logprob": -0.144901860435054, "compression_ratio": 1.73828125, "no_speech_prob": 2.494859836588148e-05}, {"id": 18, "seek": 10348, "start": 115.80000000000001, "end": 120.46000000000001, "text": " results actually account for the fact that I can't spell. That's basically the biggest", "tokens": [3542, 767, 2696, 337, 264, 1186, 300, 286, 393, 380, 9827, 13, 663, 311, 1936, 264, 3880], "temperature": 0.0, "avg_logprob": -0.144901860435054, "compression_ratio": 1.73828125, "no_speech_prob": 2.494859836588148e-05}, {"id": 19, "seek": 10348, "start": 120.46000000000001, "end": 126.88000000000001, "text": " principle with these user facing search engines is you have this concept of typo tolerance.", "tokens": [8665, 365, 613, 4195, 7170, 3164, 12982, 307, 291, 362, 341, 3410, 295, 2125, 78, 23368, 13], "temperature": 0.0, "avg_logprob": -0.144901860435054, "compression_ratio": 1.73828125, "no_speech_prob": 2.494859836588148e-05}, {"id": 20, "seek": 10348, "start": 126.88000000000001, "end": 133.44, "text": " This is a really good thing for users because users can't spell. The downside of this is", "tokens": [639, 307, 257, 534, 665, 551, 337, 5022, 570, 5022, 393, 380, 9827, 13, 440, 25060, 295, 341, 307], "temperature": 0.0, "avg_logprob": -0.144901860435054, "compression_ratio": 1.73828125, "no_speech_prob": 2.494859836588148e-05}, {"id": 21, "seek": 13344, "start": 133.44, "end": 138.68, "text": " that it has a lot of CPU time when we're checking those additional words and it makes things", "tokens": [300, 309, 575, 257, 688, 295, 13199, 565, 562, 321, 434, 8568, 729, 4497, 2283, 293, 309, 1669, 721], "temperature": 0.0, "avg_logprob": -0.12237499198135064, "compression_ratio": 1.7346153846153847, "no_speech_prob": 3.1536656024400145e-05}, {"id": 22, "seek": 13344, "start": 138.68, "end": 144.64, "text": " a lot more complicated and often documents are mutable and a lot of other things but", "tokens": [257, 688, 544, 6179, 293, 2049, 8512, 366, 5839, 712, 293, 257, 688, 295, 661, 721, 457], "temperature": 0.0, "avg_logprob": -0.12237499198135064, "compression_ratio": 1.7346153846153847, "no_speech_prob": 3.1536656024400145e-05}, {"id": 23, "seek": 13344, "start": 144.64, "end": 151.32, "text": " also when you have these nice search experiences and you want no latency something called search", "tokens": [611, 562, 291, 362, 613, 1481, 3164, 5235, 293, 291, 528, 572, 27043, 746, 1219, 3164], "temperature": 0.0, "avg_logprob": -0.12237499198135064, "compression_ratio": 1.7346153846153847, "no_speech_prob": 3.1536656024400145e-05}, {"id": 24, "seek": 13344, "start": 151.32, "end": 156.8, "text": " as you type has become more popular now and that means your amount of search as you're", "tokens": [382, 291, 2010, 575, 1813, 544, 3743, 586, 293, 300, 1355, 428, 2372, 295, 3164, 382, 291, 434], "temperature": 0.0, "avg_logprob": -0.12237499198135064, "compression_ratio": 1.7346153846153847, "no_speech_prob": 3.1536656024400145e-05}, {"id": 25, "seek": 13344, "start": 156.8, "end": 161.92, "text": " doing for a single user is increasing several times over because now every key stroke you", "tokens": [884, 337, 257, 2167, 4195, 307, 5662, 2940, 1413, 670, 570, 586, 633, 2141, 12403, 291], "temperature": 0.0, "avg_logprob": -0.12237499198135064, "compression_ratio": 1.7346153846153847, "no_speech_prob": 3.1536656024400145e-05}, {"id": 26, "seek": 16192, "start": 161.92, "end": 168.0, "text": " press is a search versus typing it all in one go hitting enter user gets a bunch of", "tokens": [1886, 307, 257, 3164, 5717, 18444, 309, 439, 294, 472, 352, 8850, 3242, 4195, 2170, 257, 3840, 295], "temperature": 0.0, "avg_logprob": -0.18214045533346473, "compression_ratio": 1.6455223880597014, "no_speech_prob": 3.269664375693537e-05}, {"id": 27, "seek": 16192, "start": 168.0, "end": 173.32, "text": " results back goes oh no I've spelt something wrong or I can't see what I want on here", "tokens": [3542, 646, 1709, 1954, 572, 286, 600, 637, 2018, 746, 2085, 420, 286, 393, 380, 536, 437, 286, 528, 322, 510], "temperature": 0.0, "avg_logprob": -0.18214045533346473, "compression_ratio": 1.6455223880597014, "no_speech_prob": 3.269664375693537e-05}, {"id": 28, "seek": 16192, "start": 173.32, "end": 178.67999999999998, "text": " so I'm going to type it again. And so that is effectively the principle of these search", "tokens": [370, 286, 478, 516, 281, 2010, 309, 797, 13, 400, 370, 300, 307, 8659, 264, 8665, 295, 613, 3164], "temperature": 0.0, "avg_logprob": -0.18214045533346473, "compression_ratio": 1.6455223880597014, "no_speech_prob": 3.269664375693537e-05}, {"id": 29, "seek": 16192, "start": 178.67999999999998, "end": 183.6, "text": " engines. You see we have Algolia at the bottom which is a very common one which I think most", "tokens": [12982, 13, 509, 536, 321, 362, 35014, 29760, 412, 264, 2767, 597, 307, 257, 588, 2689, 472, 597, 286, 519, 881], "temperature": 0.0, "avg_logprob": -0.18214045533346473, "compression_ratio": 1.6455223880597014, "no_speech_prob": 3.269664375693537e-05}, {"id": 30, "seek": 16192, "start": 183.6, "end": 190.23999999999998, "text": " people know very popular for document searching. But you know we decided hey we don't want", "tokens": [561, 458, 588, 3743, 337, 4166, 10808, 13, 583, 291, 458, 321, 3047, 4177, 321, 500, 380, 528], "temperature": 0.0, "avg_logprob": -0.18214045533346473, "compression_ratio": 1.6455223880597014, "no_speech_prob": 3.269664375693537e-05}, {"id": 31, "seek": 19024, "start": 190.24, "end": 193.92000000000002, "text": " to use one of these pre-built systems we don't want to use Elasticsearch that's big that's", "tokens": [281, 764, 472, 295, 613, 659, 12, 23018, 3652, 321, 500, 380, 528, 281, 764, 2699, 2750, 405, 1178, 300, 311, 955, 300, 311], "temperature": 0.0, "avg_logprob": -0.19086877562159257, "compression_ratio": 1.8650519031141868, "no_speech_prob": 6.525467324536294e-05}, {"id": 32, "seek": 19024, "start": 193.92000000000002, "end": 198.96, "text": " scary I don't like it. We don't want to use Algolia because I don't have that much money", "tokens": [6958, 286, 500, 380, 411, 309, 13, 492, 500, 380, 528, 281, 764, 35014, 29760, 570, 286, 500, 380, 362, 300, 709, 1460], "temperature": 0.0, "avg_logprob": -0.19086877562159257, "compression_ratio": 1.8650519031141868, "no_speech_prob": 6.525467324536294e-05}, {"id": 33, "seek": 19024, "start": 198.96, "end": 204.44, "text": " I'm just a lonely paid software developer I can't be spending thousands of pounds on", "tokens": [286, 478, 445, 257, 14236, 4835, 4722, 10754, 286, 393, 380, 312, 6434, 5383, 295, 8319, 322], "temperature": 0.0, "avg_logprob": -0.19086877562159257, "compression_ratio": 1.8650519031141868, "no_speech_prob": 6.525467324536294e-05}, {"id": 34, "seek": 19024, "start": 204.44, "end": 207.68, "text": " that. And we look at some of the others but we're going there we're just going to write", "tokens": [300, 13, 400, 321, 574, 412, 512, 295, 264, 2357, 457, 321, 434, 516, 456, 321, 434, 445, 516, 281, 2464], "temperature": 0.0, "avg_logprob": -0.19086877562159257, "compression_ratio": 1.8650519031141868, "no_speech_prob": 6.525467324536294e-05}, {"id": 35, "seek": 19024, "start": 207.68, "end": 212.24, "text": " it ourselves and that's where we have a little look because we hear something about Tantavi", "tokens": [309, 4175, 293, 300, 311, 689, 321, 362, 257, 707, 574, 570, 321, 1568, 746, 466, 314, 394, 18442], "temperature": 0.0, "avg_logprob": -0.19086877562159257, "compression_ratio": 1.8650519031141868, "no_speech_prob": 6.525467324536294e-05}, {"id": 36, "seek": 19024, "start": 212.24, "end": 218.36, "text": " we hear something about Rust that being blazingly fast as all things must be and so we go okay", "tokens": [321, 1568, 746, 466, 34952, 300, 885, 16379, 8781, 356, 2370, 382, 439, 721, 1633, 312, 293, 370, 321, 352, 1392], "temperature": 0.0, "avg_logprob": -0.19086877562159257, "compression_ratio": 1.8650519031141868, "no_speech_prob": 6.525467324536294e-05}, {"id": 37, "seek": 21836, "start": 218.36, "end": 223.32000000000002, "text": " I like this I like what it says it says yeah Apache Lucene I think I've heard that before", "tokens": [286, 411, 341, 286, 411, 437, 309, 1619, 309, 1619, 1338, 46597, 9593, 1450, 286, 519, 286, 600, 2198, 300, 949], "temperature": 0.0, "avg_logprob": -0.13724940531962626, "compression_ratio": 1.6704119850187267, "no_speech_prob": 3.37917372235097e-05}, {"id": 38, "seek": 21836, "start": 223.32000000000002, "end": 228.32000000000002, "text": " somewhere written in Rust I think I've definitely heard that before. And so we take a little", "tokens": [4079, 3720, 294, 34952, 286, 519, 286, 600, 2138, 2198, 300, 949, 13, 400, 370, 321, 747, 257, 707], "temperature": 0.0, "avg_logprob": -0.13724940531962626, "compression_ratio": 1.6704119850187267, "no_speech_prob": 3.37917372235097e-05}, {"id": 39, "seek": 21836, "start": 228.32000000000002, "end": 233.60000000000002, "text": " look at what it is and it is effectively akin to Lucene which if you don't know what that", "tokens": [574, 412, 437, 309, 307, 293, 309, 307, 8659, 47540, 281, 9593, 1450, 597, 498, 291, 500, 380, 458, 437, 300], "temperature": 0.0, "avg_logprob": -0.13724940531962626, "compression_ratio": 1.6704119850187267, "no_speech_prob": 3.37917372235097e-05}, {"id": 40, "seek": 21836, "start": 233.60000000000002, "end": 239.0, "text": " is it's a full text search engine as it's called. Tantavi in particular supports things", "tokens": [307, 309, 311, 257, 1577, 2487, 3164, 2848, 382, 309, 311, 1219, 13, 314, 394, 18442, 294, 1729, 9346, 721], "temperature": 0.0, "avg_logprob": -0.13724940531962626, "compression_ratio": 1.6704119850187267, "no_speech_prob": 3.37917372235097e-05}, {"id": 41, "seek": 21836, "start": 239.0, "end": 244.12, "text": " like BM25 scoring which is just a fancy way of saying what words are relevant to this", "tokens": [411, 15901, 6074, 22358, 597, 307, 445, 257, 10247, 636, 295, 1566, 437, 2283, 366, 7340, 281, 341], "temperature": 0.0, "avg_logprob": -0.13724940531962626, "compression_ratio": 1.6704119850187267, "no_speech_prob": 3.37917372235097e-05}, {"id": 42, "seek": 24412, "start": 244.12, "end": 248.72, "text": " query it supports something called incremental indexing which basically just means you don't", "tokens": [14581, 309, 9346, 746, 1219, 35759, 8186, 278, 597, 1936, 445, 1355, 291, 500, 380], "temperature": 0.0, "avg_logprob": -0.14523308820063524, "compression_ratio": 1.6940298507462686, "no_speech_prob": 2.7468926418805495e-05}, {"id": 43, "seek": 24412, "start": 248.72, "end": 254.08, "text": " have to re-index all of your documents every time you change one thing. You have fasted", "tokens": [362, 281, 319, 12, 471, 3121, 439, 295, 428, 8512, 633, 565, 291, 1319, 472, 551, 13, 509, 362, 2370, 292], "temperature": 0.0, "avg_logprob": -0.14523308820063524, "compression_ratio": 1.6940298507462686, "no_speech_prob": 2.7468926418805495e-05}, {"id": 44, "seek": 24412, "start": 254.08, "end": 259.08, "text": " search you have range queries and we have things like JSON fields which allow for a", "tokens": [3164, 291, 362, 3613, 24109, 293, 321, 362, 721, 411, 31828, 7909, 597, 2089, 337, 257], "temperature": 0.0, "avg_logprob": -0.14523308820063524, "compression_ratio": 1.6940298507462686, "no_speech_prob": 2.7468926418805495e-05}, {"id": 45, "seek": 24412, "start": 259.08, "end": 266.32, "text": " schemeless indexing as such. You can do aggregations which have some limitations in particular around", "tokens": [22627, 4272, 8186, 278, 382, 1270, 13, 509, 393, 360, 16743, 763, 597, 362, 512, 15705, 294, 1729, 926], "temperature": 0.0, "avg_logprob": -0.14523308820063524, "compression_ratio": 1.6940298507462686, "no_speech_prob": 2.7468926418805495e-05}, {"id": 46, "seek": 24412, "start": 266.32, "end": 272.08, "text": " JSON fields being a little bit limited but in the biggest thing is it has a cheesy logo", "tokens": [31828, 7909, 885, 257, 707, 857, 5567, 457, 294, 264, 3880, 551, 307, 309, 575, 257, 32549, 9699], "temperature": 0.0, "avg_logprob": -0.14523308820063524, "compression_ratio": 1.6940298507462686, "no_speech_prob": 2.7468926418805495e-05}, {"id": 47, "seek": 27208, "start": 272.08, "end": 278.03999999999996, "text": " with a horse which I believe Paul drew himself so I think that needs a clap on its own. But", "tokens": [365, 257, 6832, 597, 286, 1697, 4552, 12804, 3647, 370, 286, 519, 300, 2203, 257, 20760, 322, 1080, 1065, 13, 583], "temperature": 0.0, "avg_logprob": -0.11249714069538289, "compression_ratio": 1.696629213483146, "no_speech_prob": 2.2342330339597538e-05}, {"id": 48, "seek": 27208, "start": 278.03999999999996, "end": 286.76, "text": " there are other features which I just haven't yes. But there are more features which I couldn't", "tokens": [456, 366, 661, 4122, 597, 286, 445, 2378, 380, 2086, 13, 583, 456, 366, 544, 4122, 597, 286, 2809, 380], "temperature": 0.0, "avg_logprob": -0.11249714069538289, "compression_ratio": 1.696629213483146, "no_speech_prob": 2.2342330339597538e-05}, {"id": 49, "seek": 27208, "start": 286.76, "end": 292.56, "text": " fit on this slide and timers of the essence. So you might be wondering what the basic", "tokens": [3318, 322, 341, 4137, 293, 524, 433, 295, 264, 12801, 13, 407, 291, 1062, 312, 6359, 437, 264, 3875], "temperature": 0.0, "avg_logprob": -0.11249714069538289, "compression_ratio": 1.696629213483146, "no_speech_prob": 2.2342330339597538e-05}, {"id": 50, "seek": 27208, "start": 292.56, "end": 296.44, "text": " implementation of Tantavi looks like and because it's a library it's actually really quite", "tokens": [11420, 295, 314, 394, 18442, 1542, 411, 293, 570, 309, 311, 257, 6405, 309, 311, 767, 534, 1596], "temperature": 0.0, "avg_logprob": -0.11249714069538289, "compression_ratio": 1.696629213483146, "no_speech_prob": 2.2342330339597538e-05}, {"id": 51, "seek": 27208, "start": 296.44, "end": 301.4, "text": " simple to do. So we have a couple of core things starting at the top is we define what's", "tokens": [2199, 281, 360, 13, 407, 321, 362, 257, 1916, 295, 4965, 721, 2891, 412, 264, 1192, 307, 321, 6964, 437, 311], "temperature": 0.0, "avg_logprob": -0.11249714069538289, "compression_ratio": 1.696629213483146, "no_speech_prob": 2.2342330339597538e-05}, {"id": 52, "seek": 30140, "start": 301.4, "end": 308.88, "text": " called a schema. Since Tantavi was originally a schema based system still is we need some", "tokens": [1219, 257, 34078, 13, 4162, 314, 394, 18442, 390, 7993, 257, 34078, 2361, 1185, 920, 307, 321, 643, 512], "temperature": 0.0, "avg_logprob": -0.13634890388039983, "compression_ratio": 1.592920353982301, "no_speech_prob": 2.5670158720458858e-05}, {"id": 53, "seek": 30140, "start": 308.88, "end": 314.76, "text": " way of telling Tantavi what the structure of our documents are and defining what properties", "tokens": [636, 295, 3585, 314, 394, 18442, 437, 264, 3877, 295, 527, 8512, 366, 293, 17827, 437, 7221], "temperature": 0.0, "avg_logprob": -0.13634890388039983, "compression_ratio": 1.592920353982301, "no_speech_prob": 2.5670158720458858e-05}, {"id": 54, "seek": 30140, "start": 314.76, "end": 318.76, "text": " they have. We can use something like a JSON field to give the impression of a schemeless", "tokens": [436, 362, 13, 492, 393, 764, 746, 411, 257, 31828, 2519, 281, 976, 264, 9995, 295, 257, 22627, 4272], "temperature": 0.0, "avg_logprob": -0.13634890388039983, "compression_ratio": 1.592920353982301, "no_speech_prob": 2.5670158720458858e-05}, {"id": 55, "seek": 30140, "start": 318.76, "end": 325.59999999999997, "text": " index but you know schemas are good we should use them. They come with lots of nice bells", "tokens": [8186, 457, 291, 458, 22627, 296, 366, 665, 321, 820, 764, 552, 13, 814, 808, 365, 3195, 295, 1481, 25474], "temperature": 0.0, "avg_logprob": -0.13634890388039983, "compression_ratio": 1.592920353982301, "no_speech_prob": 2.5670158720458858e-05}, {"id": 56, "seek": 32560, "start": 325.6, "end": 332.44, "text": " and whistles so in this case we've created a schema with the title field and you can", "tokens": [293, 49282, 370, 294, 341, 1389, 321, 600, 2942, 257, 34078, 365, 264, 4876, 2519, 293, 291, 393], "temperature": 0.0, "avg_logprob": -0.12749882611361416, "compression_ratio": 1.8284518828451883, "no_speech_prob": 4.515519776759902e-06}, {"id": 57, "seek": 32560, "start": 332.44, "end": 338.20000000000005, "text": " see there we've added the text and stored flag which all that really says is I'm going", "tokens": [536, 456, 321, 600, 3869, 264, 2487, 293, 12187, 7166, 597, 439, 300, 534, 1619, 307, 286, 478, 516], "temperature": 0.0, "avg_logprob": -0.12749882611361416, "compression_ratio": 1.8284518828451883, "no_speech_prob": 4.515519776759902e-06}, {"id": 58, "seek": 32560, "start": 338.20000000000005, "end": 343.28000000000003, "text": " to tokenize this field and then I'm going to store it so we can retrieve it later on", "tokens": [281, 14862, 1125, 341, 2519, 293, 550, 286, 478, 516, 281, 3531, 309, 370, 321, 393, 30254, 309, 1780, 322], "temperature": 0.0, "avg_logprob": -0.12749882611361416, "compression_ratio": 1.8284518828451883, "no_speech_prob": 4.515519776759902e-06}, {"id": 59, "seek": 32560, "start": 343.28000000000003, "end": 349.28000000000003, "text": " once we've done the search. The second thing we do once we've done that is we create our", "tokens": [1564, 321, 600, 1096, 264, 3164, 13, 440, 1150, 551, 321, 360, 1564, 321, 600, 1096, 300, 307, 321, 1884, 527], "temperature": 0.0, "avg_logprob": -0.12749882611361416, "compression_ratio": 1.8284518828451883, "no_speech_prob": 4.515519776759902e-06}, {"id": 60, "seek": 32560, "start": 349.28000000000003, "end": 355.36, "text": " index writer and in this case we're just letting Tantavi select the number of threads so by", "tokens": [8186, 9936, 293, 294, 341, 1389, 321, 434, 445, 8295, 314, 394, 18442, 3048, 264, 1230, 295, 19314, 370, 538], "temperature": 0.0, "avg_logprob": -0.12749882611361416, "compression_ratio": 1.8284518828451883, "no_speech_prob": 4.515519776759902e-06}, {"id": 61, "seek": 35536, "start": 355.36, "end": 360.56, "text": " default sorry when you create this index writer and we give it a memory buffer in this case", "tokens": [7576, 2597, 562, 291, 1884, 341, 8186, 9936, 293, 321, 976, 309, 257, 4675, 21762, 294, 341, 1389], "temperature": 0.0, "avg_logprob": -0.12444237043272774, "compression_ratio": 1.7098039215686274, "no_speech_prob": 5.691826117981691e-06}, {"id": 62, "seek": 35536, "start": 360.56, "end": 368.48, "text": " about 50 megabytes. Tantavi will allocate n number of threads I think up to eight threads", "tokens": [466, 2625, 10816, 24538, 13, 314, 394, 18442, 486, 35713, 297, 1230, 295, 19314, 286, 519, 493, 281, 3180, 19314], "temperature": 0.0, "avg_logprob": -0.12444237043272774, "compression_ratio": 1.7098039215686274, "no_speech_prob": 5.691826117981691e-06}, {"id": 63, "seek": 35536, "start": 368.48, "end": 372.72, "text": " depending on what your system is using so you don't really have to put much thought", "tokens": [5413, 322, 437, 428, 1185, 307, 1228, 370, 291, 500, 380, 534, 362, 281, 829, 709, 1194], "temperature": 0.0, "avg_logprob": -0.12444237043272774, "compression_ratio": 1.7098039215686274, "no_speech_prob": 5.691826117981691e-06}, {"id": 64, "seek": 35536, "start": 372.72, "end": 377.0, "text": " into the multi-threaded indexing and then we're just adding a document really so we've", "tokens": [666, 264, 4825, 12, 392, 2538, 292, 8186, 278, 293, 550, 321, 434, 445, 5127, 257, 4166, 534, 370, 321, 600], "temperature": 0.0, "avg_logprob": -0.12444237043272774, "compression_ratio": 1.7098039215686274, "no_speech_prob": 5.691826117981691e-06}, {"id": 65, "seek": 35536, "start": 377.0, "end": 381.76, "text": " created our document we've added the text field we've given it in this case the old", "tokens": [2942, 527, 4166, 321, 600, 3869, 264, 2487, 2519, 321, 600, 2212, 309, 294, 341, 1389, 264, 1331], "temperature": 0.0, "avg_logprob": -0.12444237043272774, "compression_ratio": 1.7098039215686274, "no_speech_prob": 5.691826117981691e-06}, {"id": 66, "seek": 38176, "start": 381.76, "end": 387.4, "text": " man of the sea and we're going to put it to our indexer which is essentially just adding", "tokens": [587, 295, 264, 4158, 293, 321, 434, 516, 281, 829, 309, 281, 527, 8186, 260, 597, 307, 4476, 445, 5127], "temperature": 0.0, "avg_logprob": -0.11297323118965581, "compression_ratio": 1.7530364372469636, "no_speech_prob": 2.100975689245388e-05}, {"id": 67, "seek": 38176, "start": 387.4, "end": 392.64, "text": " it to a queue for the threads to pull off process spit out onto disk and then if we", "tokens": [309, 281, 257, 18639, 337, 264, 19314, 281, 2235, 766, 1399, 22127, 484, 3911, 12355, 293, 550, 498, 321], "temperature": 0.0, "avg_logprob": -0.11297323118965581, "compression_ratio": 1.7530364372469636, "no_speech_prob": 2.100975689245388e-05}, {"id": 68, "seek": 38176, "start": 392.64, "end": 398.44, "text": " want to actually have that be visible to our users for searching and things like that we", "tokens": [528, 281, 767, 362, 300, 312, 8974, 281, 527, 5022, 337, 10808, 293, 721, 411, 300, 321], "temperature": 0.0, "avg_logprob": -0.11297323118965581, "compression_ratio": 1.7530364372469636, "no_speech_prob": 2.100975689245388e-05}, {"id": 69, "seek": 38176, "start": 398.44, "end": 403.56, "text": " need to commit the index so in Tantavi you can either commit or you can roll back and", "tokens": [643, 281, 5599, 264, 8186, 370, 294, 314, 394, 18442, 291, 393, 2139, 5599, 420, 291, 393, 3373, 646, 293], "temperature": 0.0, "avg_logprob": -0.11297323118965581, "compression_ratio": 1.7530364372469636, "no_speech_prob": 2.100975689245388e-05}, {"id": 70, "seek": 38176, "start": 403.56, "end": 408.96, "text": " if you have a power failure midway through indexing when you reload from disk it will", "tokens": [498, 291, 362, 257, 1347, 7763, 2062, 676, 807, 8186, 278, 562, 291, 25628, 490, 12355, 309, 486], "temperature": 0.0, "avg_logprob": -0.11297323118965581, "compression_ratio": 1.7530364372469636, "no_speech_prob": 2.100975689245388e-05}, {"id": 71, "seek": 40896, "start": 408.96, "end": 413.79999999999995, "text": " be at the point of that last commit which is very very useful so you don't leave with", "tokens": [312, 412, 264, 935, 295, 300, 1036, 5599, 597, 307, 588, 588, 4420, 370, 291, 500, 380, 1856, 365], "temperature": 0.0, "avg_logprob": -0.08804531852797706, "compression_ratio": 1.8669527896995708, "no_speech_prob": 1.5674606402171776e-05}, {"id": 72, "seek": 40896, "start": 413.79999999999995, "end": 418.76, "text": " partial state and all that all that nasty things and then once we've done that we can", "tokens": [14641, 1785, 293, 439, 300, 439, 300, 17923, 721, 293, 550, 1564, 321, 600, 1096, 300, 321, 393], "temperature": 0.0, "avg_logprob": -0.08804531852797706, "compression_ratio": 1.8669527896995708, "no_speech_prob": 1.5674606402171776e-05}, {"id": 73, "seek": 40896, "start": 418.76, "end": 423.44, "text": " actually search and in this case you can either build queries using traits which are very", "tokens": [767, 3164, 293, 294, 341, 1389, 291, 393, 2139, 1322, 24109, 1228, 19526, 597, 366, 588], "temperature": 0.0, "avg_logprob": -0.08804531852797706, "compression_ratio": 1.8669527896995708, "no_speech_prob": 1.5674606402171776e-05}, {"id": 74, "seek": 40896, "start": 423.44, "end": 428.15999999999997, "text": " nice and you can mash them all together with lots of boxing and things or you can use", "tokens": [1481, 293, 291, 393, 31344, 552, 439, 1214, 365, 3195, 295, 24424, 293, 721, 420, 291, 393, 764], "temperature": 0.0, "avg_logprob": -0.08804531852797706, "compression_ratio": 1.8669527896995708, "no_speech_prob": 1.5674606402171776e-05}, {"id": 75, "seek": 40896, "start": 428.15999999999997, "end": 433.56, "text": " the query parser which basically parses a nice little query language in this case we've", "tokens": [264, 14581, 21156, 260, 597, 1936, 21156, 279, 257, 1481, 707, 14581, 2856, 294, 341, 1389, 321, 600], "temperature": 0.0, "avg_logprob": -0.08804531852797706, "compression_ratio": 1.8669527896995708, "no_speech_prob": 1.5674606402171776e-05}, {"id": 76, "seek": 43356, "start": 433.56, "end": 440.04, "text": " got a very simple phrase query as it's called trouble that up and it spits out a query for", "tokens": [658, 257, 588, 2199, 9535, 14581, 382, 309, 311, 1219, 5253, 300, 493, 293, 309, 637, 1208, 484, 257, 14581, 337], "temperature": 0.0, "avg_logprob": -0.15583571933564686, "compression_ratio": 1.7439613526570048, "no_speech_prob": 7.809615453879815e-06}, {"id": 77, "seek": 43356, "start": 440.04, "end": 446.2, "text": " us we then pass that into our search executor which in this case we're executing the query", "tokens": [505, 321, 550, 1320, 300, 666, 527, 3164, 7568, 284, 597, 294, 341, 1389, 321, 434, 32368, 264, 14581], "temperature": 0.0, "avg_logprob": -0.15583571933564686, "compression_ratio": 1.7439613526570048, "no_speech_prob": 7.809615453879815e-06}, {"id": 78, "seek": 43356, "start": 446.2, "end": 450.4, "text": " and then we're passing what called collectors and they are effectively just a simple thing", "tokens": [293, 550, 321, 434, 8437, 437, 1219, 35384, 293, 436, 366, 8659, 445, 257, 2199, 551], "temperature": 0.0, "avg_logprob": -0.15583571933564686, "compression_ratio": 1.7439613526570048, "no_speech_prob": 7.809615453879815e-06}, {"id": 79, "seek": 43356, "start": 450.4, "end": 456.56, "text": " to process the documents which are matched so in this case I believe we've got the count", "tokens": [281, 1399, 264, 8512, 597, 366, 21447, 370, 294, 341, 1389, 286, 1697, 321, 600, 658, 264, 1207], "temperature": 0.0, "avg_logprob": -0.15583571933564686, "compression_ratio": 1.7439613526570048, "no_speech_prob": 7.809615453879815e-06}, {"id": 80, "seek": 45656, "start": 456.56, "end": 463.68, "text": " collector and the top docs collector and the count collector does well it counts a big", "tokens": [23960, 293, 264, 1192, 45623, 23960, 293, 264, 1207, 23960, 775, 731, 309, 14893, 257, 955], "temperature": 0.0, "avg_logprob": -0.15388940020305356, "compression_ratio": 1.8402061855670102, "no_speech_prob": 1.070018424798036e-05}, {"id": 81, "seek": 45656, "start": 463.68, "end": 468.96, "text": " surprise there and we have the top docs which collects the top k documents up to a given", "tokens": [6365, 456, 293, 321, 362, 264, 1192, 45623, 597, 39897, 264, 1192, 350, 8512, 493, 281, 257, 2212], "temperature": 0.0, "avg_logprob": -0.15388940020305356, "compression_ratio": 1.8402061855670102, "no_speech_prob": 1.070018424798036e-05}, {"id": 82, "seek": 45656, "start": 468.96, "end": 473.72, "text": " limit so in this case we've selected 10 we only have one document to match so this doesn't", "tokens": [4948, 370, 294, 341, 1389, 321, 600, 8209, 1266, 321, 787, 362, 472, 4166, 281, 2995, 370, 341, 1177, 380], "temperature": 0.0, "avg_logprob": -0.15388940020305356, "compression_ratio": 1.8402061855670102, "no_speech_prob": 1.070018424798036e-05}, {"id": 83, "seek": 45656, "start": 473.72, "end": 479.08, "text": " matter that much but if you have more you can limit your results you can adjust how things", "tokens": [1871, 300, 709, 457, 498, 291, 362, 544, 291, 393, 4948, 428, 3542, 291, 393, 4369, 577, 721], "temperature": 0.0, "avg_logprob": -0.15388940020305356, "compression_ratio": 1.8402061855670102, "no_speech_prob": 1.070018424798036e-05}, {"id": 84, "seek": 47908, "start": 479.08, "end": 486.8, "text": " are scored etc. Now that's all well and good in this example but this doesn't actually", "tokens": [366, 18139, 5183, 13, 823, 300, 311, 439, 731, 293, 665, 294, 341, 1365, 457, 341, 1177, 380, 767], "temperature": 0.0, "avg_logprob": -0.16944945199148995, "compression_ratio": 1.6802973977695168, "no_speech_prob": 1.5813389836694114e-05}, {"id": 85, "seek": 47908, "start": 486.8, "end": 491.76, "text": " really account for spelling and as we discussed earlier users aren't very good at spelling", "tokens": [534, 2696, 337, 22254, 293, 382, 321, 7152, 3071, 5022, 3212, 380, 588, 665, 412, 22254], "temperature": 0.0, "avg_logprob": -0.16944945199148995, "compression_ratio": 1.6802973977695168, "no_speech_prob": 1.5813389836694114e-05}, {"id": 86, "seek": 47908, "start": 491.76, "end": 496.91999999999996, "text": " or at least I'm not so we maybe we want a bit of typo tolerance and in this case Tanturi", "tokens": [420, 412, 1935, 286, 478, 406, 370, 321, 1310, 321, 528, 257, 857, 295, 2125, 78, 23368, 293, 294, 341, 1389, 314, 394, 9744], "temperature": 0.0, "avg_logprob": -0.16944945199148995, "compression_ratio": 1.6802973977695168, "no_speech_prob": 1.5813389836694114e-05}, {"id": 87, "seek": 47908, "start": 496.91999999999996, "end": 501.68, "text": " does provide us with some additional way of doing this in the form of the fuzzy term query", "tokens": [775, 2893, 505, 365, 512, 4497, 636, 295, 884, 341, 294, 264, 1254, 295, 264, 34710, 1433, 14581], "temperature": 0.0, "avg_logprob": -0.16944945199148995, "compression_ratio": 1.6802973977695168, "no_speech_prob": 1.5813389836694114e-05}, {"id": 88, "seek": 47908, "start": 501.68, "end": 508.08, "text": " it uses something called lever science distance it's a very common form of effectively working", "tokens": [309, 4960, 746, 1219, 12451, 3497, 4560, 309, 311, 257, 588, 2689, 1254, 295, 8659, 1364], "temperature": 0.0, "avg_logprob": -0.16944945199148995, "compression_ratio": 1.6802973977695168, "no_speech_prob": 1.5813389836694114e-05}, {"id": 89, "seek": 50808, "start": 508.08, "end": 515.8, "text": " out how much modification you need to do to a word in order to actually get it to match", "tokens": [484, 577, 709, 26747, 291, 643, 281, 360, 281, 257, 1349, 294, 1668, 281, 767, 483, 309, 281, 2995], "temperature": 0.0, "avg_logprob": -0.12611872574378705, "compression_ratio": 1.7549019607843137, "no_speech_prob": 1.1946576705668122e-05}, {"id": 90, "seek": 50808, "start": 515.8, "end": 521.56, "text": " and we call that the edit distance as such typically you're between one and two edits", "tokens": [293, 321, 818, 300, 264, 8129, 4560, 382, 1270, 5850, 291, 434, 1296, 472, 293, 732, 41752], "temperature": 0.0, "avg_logprob": -0.12611872574378705, "compression_ratio": 1.7549019607843137, "no_speech_prob": 1.1946576705668122e-05}, {"id": 91, "seek": 50808, "start": 521.56, "end": 526.68, "text": " so you're swapping a word around you're removing it you're adding a new word a bit of magic", "tokens": [370, 291, 434, 1693, 10534, 257, 1349, 926, 291, 434, 12720, 309, 291, 434, 5127, 257, 777, 1349, 257, 857, 295, 5585], "temperature": 0.0, "avg_logprob": -0.12611872574378705, "compression_ratio": 1.7549019607843137, "no_speech_prob": 1.1946576705668122e-05}, {"id": 92, "seek": 50808, "start": 526.68, "end": 532.24, "text": " there really and as you can see at the bottom this is effectively if we use just the regular", "tokens": [456, 534, 293, 382, 291, 393, 536, 412, 264, 2767, 341, 307, 8659, 498, 321, 764, 445, 264, 3890], "temperature": 0.0, "avg_logprob": -0.12611872574378705, "compression_ratio": 1.7549019607843137, "no_speech_prob": 1.1946576705668122e-05}, {"id": 93, "seek": 53224, "start": 532.24, "end": 538.32, "text": " full text search well if we enter the term hello we'll only match with the word hello", "tokens": [1577, 2487, 3164, 731, 498, 321, 3242, 264, 1433, 7751, 321, 603, 787, 2995, 365, 264, 1349, 7751], "temperature": 0.0, "avg_logprob": -0.12500267264283735, "compression_ratio": 1.8864864864864865, "no_speech_prob": 1.733435783535242e-05}, {"id": 94, "seek": 53224, "start": 538.32, "end": 545.44, "text": " if we go with the term hell we'll only match with the word hell if we use some fuzzy term", "tokens": [498, 321, 352, 365, 264, 1433, 4921, 321, 603, 787, 2995, 365, 264, 1349, 4921, 498, 321, 764, 512, 34710, 1433], "temperature": 0.0, "avg_logprob": -0.12500267264283735, "compression_ratio": 1.8864864864864865, "no_speech_prob": 1.733435783535242e-05}, {"id": 95, "seek": 53224, "start": 545.44, "end": 550.24, "text": " query here we can actually match hell and hello which is very useful especially for", "tokens": [14581, 510, 321, 393, 767, 2995, 4921, 293, 7751, 597, 307, 588, 4420, 2318, 337], "temperature": 0.0, "avg_logprob": -0.12500267264283735, "compression_ratio": 1.8864864864864865, "no_speech_prob": 1.733435783535242e-05}, {"id": 96, "seek": 53224, "start": 550.24, "end": 557.4, "text": " the prefix search this is built upon Tanturi's inverted index which uses something called", "tokens": [264, 46969, 3164, 341, 307, 3094, 3564, 314, 394, 9744, 311, 38969, 8186, 597, 4960, 746, 1219], "temperature": 0.0, "avg_logprob": -0.12500267264283735, "compression_ratio": 1.8864864864864865, "no_speech_prob": 1.733435783535242e-05}, {"id": 97, "seek": 55740, "start": 557.4, "end": 563.92, "text": " a FST which is effectively a fancy word for saying we threw state machines at it and then", "tokens": [257, 479, 6840, 597, 307, 8659, 257, 10247, 1349, 337, 1566, 321, 11918, 1785, 8379, 412, 309, 293, 550], "temperature": 0.0, "avg_logprob": -0.11598966261919808, "compression_ratio": 1.5733333333333333, "no_speech_prob": 1.9571187294786796e-05}, {"id": 98, "seek": 55740, "start": 563.92, "end": 569.3199999999999, "text": " made them return results that's as much as I can describe how they work the person who", "tokens": [1027, 552, 2736, 3542, 300, 311, 382, 709, 382, 286, 393, 6786, 577, 436, 589, 264, 954, 567], "temperature": 0.0, "avg_logprob": -0.11598966261919808, "compression_ratio": 1.5733333333333333, "no_speech_prob": 1.9571187294786796e-05}, {"id": 99, "seek": 55740, "start": 569.3199999999999, "end": 576.3199999999999, "text": " originally wrote the FST library in Rust burnt sushi he has a blog on this goes into a lot", "tokens": [7993, 4114, 264, 479, 6840, 6405, 294, 34952, 18901, 23022, 415, 575, 257, 6968, 322, 341, 1709, 666, 257, 688], "temperature": 0.0, "avg_logprob": -0.11598966261919808, "compression_ratio": 1.5733333333333333, "no_speech_prob": 1.9571187294786796e-05}, {"id": 100, "seek": 55740, "start": 576.3199999999999, "end": 581.12, "text": " of depth really really useful for that sort of thing but I can't elaborate any more on", "tokens": [295, 7161, 534, 534, 4420, 337, 300, 1333, 295, 551, 457, 286, 393, 380, 20945, 604, 544, 322], "temperature": 0.0, "avg_logprob": -0.11598966261919808, "compression_ratio": 1.5733333333333333, "no_speech_prob": 1.9571187294786796e-05}, {"id": 101, "seek": 58112, "start": 581.12, "end": 588.0, "text": " that but all of this additional walking through our index and matching these additional words", "tokens": [300, 457, 439, 295, 341, 4497, 4494, 807, 527, 8186, 293, 14324, 613, 4497, 2283], "temperature": 0.0, "avg_logprob": -0.1295088897516698, "compression_ratio": 1.6529680365296804, "no_speech_prob": 7.197131253633415e-06}, {"id": 102, "seek": 58112, "start": 588.0, "end": 595.12, "text": " does come at the cost of some additional CPU and once we've sort of got that what we're", "tokens": [775, 808, 412, 264, 2063, 295, 512, 4497, 13199, 293, 1564, 321, 600, 1333, 295, 658, 300, 437, 321, 434], "temperature": 0.0, "avg_logprob": -0.1295088897516698, "compression_ratio": 1.6529680365296804, "no_speech_prob": 7.197131253633415e-06}, {"id": 103, "seek": 58112, "start": 595.12, "end": 600.44, "text": " left with is this nice block of data on our disks really so we have some metadata files", "tokens": [1411, 365, 307, 341, 1481, 3461, 295, 1412, 322, 527, 41617, 534, 370, 321, 362, 512, 26603, 7098], "temperature": 0.0, "avg_logprob": -0.1295088897516698, "compression_ratio": 1.6529680365296804, "no_speech_prob": 7.197131253633415e-06}, {"id": 104, "seek": 58112, "start": 600.44, "end": 606.84, "text": " here in particular meta meta.json that contains your schema along with a couple other things", "tokens": [510, 294, 1729, 19616, 19616, 13, 73, 3015, 300, 8306, 428, 34078, 2051, 365, 257, 1916, 661, 721], "temperature": 0.0, "avg_logprob": -0.1295088897516698, "compression_ratio": 1.6529680365296804, "no_speech_prob": 7.197131253633415e-06}, {"id": 105, "seek": 60684, "start": 606.84, "end": 611.4, "text": " and we have our sort of core files which look very similar if they look very similar to", "tokens": [293, 321, 362, 527, 1333, 295, 4965, 7098, 597, 574, 588, 2531, 498, 436, 574, 588, 2531, 281], "temperature": 0.0, "avg_logprob": -0.19794894487429887, "compression_ratio": 1.8333333333333333, "no_speech_prob": 2.9418231861200184e-05}, {"id": 106, "seek": 60684, "start": 611.4, "end": 617.08, "text": " these scenes that's because they are in particular we have our field norms our terms our store", "tokens": [613, 8026, 300, 311, 570, 436, 366, 294, 1729, 321, 362, 527, 2519, 24357, 527, 2115, 527, 3531], "temperature": 0.0, "avg_logprob": -0.19794894487429887, "compression_ratio": 1.8333333333333333, "no_speech_prob": 2.9418231861200184e-05}, {"id": 107, "seek": 60684, "start": 617.08, "end": 626.1600000000001, "text": " which is effectively a row level store log file our positions our IDs and our fast fields", "tokens": [597, 307, 8659, 257, 5386, 1496, 3531, 3565, 3991, 527, 8432, 527, 48212, 293, 527, 2370, 7909], "temperature": 0.0, "avg_logprob": -0.19794894487429887, "compression_ratio": 1.8333333333333333, "no_speech_prob": 2.9418231861200184e-05}, {"id": 108, "seek": 60684, "start": 626.1600000000001, "end": 636.12, "text": " and fast fields are effectively fast because we cut somewhat simple and equally vague name", "tokens": [293, 2370, 7909, 366, 8659, 2370, 570, 321, 1723, 8344, 2199, 293, 12309, 24247, 1315], "temperature": 0.0, "avg_logprob": -0.19794894487429887, "compression_ratio": 1.8333333333333333, "no_speech_prob": 2.9418231861200184e-05}, {"id": 109, "seek": 63612, "start": 636.12, "end": 642.28, "text": " but now that we've got all this stuff on disk if we wrap it up in an API we sort of we've", "tokens": [457, 586, 300, 321, 600, 658, 439, 341, 1507, 322, 12355, 498, 321, 7019, 309, 493, 294, 364, 9362, 321, 1333, 295, 321, 600], "temperature": 0.0, "avg_logprob": -0.13947267532348634, "compression_ratio": 1.703883495145631, "no_speech_prob": 1.707942101347726e-05}, {"id": 110, "seek": 63612, "start": 642.28, "end": 647.4, "text": " got that we've mostly we've got everything in this case we've got a demo of LNX working", "tokens": [658, 300, 321, 600, 5240, 321, 600, 658, 1203, 294, 341, 1389, 321, 600, 658, 257, 10723, 295, 441, 45, 55, 1364], "temperature": 0.0, "avg_logprob": -0.13947267532348634, "compression_ratio": 1.703883495145631, "no_speech_prob": 1.707942101347726e-05}, {"id": 111, "seek": 63612, "start": 647.4, "end": 653.4, "text": " here and we've got about I think 27 million documents and we're searching it with about", "tokens": [510, 293, 321, 600, 658, 466, 286, 519, 7634, 2459, 8512, 293, 321, 434, 10808, 309, 365, 466], "temperature": 0.0, "avg_logprob": -0.13947267532348634, "compression_ratio": 1.703883495145631, "no_speech_prob": 1.707942101347726e-05}, {"id": 112, "seek": 63612, "start": 653.4, "end": 659.64, "text": " millisecond latency I think in total it's about 20 gigabytes on disk compressed which", "tokens": [27940, 18882, 27043, 286, 519, 294, 3217, 309, 311, 466, 945, 42741, 322, 12355, 30353, 597], "temperature": 0.0, "avg_logprob": -0.13947267532348634, "compression_ratio": 1.703883495145631, "no_speech_prob": 1.707942101347726e-05}, {"id": 113, "seek": 65964, "start": 659.64, "end": 666.88, "text": " is pretty nice but there's sort of a bit of an issue here which is if we deploy this production", "tokens": [307, 1238, 1481, 457, 456, 311, 1333, 295, 257, 857, 295, 364, 2734, 510, 597, 307, 498, 321, 7274, 341, 4265], "temperature": 0.0, "avg_logprob": -0.12035191733882113, "compression_ratio": 1.800796812749004, "no_speech_prob": 4.970809641235974e-06}, {"id": 114, "seek": 65964, "start": 666.88, "end": 673.52, "text": " and our site is very nice we get lots of traffic things increase we go hmm well search traffic", "tokens": [293, 527, 3621, 307, 588, 1481, 321, 483, 3195, 295, 6419, 721, 3488, 321, 352, 16478, 731, 3164, 6419], "temperature": 0.0, "avg_logprob": -0.12035191733882113, "compression_ratio": 1.800796812749004, "no_speech_prob": 4.970809641235974e-06}, {"id": 115, "seek": 65964, "start": 673.52, "end": 677.52, "text": " is increased our server is not coping let's just scale up the server and we can repeat", "tokens": [307, 6505, 527, 7154, 307, 406, 32893, 718, 311, 445, 4373, 493, 264, 7154, 293, 321, 393, 7149], "temperature": 0.0, "avg_logprob": -0.12035191733882113, "compression_ratio": 1.800796812749004, "no_speech_prob": 4.970809641235974e-06}, {"id": 116, "seek": 65964, "start": 677.52, "end": 684.52, "text": " this for quite a lot and in fact things like AWS allow you a stupid amount of cores and", "tokens": [341, 337, 1596, 257, 688, 293, 294, 1186, 721, 411, 17650, 2089, 291, 257, 6631, 2372, 295, 24826, 293], "temperature": 0.0, "avg_logprob": -0.12035191733882113, "compression_ratio": 1.800796812749004, "no_speech_prob": 4.970809641235974e-06}, {"id": 117, "seek": 65964, "start": 684.52, "end": 689.24, "text": " things like that which you can scale up very easily but you keep going along with this", "tokens": [721, 411, 300, 597, 291, 393, 4373, 493, 588, 3612, 457, 291, 1066, 516, 2051, 365, 341], "temperature": 0.0, "avg_logprob": -0.12035191733882113, "compression_ratio": 1.800796812749004, "no_speech_prob": 4.970809641235974e-06}, {"id": 118, "seek": 68924, "start": 689.24, "end": 694.6800000000001, "text": " and eventually something happens and in this case your data centers burnt down if anyone", "tokens": [293, 4728, 746, 2314, 293, 294, 341, 1389, 428, 1412, 10898, 18901, 760, 498, 2878], "temperature": 0.0, "avg_logprob": -0.1481750712675207, "compression_ratio": 1.7470817120622568, "no_speech_prob": 5.6213553762063384e-05}, {"id": 119, "seek": 68924, "start": 694.6800000000001, "end": 701.72, "text": " remembers this this happened in 2021 OVH basically caught fire and that was an end of I think", "tokens": [26228, 341, 341, 2011, 294, 7201, 422, 53, 39, 1936, 5415, 2610, 293, 300, 390, 364, 917, 295, 286, 519], "temperature": 0.0, "avg_logprob": -0.1481750712675207, "compression_ratio": 1.7470817120622568, "no_speech_prob": 5.6213553762063384e-05}, {"id": 120, "seek": 68924, "start": 701.72, "end": 708.4, "text": " a lot of sleeping people and so yeah your data centers on fire search isn't able to do anything", "tokens": [257, 688, 295, 8296, 561, 293, 370, 1338, 428, 1412, 10898, 322, 2610, 3164, 1943, 380, 1075, 281, 360, 1340], "temperature": 0.0, "avg_logprob": -0.1481750712675207, "compression_ratio": 1.7470817120622568, "no_speech_prob": 5.6213553762063384e-05}, {"id": 121, "seek": 68924, "start": 708.4, "end": 712.48, "text": " you're losing losing money no one's buying anything management's breathing down your", "tokens": [291, 434, 7027, 7027, 1460, 572, 472, 311, 6382, 1340, 4592, 311, 9570, 760, 428], "temperature": 0.0, "avg_logprob": -0.1481750712675207, "compression_ratio": 1.7470817120622568, "no_speech_prob": 5.6213553762063384e-05}, {"id": 122, "seek": 68924, "start": 712.48, "end": 717.48, "text": " neck for a fix you're having to load from a backup what are you gonna do and well you", "tokens": [6189, 337, 257, 3191, 291, 434, 1419, 281, 3677, 490, 257, 14807, 437, 366, 291, 799, 360, 293, 731, 291], "temperature": 0.0, "avg_logprob": -0.1481750712675207, "compression_ratio": 1.7470817120622568, "no_speech_prob": 5.6213553762063384e-05}, {"id": 123, "seek": 71748, "start": 717.48, "end": 724.04, "text": " think ah I should have made some replicas I should have done something called high availability", "tokens": [519, 3716, 286, 820, 362, 1027, 512, 3248, 9150, 286, 820, 362, 1096, 746, 1219, 1090, 17945], "temperature": 0.0, "avg_logprob": -0.10416138526236657, "compression_ratio": 1.8744769874476988, "no_speech_prob": 1.4319646652438678e-05}, {"id": 124, "seek": 71748, "start": 724.04, "end": 728.2, "text": " and in this case what this means is we have instead of having one node on one server ready", "tokens": [293, 294, 341, 1389, 437, 341, 1355, 307, 321, 362, 2602, 295, 1419, 472, 9984, 322, 472, 7154, 1919], "temperature": 0.0, "avg_logprob": -0.10416138526236657, "compression_ratio": 1.8744769874476988, "no_speech_prob": 1.4319646652438678e-05}, {"id": 125, "seek": 71748, "start": 728.2, "end": 734.32, "text": " to burn down we have three nodes available to burn down at any point in time and in this", "tokens": [281, 5064, 760, 321, 362, 1045, 13891, 2435, 281, 5064, 760, 412, 604, 935, 294, 565, 293, 294, 341], "temperature": 0.0, "avg_logprob": -0.10416138526236657, "compression_ratio": 1.8744769874476988, "no_speech_prob": 1.4319646652438678e-05}, {"id": 126, "seek": 71748, "start": 734.32, "end": 738.0, "text": " case we hope that we put them in different what are called availability zones which", "tokens": [1389, 321, 1454, 300, 321, 829, 552, 294, 819, 437, 366, 1219, 17945, 16025, 597], "temperature": 0.0, "avg_logprob": -0.10416138526236657, "compression_ratio": 1.8744769874476988, "no_speech_prob": 1.4319646652438678e-05}, {"id": 127, "seek": 71748, "start": 738.0, "end": 742.5600000000001, "text": " mean hey if one data center burns down there's a very small likelihood or at least as it", "tokens": [914, 4177, 498, 472, 1412, 3056, 22684, 760, 456, 311, 257, 588, 1359, 22119, 420, 412, 1935, 382, 309], "temperature": 0.0, "avg_logprob": -0.10416138526236657, "compression_ratio": 1.8744769874476988, "no_speech_prob": 1.4319646652438678e-05}, {"id": 128, "seek": 74256, "start": 742.56, "end": 748.52, "text": " possible for another data center to burn down in the meantime and this allows us to effectively", "tokens": [1944, 337, 1071, 1412, 3056, 281, 5064, 760, 294, 264, 14991, 293, 341, 4045, 505, 281, 8659], "temperature": 0.0, "avg_logprob": -0.14646111488342284, "compression_ratio": 1.7411764705882353, "no_speech_prob": 1.7537216990604065e-05}, {"id": 129, "seek": 74256, "start": 748.52, "end": 753.8, "text": " operate even though one server is currently on fire or lost to the ether or I don't know", "tokens": [9651, 754, 1673, 472, 7154, 307, 4362, 322, 2610, 420, 2731, 281, 264, 37096, 420, 286, 500, 380, 458], "temperature": 0.0, "avg_logprob": -0.14646111488342284, "compression_ratio": 1.7411764705882353, "no_speech_prob": 1.7537216990604065e-05}, {"id": 130, "seek": 74256, "start": 753.8, "end": 760.16, "text": " network has torn itself to pieces and this does also mean we can upgrade if we want to", "tokens": [3209, 575, 10885, 2564, 281, 3755, 293, 341, 775, 611, 914, 321, 393, 11484, 498, 321, 528, 281], "temperature": 0.0, "avg_logprob": -0.14646111488342284, "compression_ratio": 1.7411764705882353, "no_speech_prob": 1.7537216990604065e-05}, {"id": 131, "seek": 74256, "start": 760.16, "end": 763.68, "text": " tear a server down and we want to restart it with some newer hardware we can do that", "tokens": [12556, 257, 7154, 760, 293, 321, 528, 281, 21022, 309, 365, 512, 17628, 8837, 321, 393, 360, 300], "temperature": 0.0, "avg_logprob": -0.14646111488342284, "compression_ratio": 1.7411764705882353, "no_speech_prob": 1.7537216990604065e-05}, {"id": 132, "seek": 74256, "start": 763.68, "end": 769.64, "text": " without interrupting our existing system but this is sort of a hard thing to do because", "tokens": [1553, 49455, 527, 6741, 1185, 457, 341, 307, 1333, 295, 257, 1152, 551, 281, 360, 570], "temperature": 0.0, "avg_logprob": -0.14646111488342284, "compression_ratio": 1.7411764705882353, "no_speech_prob": 1.7537216990604065e-05}, {"id": 133, "seek": 76964, "start": 769.64, "end": 774.4, "text": " now we've got to work out a way of getting the same documents across all of our nodes", "tokens": [586, 321, 600, 658, 281, 589, 484, 257, 636, 295, 1242, 264, 912, 8512, 2108, 439, 295, 527, 13891], "temperature": 0.0, "avg_logprob": -0.13413071182538877, "compression_ratio": 1.7333333333333334, "no_speech_prob": 5.173109911993379e-06}, {"id": 134, "seek": 76964, "start": 774.4, "end": 779.0, "text": " in this case it's sort of a share nothing architecture this is done by elastic search", "tokens": [294, 341, 1389, 309, 311, 1333, 295, 257, 2073, 1825, 9482, 341, 307, 1096, 538, 17115, 3164], "temperature": 0.0, "avg_logprob": -0.13413071182538877, "compression_ratio": 1.7333333333333334, "no_speech_prob": 5.173109911993379e-06}, {"id": 135, "seek": 76964, "start": 779.0, "end": 785.1999999999999, "text": " and basically most most systems so we're just replicating the documents we're not replicating", "tokens": [293, 1936, 881, 881, 3652, 370, 321, 434, 445, 3248, 30541, 264, 8512, 321, 434, 406, 3248, 30541], "temperature": 0.0, "avg_logprob": -0.13413071182538877, "compression_ratio": 1.7333333333333334, "no_speech_prob": 5.173109911993379e-06}, {"id": 136, "seek": 76964, "start": 785.1999999999999, "end": 791.1999999999999, "text": " all of that process data we've just done we need to apply them to each node and doing", "tokens": [439, 295, 300, 1399, 1412, 321, 600, 445, 1096, 321, 643, 281, 3079, 552, 281, 1184, 9984, 293, 884], "temperature": 0.0, "avg_logprob": -0.13413071182538877, "compression_ratio": 1.7333333333333334, "no_speech_prob": 5.173109911993379e-06}, {"id": 137, "seek": 76964, "start": 791.1999999999999, "end": 795.84, "text": " this approach makes it a bit simpler in reality LNX and QuickWit do something a little bit", "tokens": [341, 3109, 1669, 309, 257, 857, 18587, 294, 4103, 441, 45, 55, 293, 12101, 54, 270, 360, 746, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.13413071182538877, "compression_ratio": 1.7333333333333334, "no_speech_prob": 5.173109911993379e-06}, {"id": 138, "seek": 79584, "start": 795.84, "end": 802.0400000000001, "text": " different but this is this is easier I say this is easier because the initial solution", "tokens": [819, 457, 341, 307, 341, 307, 3571, 286, 584, 341, 307, 3571, 570, 264, 5883, 3827], "temperature": 0.0, "avg_logprob": -0.16388748310230397, "compression_ratio": 1.76953125, "no_speech_prob": 1.102140322473133e-05}, {"id": 139, "seek": 79584, "start": 802.0400000000001, "end": 806.8000000000001, "text": " would be you know just just spin up more nodes you know what can add some RPC in there what", "tokens": [576, 312, 291, 458, 445, 445, 6060, 493, 544, 13891, 291, 458, 437, 393, 909, 512, 497, 12986, 294, 456, 437], "temperature": 0.0, "avg_logprob": -0.16388748310230397, "compression_ratio": 1.76953125, "no_speech_prob": 1.102140322473133e-05}, {"id": 140, "seek": 79584, "start": 806.8000000000001, "end": 812.2, "text": " can go wrong and then deep down you work out it's like oh do you mean networks are reliable", "tokens": [393, 352, 2085, 293, 550, 2452, 760, 291, 589, 484, 309, 311, 411, 1954, 360, 291, 914, 9590, 366, 12924], "temperature": 0.0, "avg_logprob": -0.16388748310230397, "compression_ratio": 1.76953125, "no_speech_prob": 1.102140322473133e-05}, {"id": 141, "seek": 79584, "start": 812.2, "end": 817.96, "text": " what's a raft and things like that and so at that point you go okay this is this is harder", "tokens": [437, 311, 257, 43863, 293, 721, 411, 300, 293, 370, 412, 300, 935, 291, 352, 1392, 341, 307, 341, 307, 6081], "temperature": 0.0, "avg_logprob": -0.16388748310230397, "compression_ratio": 1.76953125, "no_speech_prob": 1.102140322473133e-05}, {"id": 142, "seek": 79584, "start": 817.96, "end": 822.5600000000001, "text": " than I thought and you realize the world is in fact a scary place outside your happy little", "tokens": [813, 286, 1194, 293, 291, 4325, 264, 1002, 307, 294, 1186, 257, 6958, 1081, 2380, 428, 2055, 707], "temperature": 0.0, "avg_logprob": -0.16388748310230397, "compression_ratio": 1.76953125, "no_speech_prob": 1.102140322473133e-05}, {"id": 143, "seek": 82256, "start": 822.56, "end": 830.0, "text": " data center and you need some way of organizing states independent on things catching on fire", "tokens": [1412, 3056, 293, 291, 643, 512, 636, 295, 17608, 4368, 6695, 322, 721, 16124, 322, 2610], "temperature": 0.0, "avg_logprob": -0.1384457996913365, "compression_ratio": 1.7213740458015268, "no_speech_prob": 8.088776667136699e-06}, {"id": 144, "seek": 82256, "start": 830.0, "end": 835.3199999999999, "text": " and this is this is a hard problem to solve and so you have a little look around and you", "tokens": [293, 341, 307, 341, 307, 257, 1152, 1154, 281, 5039, 293, 370, 291, 362, 257, 707, 574, 926, 293, 291], "temperature": 0.0, "avg_logprob": -0.1384457996913365, "compression_ratio": 1.7213740458015268, "no_speech_prob": 8.088776667136699e-06}, {"id": 145, "seek": 82256, "start": 835.3199999999999, "end": 840.92, "text": " go well Rust is quite a new system it's quite a young ecosystem we're quite limited so we", "tokens": [352, 731, 34952, 307, 1596, 257, 777, 1185, 309, 311, 1596, 257, 2037, 11311, 321, 434, 1596, 5567, 370, 321], "temperature": 0.0, "avg_logprob": -0.1384457996913365, "compression_ratio": 1.7213740458015268, "no_speech_prob": 8.088776667136699e-06}, {"id": 146, "seek": 82256, "start": 840.92, "end": 846.16, "text": " can't necessarily pick a Paxos implementation off the shelf we maybe have something called", "tokens": [393, 380, 4725, 1888, 257, 430, 2797, 329, 11420, 766, 264, 15222, 321, 1310, 362, 746, 1219], "temperature": 0.0, "avg_logprob": -0.1384457996913365, "compression_ratio": 1.7213740458015268, "no_speech_prob": 8.088776667136699e-06}, {"id": 147, "seek": 82256, "start": 846.16, "end": 852.4, "text": " raft so that's a leader-based approach and that means we elect a leader and we say okay", "tokens": [43863, 370, 300, 311, 257, 5263, 12, 6032, 3109, 293, 300, 1355, 321, 2185, 257, 5263, 293, 321, 584, 1392], "temperature": 0.0, "avg_logprob": -0.1384457996913365, "compression_ratio": 1.7213740458015268, "no_speech_prob": 8.088776667136699e-06}, {"id": 148, "seek": 85240, "start": 852.4, "end": 858.36, "text": " leader tell us what to do and it will say okay you you handle these documents go go do things", "tokens": [5263, 980, 505, 437, 281, 360, 293, 309, 486, 584, 1392, 291, 291, 4813, 613, 8512, 352, 352, 360, 721], "temperature": 0.0, "avg_logprob": -0.15791606903076172, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.8668559278012253e-05}, {"id": 149, "seek": 85240, "start": 858.36, "end": 863.4, "text": " with them it's a very well-known algorithm very easy to understand it's probably the", "tokens": [365, 552, 309, 311, 257, 588, 731, 12, 6861, 9284, 588, 1858, 281, 1223, 309, 311, 1391, 264], "temperature": 0.0, "avg_logprob": -0.15791606903076172, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.8668559278012253e-05}, {"id": 150, "seek": 85240, "start": 863.4, "end": 868.24, "text": " only algorithm which is really implemented widely in Rust so there's two implementations", "tokens": [787, 9284, 597, 307, 534, 12270, 13371, 294, 34952, 370, 456, 311, 732, 4445, 763], "temperature": 0.0, "avg_logprob": -0.15791606903076172, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.8668559278012253e-05}, {"id": 151, "seek": 85240, "start": 868.24, "end": 873.88, "text": " one of them by the pink cap group called raft RS and the other by data fuse labs called", "tokens": [472, 295, 552, 538, 264, 7022, 1410, 1594, 1219, 43863, 25855, 293, 264, 661, 538, 1412, 31328, 20339, 1219], "temperature": 0.0, "avg_logprob": -0.15791606903076172, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.8668559278012253e-05}, {"id": 152, "seek": 87388, "start": 873.88, "end": 883.4399999999999, "text": " open raft varying levels of completion or pre-made so in this case you think okay I don't really", "tokens": [1269, 43863, 22984, 4358, 295, 19372, 420, 659, 12, 10341, 370, 294, 341, 1389, 291, 519, 1392, 286, 500, 380, 534], "temperature": 0.0, "avg_logprob": -0.11447245103341562, "compression_ratio": 1.6977611940298507, "no_speech_prob": 1.5545085261692293e-05}, {"id": 153, "seek": 87388, "start": 883.4399999999999, "end": 889.72, "text": " know what I'm doing here so maybe I shouldn't be managing my own raft cluster and you hear", "tokens": [458, 437, 286, 478, 884, 510, 370, 1310, 286, 4659, 380, 312, 11642, 452, 1065, 43863, 13630, 293, 291, 1568], "temperature": 0.0, "avg_logprob": -0.11447245103341562, "compression_ratio": 1.6977611940298507, "no_speech_prob": 1.5545085261692293e-05}, {"id": 154, "seek": 87388, "start": 889.72, "end": 894.64, "text": " something about eventual consistency and you hear oh it's it's leaderless any any node can", "tokens": [746, 466, 33160, 14416, 293, 291, 1568, 1954, 309, 311, 309, 311, 5263, 1832, 604, 604, 9984, 393], "temperature": 0.0, "avg_logprob": -0.11447245103341562, "compression_ratio": 1.6977611940298507, "no_speech_prob": 1.5545085261692293e-05}, {"id": 155, "seek": 87388, "start": 894.64, "end": 898.2, "text": " handle the rights and then ship off to the other nodes as long as the operations are", "tokens": [4813, 264, 4601, 293, 550, 5374, 766, 281, 264, 661, 13891, 382, 938, 382, 264, 7705, 366], "temperature": 0.0, "avg_logprob": -0.11447245103341562, "compression_ratio": 1.6977611940298507, "no_speech_prob": 1.5545085261692293e-05}, {"id": 156, "seek": 87388, "start": 898.2, "end": 903.0, "text": " idempotent and that's a very key point which means you can basically ship the same document", "tokens": [4496, 15970, 310, 317, 293, 300, 311, 257, 588, 2141, 935, 597, 1355, 291, 393, 1936, 5374, 264, 912, 4166], "temperature": 0.0, "avg_logprob": -0.11447245103341562, "compression_ratio": 1.6977611940298507, "no_speech_prob": 1.5545085261692293e-05}, {"id": 157, "seek": 90300, "start": 903.0, "end": 907.96, "text": " over and over and over again and they're not going to duplicate themselves or at least", "tokens": [670, 293, 670, 293, 670, 797, 293, 436, 434, 406, 516, 281, 23976, 2969, 420, 412, 1935], "temperature": 0.0, "avg_logprob": -0.10805012629582332, "compression_ratio": 1.6859903381642511, "no_speech_prob": 4.539077417575754e-05}, {"id": 158, "seek": 90300, "start": 907.96, "end": 914.08, "text": " they don't act like they duplicate and this gives us realistically a bit more freedom", "tokens": [436, 500, 380, 605, 411, 436, 23976, 293, 341, 2709, 505, 40734, 257, 857, 544, 5645], "temperature": 0.0, "avg_logprob": -0.10805012629582332, "compression_ratio": 1.6859903381642511, "no_speech_prob": 4.539077417575754e-05}, {"id": 159, "seek": 90300, "start": 914.08, "end": 920.8, "text": " if we want to change we can change and so we decide let's go with eventual consistency", "tokens": [498, 321, 528, 281, 1319, 321, 393, 1319, 293, 370, 321, 4536, 718, 311, 352, 365, 33160, 14416], "temperature": 0.0, "avg_logprob": -0.10805012629582332, "compression_ratio": 1.6859903381642511, "no_speech_prob": 4.539077417575754e-05}, {"id": 160, "seek": 90300, "start": 920.8, "end": 929.48, "text": " because yeah I like an easy life and it seemed easier yes people laughing will agree that", "tokens": [570, 1338, 286, 411, 364, 1858, 993, 293, 309, 6576, 3571, 2086, 561, 5059, 486, 3986, 300], "temperature": 0.0, "avg_logprob": -0.10805012629582332, "compression_ratio": 1.6859903381642511, "no_speech_prob": 4.539077417575754e-05}, {"id": 161, "seek": 92948, "start": 929.48, "end": 935.76, "text": " yes things that seem easier probably aren't and so our diagram sort of looks something", "tokens": [2086, 721, 300, 1643, 3571, 1391, 3212, 380, 293, 370, 527, 10686, 1333, 295, 1542, 746], "temperature": 0.0, "avg_logprob": -0.11295164411313066, "compression_ratio": 1.7892561983471074, "no_speech_prob": 1.2382143722788896e-05}, {"id": 162, "seek": 92948, "start": 935.76, "end": 939.72, "text": " like this and I'm scared to cross the white line so I'll try and point but we have step", "tokens": [411, 341, 293, 286, 478, 5338, 281, 3278, 264, 2418, 1622, 370, 286, 603, 853, 293, 935, 457, 321, 362, 1823], "temperature": 0.0, "avg_logprob": -0.11295164411313066, "compression_ratio": 1.7892561983471074, "no_speech_prob": 1.2382143722788896e-05}, {"id": 163, "seek": 92948, "start": 939.72, "end": 946.44, "text": " one a client sends the documents to a any node it doesn't really care which one that", "tokens": [472, 257, 6423, 14790, 264, 8512, 281, 257, 604, 9984, 309, 1177, 380, 534, 1127, 597, 472, 300], "temperature": 0.0, "avg_logprob": -0.11295164411313066, "compression_ratio": 1.7892561983471074, "no_speech_prob": 1.2382143722788896e-05}, {"id": 164, "seek": 92948, "start": 946.44, "end": 950.6800000000001, "text": " client then goes okay I'm going to send it to some of my peers and then wait for them", "tokens": [6423, 550, 1709, 1392, 286, 478, 516, 281, 2845, 309, 281, 512, 295, 452, 16739, 293, 550, 1699, 337, 552], "temperature": 0.0, "avg_logprob": -0.11295164411313066, "compression_ratio": 1.7892561983471074, "no_speech_prob": 1.2382143722788896e-05}, {"id": 165, "seek": 92948, "start": 950.6800000000001, "end": 954.96, "text": " to tell me that they've got the document it's safe and then once we've got the majority", "tokens": [281, 980, 385, 300, 436, 600, 658, 264, 4166, 309, 311, 3273, 293, 550, 1564, 321, 600, 658, 264, 6286], "temperature": 0.0, "avg_logprob": -0.11295164411313066, "compression_ratio": 1.7892561983471074, "no_speech_prob": 1.2382143722788896e-05}, {"id": 166, "seek": 95496, "start": 954.96, "end": 960.24, "text": " which is a very common approach in these systems we can tell the client okay your document", "tokens": [597, 307, 257, 588, 2689, 3109, 294, 613, 3652, 321, 393, 980, 264, 6423, 1392, 428, 4166], "temperature": 0.0, "avg_logprob": -0.10594641367594401, "compression_ratio": 1.700374531835206, "no_speech_prob": 1.291633270739112e-05}, {"id": 167, "seek": 95496, "start": 960.24, "end": 965.6800000000001, "text": " is safe even if OHV burns down again we're probably going to be okay it doesn't need", "tokens": [307, 3273, 754, 498, 13931, 53, 22684, 760, 797, 321, 434, 1391, 516, 281, 312, 1392, 309, 1177, 380, 643], "temperature": 0.0, "avg_logprob": -0.10594641367594401, "compression_ratio": 1.700374531835206, "no_speech_prob": 1.291633270739112e-05}, {"id": 168, "seek": 95496, "start": 965.6800000000001, "end": 970.08, "text": " to wait for all of the nodes to respond because otherwise you're not really highly available", "tokens": [281, 1699, 337, 439, 295, 264, 13891, 281, 4196, 570, 5911, 291, 434, 406, 534, 5405, 2435], "temperature": 0.0, "avg_logprob": -0.10594641367594401, "compression_ratio": 1.700374531835206, "no_speech_prob": 1.291633270739112e-05}, {"id": 169, "seek": 95496, "start": 970.08, "end": 975.84, "text": " because if one node goes down you can't progress and so this system is this system is pretty", "tokens": [570, 498, 472, 9984, 1709, 760, 291, 393, 380, 4205, 293, 370, 341, 1185, 307, 341, 1185, 307, 1238], "temperature": 0.0, "avg_logprob": -0.10594641367594401, "compression_ratio": 1.700374531835206, "no_speech_prob": 1.291633270739112e-05}, {"id": 170, "seek": 95496, "start": 975.84, "end": 983.08, "text": " good there's just one small problem which is how in God's name do you do this many questions", "tokens": [665, 456, 311, 445, 472, 1359, 1154, 597, 307, 577, 294, 1265, 311, 1315, 360, 291, 360, 341, 867, 1651], "temperature": 0.0, "avg_logprob": -0.10594641367594401, "compression_ratio": 1.700374531835206, "no_speech_prob": 1.291633270739112e-05}, {"id": 171, "seek": 98308, "start": 983.08, "end": 987.4000000000001, "text": " need to be answered many things how do you test this or who's going to have the time", "tokens": [643, 281, 312, 10103, 867, 721, 577, 360, 291, 1500, 341, 420, 567, 311, 516, 281, 362, 264, 565], "temperature": 0.0, "avg_logprob": -0.13611589945279634, "compression_ratio": 1.8083333333333333, "no_speech_prob": 1.822219928726554e-05}, {"id": 172, "seek": 98308, "start": 987.4000000000001, "end": 992.96, "text": " to do this and well luckily someone aka me spent the better part of six months of their", "tokens": [281, 360, 341, 293, 731, 22880, 1580, 28042, 385, 4418, 264, 1101, 644, 295, 2309, 2493, 295, 641], "temperature": 0.0, "avg_logprob": -0.13611589945279634, "compression_ratio": 1.8083333333333333, "no_speech_prob": 1.822219928726554e-05}, {"id": 173, "seek": 98308, "start": 992.96, "end": 1000.1600000000001, "text": " free time dealing with this and so I made a library and in this case it's called data", "tokens": [1737, 565, 6260, 365, 341, 293, 370, 286, 1027, 257, 6405, 293, 294, 341, 1389, 309, 311, 1219, 1412], "temperature": 0.0, "avg_logprob": -0.13611589945279634, "compression_ratio": 1.8083333333333333, "no_speech_prob": 1.822219928726554e-05}, {"id": 174, "seek": 98308, "start": 1000.1600000000001, "end": 1005.6800000000001, "text": " cake whoo yes in this case this is called data cake I originally was going to call it", "tokens": [5908, 567, 78, 2086, 294, 341, 1389, 341, 307, 1219, 1412, 5908, 286, 7993, 390, 516, 281, 818, 309], "temperature": 0.0, "avg_logprob": -0.13611589945279634, "compression_ratio": 1.8083333333333333, "no_speech_prob": 1.822219928726554e-05}, {"id": 175, "seek": 98308, "start": 1005.6800000000001, "end": 1010.32, "text": " data lake but unfortunately that already exists so we added cake at the end and called it", "tokens": [1412, 11001, 457, 7015, 300, 1217, 8198, 370, 321, 3869, 5908, 412, 264, 917, 293, 1219, 309], "temperature": 0.0, "avg_logprob": -0.13611589945279634, "compression_ratio": 1.8083333333333333, "no_speech_prob": 1.822219928726554e-05}, {"id": 176, "seek": 101032, "start": 1010.32, "end": 1016.96, "text": " a day it is effectively a tooling to create your own distributed systems it doesn't have", "tokens": [257, 786, 309, 307, 8659, 257, 46593, 281, 1884, 428, 1065, 12631, 3652, 309, 1177, 380, 362], "temperature": 0.0, "avg_logprob": -0.09121429218965418, "compression_ratio": 1.7042801556420233, "no_speech_prob": 2.8022946935379878e-05}, {"id": 177, "seek": 101032, "start": 1016.96, "end": 1022.8000000000001, "text": " to be eventually consistent but it just is designed to make your life a lot easier and", "tokens": [281, 312, 4728, 8398, 457, 309, 445, 307, 4761, 281, 652, 428, 993, 257, 688, 3571, 293], "temperature": 0.0, "avg_logprob": -0.09121429218965418, "compression_ratio": 1.7042801556420233, "no_speech_prob": 2.8022946935379878e-05}, {"id": 178, "seek": 101032, "start": 1022.8000000000001, "end": 1028.64, "text": " it only took about six rewrites to get it to the stage that it is because yeah things", "tokens": [309, 787, 1890, 466, 2309, 319, 86, 30931, 281, 483, 309, 281, 264, 3233, 300, 309, 307, 570, 1338, 721], "temperature": 0.0, "avg_logprob": -0.09121429218965418, "compression_ratio": 1.7042801556420233, "no_speech_prob": 2.8022946935379878e-05}, {"id": 179, "seek": 101032, "start": 1028.64, "end": 1033.72, "text": " are hard and trying to work out what you want to do with something like that is awkward", "tokens": [366, 1152, 293, 1382, 281, 589, 484, 437, 291, 528, 281, 360, 365, 746, 411, 300, 307, 11411], "temperature": 0.0, "avg_logprob": -0.09121429218965418, "compression_ratio": 1.7042801556420233, "no_speech_prob": 2.8022946935379878e-05}, {"id": 180, "seek": 101032, "start": 1033.72, "end": 1038.56, "text": " but some of the features it includes is it includes the zero copy RPC framework and this", "tokens": [457, 512, 295, 264, 4122, 309, 5974, 307, 309, 5974, 264, 4018, 5055, 497, 12986, 8388, 293, 341], "temperature": 0.0, "avg_logprob": -0.09121429218965418, "compression_ratio": 1.7042801556420233, "no_speech_prob": 2.8022946935379878e-05}, {"id": 181, "seek": 103856, "start": 1038.56, "end": 1043.52, "text": " is built upon the popular archive framework which is really really useful if you're shipping", "tokens": [307, 3094, 3564, 264, 3743, 23507, 8388, 597, 307, 534, 534, 4420, 498, 291, 434, 14122], "temperature": 0.0, "avg_logprob": -0.10592890668798376, "compression_ratio": 1.7045454545454546, "no_speech_prob": 1.2706718734989408e-05}, {"id": 182, "seek": 103856, "start": 1043.52, "end": 1047.24, "text": " a lot of data because you don't actually have to deserialize and allocate everything all", "tokens": [257, 688, 295, 1412, 570, 291, 500, 380, 767, 362, 281, 730, 260, 831, 1125, 293, 35713, 1203, 439], "temperature": 0.0, "avg_logprob": -0.10592890668798376, "compression_ratio": 1.7045454545454546, "no_speech_prob": 1.2706718734989408e-05}, {"id": 183, "seek": 103856, "start": 1047.24, "end": 1052.44, "text": " over again you can just treat an initial buffer as if it's the data which if that sounds wildly", "tokens": [670, 797, 291, 393, 445, 2387, 364, 5883, 21762, 382, 498, 309, 311, 264, 1412, 597, 498, 300, 3263, 34731], "temperature": 0.0, "avg_logprob": -0.10592890668798376, "compression_ratio": 1.7045454545454546, "no_speech_prob": 1.2706718734989408e-05}, {"id": 184, "seek": 103856, "start": 1052.44, "end": 1061.6799999999998, "text": " and safe it is but there's a lot of tests and I didn't write it so you're safe.", "tokens": [293, 3273, 309, 307, 457, 456, 311, 257, 688, 295, 6921, 293, 286, 994, 380, 2464, 309, 370, 291, 434, 3273, 13], "temperature": 0.0, "avg_logprob": -0.10592890668798376, "compression_ratio": 1.7045454545454546, "no_speech_prob": 1.2706718734989408e-05}, {"id": 185, "seek": 103856, "start": 1061.6799999999998, "end": 1067.56, "text": " We also add the membership and failure detection and this is done using chit chat which is a", "tokens": [492, 611, 909, 264, 16560, 293, 7763, 17784, 293, 341, 307, 1096, 1228, 417, 270, 5081, 597, 307, 257], "temperature": 0.0, "avg_logprob": -0.10592890668798376, "compression_ratio": 1.7045454545454546, "no_speech_prob": 1.2706718734989408e-05}, {"id": 186, "seek": 106756, "start": 1067.56, "end": 1072.04, "text": " library we made at quick quit it uses the same algorithm as something like Cassandra", "tokens": [6405, 321, 1027, 412, 1702, 10366, 309, 4960, 264, 912, 9284, 382, 746, 411, 18208, 18401], "temperature": 0.0, "avg_logprob": -0.11691710875206388, "compression_ratio": 1.73046875, "no_speech_prob": 2.0826439140364528e-05}, {"id": 187, "seek": 106756, "start": 1072.04, "end": 1077.24, "text": " or DynamoDB and this allows the system to essentially work out what nodes are actually", "tokens": [420, 22947, 78, 27735, 293, 341, 4045, 264, 1185, 281, 4476, 589, 484, 437, 13891, 366, 767], "temperature": 0.0, "avg_logprob": -0.11691710875206388, "compression_ratio": 1.73046875, "no_speech_prob": 2.0826439140364528e-05}, {"id": 188, "seek": 106756, "start": 1077.24, "end": 1085.52, "text": " its friends and what it can do and in this case we've also implemented an eventually", "tokens": [1080, 1855, 293, 437, 309, 393, 360, 293, 294, 341, 1389, 321, 600, 611, 12270, 364, 4728], "temperature": 0.0, "avg_logprob": -0.11691710875206388, "compression_ratio": 1.73046875, "no_speech_prob": 2.0826439140364528e-05}, {"id": 189, "seek": 106756, "start": 1085.52, "end": 1093.0, "text": " consistent store in the form of a key value system which only requires one trait to implement", "tokens": [8398, 3531, 294, 264, 1254, 295, 257, 2141, 2158, 1185, 597, 787, 7029, 472, 22538, 281, 4445], "temperature": 0.0, "avg_logprob": -0.11691710875206388, "compression_ratio": 1.73046875, "no_speech_prob": 2.0826439140364528e-05}, {"id": 190, "seek": 106756, "start": 1093.0, "end": 1096.6399999999999, "text": " and the reason why I went with this is because if you implement anything more than one trait", "tokens": [293, 264, 1778, 983, 286, 1437, 365, 341, 307, 570, 498, 291, 4445, 1340, 544, 813, 472, 22538], "temperature": 0.0, "avg_logprob": -0.11691710875206388, "compression_ratio": 1.73046875, "no_speech_prob": 2.0826439140364528e-05}, {"id": 191, "seek": 109664, "start": 1096.64, "end": 1102.4, "text": " people seem to turn off and frankly I did when I looked at the raft implementations.", "tokens": [561, 1643, 281, 1261, 766, 293, 11939, 286, 630, 562, 286, 2956, 412, 264, 43863, 4445, 763, 13], "temperature": 0.0, "avg_logprob": -0.14444717558303682, "compression_ratio": 1.608365019011407, "no_speech_prob": 1.740764855640009e-05}, {"id": 192, "seek": 109664, "start": 1102.4, "end": 1106.6000000000001, "text": " So we went with one storage trait that's all you need to get this to work.", "tokens": [407, 321, 1437, 365, 472, 6725, 22538, 300, 311, 439, 291, 643, 281, 483, 341, 281, 589, 13], "temperature": 0.0, "avg_logprob": -0.14444717558303682, "compression_ratio": 1.608365019011407, "no_speech_prob": 1.740764855640009e-05}, {"id": 193, "seek": 109664, "start": 1106.6000000000001, "end": 1111.44, "text": " We also have some pre-built implementations I particularly like abusing SQLite so there", "tokens": [492, 611, 362, 512, 659, 12, 23018, 4445, 763, 286, 4098, 411, 410, 7981, 19200, 642, 370, 456], "temperature": 0.0, "avg_logprob": -0.14444717558303682, "compression_ratio": 1.608365019011407, "no_speech_prob": 1.740764855640009e-05}, {"id": 194, "seek": 109664, "start": 1111.44, "end": 1117.4, "text": " is an SQLite implementation and a memory version and it also gives you some CRDTs which", "tokens": [307, 364, 19200, 642, 11420, 293, 257, 4675, 3037, 293, 309, 611, 2709, 291, 512, 14123, 35, 33424, 597], "temperature": 0.0, "avg_logprob": -0.14444717558303682, "compression_ratio": 1.608365019011407, "no_speech_prob": 1.740764855640009e-05}, {"id": 195, "seek": 109664, "start": 1117.4, "end": 1124.72, "text": " are conflict-free replicated data types I should say and also something called a hybrid", "tokens": [366, 6596, 12, 10792, 46365, 1412, 3467, 286, 820, 584, 293, 611, 746, 1219, 257, 13051], "temperature": 0.0, "avg_logprob": -0.14444717558303682, "compression_ratio": 1.608365019011407, "no_speech_prob": 1.740764855640009e-05}, {"id": 196, "seek": 112472, "start": 1124.72, "end": 1128.72, "text": " logical clock which means it's a clock which you can have across your cluster where the", "tokens": [14978, 7830, 597, 1355, 309, 311, 257, 7830, 597, 291, 393, 362, 2108, 428, 13630, 689, 264], "temperature": 0.0, "avg_logprob": -0.118274602023038, "compression_ratio": 1.6706827309236947, "no_speech_prob": 2.2923732103663497e-05}, {"id": 197, "seek": 112472, "start": 1128.72, "end": 1134.0, "text": " nodes will stabilize themselves and prevent you from effectively having to deal with this", "tokens": [13891, 486, 31870, 2969, 293, 4871, 291, 490, 8659, 1419, 281, 2028, 365, 341], "temperature": 0.0, "avg_logprob": -0.118274602023038, "compression_ratio": 1.6706827309236947, "no_speech_prob": 2.2923732103663497e-05}, {"id": 198, "seek": 112472, "start": 1134.0, "end": 1140.72, "text": " concept of causality and causality is definitely the biggest issue you will ever run into with", "tokens": [3410, 295, 3302, 1860, 293, 3302, 1860, 307, 2138, 264, 3880, 2734, 291, 486, 1562, 1190, 666, 365], "temperature": 0.0, "avg_logprob": -0.118274602023038, "compression_ratio": 1.6706827309236947, "no_speech_prob": 2.2923732103663497e-05}, {"id": 199, "seek": 112472, "start": 1140.72, "end": 1146.16, "text": " distributed systems because time is suddenly not reliable.", "tokens": [12631, 3652, 570, 565, 307, 5800, 406, 12924, 13], "temperature": 0.0, "avg_logprob": -0.118274602023038, "compression_ratio": 1.6706827309236947, "no_speech_prob": 2.2923732103663497e-05}, {"id": 200, "seek": 112472, "start": 1146.16, "end": 1150.72, "text": " And so we go back to our original thing of well first we actually need a cluster and", "tokens": [400, 370, 321, 352, 646, 281, 527, 3380, 551, 295, 731, 700, 321, 767, 643, 257, 13630, 293], "temperature": 0.0, "avg_logprob": -0.118274602023038, "compression_ratio": 1.6706827309236947, "no_speech_prob": 2.2923732103663497e-05}, {"id": 201, "seek": 115072, "start": 1150.72, "end": 1156.64, "text": " this case it's really simple to do all we need to do is we just create our node builder", "tokens": [341, 1389, 309, 311, 534, 2199, 281, 360, 439, 321, 643, 281, 360, 307, 321, 445, 1884, 527, 9984, 27377], "temperature": 0.0, "avg_logprob": -0.1577506595187717, "compression_ratio": 1.7272727272727273, "no_speech_prob": 1.0425639629829675e-05}, {"id": 202, "seek": 115072, "start": 1156.64, "end": 1162.32, "text": " we tell data cake okay we've got your address is this your peers are this or you can start", "tokens": [321, 980, 1412, 5908, 1392, 321, 600, 658, 428, 2985, 307, 341, 428, 16739, 366, 341, 420, 291, 393, 722], "temperature": 0.0, "avg_logprob": -0.1577506595187717, "compression_ratio": 1.7272727272727273, "no_speech_prob": 1.0425639629829675e-05}, {"id": 203, "seek": 115072, "start": 1162.32, "end": 1167.52, "text": " with one peer and they'll discover themselves who their neighbors are and you give them", "tokens": [365, 472, 15108, 293, 436, 603, 4411, 2969, 567, 641, 12512, 366, 293, 291, 976, 552], "temperature": 0.0, "avg_logprob": -0.1577506595187717, "compression_ratio": 1.7272727272727273, "no_speech_prob": 1.0425639629829675e-05}, {"id": 204, "seek": 115072, "start": 1167.52, "end": 1168.52, "text": " a node ID.", "tokens": [257, 9984, 7348, 13], "temperature": 0.0, "avg_logprob": -0.1577506595187717, "compression_ratio": 1.7272727272727273, "no_speech_prob": 1.0425639629829675e-05}, {"id": 205, "seek": 115072, "start": 1168.52, "end": 1172.4, "text": " They're integers they're not strings and the reason for that is because there's a lot", "tokens": [814, 434, 41674, 436, 434, 406, 13985, 293, 264, 1778, 337, 300, 307, 570, 456, 311, 257, 688], "temperature": 0.0, "avg_logprob": -0.1577506595187717, "compression_ratio": 1.7272727272727273, "no_speech_prob": 1.0425639629829675e-05}, {"id": 206, "seek": 115072, "start": 1172.4, "end": 1177.64, "text": " of bit packing of certain data types going on and strings do not do well.", "tokens": [295, 857, 20815, 295, 1629, 1412, 3467, 516, 322, 293, 13985, 360, 406, 360, 731, 13], "temperature": 0.0, "avg_logprob": -0.1577506595187717, "compression_ratio": 1.7272727272727273, "no_speech_prob": 1.0425639629829675e-05}, {"id": 207, "seek": 117764, "start": 1177.64, "end": 1182.68, "text": " And here we can also effectively wait for nodes to come onto the system so our cluster", "tokens": [400, 510, 321, 393, 611, 8659, 1699, 337, 13891, 281, 808, 3911, 264, 1185, 370, 527, 13630], "temperature": 0.0, "avg_logprob": -0.11627689160798725, "compression_ratio": 1.7081712062256809, "no_speech_prob": 8.663222615723498e-06}, {"id": 208, "seek": 117764, "start": 1182.68, "end": 1186.8400000000001, "text": " is stable and ready to go before we actually do anything else.", "tokens": [307, 8351, 293, 1919, 281, 352, 949, 321, 767, 360, 1340, 1646, 13], "temperature": 0.0, "avg_logprob": -0.11627689160798725, "compression_ratio": 1.7081712062256809, "no_speech_prob": 8.663222615723498e-06}, {"id": 209, "seek": 117764, "start": 1186.8400000000001, "end": 1191.0, "text": " And by the time we get to this point our RPC systems are working nodes are communicating", "tokens": [400, 538, 264, 565, 321, 483, 281, 341, 935, 527, 497, 12986, 3652, 366, 1364, 13891, 366, 17559], "temperature": 0.0, "avg_logprob": -0.11627689160798725, "compression_ratio": 1.7081712062256809, "no_speech_prob": 8.663222615723498e-06}, {"id": 210, "seek": 117764, "start": 1191.0, "end": 1195.72, "text": " your clocks have synchronized themselves mostly and you can actually start adding something", "tokens": [428, 41528, 362, 19331, 1602, 2969, 5240, 293, 291, 393, 767, 722, 5127, 746], "temperature": 0.0, "avg_logprob": -0.11627689160798725, "compression_ratio": 1.7081712062256809, "no_speech_prob": 8.663222615723498e-06}, {"id": 211, "seek": 117764, "start": 1195.72, "end": 1198.1200000000001, "text": " called extensions.", "tokens": [1219, 25129, 13], "temperature": 0.0, "avg_logprob": -0.11627689160798725, "compression_ratio": 1.7081712062256809, "no_speech_prob": 8.663222615723498e-06}, {"id": 212, "seek": 117764, "start": 1198.1200000000001, "end": 1205.44, "text": " Now extensions essentially allow you to extend your existing cluster you don't you can do", "tokens": [823, 25129, 4476, 2089, 291, 281, 10101, 428, 6741, 13630, 291, 500, 380, 291, 393, 360], "temperature": 0.0, "avg_logprob": -0.11627689160798725, "compression_ratio": 1.7081712062256809, "no_speech_prob": 8.663222615723498e-06}, {"id": 213, "seek": 120544, "start": 1205.44, "end": 1210.48, "text": " this at runtime they can be added and they can be unloaded all at runtime without any", "tokens": [341, 412, 34474, 436, 393, 312, 3869, 293, 436, 393, 312, 32165, 292, 439, 412, 34474, 1553, 604], "temperature": 0.0, "avg_logprob": -0.1154296181418679, "compression_ratio": 1.7248062015503876, "no_speech_prob": 1.4097472558205482e-05}, {"id": 214, "seek": 120544, "start": 1210.48, "end": 1216.0, "text": " with state cleanup and everything else which makes life a lot easier especially for testing.", "tokens": [365, 1785, 40991, 293, 1203, 1646, 597, 1669, 993, 257, 688, 3571, 2318, 337, 4997, 13], "temperature": 0.0, "avg_logprob": -0.1154296181418679, "compression_ratio": 1.7248062015503876, "no_speech_prob": 1.4097472558205482e-05}, {"id": 215, "seek": 120544, "start": 1216.0, "end": 1220.48, "text": " They have access to the running node on this local system which allows you to access things", "tokens": [814, 362, 2105, 281, 264, 2614, 9984, 322, 341, 2654, 1185, 597, 4045, 291, 281, 2105, 721], "temperature": 0.0, "avg_logprob": -0.1154296181418679, "compression_ratio": 1.7248062015503876, "no_speech_prob": 1.4097472558205482e-05}, {"id": 216, "seek": 120544, "start": 1220.48, "end": 1225.68, "text": " like the cluster clock the RPC network as it's called which is the pre-established RPC", "tokens": [411, 264, 13630, 7830, 264, 497, 12986, 3209, 382, 309, 311, 1219, 597, 307, 264, 659, 12, 33542, 4173, 497, 12986], "temperature": 0.0, "avg_logprob": -0.1154296181418679, "compression_ratio": 1.7248062015503876, "no_speech_prob": 1.4097472558205482e-05}, {"id": 217, "seek": 120544, "start": 1225.68, "end": 1231.92, "text": " connections and you can essentially make this as simple or as complex as possible which", "tokens": [9271, 293, 291, 393, 4476, 652, 341, 382, 2199, 420, 382, 3997, 382, 1944, 597], "temperature": 0.0, "avg_logprob": -0.1154296181418679, "compression_ratio": 1.7248062015503876, "no_speech_prob": 1.4097472558205482e-05}, {"id": 218, "seek": 123192, "start": 1231.92, "end": 1237.2, "text": " is essentially what I've done here so I've created this nice little extension which is", "tokens": [307, 4476, 437, 286, 600, 1096, 510, 370, 286, 600, 2942, 341, 1481, 707, 10320, 597, 307], "temperature": 0.0, "avg_logprob": -0.11214823072606867, "compression_ratio": 1.6652360515021458, "no_speech_prob": 8.633796824142337e-06}, {"id": 219, "seek": 123192, "start": 1237.2, "end": 1240.72, "text": " absolutely nothing other than print what the current time is which realistically I could", "tokens": [3122, 1825, 661, 813, 4482, 437, 264, 2190, 565, 307, 597, 40734, 286, 727], "temperature": 0.0, "avg_logprob": -0.11214823072606867, "compression_ratio": 1.6652360515021458, "no_speech_prob": 8.633796824142337e-06}, {"id": 220, "seek": 123192, "start": 1240.72, "end": 1245.2, "text": " do without but nonetheless I went with it.", "tokens": [360, 1553, 457, 26756, 286, 1437, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.11214823072606867, "compression_ratio": 1.6652360515021458, "no_speech_prob": 8.633796824142337e-06}, {"id": 221, "seek": 123192, "start": 1245.2, "end": 1249.48, "text": " And this is what the eventual consistency store actually does under the hood is it's", "tokens": [400, 341, 307, 437, 264, 33160, 14416, 3531, 767, 775, 833, 264, 13376, 307, 309, 311], "temperature": 0.0, "avg_logprob": -0.11214823072606867, "compression_ratio": 1.6652360515021458, "no_speech_prob": 8.633796824142337e-06}, {"id": 222, "seek": 123192, "start": 1249.48, "end": 1255.04, "text": " just an extension and here we can see that we're passing in a I can't point that far", "tokens": [445, 364, 10320, 293, 510, 321, 393, 536, 300, 321, 434, 8437, 294, 257, 286, 393, 380, 935, 300, 1400], "temperature": 0.0, "avg_logprob": -0.11214823072606867, "compression_ratio": 1.6652360515021458, "no_speech_prob": 8.633796824142337e-06}, {"id": 223, "seek": 125504, "start": 1255.04, "end": 1263.68, "text": " but we pass in a mem store which is our storage trait we pass in our create our eventual consistency", "tokens": [457, 321, 1320, 294, 257, 1334, 3531, 597, 307, 527, 6725, 22538, 321, 1320, 294, 527, 1884, 527, 33160, 14416], "temperature": 0.0, "avg_logprob": -0.13169018815203412, "compression_ratio": 1.86, "no_speech_prob": 1.934666443048627e-06}, {"id": 224, "seek": 125504, "start": 1263.68, "end": 1271.04, "text": " extension using this and we pass it to the data cake node and say okay go add this extension", "tokens": [10320, 1228, 341, 293, 321, 1320, 309, 281, 264, 1412, 5908, 9984, 293, 584, 1392, 352, 909, 341, 10320], "temperature": 0.0, "avg_logprob": -0.13169018815203412, "compression_ratio": 1.86, "no_speech_prob": 1.934666443048627e-06}, {"id": 225, "seek": 125504, "start": 1271.04, "end": 1276.04, "text": " give me the result back when you're ready and in this case our eventual consistency cluster", "tokens": [976, 385, 264, 1874, 646, 562, 291, 434, 1919, 293, 294, 341, 1389, 527, 33160, 14416, 13630], "temperature": 0.0, "avg_logprob": -0.13169018815203412, "compression_ratio": 1.86, "no_speech_prob": 1.934666443048627e-06}, {"id": 226, "seek": 125504, "start": 1276.04, "end": 1281.0, "text": " actually returns us a storage handle which allows us to do basically all of our lovely", "tokens": [767, 11247, 505, 257, 6725, 4813, 597, 4045, 505, 281, 360, 1936, 439, 295, 527, 7496], "temperature": 0.0, "avg_logprob": -0.13169018815203412, "compression_ratio": 1.86, "no_speech_prob": 1.934666443048627e-06}, {"id": 227, "seek": 128100, "start": 1281.0, "end": 1288.24, "text": " key value operations should we wish including delete, put, get that's about all there is", "tokens": [2141, 2158, 7705, 820, 321, 3172, 3009, 12097, 11, 829, 11, 483, 300, 311, 466, 439, 456, 307], "temperature": 0.0, "avg_logprob": -0.1384469503643869, "compression_ratio": 1.5975103734439835, "no_speech_prob": 6.048047907825094e-06}, {"id": 228, "seek": 128100, "start": 1288.24, "end": 1293.68, "text": " on the key value store but there are also some bulk operations which allow for much", "tokens": [322, 264, 2141, 2158, 3531, 457, 456, 366, 611, 512, 16139, 7705, 597, 2089, 337, 709], "temperature": 0.0, "avg_logprob": -0.1384469503643869, "compression_ratio": 1.5975103734439835, "no_speech_prob": 6.048047907825094e-06}, {"id": 229, "seek": 128100, "start": 1293.68, "end": 1297.76, "text": " more efficient replication of data.", "tokens": [544, 7148, 39911, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1384469503643869, "compression_ratio": 1.5975103734439835, "no_speech_prob": 6.048047907825094e-06}, {"id": 230, "seek": 128100, "start": 1297.76, "end": 1302.08, "text": " The only problem with this approach is it's not suitable for billion scale databases so", "tokens": [440, 787, 1154, 365, 341, 3109, 307, 309, 311, 406, 12873, 337, 5218, 4373, 22380, 370], "temperature": 0.0, "avg_logprob": -0.1384469503643869, "compression_ratio": 1.5975103734439835, "no_speech_prob": 6.048047907825094e-06}, {"id": 231, "seek": 128100, "start": 1302.08, "end": 1306.88, "text": " if you're trying to make the next Cassandra or Silla don't use this particular extension", "tokens": [498, 291, 434, 1382, 281, 652, 264, 958, 18208, 18401, 420, 318, 5291, 500, 380, 764, 341, 1729, 10320], "temperature": 0.0, "avg_logprob": -0.1384469503643869, "compression_ratio": 1.5975103734439835, "no_speech_prob": 6.048047907825094e-06}, {"id": 232, "seek": 130688, "start": 1306.88, "end": 1313.44, "text": " because it keeps the key value or the keys sorry in memory which it uses to work out", "tokens": [570, 309, 5965, 264, 2141, 2158, 420, 264, 9317, 2597, 294, 4675, 597, 309, 4960, 281, 589, 484], "temperature": 0.0, "avg_logprob": -0.12077875235646042, "compression_ratio": 1.7370517928286853, "no_speech_prob": 5.052103915659245e-06}, {"id": 233, "seek": 130688, "start": 1313.44, "end": 1319.0400000000002, "text": " what keys have and have not been processed and the reason for this is effectively because", "tokens": [437, 9317, 362, 293, 362, 406, 668, 18846, 293, 264, 1778, 337, 341, 307, 8659, 570], "temperature": 0.0, "avg_logprob": -0.12077875235646042, "compression_ratio": 1.7370517928286853, "no_speech_prob": 5.052103915659245e-06}, {"id": 234, "seek": 130688, "start": 1319.0400000000002, "end": 1324.24, "text": " I didn't really trust users implementing this on the storage site correctly which turned", "tokens": [286, 994, 380, 534, 3361, 5022, 18114, 341, 322, 264, 6725, 3621, 8944, 597, 3574], "temperature": 0.0, "avg_logprob": -0.12077875235646042, "compression_ratio": 1.7370517928286853, "no_speech_prob": 5.052103915659245e-06}, {"id": 235, "seek": 130688, "start": 1324.24, "end": 1327.5600000000002, "text": " out to be a good choice because the amount of unit tests that this failed initially", "tokens": [484, 281, 312, 257, 665, 3922, 570, 264, 2372, 295, 4985, 6921, 300, 341, 7612, 9105], "temperature": 0.0, "avg_logprob": -0.12077875235646042, "compression_ratio": 1.7370517928286853, "no_speech_prob": 5.052103915659245e-06}, {"id": 236, "seek": 130688, "start": 1327.5600000000002, "end": 1333.88, "text": " was a lot and so now we've sort of got this ability to replicate our key values our life", "tokens": [390, 257, 688, 293, 370, 586, 321, 600, 1333, 295, 658, 341, 3485, 281, 25356, 527, 2141, 4190, 527, 993], "temperature": 0.0, "avg_logprob": -0.12077875235646042, "compression_ratio": 1.7370517928286853, "no_speech_prob": 5.052103915659245e-06}, {"id": 237, "seek": 133388, "start": 1333.88, "end": 1340.72, "text": " is a lot easier in particular we can actually go as far as essentially saying okay we've", "tokens": [307, 257, 688, 3571, 294, 1729, 321, 393, 767, 352, 382, 1400, 382, 4476, 1566, 1392, 321, 600], "temperature": 0.0, "avg_logprob": -0.194816287559799, "compression_ratio": 1.5739910313901346, "no_speech_prob": 6.434781425923575e-06}, {"id": 238, "seek": 133388, "start": 1340.72, "end": 1345.8400000000001, "text": " established our data connection our key values let's just use Tantive as our persistence", "tokens": [7545, 527, 1412, 4984, 527, 2141, 4190, 718, 311, 445, 764, 314, 394, 488, 382, 527, 37617], "temperature": 0.0, "avg_logprob": -0.194816287559799, "compression_ratio": 1.5739910313901346, "no_speech_prob": 6.434781425923575e-06}, {"id": 239, "seek": 133388, "start": 1345.8400000000001, "end": 1352.4, "text": " store and this is effectively the simplest way to do it and I've made a little demo here", "tokens": [3531, 293, 341, 307, 8659, 264, 22811, 636, 281, 360, 309, 293, 286, 600, 1027, 257, 707, 10723, 510], "temperature": 0.0, "avg_logprob": -0.194816287559799, "compression_ratio": 1.5739910313901346, "no_speech_prob": 6.434781425923575e-06}, {"id": 240, "seek": 133388, "start": 1352.4, "end": 1358.44, "text": " which you can go to that link I basically abused and slightly ignored certain things", "tokens": [597, 291, 393, 352, 281, 300, 2113, 286, 1936, 27075, 293, 4748, 19735, 1629, 721], "temperature": 0.0, "avg_logprob": -0.194816287559799, "compression_ratio": 1.5739910313901346, "no_speech_prob": 6.434781425923575e-06}, {"id": 241, "seek": 135844, "start": 1358.44, "end": 1364.96, "text": " in particular correctness but this will replicate your data you may end up with duplicate documents", "tokens": [294, 1729, 3006, 1287, 457, 341, 486, 25356, 428, 1412, 291, 815, 917, 493, 365, 23976, 8512], "temperature": 0.0, "avg_logprob": -0.12356778078301009, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.0903871043410618e-05}, {"id": 242, "seek": 135844, "start": 1364.96, "end": 1369.8400000000001, "text": " because I didn't handle de-duping but in this case we can fetch we can delete and we can", "tokens": [570, 286, 994, 380, 4813, 368, 12, 769, 3381, 457, 294, 341, 1389, 321, 393, 23673, 321, 393, 12097, 293, 321, 393], "temperature": 0.0, "avg_logprob": -0.12356778078301009, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.0903871043410618e-05}, {"id": 243, "seek": 135844, "start": 1369.8400000000001, "end": 1373.8400000000001, "text": " index documents with Tantive and that's our persistence store and here you can see we're", "tokens": [8186, 8512, 365, 314, 394, 488, 293, 300, 311, 527, 37617, 3531, 293, 510, 291, 393, 536, 321, 434], "temperature": 0.0, "avg_logprob": -0.12356778078301009, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.0903871043410618e-05}, {"id": 244, "seek": 135844, "start": 1373.8400000000001, "end": 1384.44, "text": " doing about 20,000 documents in 400 milliseconds in the local cluster yes and that is effectively", "tokens": [884, 466, 945, 11, 1360, 8512, 294, 8423, 34184, 294, 264, 2654, 13630, 2086, 293, 300, 307, 8659], "temperature": 0.0, "avg_logprob": -0.12356778078301009, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.0903871043410618e-05}, {"id": 245, "seek": 138444, "start": 1384.44, "end": 1406.72, "text": " the end so are there any questions how long do we have left how long do we have left 15", "tokens": [264, 917, 370, 366, 456, 604, 1651, 577, 938, 360, 321, 362, 1411, 577, 938, 360, 321, 362, 1411, 2119], "temperature": 0.0, "avg_logprob": -0.3945736090342204, "compression_ratio": 1.2794117647058822, "no_speech_prob": 0.00023869166034273803}, {"id": 246, "seek": 140672, "start": 1406.72, "end": 1429.0, "text": " minutes so actually kind of so in there do you have like a way to provide from outside", "tokens": [2077, 370, 767, 733, 295, 370, 294, 456, 360, 291, 362, 411, 257, 636, 281, 2893, 490, 2380], "temperature": 0.0, "avg_logprob": -0.49616518887606537, "compression_ratio": 1.1466666666666667, "no_speech_prob": 0.00223942962475121}, {"id": 247, "seek": 142900, "start": 1429.0, "end": 1436.6, "text": " to the Tantive transaction or links transaction an external ID that I can use to integrate", "tokens": [281, 264, 314, 394, 488, 14425, 420, 6123, 14425, 364, 8320, 7348, 300, 286, 393, 764, 281, 13365], "temperature": 0.0, "avg_logprob": -0.2668114381677964, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.000513723527546972}, {"id": 248, "seek": 142900, "start": 1436.6, "end": 1443.08, "text": " with the standard storage so change the question would be an easier way do you have a way to", "tokens": [365, 264, 3832, 6725, 370, 1319, 264, 1168, 576, 312, 364, 3571, 636, 360, 291, 362, 257, 636, 281], "temperature": 0.0, "avg_logprob": -0.2668114381677964, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.000513723527546972}, {"id": 249, "seek": 142900, "start": 1443.08, "end": 1450.44, "text": " say which which level of data has been indexed yes in this case I've sort of glossed over", "tokens": [584, 597, 597, 1496, 295, 1412, 575, 668, 8186, 292, 2086, 294, 341, 1389, 286, 600, 1333, 295, 19574, 292, 670], "temperature": 0.0, "avg_logprob": -0.2668114381677964, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.000513723527546972}, {"id": 250, "seek": 142900, "start": 1450.44, "end": 1454.92, "text": " it a little bit because in reality it's a little bit more complicated when you implement", "tokens": [309, 257, 707, 857, 570, 294, 4103, 309, 311, 257, 707, 857, 544, 6179, 562, 291, 4445], "temperature": 0.0, "avg_logprob": -0.2668114381677964, "compression_ratio": 1.6529680365296804, "no_speech_prob": 0.000513723527546972}, {"id": 251, "seek": 145492, "start": 1454.92, "end": 1460.8000000000002, "text": " it so in reality when you actually implement this you would probably have a essentially", "tokens": [309, 370, 294, 4103, 562, 291, 767, 4445, 341, 291, 576, 1391, 362, 257, 4476], "temperature": 0.0, "avg_logprob": -0.11093506914504031, "compression_ratio": 1.9251101321585904, "no_speech_prob": 0.0001846588565967977}, {"id": 252, "seek": 145492, "start": 1460.8000000000002, "end": 1465.2, "text": " use the replication to replicate the initial documents and then you would have a check", "tokens": [764, 264, 39911, 281, 25356, 264, 5883, 8512, 293, 550, 291, 576, 362, 257, 1520], "temperature": 0.0, "avg_logprob": -0.11093506914504031, "compression_ratio": 1.9251101321585904, "no_speech_prob": 0.0001846588565967977}, {"id": 253, "seek": 145492, "start": 1465.2, "end": 1469.52, "text": " mark to essentially work out what documents have and have not been indexed yet or you", "tokens": [1491, 281, 4476, 589, 484, 437, 8512, 362, 293, 362, 406, 668, 8186, 292, 1939, 420, 291], "temperature": 0.0, "avg_logprob": -0.11093506914504031, "compression_ratio": 1.9251101321585904, "no_speech_prob": 0.0001846588565967977}, {"id": 254, "seek": 145492, "start": 1469.52, "end": 1475.0, "text": " would add some additional step like a right ahead log so that way you know that as long", "tokens": [576, 909, 512, 4497, 1823, 411, 257, 558, 2286, 3565, 370, 300, 636, 291, 458, 300, 382, 938], "temperature": 0.0, "avg_logprob": -0.11093506914504031, "compression_ratio": 1.9251101321585904, "no_speech_prob": 0.0001846588565967977}, {"id": 255, "seek": 145492, "start": 1475.0, "end": 1480.04, "text": " as the documents are there you can make sure that your check your commit point is always", "tokens": [382, 264, 8512, 366, 456, 291, 393, 652, 988, 300, 428, 1520, 428, 5599, 935, 307, 1009], "temperature": 0.0, "avg_logprob": -0.11093506914504031, "compression_ratio": 1.9251101321585904, "no_speech_prob": 0.0001846588565967977}, {"id": 256, "seek": 148004, "start": 1480.04, "end": 1487.24, "text": " updated to the latest thing in the next it's actually a little bit different again because", "tokens": [10588, 281, 264, 6792, 551, 294, 264, 958, 309, 311, 767, 257, 707, 857, 819, 797, 570], "temperature": 0.0, "avg_logprob": -0.1740949394997586, "compression_ratio": 1.691588785046729, "no_speech_prob": 5.5434553360100836e-05}, {"id": 257, "seek": 148004, "start": 1487.24, "end": 1492.96, "text": " the way it creates indexes is they are per check point so in a new index is created every", "tokens": [264, 636, 309, 7829, 8186, 279, 307, 436, 366, 680, 1520, 935, 370, 294, 257, 777, 8186, 307, 2942, 633], "temperature": 0.0, "avg_logprob": -0.1740949394997586, "compression_ratio": 1.691588785046729, "no_speech_prob": 5.5434553360100836e-05}, {"id": 258, "seek": 148004, "start": 1492.96, "end": 1500.76, "text": " commit effectively but you don't have to do that and in this method I didn't so you could", "tokens": [5599, 8659, 457, 291, 500, 380, 362, 281, 360, 300, 293, 294, 341, 3170, 286, 994, 380, 370, 291, 727], "temperature": 0.0, "avg_logprob": -0.1740949394997586, "compression_ratio": 1.691588785046729, "no_speech_prob": 5.5434553360100836e-05}, {"id": 259, "seek": 148004, "start": 1500.76, "end": 1506.04, "text": " you can it doesn't do it here but you can add a right ahead log and do you can do basically", "tokens": [291, 393, 309, 1177, 380, 360, 309, 510, 457, 291, 393, 909, 257, 558, 2286, 3565, 293, 360, 291, 393, 360, 1936], "temperature": 0.0, "avg_logprob": -0.1740949394997586, "compression_ratio": 1.691588785046729, "no_speech_prob": 5.5434553360100836e-05}, {"id": 260, "seek": 150604, "start": 1506.04, "end": 1522.72, "text": " do anything as long as the trait is implemented hello hello hi yeah all right so congratulations", "tokens": [360, 1340, 382, 938, 382, 264, 22538, 307, 12270, 7751, 7751, 4879, 1338, 439, 558, 370, 13568], "temperature": 0.0, "avg_logprob": -0.3610103243873233, "compression_ratio": 1.2467532467532467, "no_speech_prob": 0.00020415117614902556}, {"id": 261, "seek": 152272, "start": 1522.72, "end": 1552.64, "text": " for the presentation sorry I think I can see you yes hello so let me see if I can", "tokens": [337, 264, 5860, 2597, 286, 519, 286, 393, 536, 291, 2086, 7751, 370, 718, 385, 536, 498, 286, 393], "temperature": 0.0, "avg_logprob": -0.28602293263310974, "compression_ratio": 1.1408450704225352, "no_speech_prob": 0.0006093845586292446}, {"id": 262, "seek": 155264, "start": 1552.64, "end": 1557.88, "text": " got that question right so you was is that about it sending time to me so if you want", "tokens": [658, 300, 1168, 558, 370, 291, 390, 307, 300, 466, 309, 7750, 565, 281, 385, 370, 498, 291, 528], "temperature": 0.0, "avg_logprob": -0.3164878282390657, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0002541716967243701}, {"id": 263, "seek": 155264, "start": 1557.88, "end": 1572.64, "text": " to go beyond something like bm25 or leave a size distance and things like that things", "tokens": [281, 352, 4399, 746, 411, 272, 76, 6074, 420, 1856, 257, 2744, 4560, 293, 721, 411, 300, 721], "temperature": 0.0, "avg_logprob": -0.3164878282390657, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0002541716967243701}, {"id": 264, "seek": 155264, "start": 1572.64, "end": 1577.96, "text": " like I think things like vector search or word embedding search is still something which", "tokens": [411, 286, 519, 721, 411, 8062, 3164, 420, 1349, 12240, 3584, 3164, 307, 920, 746, 597], "temperature": 0.0, "avg_logprob": -0.3164878282390657, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0002541716967243701}, {"id": 265, "seek": 157796, "start": 1577.96, "end": 1583.76, "text": " is quite far away and we need quite a big push to do with time to be specifically but", "tokens": [307, 1596, 1400, 1314, 293, 321, 643, 1596, 257, 955, 2944, 281, 360, 365, 565, 281, 312, 4682, 457], "temperature": 0.0, "avg_logprob": -0.1731755916888897, "compression_ratio": 1.8489795918367347, "no_speech_prob": 9.022037556860596e-05}, {"id": 266, "seek": 157796, "start": 1583.76, "end": 1587.6000000000001, "text": " if you want to add additional queries or additional functionality it's quite easy to add with", "tokens": [498, 291, 528, 281, 909, 4497, 24109, 420, 4497, 14980, 309, 311, 1596, 1858, 281, 909, 365], "temperature": 0.0, "avg_logprob": -0.1731755916888897, "compression_ratio": 1.8489795918367347, "no_speech_prob": 9.022037556860596e-05}, {"id": 267, "seek": 157796, "start": 1587.6000000000001, "end": 1592.8400000000001, "text": " time to be so it's actually just a query trait so one of the things that and the next does", "tokens": [565, 281, 312, 370, 309, 311, 767, 445, 257, 14581, 22538, 370, 472, 295, 264, 721, 300, 293, 264, 958, 775], "temperature": 0.0, "avg_logprob": -0.1731755916888897, "compression_ratio": 1.8489795918367347, "no_speech_prob": 9.022037556860596e-05}, {"id": 268, "seek": 157796, "start": 1592.8400000000001, "end": 1599.24, "text": " it actually has another a query mode called fast fuzzy which actually uses another algorithm", "tokens": [309, 767, 575, 1071, 257, 14581, 4391, 1219, 2370, 34710, 597, 767, 4960, 1071, 9284], "temperature": 0.0, "avg_logprob": -0.1731755916888897, "compression_ratio": 1.8489795918367347, "no_speech_prob": 9.022037556860596e-05}, {"id": 269, "seek": 157796, "start": 1599.24, "end": 1604.8, "text": " for pre-computing dictionaries in order to do the edit distance lookup and that basically", "tokens": [337, 659, 12, 1112, 2582, 278, 22352, 4889, 294, 1668, 281, 360, 264, 8129, 4560, 574, 1010, 293, 300, 1936], "temperature": 0.0, "avg_logprob": -0.1731755916888897, "compression_ratio": 1.8489795918367347, "no_speech_prob": 9.022037556860596e-05}, {"id": 270, "seek": 160480, "start": 1604.8, "end": 1610.76, "text": " is just involves creating another query and you can customize effectively all of your query", "tokens": [307, 445, 11626, 4084, 1071, 14581, 293, 291, 393, 19734, 8659, 439, 295, 428, 14581], "temperature": 0.0, "avg_logprob": -0.09211590949525224, "compression_ratio": 1.7992125984251968, "no_speech_prob": 5.689262252417393e-05}, {"id": 271, "seek": 160480, "start": 1610.76, "end": 1615.8, "text": " logic all of your collecting logic and things like that so providing your within the scope", "tokens": [9952, 439, 295, 428, 12510, 9952, 293, 721, 411, 300, 370, 6530, 428, 1951, 264, 11923], "temperature": 0.0, "avg_logprob": -0.09211590949525224, "compression_ratio": 1.7992125984251968, "no_speech_prob": 5.689262252417393e-05}, {"id": 272, "seek": 160480, "start": 1615.8, "end": 1621.32, "text": " of the API time to be will allow you to implement it yourself otherwise things like the word", "tokens": [295, 264, 9362, 565, 281, 312, 486, 2089, 291, 281, 4445, 309, 1803, 5911, 721, 411, 264, 1349], "temperature": 0.0, "avg_logprob": -0.09211590949525224, "compression_ratio": 1.7992125984251968, "no_speech_prob": 5.689262252417393e-05}, {"id": 273, "seek": 160480, "start": 1621.32, "end": 1625.3999999999999, "text": " embeddings which are a little bit more complicated and require a bit more on the storage side", "tokens": [12240, 29432, 597, 366, 257, 707, 857, 544, 6179, 293, 3651, 257, 857, 544, 322, 264, 6725, 1252], "temperature": 0.0, "avg_logprob": -0.09211590949525224, "compression_ratio": 1.7992125984251968, "no_speech_prob": 5.689262252417393e-05}, {"id": 274, "seek": 160480, "start": 1625.3999999999999, "end": 1630.24, "text": " would need to an issue and a very motivated individual to probably implement that which", "tokens": [576, 643, 281, 364, 2734, 293, 257, 588, 14515, 2609, 281, 1391, 4445, 300, 597], "temperature": 0.0, "avg_logprob": -0.09211590949525224, "compression_ratio": 1.7992125984251968, "no_speech_prob": 5.689262252417393e-05}, {"id": 275, "seek": 163024, "start": 1630.24, "end": 1651.32, "text": " currently we we don't really have so it's pretty little question on all your sketches", "tokens": [4362, 321, 321, 500, 380, 534, 362, 370, 309, 311, 1238, 707, 1168, 322, 439, 428, 34547], "temperature": 0.0, "avg_logprob": -0.1606339780907882, "compression_ratio": 1.4083333333333334, "no_speech_prob": 0.0006449202774092555}, {"id": 276, "seek": 163024, "start": 1651.32, "end": 1658.76, "text": " the network the subject network was fully connected is that important let me see if", "tokens": [264, 3209, 264, 3983, 3209, 390, 4498, 4582, 307, 300, 1021, 718, 385, 536, 498], "temperature": 0.0, "avg_logprob": -0.1606339780907882, "compression_ratio": 1.4083333333333334, "no_speech_prob": 0.0006449202774092555}, {"id": 277, "seek": 165876, "start": 1658.76, "end": 1666.32, "text": " I can find which one that was was it was it this one or was it this one well on this one", "tokens": [286, 393, 915, 597, 472, 300, 390, 390, 309, 390, 309, 341, 472, 420, 390, 309, 341, 472, 731, 322, 341, 472], "temperature": 0.0, "avg_logprob": -0.1671862543365102, "compression_ratio": 1.7004830917874396, "no_speech_prob": 0.0003965710347983986}, {"id": 278, "seek": 165876, "start": 1666.32, "end": 1672.12, "text": " it's it does not look fully connected but I'm not sure if these diagram depicts kind", "tokens": [309, 311, 309, 775, 406, 574, 4498, 4582, 457, 286, 478, 406, 988, 498, 613, 10686, 48949, 733], "temperature": 0.0, "avg_logprob": -0.1671862543365102, "compression_ratio": 1.7004830917874396, "no_speech_prob": 0.0003965710347983986}, {"id": 279, "seek": 165876, "start": 1672.12, "end": 1680.4, "text": " of connectivity connect home or just which messages has actually been dispatched so I'm", "tokens": [295, 21095, 1745, 1280, 420, 445, 597, 7897, 575, 767, 668, 4920, 24102, 370, 286, 478], "temperature": 0.0, "avg_logprob": -0.1671862543365102, "compression_ratio": 1.7004830917874396, "no_speech_prob": 0.0003965710347983986}, {"id": 280, "seek": 165876, "start": 1680.4, "end": 1686.24, "text": " going to cross the forbidden white line here because we're doing questions and effectively", "tokens": [516, 281, 3278, 264, 25990, 2418, 1622, 510, 570, 321, 434, 884, 1651, 293, 8659], "temperature": 0.0, "avg_logprob": -0.1671862543365102, "compression_ratio": 1.7004830917874396, "no_speech_prob": 0.0003965710347983986}, {"id": 281, "seek": 168624, "start": 1686.24, "end": 1692.32, "text": " these are just indicating sending responses and getting things back so these notes don't", "tokens": [613, 366, 445, 25604, 7750, 13019, 293, 1242, 721, 646, 370, 613, 5570, 500, 380], "temperature": 0.0, "avg_logprob": -0.16128847436997498, "compression_ratio": 1.772549019607843, "no_speech_prob": 8.326200622832403e-05}, {"id": 282, "seek": 168624, "start": 1692.32, "end": 1697.92, "text": " actually in a real system that you could have a network petition here and your node one", "tokens": [767, 294, 257, 957, 1185, 300, 291, 727, 362, 257, 3209, 22661, 510, 293, 428, 9984, 472], "temperature": 0.0, "avg_logprob": -0.16128847436997498, "compression_ratio": 1.772549019607843, "no_speech_prob": 8.326200622832403e-05}, {"id": 283, "seek": 168624, "start": 1697.92, "end": 1702.48, "text": " can no longer talk to no three it's effectively lost to the ether and maybe no two can also", "tokens": [393, 572, 2854, 751, 281, 572, 1045, 309, 311, 8659, 2731, 281, 264, 37096, 293, 1310, 572, 732, 393, 611], "temperature": 0.0, "avg_logprob": -0.16128847436997498, "compression_ratio": 1.772549019607843, "no_speech_prob": 8.326200622832403e-05}, {"id": 284, "seek": 168624, "start": 1702.48, "end": 1708.72, "text": " not do it and in this case it doesn't actually really care all that you need to do is you", "tokens": [406, 360, 309, 293, 294, 341, 1389, 309, 1177, 380, 767, 534, 1127, 439, 300, 291, 643, 281, 360, 307, 291], "temperature": 0.0, "avg_logprob": -0.16128847436997498, "compression_ratio": 1.772549019607843, "no_speech_prob": 8.326200622832403e-05}, {"id": 285, "seek": 168624, "start": 1708.72, "end": 1714.56, "text": " need to achieve what's called a consistency level so which means that if you want to progress", "tokens": [643, 281, 4584, 437, 311, 1219, 257, 14416, 1496, 370, 597, 1355, 300, 498, 291, 528, 281, 4205], "temperature": 0.0, "avg_logprob": -0.16128847436997498, "compression_ratio": 1.772549019607843, "no_speech_prob": 8.326200622832403e-05}, {"id": 286, "seek": 171456, "start": 1714.56, "end": 1720.12, "text": " you have to reach that level otherwise things are counted as not happening and so in this", "tokens": [291, 362, 281, 2524, 300, 1496, 5911, 721, 366, 20150, 382, 406, 2737, 293, 370, 294, 341], "temperature": 0.0, "avg_logprob": -0.16693555961534814, "compression_ratio": 1.8577405857740585, "no_speech_prob": 4.718578566098586e-05}, {"id": 287, "seek": 171456, "start": 1720.12, "end": 1726.3999999999999, "text": " case if no three is down or can't be contacted as long as node one can contact node two and", "tokens": [1389, 498, 572, 1045, 307, 760, 420, 393, 380, 312, 21546, 382, 938, 382, 9984, 472, 393, 3385, 9984, 732, 293], "temperature": 0.0, "avg_logprob": -0.16693555961534814, "compression_ratio": 1.8577405857740585, "no_speech_prob": 4.718578566098586e-05}, {"id": 288, "seek": 171456, "start": 1726.3999999999999, "end": 1730.9199999999998, "text": " no two acknowledges the messages things can still progress this is the same with raft", "tokens": [572, 732, 15195, 2880, 264, 7897, 721, 393, 920, 4205, 341, 307, 264, 912, 365, 43863], "temperature": 0.0, "avg_logprob": -0.16693555961534814, "compression_ratio": 1.8577405857740585, "no_speech_prob": 4.718578566098586e-05}, {"id": 289, "seek": 171456, "start": 1730.9199999999998, "end": 1737.08, "text": " as well so raft operates on what's called a quorum which yeah but effectively any node", "tokens": [382, 731, 370, 43863, 22577, 322, 437, 311, 1219, 257, 421, 36543, 597, 1338, 457, 8659, 604, 9984], "temperature": 0.0, "avg_logprob": -0.16693555961534814, "compression_ratio": 1.8577405857740585, "no_speech_prob": 4.718578566098586e-05}, {"id": 290, "seek": 171456, "start": 1737.08, "end": 1741.84, "text": " any one node can go down in a three node group and the other two nodes can still progress", "tokens": [604, 472, 9984, 393, 352, 760, 294, 257, 1045, 9984, 1594, 293, 264, 661, 732, 13891, 393, 920, 4205], "temperature": 0.0, "avg_logprob": -0.16693555961534814, "compression_ratio": 1.8577405857740585, "no_speech_prob": 4.718578566098586e-05}, {"id": 291, "seek": 174184, "start": 1741.84, "end": 1746.52, "text": " providing they have what's called what's the majority so I understand full connection", "tokens": [6530, 436, 362, 437, 311, 1219, 437, 311, 264, 6286, 370, 286, 1223, 1577, 4984], "temperature": 0.0, "avg_logprob": -0.22056188254520812, "compression_ratio": 1.5568862275449102, "no_speech_prob": 0.000841617351397872}, {"id": 292, "seek": 174184, "start": 1746.52, "end": 1762.08, "text": " of the network is not an important factor here well it's nice to know thank you thank", "tokens": [295, 264, 3209, 307, 406, 364, 1021, 5952, 510, 731, 309, 311, 1481, 281, 458, 1309, 291, 1309], "temperature": 0.0, "avg_logprob": -0.22056188254520812, "compression_ratio": 1.5568862275449102, "no_speech_prob": 0.000841617351397872}, {"id": 293, "seek": 174184, "start": 1762.08, "end": 1768.72, "text": " you for our talk I see here that there is basically a consistency mechanism for indexing", "tokens": [291, 337, 527, 751, 286, 536, 510, 300, 456, 307, 1936, 257, 14416, 7513, 337, 8186, 278], "temperature": 0.0, "avg_logprob": -0.22056188254520812, "compression_ratio": 1.5568862275449102, "no_speech_prob": 0.000841617351397872}, {"id": 294, "seek": 176872, "start": 1768.72, "end": 1774.52, "text": " do you check as well for that on over nodes when there is a search request as well say", "tokens": [360, 291, 1520, 382, 731, 337, 300, 322, 670, 13891, 562, 456, 307, 257, 3164, 5308, 382, 731, 584], "temperature": 0.0, "avg_logprob": -0.17518554613428208, "compression_ratio": 1.858974358974359, "no_speech_prob": 0.00017855579790193588}, {"id": 295, "seek": 176872, "start": 1774.52, "end": 1778.76, "text": " that again sorry I didn't quite pick that up do you check the data on over nodes when", "tokens": [300, 797, 2597, 286, 994, 380, 1596, 1888, 300, 493, 360, 291, 1520, 264, 1412, 322, 670, 13891, 562], "temperature": 0.0, "avg_logprob": -0.17518554613428208, "compression_ratio": 1.858974358974359, "no_speech_prob": 0.00017855579790193588}, {"id": 296, "seek": 176872, "start": 1778.76, "end": 1784.48, "text": " there is a search request not an indexing request in this case we have relaxed reads", "tokens": [456, 307, 257, 3164, 5308, 406, 364, 8186, 278, 5308, 294, 341, 1389, 321, 362, 14628, 15700], "temperature": 0.0, "avg_logprob": -0.17518554613428208, "compression_ratio": 1.858974358974359, "no_speech_prob": 0.00017855579790193588}, {"id": 297, "seek": 176872, "start": 1784.48, "end": 1789.96, "text": " essentially so we don't do we're not it's searching across several nodes and getting", "tokens": [4476, 370, 321, 500, 380, 360, 321, 434, 406, 309, 311, 10808, 2108, 2940, 13891, 293, 1242], "temperature": 0.0, "avg_logprob": -0.17518554613428208, "compression_ratio": 1.858974358974359, "no_speech_prob": 0.00017855579790193588}, {"id": 298, "seek": 176872, "start": 1789.96, "end": 1794.24, "text": " the most updated version from that which is part of the trade-off you make with the eventual", "tokens": [264, 881, 10588, 3037, 490, 300, 597, 307, 644, 295, 264, 4923, 12, 4506, 291, 652, 365, 264, 33160], "temperature": 0.0, "avg_logprob": -0.17518554613428208, "compression_ratio": 1.858974358974359, "no_speech_prob": 0.00017855579790193588}, {"id": 299, "seek": 179424, "start": 1794.24, "end": 1799.1200000000001, "text": " consistency you will have that with raft as well effectively unless you contact the leader", "tokens": [14416, 291, 486, 362, 300, 365, 43863, 382, 731, 8659, 5969, 291, 3385, 264, 5263], "temperature": 0.0, "avg_logprob": -0.12540274250264072, "compression_ratio": 1.923728813559322, "no_speech_prob": 8.3598104538396e-05}, {"id": 300, "seek": 179424, "start": 1799.1200000000001, "end": 1804.76, "text": " you won't have the most update data when searching but one of the things you do have to do if", "tokens": [291, 1582, 380, 362, 264, 881, 5623, 1412, 562, 10808, 457, 472, 295, 264, 721, 291, 360, 362, 281, 360, 498], "temperature": 0.0, "avg_logprob": -0.12540274250264072, "compression_ratio": 1.923728813559322, "no_speech_prob": 8.3598104538396e-05}, {"id": 301, "seek": 179424, "start": 1804.76, "end": 1813.68, "text": " you go with the eventual consistency eventual consistency approach like we do here is you", "tokens": [291, 352, 365, 264, 33160, 14416, 33160, 14416, 3109, 411, 321, 360, 510, 307, 291], "temperature": 0.0, "avg_logprob": -0.12540274250264072, "compression_ratio": 1.923728813559322, "no_speech_prob": 8.3598104538396e-05}, {"id": 302, "seek": 179424, "start": 1813.68, "end": 1818.32, "text": " would need to effectively handle the idea that maybe you will have duplicate documents", "tokens": [576, 643, 281, 8659, 4813, 264, 1558, 300, 1310, 291, 486, 362, 23976, 8512], "temperature": 0.0, "avg_logprob": -0.12540274250264072, "compression_ratio": 1.923728813559322, "no_speech_prob": 8.3598104538396e-05}, {"id": 303, "seek": 179424, "start": 1818.32, "end": 1824.2, "text": " because something's been recent in the meantime and so you'll need to be able to deduplicate", "tokens": [570, 746, 311, 668, 5162, 294, 264, 14991, 293, 370, 291, 603, 643, 281, 312, 1075, 281, 4172, 84, 4770, 473], "temperature": 0.0, "avg_logprob": -0.12540274250264072, "compression_ratio": 1.923728813559322, "no_speech_prob": 8.3598104538396e-05}, {"id": 304, "seek": 182420, "start": 1824.2, "end": 1828.88, "text": " that when you're searching or have some other method of handling it and deleting it from", "tokens": [300, 562, 291, 434, 10808, 420, 362, 512, 661, 3170, 295, 13175, 309, 293, 48946, 309, 490], "temperature": 0.0, "avg_logprob": -0.1589031219482422, "compression_ratio": 1.9448529411764706, "no_speech_prob": 0.00014224294864106923}, {"id": 305, "seek": 182420, "start": 1828.88, "end": 1834.2, "text": " the index so that means that effectively every node must have a copy of the data like I cannot", "tokens": [264, 8186, 370, 300, 1355, 300, 8659, 633, 9984, 1633, 362, 257, 5055, 295, 264, 1412, 411, 286, 2644], "temperature": 0.0, "avg_logprob": -0.1589031219482422, "compression_ratio": 1.9448529411764706, "no_speech_prob": 0.00014224294864106923}, {"id": 306, "seek": 182420, "start": 1834.2, "end": 1839.24, "text": " have five nodes unlike a free with the car system or something about yeah so as long", "tokens": [362, 1732, 13891, 8343, 257, 1737, 365, 264, 1032, 1185, 420, 746, 466, 1338, 370, 382, 938], "temperature": 0.0, "avg_logprob": -0.1589031219482422, "compression_ratio": 1.9448529411764706, "no_speech_prob": 0.00014224294864106923}, {"id": 307, "seek": 182420, "start": 1839.24, "end": 1844.8400000000001, "text": " as if you've got like a five node cluster and three nodes respond you can immediately", "tokens": [382, 498, 291, 600, 658, 411, 257, 1732, 9984, 13630, 293, 1045, 13891, 4196, 291, 393, 4258], "temperature": 0.0, "avg_logprob": -0.1589031219482422, "compression_ratio": 1.9448529411764706, "no_speech_prob": 0.00014224294864106923}, {"id": 308, "seek": 182420, "start": 1844.8400000000001, "end": 1848.3600000000001, "text": " search from if those three nodes have got the data they can immediately be searched from", "tokens": [3164, 490, 498, 729, 1045, 13891, 362, 658, 264, 1412, 436, 393, 4258, 312, 22961, 490], "temperature": 0.0, "avg_logprob": -0.1589031219482422, "compression_ratio": 1.9448529411764706, "no_speech_prob": 0.00014224294864106923}, {"id": 309, "seek": 182420, "start": 1848.3600000000001, "end": 1853.16, "text": " effectively if you want but the other nodes may take a little bit of time to catch up", "tokens": [8659, 498, 291, 528, 457, 264, 661, 13891, 815, 747, 257, 707, 857, 295, 565, 281, 3745, 493], "temperature": 0.0, "avg_logprob": -0.1589031219482422, "compression_ratio": 1.9448529411764706, "no_speech_prob": 0.00014224294864106923}, {"id": 310, "seek": 185316, "start": 1853.16, "end": 1857.6000000000001, "text": " which is the principle with eventual consistency they'll eventually align themselves but they're", "tokens": [597, 307, 264, 8665, 365, 33160, 14416, 436, 603, 4728, 7975, 2969, 457, 436, 434], "temperature": 0.0, "avg_logprob": -0.2162856695786962, "compression_ratio": 1.6094674556213018, "no_speech_prob": 0.0001054820095305331}, {"id": 311, "seek": 185316, "start": 1857.6000000000001, "end": 1868.68, "text": " not all immediately immediately able to reflect changes hello just simple one in hindsight", "tokens": [406, 439, 4258, 4258, 1075, 281, 5031, 2962, 7751, 445, 2199, 472, 294, 44357], "temperature": 0.0, "avg_logprob": -0.2162856695786962, "compression_ratio": 1.6094674556213018, "no_speech_prob": 0.0001054820095305331}, {"id": 312, "seek": 185316, "start": 1868.68, "end": 1876.6000000000001, "text": " would you take the raft part in hindsight probably not still and the reason for that", "tokens": [576, 291, 747, 264, 43863, 644, 294, 44357, 1391, 406, 920, 293, 264, 1778, 337, 300], "temperature": 0.0, "avg_logprob": -0.2162856695786962, "compression_ratio": 1.6094674556213018, "no_speech_prob": 0.0001054820095305331}, {"id": 313, "seek": 187660, "start": 1876.6, "end": 1885.8, "text": " is because the current state of the rust ecosystem with it means that there's a lot of black", "tokens": [307, 570, 264, 2190, 1785, 295, 264, 15259, 11311, 365, 309, 1355, 300, 456, 311, 257, 688, 295, 2211], "temperature": 0.0, "avg_logprob": -0.12555914214163116, "compression_ratio": 1.8451882845188285, "no_speech_prob": 2.074282747344114e-05}, {"id": 314, "seek": 187660, "start": 1885.8, "end": 1891.48, "text": " holes effectively around it and so you either going with an implementation which is very", "tokens": [8118, 8659, 926, 309, 293, 370, 291, 2139, 516, 365, 364, 11420, 597, 307, 588], "temperature": 0.0, "avg_logprob": -0.12555914214163116, "compression_ratio": 1.8451882845188285, "no_speech_prob": 2.074282747344114e-05}, {"id": 315, "seek": 187660, "start": 1891.48, "end": 1895.6, "text": " very stripped down just the state machine part or going with an implementation which", "tokens": [588, 33221, 760, 445, 264, 1785, 3479, 644, 420, 516, 365, 364, 11420, 597], "temperature": 0.0, "avg_logprob": -0.12555914214163116, "compression_ratio": 1.8451882845188285, "no_speech_prob": 2.074282747344114e-05}, {"id": 316, "seek": 187660, "start": 1895.6, "end": 1901.24, "text": " is very very trait heavy and is a little bit opaque around what you need to test what you", "tokens": [307, 588, 588, 22538, 4676, 293, 307, 257, 707, 857, 42687, 926, 437, 291, 643, 281, 1500, 437, 291], "temperature": 0.0, "avg_logprob": -0.12555914214163116, "compression_ratio": 1.8451882845188285, "no_speech_prob": 2.074282747344114e-05}, {"id": 317, "seek": 187660, "start": 1901.24, "end": 1906.4399999999998, "text": " don't need to test and how it behaves under failure so in this case it's I like this", "tokens": [500, 380, 643, 281, 1500, 293, 577, 309, 36896, 833, 7763, 370, 294, 341, 1389, 309, 311, 286, 411, 341], "temperature": 0.0, "avg_logprob": -0.12555914214163116, "compression_ratio": 1.8451882845188285, "no_speech_prob": 2.074282747344114e-05}, {"id": 318, "seek": 190644, "start": 1906.44, "end": 1911.8, "text": " approach more because it may allow me to implement things like network simulation which the RPC", "tokens": [3109, 544, 570, 309, 815, 2089, 385, 281, 4445, 721, 411, 3209, 16575, 597, 264, 497, 12986], "temperature": 0.0, "avg_logprob": -0.1665982677511973, "compression_ratio": 1.6866359447004609, "no_speech_prob": 2.68880703515606e-05}, {"id": 319, "seek": 190644, "start": 1911.8, "end": 1917.52, "text": " program supports so we can actually simulate network fit networks failing locally in tests", "tokens": [1461, 9346, 370, 321, 393, 767, 27817, 3209, 3318, 9590, 18223, 16143, 294, 6921], "temperature": 0.0, "avg_logprob": -0.1665982677511973, "compression_ratio": 1.6866359447004609, "no_speech_prob": 2.68880703515606e-05}, {"id": 320, "seek": 190644, "start": 1917.52, "end": 1923.64, "text": " and things like that which makes me feel a little bit more confident than trying to just", "tokens": [293, 721, 411, 300, 597, 1669, 385, 841, 257, 707, 857, 544, 6679, 813, 1382, 281, 445], "temperature": 0.0, "avg_logprob": -0.1665982677511973, "compression_ratio": 1.6866359447004609, "no_speech_prob": 2.68880703515606e-05}, {"id": 321, "seek": 190644, "start": 1923.64, "end": 1929.0800000000002, "text": " have the state machine and implement everything and all the handling correctly but I think", "tokens": [362, 264, 1785, 3479, 293, 4445, 1203, 293, 439, 264, 13175, 8944, 457, 286, 519], "temperature": 0.0, "avg_logprob": -0.1665982677511973, "compression_ratio": 1.6866359447004609, "no_speech_prob": 2.68880703515606e-05}, {"id": 322, "seek": 192908, "start": 1929.08, "end": 1937.36, "text": " in future yeah you could you could use it but it's just not not quite at that state", "tokens": [294, 2027, 1338, 291, 727, 291, 727, 764, 309, 457, 309, 311, 445, 406, 406, 1596, 412, 300, 1785], "temperature": 0.0, "avg_logprob": -0.3540512997171153, "compression_ratio": 1.2205882352941178, "no_speech_prob": 0.0007609706372022629}, {"id": 323, "seek": 193736, "start": 1937.36, "end": 1961.1599999999999, "text": " so I'm not sure I quite got how how if the engine actually does any data sharding or", "tokens": [370, 286, 478, 406, 988, 286, 1596, 658, 577, 577, 498, 264, 2848, 767, 775, 604, 1412, 402, 515, 278, 420], "temperature": 0.0, "avg_logprob": -0.3156504249572754, "compression_ratio": 1.0769230769230769, "no_speech_prob": 0.001411702367477119}, {"id": 324, "seek": 196116, "start": 1961.16, "end": 1969.52, "text": " there's a hatchery yeah in this so in this approach it's simplicity of time really we're", "tokens": [456, 311, 257, 17387, 2109, 1338, 294, 341, 370, 294, 341, 3109, 309, 311, 25632, 295, 565, 534, 321, 434], "temperature": 0.0, "avg_logprob": -0.18907949148890485, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.00011667866056086496}, {"id": 325, "seek": 196116, "start": 1969.52, "end": 1975.64, "text": " not actually doing any data sharding servers are really quite big nowadays so you can even", "tokens": [406, 767, 884, 604, 1412, 402, 515, 278, 15909, 366, 534, 1596, 955, 13434, 370, 291, 393, 754], "temperature": 0.0, "avg_logprob": -0.18907949148890485, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.00011667866056086496}, {"id": 326, "seek": 196116, "start": 1975.64, "end": 1980.76, "text": " for your e-commerce website you can get a pretty huge server and the biggest issue tends", "tokens": [337, 428, 308, 12, 26926, 3144, 291, 393, 483, 257, 1238, 2603, 7154, 293, 264, 3880, 2734, 12258], "temperature": 0.0, "avg_logprob": -0.18907949148890485, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.00011667866056086496}, {"id": 327, "seek": 196116, "start": 1980.76, "end": 1987.1200000000001, "text": " to be replication and the high availability the data sharding is something that some quick", "tokens": [281, 312, 39911, 293, 264, 1090, 17945, 264, 1412, 402, 515, 278, 307, 746, 300, 512, 1702], "temperature": 0.0, "avg_logprob": -0.18907949148890485, "compression_ratio": 1.7177033492822966, "no_speech_prob": 0.00011667866056086496}, {"id": 328, "seek": 198712, "start": 1987.12, "end": 1991.6, "text": " wits is something that would be concerned about because you've got so much data you", "tokens": [261, 1208, 307, 746, 300, 576, 312, 5922, 466, 570, 291, 600, 658, 370, 709, 1412, 291], "temperature": 0.0, "avg_logprob": -0.12865449190139772, "compression_ratio": 1.7326732673267327, "no_speech_prob": 0.00014751109119970351}, {"id": 329, "seek": 198712, "start": 1991.6, "end": 1996.7199999999998, "text": " need to spread it across machines and things like that when you're searching but in e-commerce", "tokens": [643, 281, 3974, 309, 2108, 8379, 293, 721, 411, 300, 562, 291, 434, 10808, 457, 294, 308, 12, 26926], "temperature": 0.0, "avg_logprob": -0.12865449190139772, "compression_ratio": 1.7326732673267327, "no_speech_prob": 0.00014751109119970351}, {"id": 330, "seek": 198712, "start": 1996.7199999999998, "end": 2000.76, "text": " at the point in which you're searching across multiple machines you're probably going to", "tokens": [412, 264, 935, 294, 597, 291, 434, 10808, 2108, 3866, 8379, 291, 434, 1391, 516, 281], "temperature": 0.0, "avg_logprob": -0.12865449190139772, "compression_ratio": 1.7326732673267327, "no_speech_prob": 0.00014751109119970351}, {"id": 331, "seek": 198712, "start": 2000.76, "end": 2004.9599999999998, "text": " be looking at the higher latencies so you would you'd be better off dedicating one", "tokens": [312, 1237, 412, 264, 2946, 4465, 6464, 370, 291, 576, 291, 1116, 312, 1101, 766, 4172, 30541, 472], "temperature": 0.0, "avg_logprob": -0.12865449190139772, "compression_ratio": 1.7326732673267327, "no_speech_prob": 0.00014751109119970351}, {"id": 332, "seek": 200496, "start": 2004.96, "end": 2021.6000000000001, "text": " machine per search rather than several machines per per search really.", "tokens": [50364, 3479, 680, 3164, 2831, 813, 2940, 8379, 680, 680, 3164, 534, 13, 51196], "temperature": 0.0, "avg_logprob": -0.42064476013183594, "compression_ratio": 1.206896551724138, "no_speech_prob": 7.567733700852841e-05}], "language": "en"}